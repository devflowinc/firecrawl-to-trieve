link: https://signoz.io/docs/introduction/
tag_set: introduction
image_urls: 
tracking_id: docs-introduction
group_tracking_ids: docs-introduction
<h2>Introduction</h2>
<p>SigNoz is an open-source observability tool that helps you monitor your applications and troubleshoot problems. It provides traces, metrics, and logs under a single pane of glass. It is available both as an <a href="https://github.com/SigNoz/signoz">open-source software</a> and a <a href="https://signoz.io/teams/">cloud offering</a>.</p>
<p>With SigNoz, you can do the following:</p>
<ul>
<li>Visualise Traces, Metrics, and Logs in a single pane of glass</li>
<li>Monitor application metrics like p99 latency, error rates for your services, external API calls, and individual endpoints.</li>
<li>Find the root cause of the problem by going to the exact traces which are causing the problem and see detailed flamegraphs of individual request traces.</li>
<li>Run aggregates on trace data to get business-relevant metrics</li>
<li>Filter and query logs, build dashboards and alerts based on attributes in logs</li>
<li>Monitor infrastructure metrics such as CPU utilization or memory usage</li>
<li>Record exceptions automatically in Python, Java, Ruby, and Javascript</li>
<li>Easy to set alerts with DIY query builder</li>
</ul>
<h2>Get Started</h2>
<hr />
<p>You can either self-host SigNoz or try SigNoz Cloud. Once SigNoz is up and running, you can instrument your application to send data to SigNoz.</p>
<ul>
<li><a href="/docs/cloud/">SigNoz Cloud: Easy way to get started with SigNoz</a></li>
<li><a href="/docs/install">Self-host SigNoz: Learn how to install and self-host SigNoz</a></li>
</ul>
<h2>How Does SigNoz Work?</h2>
<hr />
<p>SigNoz collects data using <a href="https://opentelemetry.io/">OpenTelemetry</a>
, an open-source observability solution. OpenTelemetry is backed by Cloud Native Computing Foundation. The project aims to standardize how we instrument our applications for generating telemetry data(traces, metrics, and logs).</p>
<p>SigNoz supports all the frameworks and languages supported by OpenTelemetry. You can find the complete list of supported languages on the <a href="https://opentelemetry.io/docs/instrumentation/">instrumentation</a> page of the OpenTelemetry documentation.</p>
<p>Once you instrument your application with OpenTelemetry, you can send the data to SigNoz for storage, analysis, and visualization.</p>
<h3>## Architecture</h3>
<p><img src="https://signoz.io/img/architecture-signoz-clickhouse.svg" alt="acrhitecture-diagram-clickhouse" /></p>
<p>SigNoz includes the following components:</p>
<ul>
<li><strong>OpenTelemetry Collector</strong>: Collects telemetry data from your services and applications.</li>
<li><strong>ClickHouse</strong>: An open-source, high performance columnar OLAP database management system.</li>
<li><strong>Query Service</strong>: The interface between the front-end and ClickHouse</li>
<li><strong>Frontend</strong>: The user interface, built in ReactJS and TypeScript.</li>
</ul>
<p>To learn more about the architecture of SigNoz, see the <a href="/docs/architecture/">Architecture</a> page.</p>
<h2>Use SigNoz</h2>
<hr />
<p>The topics in this section provide details on using SigNoz to monitor your application.</p>
<ul>
<li><a href="/docs/instrumentation/overview/">1. Instrumentation: Learn how to instrument your applications to send data to SigNoz</a></li>
<li><a href="/docs/tutorials/">2. Tutorials: Tutorials about monitoring your applications and infrastructure</a></li>
<li><a href="/docs/operate/">3. Operate: This section explains how to manage SigNoz</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/cloud/
tag_set: cloud
image_urls: 
tracking_id: docs-cloud
group_tracking_ids: docs-cloud
<h2>SigNoz Cloud</h2>
<p>SigNoz Cloud is an easy way to get started with SigNoz. You don't need to install or maintain your own instance.</p>
<h2>Getting started with SigNoz Cloud</h2>
<hr />
<p>SigNoz offers a <strong>free trial of 30 days with full access to all features</strong>. If you don‚Äôt already have an account, you can sign up <a href="https://signoz.io/teams/">here</a>.</p>
<h2>Send Traces to SigNoz Cloud</h2>
<hr />
<p>SigNoz supports tracing for major programming languages. With tracing you can get started with great out-of-box charts for application performance like p99 latency, request rates, error rates, and top end-points of your application.</p>
<p>You can also visualize user requests in their entirety as it travels across components of your application. Here are instructions for sending traces to SigNoz cloud in different languages:</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/instrumentation/java/#send-traces-to-signoz-cloud">Java</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/instrumentation/python/#send-traces-to-signoz-cloud">Python</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/instrumentation/javascript/">Nodejs</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/instrumentation/golang/#send-traces-to-signoz-cloud">Golang</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/instrumentation/ruby-on-rails/">Ruby on Rails</a></p>
</li>
</ul>
<h2>Collect Hostmetrics from VM</h2>
<hr />
<p>Find <a href="https://signoz.io/docs/userguide/hostmetrics/">instructions</a> to send hostmetrics to SigNoz Cloud using OpenTelemetry Collector.</p>
<h2>Collect Kubernetes Infra Metrics</h2>
<hr />
<p>Find <a href="https://signoz.io/docs/tutorial/kubernetes-infra-metrics/">instructions</a> to send Kubernetes infra metrics and logs to SigNoz Cloud using OpenTelemetry Collector.</p>
<h2>Send Logs to SigNoz Cloud</h2>
<hr />
<p>SigNoz provides log management with and advanced query builder to quickly search and filter logs. Here's an <a href="https://signoz.io/docs/userguide/logs/#collecting-logs-in-signoz-cloud">overview</a> of how to collect and send logs to SigNoz cloud.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/
tag_set: install
image_urls: 
tracking_id: docs-install
group_tracking_ids: docs-install
<h2>Install SigNoz</h2>
<p>To install SigNoz, follow the instructions in the sections below. You can either self-host SigNoz or try SigNoz Cloud.</p>
<ul>
<li><a href="/docs/cloud/">üìÑÔ∏è Setup SigNoz Cloud: Easy way to get started with SigNoz</a></li>
<li><a href="/docs/install/docker">üìÑÔ∏è Self Host SigNoz: Docker, Docker Swarm, Kubernetes</a></li>
<li><a href="/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine">üìÑÔ∏è Install OTel Collector: VM, Kubernetes</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/architecture/
tag_set: architecture
image_urls: 
tracking_id: docs-architecture
group_tracking_ids: docs-architecture
<h2>Technical Architecture</h2>
<p><img src="https://signoz.io/img/architecture-signoz-clickhouse.svg" alt="acrhitecture-diagram-clickhouse" /></p>
<h3>## Architecture Components</h3>
<ul>
<li>SigNoz OpenTelemetry Collector</li>
<li>ClickHouse</li>
<li>Query Service</li>
<li>Frontend</li>
<li>Alert Manager</li>
</ul>
<p><strong>OpenTelemetry Collector</strong> can receive data in multiple formats. Here are some of the commonly used receivers:</p>
<ul>
<li>Jaeger Receiver</li>
<li>Kafka Receiver</li>
<li>OpenCensus Receiver</li>
<li>OTLP Receiver</li>
<li>Zipkin Receiver</li>
</ul>
<p>One can send data from their applications directly to SigNoz Otel collector or external otel collectors can be used for collecting telemetry data &amp; sending to SigNoz otel collector. These external otel collectors are then working effectively as an agent to collect data first and then send to SigNoz Otel collector.</p>
<p><strong>Query Service</strong> is the interface between Frontend and ClickHouse. It provides APIs to be consumed by frontend application and queries ClickHouse to fetch data and processes data before responding back to the frontend.</p>
<p><strong>Frontend</strong> is the UI, built in ReactJS and Typescript and provides advanced trace/span filtering capabilities and plot metrics to provide service overviews.</p>
<p><strong>Alert Manager</strong> evaluates different alert rules set by the users and triggers an alert if a threshold is crossed.</p>
<h3>## Opentelemetry Introduction</h3>
<p>SigNoz uses OpenTelemetry for instrumenting applications and for collecting telemetry data. The following docs may be useful to get familiar with the basic concepts of OpenTelemetry</p>
<ul>
<li>
<p><a href="https://opentelemetry.io/docs/concepts/data-collection/">OpenTelemetry Data Collection</a></p>
</li>
<li>
<p><a href="https://opentelemetry.io/docs/collector/configuration/">OpenTelemetry Collector Configuration</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/overview/
tag_set: instrumentation, overview
image_urls: 
tracking_id: docs-instrumentation-overview
group_tracking_ids: docs-instrumentation-overview
<h2>Overview</h2>
<h2>APM and Distributed Tracing - Overview</h2>
<hr />
<p>Instrumentation is the process of enabling your application code to generate telemetry data - anything that assists you in monitoring or measuring the performance and state of your application.</p>
<p>SigNoz supports <a href="https://opentelemetry.io/">OpenTelemetry</a> for instrumenting applications.</p>
<p><strong>OpenTelemetry</strong> is the leading open-source project that provides instrumentation libraries for major programming languages and popular frameworks. It is a project under Cloud Native Computing Foundation and is backed by a huge community. It provides a standardized data format for collected data, eliminating the need for specific vendor integrations.</p>
<p>This <a href="https://opentelemetry.io/docs/concepts/instrumenting">guide</a> introduces the basic concepts of instrumentation using OpenTelemetry. OpenTelemetry also has an ecosystem of libraries, plugins, integrations, and other useful tools which extend it. You can find these resources at Otel Registry <a href="https://opentelemetry.io/registry/">here</a>.</p>
<p><em>You can instrument using any open-standard library and use SigNoz as your observability backend to ingest, analyse and visualize data.</em></p>
<p>For instrumenting your code, you can use the instruction provided by OpenTelemetry for specific langauges.</p>
<p>SigNoz currently provides simple ways to instrument NodeJS, Java, Python and Golang applications using OpenTelemetry. Please follow the below guides.</p>
<ol>
<li>
<p><a href="/docs/instrumentation/python/">OpenTelemetry Python Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/javascript/">OpenTelemetry Javascript Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/java/">OpenTelemetry Java Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/golang/">OpenTelemetry Go Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/php/">OpenTelemetry PHP Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/dotnet/">OpenTelemetry .NET Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/ruby-on-rails/">OpenTelemetry Ruby on Rails Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/elixir/">OpenTelemetry Elixir Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/rust/">OpenTelemetry Rust Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/swift/">OpenTelemetry Swift Instrumentation</a></p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorials/
tag_set: tutorials
image_urls: 
tracking_id: docs-tutorials
group_tracking_ids: docs-tutorials
<h2>Tutorials</h2>
<p>SigNoz tutorials are step-by-step training exercises that guide you through monitoring your applications and infrastructure.</p>
<ul>
<li><a href="/docs/tutorial/jvm-metrics/">üìÑÔ∏è Spring Boot JVM Metrics: View JVM metrics from Spring Boot applications in SigNoz</a></li>
<li><a href="/docs/tutorial/jmx-metrics/">üìÑÔ∏è JMX Metrics: Collect JMX metrics from Java services</a></li>
<li><a href="/docs/tutorial/mongodb-metrics/">üìÑÔ∏è MongoDB Metrics: View MongoDB metrics in SigNoz</a></li>
<li><a href="/docs/tutorial/instrumenting-angular-frontend/">üìÑÔ∏è Instrumenting Angular Frontend Web App: Instrument your Angular frontend app.</a></li>
<li><a href="/docs/tutorial/s3-integration-iam-role-eks/">üìÑÔ∏è S3 Integration With AWS IAM role in EKS: Integrate S3 cold storage in AWS EKS with IAM Role.</a></li>
<li><a href="/docs/tutorial/oci-bucket-cold-storage-integration/">üìÑÔ∏è OCI Bucket Cold Storage Integration: Integrate OCI Bucket as Cold Storage.</a></li>
<li><a href="/docs/tutorial/opentelemetry-operator-usage/">üìÑÔ∏è OpenTelemetry Operator Usage: How to use OpenTelemetry Operator to ease Otelcol deployment and instrumentation in SigNoz.</a></li>
<li><a href="/docs/tutorial/setting-up-tls-for-signoz/">üìÑÔ∏è Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager: Set up TLS for SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager.</a></li>
<li><a href="/docs/tutorial/setting-up-sso-saml-with-keycloak/">üìÑÔ∏è Setting Up SSO SAML 2.0 With Keycloak: Setting up Single Sign-On (SSO) SAML 2.0 with Keycloak.</a></li>
<li><a href="/docs/tutorial/writing-clickhouse-queries-in-dashboard/">üìÑÔ∏è ClickHouse Queries for Building Dashboards and Alerts: Example ClickHouse queries to run analytics on observability data.</a></li>
<li><a href="/docs/tutorial/traefik-observability">üìÑÔ∏è Traefik Observability: Tutorial to export Traefik metrics and traces to SigNoz.</a></li>
<li><a href="/docs/tutorial/infinite-retention-aws-s3">üìÑÔ∏è Infinite retention using AWS S3: Tutorial to backup data for longer duration using AWS S3</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/
tag_set: operate
image_urls: 
tracking_id: docs-operate
group_tracking_ids: docs-operate
<h2>Operate Self-Hosted SigNoz</h2>
<ul>
<li><a href="/docs/operate/configuration/">üìÑÔ∏è Configuration: Learn how to configure SigNoz</a></li>
<li><a href="/docs/operate/docker-standalone/">üìÑÔ∏è Docker Standalone: Learn how to operate SigNoz on Docker Standalone</a></li>
<li><a href="/docs/operate/docker-swarm/">üìÑÔ∏è Docker Swarm: Learn how to operate SigNoz on Docker Swarm</a></li>
<li><a href="/docs/operate/kubernetes/">üìÑÔ∏è Kubernetes: Learn how to operate SigNoz on Kubernetes</a></li>
<li><a href="/docs/operate/migration/">üìÑÔ∏è Migration Guide: 11 Items</a></li>
<li><a href="/docs/operate/clickhouse/">üìÑÔ∏è ClickHouse: 4 Items</a></li>
<li><a href="/docs/operate/query-service/">üìÑÔ∏è Query Service: 2 Items</a></li>
<li><a href="/docs/operate/feature-flags/">üìÑÔ∏è Feature Flags: Learn how to use feature flags in SigNoz</a></li>
<li><a href="/docs/production-readiness/">üìÑÔ∏è Best Practices for Production: Learn to use SigNoz's best practices for production use cases</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/
tag_set: install
image_urls: 
tracking_id: docs-install
group_tracking_ids: docs-install
<h2>Install SigNoz</h2>
<p>To install SigNoz, follow the instructions in the sections below. You can either self-host SigNoz or try SigNoz Cloud.</p>
<ul>
<li><a href="/docs/cloud/">üìÑÔ∏è Setup SigNoz Cloud: Easy way to get started with SigNoz</a></li>
<li><a href="/docs/install/docker">üìÑÔ∏è Self Host SigNoz: Docker, Docker Swarm, Kubernetes</a></li>
<li><a href="/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine">üìÑÔ∏è Install OTel Collector: VM, Kubernetes</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/contributing/
tag_set: contributing
image_urls: 
tracking_id: docs-contributing
group_tracking_ids: docs-contributing
<h2>Contribution Guidelines</h2>
<h2>Welcome to SigNoz Contributing section üéâ</h2>
<hr />
<p>Hi there! We're thrilled that you'd like to contribute to this project, thank you for your interest. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p>
<p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>
<ul>
<li>
<p>We accept contributions made to the <a href="https://github.com/SigNoz/signoz/tree/develop">SigNoz <code>develop</code> branch</a></p>
</li>
<li>
<p>Find all SigNoz Docker Hub images here</p>
<ul>
<li>
<p><a href="https://hub.docker.com/r/signoz/frontend">signoz/frontend</a></p>
</li>
<li>
<p><a href="https://hub.docker.com/r/signoz/query-service">signoz/query-service</a></p>
</li>
<li>
<p><a href="https://hub.docker.com/r/signoz/otelcontribcol">signoz/otelcontribcol</a></p>
</li>
</ul>
</li>
</ul>
<h2>Finding contributions to work on üí¨</h2>
<hr />
<p>Looking at the existing issues is a great way to find something to contribute on. Also, have a look at these <a href="https://github.com/SigNoz/signoz/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">good first issues label</a> to start with.</p>
<h2>How to Contribute</h2>
<hr />
<p>Please check <a href="https://github.com/SigNoz/signoz/blob/develop/CONTRIBUTING.md">Contributing.md</a> file for instructions on how to contribute to SigNoz.</p>
<ul>
<li>You can create a PR (Pull Request) for contributing features, bug fixes to the project.</li>
<li>If you find any bugs, please create an issue.</li>
<li>If you find anything missing in documentation, you can create an issue with label <strong>documentation</strong>.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/
tag_set: instrumentation, python
image_urls: https://signoz.io/img/docs/otel_python_instrumentation.webp
tracking_id: docs-instrumentation-python
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Python applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Python application.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Python application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/otel_python_instrumentation.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Python applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in Python.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#requirements
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-requirements
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Requirements</h2>
<ul>
<li>Python 3.8 or newer</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#send-traces-to-signoz-cloud
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindowsDocker</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h4>## Send traces directly to SigNoz Cloud</h4>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p><strong>Step 4.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
opentelemetry-instrument &lt;your_run_command&gt;
</code></pre>
<ul>
<li><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</li>
<li><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></li>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the api token provided by SigNoz. You can find it in the email sent by SigNoz with your cloud account details.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p><strong>Step 5.</strong> Validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-signoz-installation">troubleshooting section</a>.</p>
<hr />
<h4>## Send traces via OTel Collector binary</h4>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Python application.</p>
<p><strong>Step 1.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p><strong>Step 2.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p><strong>Step 3.</strong> To run your application and send data to collector in same VM:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
opentelemetry-instrument &lt;your_run_command&gt;
</code></pre>
<p>where,</p>
<ul>
<li><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</li>
<li><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></li>
</ul>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4317 with <code>&lt;IP Address of the VM&gt;:4317</code>.</p>
<p><strong>Step 4.</strong> You can validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-signoz-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can use OpenTelemetry Python to send your traces directly to SigNoz. OpenTelemetry provides a handy distro in Python that can help you get started with automatic instrumentation. We recommend using it to get started quickly.</p>
<h3>## Steps to auto-instrument Python app for traces</h3>
<p>‚úÖ Info</p>
<p>If you are on K8s, you should checkout <a href="/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">opentelemetry operators</a> which enable auto instrumenting Python applications very easily.</p>
<ol>
<li>
<p><strong>Create a virtual environment</strong></p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
</li>
<li>
<p><strong>Install the OpenTelemetry dependencies</strong></p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
</li>
<li>
<p><strong>Add automatic instrumentation</strong><br />
The below command inspects the dependencies of your application and installs the instrumentation packages relevant for your Python application.</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
</li>
<li>
<p><strong>Run your application</strong></p>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, if you use <code>export FLASK_ENV=development</code>, it enables the reloader mode which breaks OpenTelemetry instrumentation.</p>
<p>To start sending data to SigNoz, use the following run command:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></p>
<p><code>IP of SigNoz backend</code> is the IP of the machine where you installed SigNoz. If you have installed SigNoz on <code>localhost</code>, the endpoint will be <code>http://localhost:4317</code> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>Replacing these environment variables, a sample final run command will look like this:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=python_app OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument python3 app.py
</code></pre>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively. Remember to allow incoming requests to port¬†<strong>4317</strong>/<strong>4318</strong>¬†of machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-signoz-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, python
image_urls: https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp
tracking_id: docs-instrumentation-python-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp" alt="Python Application in the list of services being monitored in SigNoz" /></p>
<p><em>Python Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#database-instrumentation
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-database-instrumentation
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Database Instrumentation</h2>
<p>Make sure that the DB client library you are using has the corresponding instrumentation library, and the version of the DB client library is supported by OpenTelemetry.</p>
<h3>## MongoDB</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MongoDB database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MongoDB, the instrumentation library is <code>opentelemetry-instrumentation-pymongo</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Redis</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your Redis database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Redis, the instrumentation library is <code>opentelemetry-instrumentation-redis</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## MySQL</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MySQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MySQL, we have two instrumentation libraries:</p>
<ul>
<li>opentelemetry-instrumentation-mysql</li>
<li>opentelemetry-instrumentation-pymysql</li>
</ul>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Postgres</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your PostgreSQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Postgres, the instrumentation library is <code>opentelemetry-instrumentation-psycopg2</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<p>üìù Note</p>
<p><code>psycopg2-binary</code> is not supported by opentelemetry auto instrumentation libraries as it is not recommended for production use. Please use <code>psycopg2</code> to see DB calls also in your trace data in SigNoz</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#running-applications-with-gunicorn-uwsgi
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-running-applications-with-gunicorn-uwsgi
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Running applications with Gunicorn, uWSGI</h2>
<p>For application servers which are based on pre fork model like Gunicorn, uWSGI you have to add a <code>post_fork</code> hook or a <code>@postfork</code> decorator in your configuration.</p>
<p>Check this <a href="https://opentelemetry-python.readthedocs.io/en/latest/examples/fork-process-model/README.html">documentation</a> from OpenTelemetry on how to set it up.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-python/tree/main/docs/examples/fork-process-model">Here's</a> a working example configured with a gunicorn server with <code>post_fork</code> hook.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#troubleshooting-your-signoz-installation
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-troubleshooting-your-signoz-installation
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Troubleshooting your SigNoz installation</h2>
<h4>## Application servers such as Uvicorn, Hypercorn, etc.</h4>
<ul>
<li>Uvicorn with <code>--workers</code> flag is not supported. The work around for this is to use <code>gunicorn</code> with uvicorn as the worker class <code>gunicorn -k uvicorn.workers.UvicornWorker</code>.</li>
<li>Hypercorn is not supported. There is no workaround for this. Please follow the issue <a href="https://github.com/pgjones/hypercorn/issues/215">https://github.com/pgjones/hypercorn/issues/215</a></li>
</ul>
<h4>## Spans are not being reported</h4>
<p>If spans are not being reported to SigNoz, try enabling debug exporter which writes the JSON formatted trace data to the console by setting env var OTEL_TRACES_EXPORTER=console.</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=python_app OTEL_TRACES_EXPORTER=console opentelemetry-instrument &lt;your run command&gt;


{
    &quot;name&quot;: &quot;alice&quot;,
    &quot;context&quot;: {
        &quot;trace_id&quot;: &quot;0xedb7caf0c8b082a9578460a201759193&quot;,
        &quot;span_id&quot;: &quot;0x57cf7eee198e1fed&quot;,
        &quot;trace_state&quot;: &quot;[]&quot;
    },
    &quot;kind&quot;: &quot;SpanKind.INTERNAL&quot;,
    &quot;parent_id&quot;: null,
    &quot;start_time&quot;: &quot;2022-03-27T14:55:18.804758Z&quot;,
    &quot;end_time&quot;: &quot;2022-03-27T14:55:18.804805Z&quot;,
    &quot;status&quot;: {
        &quot;status_code&quot;: &quot;UNSET&quot;
    },
    &quot;attributes&quot;: {},
    &quot;events&quot;: [],
    &quot;links&quot;: [],
    &quot;resource&quot;: {
        &quot;telemetry.sdk.language&quot;: &quot;python&quot;,
        &quot;telemetry.sdk.name&quot;: &quot;opentelemetry&quot;,
        &quot;telemetry.sdk.version&quot;: &quot;1.10.0&quot;,
        &quot;service.name&quot;: &quot;my-service&quot;
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#sample-application
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-sample-application
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Sample Application</h2>
<ul>
<li><a href="https://github.com/SigNoz/opentelemetry-python/tree/main/docs/examples/fork-process-model">Working example</a> where we have configured a gunicorn server with <code>post_fork</code> hook.</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation</h2>
<p>This document contains OpenTelemetry instrumentation instructions for Javascript backend frameworks and modules based on Nodejs. If you're using self-hosted SigNoz refer to this <a href="#send-traces-to-self-hosted-signoz">section</a>. If you're using SigNoz cloud, refer to this <a href="#send-traces-to-signoz-cloud">section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-to-signoz-cloud
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
<ul>
<li><a href="#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
<ul>
<li><a href="#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;&lt;SIGNOZ_ENDPOINT&gt;&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_ACCESS_TOKEN&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-directly-to-signoz-cloud--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api@^1.6.0
npm install --save @opentelemetry/sdk-node@^0.45.0
npm install --save @opentelemetry/auto-instrumentations-node@^0.39.4
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.45.0
</code></pre>
<p><strong>Step 2.</strong> Create tracing.js file<br />
You need to configure the endpoint for SigNoz cloud in this file. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
<pre><code>// tracing.js
'use strict'
const process = require('process')
const opentelemetry = require('@opentelemetry/sdk-node')
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node')
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')
const { Resource } = require('@opentelemetry/resources')
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions')

// do not set headers in exporterOptions, the OTel spec recommends setting headers through ENV variables
// https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/exporter.md#specifying-headers-via-environment-variables

// highlight-start
const exporterOptions = {
  url: 'https://ingest.{region}.signoz.cloud:443/v1/traces',
}
// highlight-end

const traceExporter = new OTLPTraceExporter(exporterOptions)
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({
    // highlight-next-line
    [SemanticResourceAttributes.SERVICE_NAME]: 'node_app',
  }),
})

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk
    .shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0))
})
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443/v1/traces</td>
</tr>
</tbody>
</table>
<p><strong>Step 3.</strong> Run the application<br />
Make sure you set the <code>OTEL_EXPORTER_OTLP_HEADERS</code> env as follows</p>
<pre><code>OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;&quot; node -r ./tracing.js app.js
</code></pre>
<p><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
<p><strong>Step 4.</strong> You can validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-via-otel-collector-binary--no-code-automatic-instrumentation
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - No Code Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended-1
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended-1
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4318/v1/traces&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-via-otel-collector-binary--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - Code Level Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api@^1.6.0
npm install --save @opentelemetry/sdk-node@^0.45.0
npm install --save @opentelemetry/auto-instrumentations-node@^0.39.4
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.45.0
</code></pre>
<p><strong>Step 2.</strong> Create tracing.js file</p>
<pre><code>// tracing.js
'use strict'
const process = require('process')
const opentelemetry = require('@opentelemetry/sdk-node')
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node')
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')
const { Resource } = require('@opentelemetry/resources')
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions')

const exporterOptions = {
  url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces',
}

const traceExporter = new OTLPTraceExporter(exporterOptions)
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({
    // highlight-next-line
    [SemanticResourceAttributes.SERVICE_NAME]: 'node_app',
  }),
})

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk
    .shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0))
})
</code></pre>
<p><strong>Step 3.</strong> Run the application</p>
<pre><code>node -r ./tracing.js app.js
</code></pre>
<p><strong>Step 4.</strong> You can validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Send Traces to Self-Hosted SigNoz</h2>
<p><strong>Requirements</strong></p>
<ul>
<li>Node.js version 14 or newer (<a href="https://github.com/open-telemetry/opentelemetry-js#supported-runtimes">See here</a>
)</li>
</ul>
<p>You can use <a href="https://signoz.io/opentelemetry/nodejs/">OpenTelemetry Nodejs</a> client libraries to send your traces directly to SigNoz. You have two choices for instrumenting your Nodejs application with OpenTelemetry.</p>
<ul>
<li>
<p><strong><a href="#using-the-all-in-one-auto-instrumentation-library">Use the all-in-one auto-instrumentation library(Recommended)</a></strong><br />
The auto-instrumentation library of OpenTelemetry is a meta package that provides a simple way to initialize multiple Nodejs instrumnetations.</p>
<p>‚úÖ Info</p>
<p>If you are on K8s, you should checkout <a href="/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">opentelemetry operators</a> which enable auto instrumenting Javascript applications very easily.</p>
</li>
<li>
<p><strong><a href="#using-a-specific-auto-instrumentation-library">Use a specific auto-instrumentation library</a></strong><br />
You can use individual auto-instrumentation libraries too for a specific component of your application. For example, you can use <code>@opentelemetry/instrumentation-express</code> for instrumenting the Express web framework.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#using-the-all-in-one-auto-instrumentation-library
tag_set: instrumentation, javascript
image_urls: https://signoz.io/img/docs/all_in_one_auto_instrumentation.webp
tracking_id: docs-instrumentation-javascript-using-the-all-in-one-auto-instrumentation-library
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Using the all-in-one auto-instrumentation library</p>
<p>The recommended way to instrument your Javascript Nodejs application is to use the all-in-one auto-instrumentation library - <code>@opentelemetry/auto-instrumentations-node</code>. It provides a simple way to initialize multiple Nodejs instrumentations.</p>
<p>Internally, it calls the specific auto-instrumentation library for components used in the application. You can see the complete list <a href="https://github.com/open-telemetry/opentelemetry-js-contrib/tree/main/metapackages/auto-instrumentations-node#supported-instrumentations">here</a>.</p>
<p>The instrumentation automatically identifies the following within your application:</p>
<ul>
<li>Frameworks, such as Express, Nestjs</li>
<li>Common protocols such as HTTP, HTTPS, and gRPC</li>
<li>Databases, such as MySQL, MongoDB, Redis, etc.</li>
<li>Other libraries used in the application</li>
</ul>
<p><img src="https://signoz.io/img/docs/all_in_one_auto_instrumentation.webp" alt="All in one OpenTelemetry nodejs instrumentation " /></p>
<p>_</p>
<p>All in one auto instrumentation library - identifies and instruments packages used by your Nodejs application</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#steps-to-auto-instrument-nodejs-application
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-steps-to-auto-instrument-nodejs-application
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Nodejs application: Steps to auto-instrument Nodejs application: Steps to auto-instrument Nodejs application</p>
<ol>
<li>
<p>Install the dependencies<br />
We start by installing the relevant dependencies.</p>
<pre><code>npm install --save @opentelemetry/sdk-node
npm install --save @opentelemetry/auto-instrumentations-node
npm install --save @opentelemetry/exporter-trace-otlp-http
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>@opentelemetry/sdk-node</code> - This package provides the full OpenTelemetry SDK for Node.js including tracing and metrics.</p>
<p><code>@opentelemetry/auto-instrumentations-node</code> - This module provides a simple way to initialize multiple Node instrumentations.</p>
<p><code>@opentelemetry/exporter-trace-otlp-http</code> - This module provides the exporter to be used with OTLP (<code>http/json</code>) compatible receivers.</p>
<p>üìù Note</p>
<p>If you run into any error, you might want to use these pinned versions of OpenTelemetry libraries used in this <a href="https://github.com/SigNoz/sample-nodejs-app/blob/master/package.json">GitHub repo</a>
.</p>
</li>
<li>
<p><strong>Create a <code>tracing.js</code> file</strong><br />
The <code>tracing.js</code> file will contain the tracing setup code. Notice, that we have set some environment variables in the code(highlighted). You can update these variables based on your environment.</p>
<p>// tracing.js
'use strict'
const process = require('process')
const opentelemetry = require('@opentelemetry/sdk-node')
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node')
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')
const { Resource } = require('@opentelemetry/resources')
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions')</p>
<p>const exporterOptions = {
// highlight-next-line
url: 'http://localhost:4318/v1/traces',
}</p>
<p>const traceExporter = new OTLPTraceExporter(exporterOptions)
const sdk = new opentelemetry.NodeSDK({
traceExporter,
instrumentations: [getNodeAutoInstrumentations()],
// highlight-start
resource: new Resource({
[SemanticResourceAttributes.SERVICE_NAME]: 'node_app',
}),
// highlight-end
})</p>
<p>// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()</p>
<p>// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
sdk
.shutdown()
.then(() =&gt; console.log('Tracing terminated'))
.catch((error) =&gt; console.log('Error terminating tracing', error))
.finally(() =&gt; process.exit(0))
})</p>
</li>
</ol>
<p>OpenTelemetry Node SDK currently does not detect the <code>OTEL_RESOURCE_ATTRIBUTES</code> from <code>.env</code> files as of today. That‚Äôs why we need to include the variables in the <code>tracing.js</code> file itself.</p>
<p>About environment variables:</p>
<p><code>service_name</code>¬†: node_app (you can give whatever name that suits you)</p>
<p><code>http://localhost:4318/v1/traces</code> is the default url for sending your tracing data. We are assuming you have installed SigNoz on your <code>localhost</code>. Based on your environment, you can update it accordingly. It should be in the following format:</p>
<pre><code>http://&lt;IP of SigNoz backend&gt;:4318/v1/traces
</code></pre>
<p>üìù Note</p>
<p>Remember to allow incoming requests to port 4318 of machine where SigNoz backend is hosted.</p>
<ol start="3">
<li>
<p><strong>Run the application</strong><br />
The tracing configuration should be run before your application code. We will use the <a href="https://nodejs.org/api/cli.html#cli_r_require_module"><code>-r, ‚Äîrequire module</code></a> flag for that.</p>
<pre><code>node -r ./tracing.js app.js
</code></pre>
<p>üìù Note</p>
<p>If you're running your nodejs application in PM2 cluster mode, it doesn't support node args: <a href="https://github.com/Unitech/pm2/issues/3227">Unitech/pm2#3227</a>
. As above sample app instrumentation requires to load <code>tracing.js</code> before app load by passing node arg, so nodejs instrumentation doesn't work in PM2 cluster mode. So you need to import <code>tracing.js</code> in your main application. The <code>import ./tracing.js</code> should be the first line of your application code and initialize it before any other function. Here's the <a href="https://github.com/SigNoz/sample-nodejs-app/tree/init-tracer-main">sample github repo</a> which shows the implementation.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, javascript
image_urls: https://signoz.io/img/docs/nodejs_in_services_list.webp
tracking_id: docs-instrumentation-javascript-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the <code>Services</code> tab. Hit the <code>Refresh</code> button on the top right corner, and your application should appear in the list of <code>Applications</code>.</li>
<li>Go to the <code>Traces</code> tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs <a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/nodejs_in_services_list.webp" alt="Node Application in the list of services being monitored in SigNoz" /></p>
<p><em>Node Application in the list of services being monitored in SigNoz</em></p>
<p>If you don't see your application reported in the list of services, try our <a href="https://signoz.io/docs/install/troubleshooting/">troubleshooting</a> guide.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#using-a-specific-auto-instrumentation-library
tag_set: instrumentation, javascript
image_urls: https://signoz.io/img/docs/individual_auto_instrumentation_libraries.webp
tracking_id: docs-instrumentation-javascript-using-a-specific-auto-instrumentation-library
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Using a specific auto-instrumentation library: Using a specific auto-instrumentation library: Using a specific auto-instrumentation library</p>
<p>If total installation size is not constrained, it is recommended to use the¬†<code>@opentelemetry/auto-instrumentations-node</code> bundle with <code>@opentelemetry/sdk-node</code> for the most seamless instrumentation experience.</p>
<p>But you can also install specific auto-instrumenation packages for the components used by your application.</p>
<p><img src="https://signoz.io/img/docs/individual_auto_instrumentation_libraries.webp" alt="All in one OpenTelemetry nodejs instrumentation " /></p>
<p>_</p>
<p>You can also choose individual auto-instrumenation libraries, but the all-in-one library is recommended to get started</p>
<p>_</p>
<p>If an application uses Express, HTTP, and MongoDB, we can instrument the application using the following modules:</p>
<ul>
<li>opentelemetry-instrumentation-express</li>
<li>opentelemetry/instrumentation-mongodb</li>
<li>opentelemetry/instrumentation-http</li>
</ul>
<p>If you are using Express, the instrumentation relies on HTTP calls to also be instrumented. That‚Äôs why we‚Äôre also including the module for http instrumentation. Let‚Äôs see the steps required.</p>
<p><strong>Steps to use specific auto-instrumentation libraries</strong></p>
<ol>
<li>
<p><strong>Install the dependencies</strong><br />
We start by installing the relevant dependencies.</p>
<pre><code>npm install --save @opentelemetry/sdk-node
npm install --save @opentelemetry/exporter-trace-otlp-http
npm install --save @opentelemetry/instrumentation-express
npm install --save @opentelemetry/instrumentation-mongodb
npm install --save @opentelemetry/instrumentation-http
</code></pre>
</li>
<li>
<p><strong>Creat a <code>tracing.js</code> file</strong><br />
The <code>tracing.js</code> file will contain the following tracing setup code.</p>
<pre><code>// tracing.js
'use strict'
const process = require('process')
//OpenTelemetry
const opentelemetry = require('@opentelemetry/sdk-node')
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')
//instrumentations
const { ExpressInstrumentation } = require('@opentelemetry/instrumentation-express')
const { MongoDBInstrumentation } = require('@opentelemetry/instrumentation-mongodb')
const { HttpInstrumentation } = require('@opentelemetry/instrumentation-http')

const { Resource } = require('@opentelemetry/resources')
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions')

const exporterOptions = {
  url: 'http://localhost:4318/v1/traces',
}

const traceExporter = new OTLPTraceExporter(exporterOptions)
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [\
    new ExpressInstrumentation(),\
    new MongoDBInstrumentation(),\
    new HttpInstrumentation(),\
  ],
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: 'node_app',
  }),
})

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk
    .shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0))
})
</code></pre>
<p>OpenTelemetry Node SDK currently does not detect the <code>OTEL_RESOURCE_ATTRIBUTES</code> from <code>.env</code> files as of today. That‚Äôs why we need to include the variables in the <code>tracing.js</code> file itself.</p>
<p>About environment variables:</p>
<p><code>service_name</code>¬†: node_app (you can give whatever name that suits you)</p>
<p><code>http://localhost:4318/v1/traces</code> is the default url for sending your tracing data. We are assuming you have installed SigNoz on your <code>localhost</code>. Based on your environment, you can update it accordingly. It should be in the following format:</p>
<p><code>http://&lt;IP of SigNoz backend&gt;:4318/v1/traces</code></p>
<p>üìù Note</p>
<p>Remember to allow incoming requests to port 4318 of machine where SigNoz backend is hosted.</p>
</li>
<li>
<p><strong>Run the application</strong><br />
The tracing configuration should be run before your application code. We will use the <a href="https://nodejs.org/api/cli.html#cli_r_require_module"><code>-r, ‚Äîrequire module</code></a> flag for that.</p>
<pre><code>node -r ./tracing.js app.js
</code></pre>
<p>üìù Note</p>
<p>If you're running your nodejs application in PM2 cluster mode, it doesn't support node args: <a href="https://github.com/Unitech/pm2/issues/3227">Unitech/pm2#3227</a>
. As above sample app instrumentation requires to load <code>tracing.js</code> before app load by passing node arg, so nodejs instrumentation doesn't work in PM2 cluster mode. So you need to import <code>tracing.js</code> in your main application. The <code>import ./tracing.js</code> should be the first line of your application code and initialize it before any other function. Here's the <a href="https://github.com/SigNoz/sample-nodejs-app/tree/init-tracer-main">sample github repo</a> which shows the implementation.</p>
</li>
</ol>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by <a href="#validating-instrumentation-by-checking-for-traces">validating</a> if your traces are being to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#manual-instrumentation-in-javascript
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-manual-instrumentation-in-javascript
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Manual Instrumentation in JavaScript</h2>
<p>For those looking to gain deeper insights into their application's performance and behavior, manual instrumentation provides a powerful way to achieve this.</p>
<p>Refer to the documentation for <a href="/docs/instrumentation/manual-instrumentation/javascript/nodejs/">Manual Instrumentation in NodeJS</a> to delve into the step-by-step process of adding custom tracing to your Node.js applications.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#instrumentation-modules-for-databases
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-instrumentation-modules-for-databases
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Instrumentation Modules for Databases</h2>
<p>The <code>@opentelemetry/auto-instrumentations-node</code> can inititialize instrumentation for popular databases. Hence it‚Äôs recommended to <a href="#using-the-all-in-one-auto-instrumentation-library">get started</a> with it.</p>
<p>But if you are using <a href="#using-a-specific-auto-instrumentation-library">specific auto-instrumentation packages</a>
, here‚Äôs a list of packages for popular databases.</p>
<h3>## MongoDB instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>‚Ä¢ <code>&gt;=3.3 &lt;5</code></p>
<p>Module that provides automatic instrumentation for MongoDB:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-mongodb
</code></pre>
<h3>## Redis Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>This package supports¬†<code>redis@^2.6.0</code> and¬†<code>redis@^3.0.0</code> For version¬†<code>redis@^4.0.0</code>, please use¬†<code>@opentelemetry/instrumentation-redis-4</code></p>
<pre><code>npm install --save @opentelemetry/instrumentation-redis
</code></pre>
<h3>## MySQL Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>‚Ä¢ <code>2.x</code></p>
<p>Module that provides automatic instrumentation for MySQL:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-mysql
</code></pre>
<h3>## Memcached Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<ul>
<li><code>&gt;=2.2</code></li>
</ul>
<p>Module that provides automatic instrumentation for Memcached:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-memcached
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#troubleshooting-your-installation
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Troubleshooting your installation</h2>
<p>Set an environment variable to run the OpenTelemetry launcher in debug mode, where it logs details about the configuration and emitted spans:</p>
<pre><code>export OTEL_LOG_LEVEL=debug
</code></pre>
<p>The output may be very verbose with some benign errors. Early in the console output, look for logs about the configuration. Next, look for lines like the ones below, which are emitted when spans are emitted to SigNoz.</p>
<pre><code>{
  &quot;traceId&quot;: &quot;985b66d592a1299f7d12ebca56ca1fe3&quot;,
  &quot;parentId&quot;: &quot;8d62a70aa335a227&quot;,
  &quot;name&quot;: &quot;bar&quot;,
  &quot;id&quot;: &quot;17ada85c3d55376a&quot;,
  &quot;kind&quot;: 0,
  &quot;timestamp&quot;: 1685674607399000,
  &quot;duration&quot;: 299,
  &quot;attributes&quot;: {},
  &quot;status&quot;: { &quot;code&quot;: 0 },
  &quot;events&quot;: []
}
{
  &quot;traceId&quot;: &quot;985b66d592a1299f7d12ebca56ca1fe3&quot;,
  &quot;name&quot;: &quot;foo&quot;,
  &quot;id&quot;: &quot;8d62a70aa335a227&quot;,
  &quot;kind&quot;: 0,
  &quot;timestamp&quot;: 1585130342183948,
  &quot;duration&quot;: 315,
  &quot;attributes&quot;: {
    &quot;name&quot;: &quot;value&quot;
  },
  &quot;status&quot;: { &quot;code&quot;: 0 },
  &quot;events&quot;: [\
    {\
      &quot;name&quot;: &quot;event in foo&quot;,\
      &quot;time&quot;: [1585130342, 184213041]\
    }\
  ]
}
</code></pre>
<p><em>Running short applications (Lambda/Serverless/etc)</em> If your application exits quickly after startup, you may need to explicitly shutdown the tracer to ensure that all spans are flushed:</p>
<pre><code>opentelemetry.trace.getTracer('your_tracer_name').getActiveSpanProcessor().shutdown()
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#sample-javascript-app
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-sample-javascript-app
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Sample Javascript App</h2>
<ul>
<li>We have included a sample applications at:
<ul>
<li>
<p><a href="https://github.com/SigNoz/sample-reactjs-app">Sample React App Github Repo</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/sample-nodejs-app">Sample NodeJs App Github Repo</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/distributed-tracing-nodejs-sample">Sample Distributed Tracing NodeJs App Github Repo</a></p>
</li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#further-reading
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-further-reading
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Further Reading</h2>
<ul>
<li>
<p><a href="https://signoz.io/blog/nodejs-performance-monitoring/">Nodejs Performance Monitoring</a></p>
</li>
<li>
<p><a href="https://signoz.io/blog/distributed-tracing-nodejs/">Implementing Distributed Tracing in a Nodejs application</a></p>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/
tag_set: instrumentation, java
image_urls: https://signoz.io/img/docs/opentelemetry_java_instrument.webp
tracking_id: docs-instrumentation-java
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Java applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Java application.</p>
<p>OpenTelemetry Java is the language-specific implementation of OpenTelemetry in Java.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Java application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/opentelemetry_java_instrument.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Java applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>There are two types of application instrumentation:</p>
<ul>
<li>
<p><strong>Auto Instrumentation</strong><br />
A completely automatic and out of box experience, with minimal code changes. For your Java application, we recommend getting started with auto instrumentation.</p>
<p>‚úÖ Info</p>
<p>If you are on K8s, you should checkout <a href="/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">opentelemetry operators</a> which enable auto instrumenting Java applications very easily.</p>
</li>
<li>
<p><strong>Manual Instrumentation</strong><br />
It involves writing instrumentation using OpenTelemetry SDK and API manually. You would need to get a handle to an instance of the <code>OpenTelemetry</code> interface, acquire a tracer, and create <a href="https://signoz.io/blog/distributed-tracing-span/">spans</a> manually. Manual isntrumentation might also be used along with auto instrumentation.</p>
</li>
</ul>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in Java. If you're using SigNoz cloud, refer to this <a href="#send-traces-to-signoz-cloud">section</a>. If you're using self-hosted SigNoz refer to this <a href="#send-traces-to-self-hosted-signoz">section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#requirements
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-requirements
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Requirements</h2>
<p>Java 8 or higher</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#send-traces-to-signoz-cloud
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud</h2>
<p>OpenTelemetry provides a handy Java JAR agent that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetes</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h4>## Send traces directly to SigNoz Cloud</h4>
<p>OpenTelemetry Java agent can send traces directly to SigNoz Cloud.</p>
<p><strong>Step 1.</strong> Download otel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.{region}.signoz.cloud:443 \
java -javaagent:$PWD/opentelemetry-javaagent.jar -jar &lt;my-app&gt;.jar
</code></pre>
<ul>
<li><code>&lt;app_name&gt;</code> is the name for your application</li>
<li><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />
<h4>## Send traces via OTel Collector binary</h4>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Java application.</p>
<p><strong>Step 1.</strong> Download OTel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Run your application</p>
<pre><code>java -javaagent:$PWD/opentelemetry-javaagent.jar -jar &lt;myapp&gt;.jar
</code></pre>
<ul>
<li><code>&lt;myapp&gt;</code> is the name of your application jar file</li>
<li>In case you download <code>opentelemetry-javaagent.jar</code> file in different directory than that of the project, replace <code>$PWD</code> with the path of the otel jar file.</li>
</ul>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can use OpenTelemetry Java to send your traces directly to SigNoz. OpenTelemetry provides a <strong>handy Java JAR agent</strong> that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<h3>## Steps to auto-instrument Java applications for traces</h3>
<p><a href="https://signoz.io/opentelemetry/java-auto-instrumentation/">OpenTelemetry Java auto-instrumentation</a> supports collecting telemetry data from a huge number of libraries and frameworks. You can check out the full list <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/supported-libraries.md">here</a>.</p>
<ol>
<li>
<p><strong>Download the latest OpenTelemetry Java JAR agent</strong><br />
Download the latest <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar">Java JAR agent</a>
. You can also use the terminal to get the file using the following command:</p>
<pre><code> wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
</li>
<li>
<p><strong>Enable the instrumentation agent and run your application</strong><br />
If you run your Java application as a JAR file, run your application using the following command:</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; java -javaagent:/path/to/opentelemetry-javaagent.jar -jar  &lt;myapp&gt;.jar
</code></pre>
<p>where <code>&lt;app_name&gt;</code> is the name you want to set for your application.¬†<code>path</code> should be updated to the path of the downloaded Java JAR agent.</p>
<p>In the above command, we are configuring the exporter to send data to SigNoz backend. By default, OpenTelemetry Java agent uses¬†<a href="https://github.com/open-telemetry/opentelemetry-java/tree/main/exporters/otlp">OTLP exporter</a> configured to send data.</p>
<p>Two things to note about the command:</p>
<p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code> - This is the endpoint of the machine where SigNoz is installed.</p>
<p><code>path/to</code> - Update it to the path of your downloaded Java JAR agent.</p>
<p>If you have installed SigNoz on your <code>localhost</code> and your Java JAR agent is saved at <code>/Users/john/Downloads/</code>, then the final command looks like:</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=javaApp java -javaagent:/Users/john/Downloads/opentelemetry-javaagent.jar -jar target/*.jar
</code></pre>
<p>Here‚Äôs a handy <a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> to figure out which address to use to send data to SigNoz.</p>
<p>You can also specify environment variables in the following way:</p>
<pre><code>java -javaagent:/path/opentelemetry-javaagent.jar \
    -Dotel.exporter.otlp.endpoint=http://&lt;IP of SigNoz Backend&gt;:4317 \
    -Dotel.resource.attributes=service.name=&lt;app_name&gt; \
    -jar &lt;myapp&gt;.jar
</code></pre>
</li>
</ol>
<p>üìù Note</p>
<p>üí° Remember to allow incoming requests to port 4317 of the machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, java
image_urls: https://signoz.io/img/docs/java_app_services_list.webp
tracking_id: docs-instrumentation-java-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>. Ensure that you're checking data for the <code>time range filter</code> applied in the top right corner.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using self-hosted SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/java_app_services_list.webp" alt="Java Application in the list of services being monitored in SigNoz" /></p>
<p><em>Java Application in the list of services being monitored in SigNoz</em></p>
<p>If you don't see your application reported in the list of services, try our <a href="https://signoz.io/docs/install/troubleshooting/">troubleshooting</a> guide.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#configuring-the-agent
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-configuring-the-agent
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Configuring the agent</h2>
<p>The agent is highly configurable. You can check out all the configuration options available <a href="https://opentelemetry.io/docs/instrumentation/java/automatic/agent-config/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#disabled-instrumentations
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-disabled-instrumentations
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Disabled instrumentations</h2>
<p>Some instrumentations can produce too many spans and make traces very noisy. For this reason, the following instrumentations are disabled by default:</p>
<ul>
<li><code>jdbc-datasource</code>¬†which creates spans whenever the¬†<code>java.sql.DataSource#getConnection</code>¬†method is called.</li>
<li><code>dropwizard-metrics</code>,¬†which might create very low-quality metrics data because of the lack of label/attribute support in the Dropwizard metrics API.</li>
</ul>
<p>To enable them, add the¬†<code>otel.instrumentation.&lt;name&gt;.enabled</code>¬†system property:¬†<code>-Dotel.instrumentation.jdbc-datasource.enabled=true</code></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#manual-instrumentation
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-manual-instrumentation
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Manual Instrumentation</h2>
<p>For manual instrumentation of Java application, refer to the docs <a href="https://opentelemetry.io/docs/instrumentation/java/manual/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#instrumentation-using-otel-buildpack-paketo-for-java
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-instrumentation-using-otel-buildpack-paketo-for-java
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Instrumentation using Otel buildpack (paketo) for Java</h2>
<ol>
<li>
<p><strong>Clone OTel buildpack repo:</strong></p>
<pre><code>git clone https://github.com/paketo-buildpacks/opentelemetry.git
</code></pre>
</li>
<li>
<p><strong>Switch to config-binding branch:</strong></p>
<pre><code>git checkout config-binding
</code></pre>
</li>
<li>
<p><strong>Run the following command:</strong></p>
<pre><code>scripts/build.sh
</code></pre>
</li>
<li>
<p><strong>Now run below command to build a pack:</strong></p>
<pre><code>pack build paketo-demo-app \
  --path /Users/makeavish/samples/java/maven \
  --buildpack paketo-buildpacks/java \
  --buildpack . \
  --builder paketobuildpacks/builder:base \
  --verbose --trust-builder \
  -e BP_JVM_VERSION=17 -e BP_OPENTELEMETRY_ENABLED=true
</code></pre>
</li>
<li>
<p>Pass environment variables to enable java agent <code>OTEL_JAVAAGENT_ENABLED=true</code> set exporter endpoint <code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317</code> and set service name <code>OTEL_RESOURCE_ATTRIBUTES=service.name=javaApp</code></p>
</li>
<li>
<p>Other otel configurations can be updated by passing more environment variables. Refer to <a href="https://opentelemetry.io/docs/instrumentation/java/automatic/agent-config/">official docs</a> for more such configurations.</p>
</li>
<li>
<p>Run docker command:</p>
<pre><code>docker run -d -p 8080:8080 -e PORT=8080 -e OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 -e OTEL_RESOURCE_ATTRIBUTES=service.name=javaApp -e OTEL_JAVAAGENT_ENABLED=true paketo-demo-app
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#troubleshooting-your-installation
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Troubleshooting your installation</h2>
<p>If spans are not being reported to SigNoz, try running in debug mode by setting <code>OTEL_LOG_LEVEL=debug</code>:</p>
<p>The debug log level will print out the configuration information. It will also emit every span to the console, which should look something like:</p>
<pre><code>Span {
  attributes: {},
  links: [],
  events: [],
  status: { code: 0 },
  endTime: [ 1597810686, 885498645 ],
  _ended: true,
  _duration: [ 0, 43333 ],
  name: 'bar',
  spanContext: {
    traceId: 'eca3cc297720bd705e734f4941bca45a',
    spanId: '891016e5f8c134ad',
    traceFlags: 1,
    traceState: undefined
  },
  parentSpanId: 'cff3a2c6bfd4bbef',
  kind: 0,
  startTime: [ 1597810686, 885455312 ],
  resource: Resource { labels: [Object] },
  instrumentationLibrary: { name: 'example', version: '*' },
  _logger: ConsoleLogger {
    debug: [Function],
    info: [Function],
    warn: [Function],
    error: [Function]
  },
  _traceParams: {
    numberOfAttributesPerSpan: 32,
    numberOfLinksPerSpan: 32,
    numberOfEventsPerSpan: 128
  },
  _spanProcessor: MultiSpanProcessor { _spanProcessors: [Array] }
},
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#sample-java-application
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-sample-java-application
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Sample Java Application</h2>
<ul>
<li>We have included a sample Java application with README.md at <a href="https://github.com/SigNoz/distributed-tracing-java-sample">Sample Java App Github Repo.</a></li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Go applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Go application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#send-traces-to-signoz-cloud
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-golang
<p>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud</p>
<ol>
<li>
<p><strong>Install Dependencies</strong><br />
Dependencies related to OpenTelemetry exporter and SDK have to be installed first. Note that we are assuming you are using <code>gin</code> request router. If you are using other request routers, check out the <a href="#request-routers">corresponding package</a>
.</p>
<p>Run the below commands after navigating to the application source folder:</p>
<pre><code>go get go.opentelemetry.io/otel \
  go.opentelemetry.io/otel/trace \
  go.opentelemetry.io/otel/sdk \
  go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc
</code></pre>
</li>
<li>
<p><strong>Declare environment variables for configuring OpenTelemetry</strong><br />
Declare the following global variables in¬†<code>main.go</code> which we will use to configure OpenTelemetry:</p>
<pre><code> var (
     serviceName  = os.Getenv(&quot;SERVICE_NAME&quot;)
     collectorURL = os.Getenv(&quot;OTEL_EXPORTER_OTLP_ENDPOINT&quot;)
     insecure     = os.Getenv(&quot;INSECURE_MODE&quot;)
 )
</code></pre>
</li>
<li>
<p><strong>Instrument your Go application with OpenTelemetry</strong><br />
To configure your application to send data we will need a function to initialize OpenTelemetry. Add the following snippet of code in your¬†<code>main.go</code> file.</p>
<pre><code>import (
    .....

    &quot;google.golang.org/grpc/credentials&quot;
    &quot;github.com/gin-gonic/gin&quot;
    &quot;go.opentelemetry.io/otel&quot;
    &quot;go.opentelemetry.io/otel/attribute&quot;
    &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace&quot;
    &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc&quot;

    &quot;go.opentelemetry.io/otel/sdk/resource&quot;
    sdktrace &quot;go.opentelemetry.io/otel/sdk/trace&quot;
)

func initTracer() func(context.Context) error {

    var secureOption otlptracegrpc.Option

    if strings.ToLower(insecure) == &quot;false&quot; || insecure == &quot;0&quot; || strings.ToLower(insecure) == &quot;f&quot; {
        secureOption = otlptracegrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, &quot;&quot;))
    } else {
        secureOption = otlptracegrpc.WithInsecure()
    }

    exporter, err := otlptrace.New(
        context.Background(),
        otlptracegrpc.NewClient(
            secureOption,
            otlptracegrpc.WithEndpoint(collectorURL),
        ),
    )

    if err != nil {
        log.Fatalf(&quot;Failed to create exporter: %v&quot;, err)
    }
    resources, err := resource.New(
        context.Background(),
        resource.WithAttributes(
            attribute.String(&quot;service.name&quot;, serviceName),
            attribute.String(&quot;library.language&quot;, &quot;go&quot;),
        ),
    )
    if err != nil {
        log.Fatalf(&quot;Could not set resources: %v&quot;, err)
    }

    otel.SetTracerProvider(
        sdktrace.NewTracerProvider(
            sdktrace.WithSampler(sdktrace.AlwaysSample()),
            sdktrace.WithBatcher(exporter),
            sdktrace.WithResource(resources),
        ),
    )
    return exporter.Shutdown
}
</code></pre>
</li>
<li>
<p><strong>Initialize the tracer in main.go</strong><br />
Modify the main function to initialise the tracer in¬†<code>main.go</code>. Initiate the tracer at the very beginning of our main function.</p>
<pre><code>func main() {
    cleanup := initTracer()
    defer cleanup(context.Background())

    ......
}
</code></pre>
</li>
<li>
<p><strong>Add the OpenTelemetry Gin middleware</strong><br />
Configure Gin to use the middleware by adding the following lines in¬†<code>main.go</code>.</p>
<pre><code>import (
    ....
  &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin&quot;
)

func main() {
    ......
    r := gin.Default()
    r.Use(otelgin.Middleware(serviceName))
    ......
}
</code></pre>
</li>
<li>
<p><strong>Set environment variables and run your Go Gin application</strong><br />
The run command must have some environment variables to send data to SigNoz cloud. The run command:</p>
<pre><code>SERVICE_NAME=goApp INSECURE_MODE=false OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ-INGESTION-TOKEN&gt; OTEL_EXPORTER_OTLP_ENDPOINT=ingest.{region}.signoz.cloud:443 go run main.go
</code></pre>
<p>We can replace the placeholders based on our environment.</p>
<p><code>SERVICE_NAME</code>: goGinApp (you can name it whatever you want)</p>
<p><code>OTEL_EXPORTER_OTLP_HEADERS</code>: <code>signoz-access-token=&lt;SIGNOZ-INGESTION-TOKEN&gt;</code>. Update <code>&lt;SIGNOZ-INGESTION-TOKEN&gt;</code> with the ingestion token provided by SigNoz</p>
<p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code>: ingest.<code>{region}</code>.signoz.cloud:443. Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
</li>
</ol>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#send-traces-via-otel-collector-binary
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-golang
<p>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Golang application.</p>
<ol>
<li>
<p><strong>Install Dependencies</strong><br />
Dependencies related to OpenTelemetry exporter and SDK have to be installed first. Note that we are assuming you are using <code>gin</code> request router. If you are using other request routers, check out the <a href="#request-routers">corresponding package</a>
.</p>
<p>Run the below commands after navigating to the application source folder:</p>
<pre><code>go get go.opentelemetry.io/otel \
  go.opentelemetry.io/otel/trace \
  go.opentelemetry.io/otel/sdk \
  go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc
</code></pre>
</li>
<li>
<p><strong>Declare environment variables for configuring OpenTelemetry</strong><br />
Declare the following global variables in¬†<code>main.go</code> which we will use to configure OpenTelemetry:</p>
<pre><code> var (
     serviceName  = os.Getenv(&quot;SERVICE_NAME&quot;)
     collectorURL = os.Getenv(&quot;OTEL_EXPORTER_OTLP_ENDPOINT&quot;)
     insecure     = os.Getenv(&quot;INSECURE_MODE&quot;)
 )
</code></pre>
</li>
<li>
<p><strong>Instrument your Go application with OpenTelemetry</strong><br />
To configure your application to send data we will need a function to initialize OpenTelemetry. Add the following snippet of code in your¬†<code>main.go</code> file.</p>
<pre><code> import (
     .....

     &quot;google.golang.org/grpc/credentials&quot;
     &quot;github.com/gin-gonic/gin&quot;
     &quot;go.opentelemetry.io/otel&quot;
     &quot;go.opentelemetry.io/otel/attribute&quot;
     &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace&quot;
     &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc&quot;

     &quot;go.opentelemetry.io/otel/sdk/resource&quot;
     sdktrace &quot;go.opentelemetry.io/otel/sdk/trace&quot;
 )

 func initTracer() func(context.Context) error {

     var secureOption otlptracegrpc.Option

     if strings.ToLower(insecure) == &quot;false&quot; || insecure == &quot;0&quot; || strings.ToLower(insecure) == &quot;f&quot; {
         secureOption = otlptracegrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, &quot;&quot;))
     } else {
         secureOption = otlptracegrpc.WithInsecure()
     }

     exporter, err := otlptrace.New(
         context.Background(),
         otlptracegrpc.NewClient(
             secureOption,
             otlptracegrpc.WithEndpoint(collectorURL),
         ),
     )

     if err != nil {
         log.Fatalf(&quot;Failed to create exporter: %v&quot;, err)
     }
     resources, err := resource.New(
         context.Background(),
         resource.WithAttributes(
             attribute.String(&quot;service.name&quot;, serviceName),
             attribute.String(&quot;library.language&quot;, &quot;go&quot;),
         ),
     )
     if err != nil {
         log.Fatalf(&quot;Could not set resources: %v&quot;, err)
     }

     otel.SetTracerProvider(
         sdktrace.NewTracerProvider(
             sdktrace.WithSampler(sdktrace.AlwaysSample()),
             sdktrace.WithBatcher(exporter),
             sdktrace.WithResource(resources),
         ),
     )
     return exporter.Shutdown
 }
</code></pre>
</li>
<li>
<p><strong>Initialize the tracer in main.go</strong><br />
Modify the main function to initialise the tracer in¬†<code>main.go</code>. Initiate the tracer at the very beginning of our main function.</p>
<pre><code>func main() {
    cleanup := initTracer()
    defer cleanup(context.Background())

    ......
}
</code></pre>
</li>
<li>
<p><strong>Add the OpenTelemetry Gin middleware</strong><br />
Configure Gin to use the middleware by adding the following lines in¬†<code>main.go</code>.</p>
<pre><code>import (
    ....
  &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin&quot;
)

func main() {
    ......
    r := gin.Default()
    r.Use(otelgin.Middleware(serviceName))
    ......
}
</code></pre>
</li>
<li>
<p><strong>Set environment variables and run your Go Gin application</strong><br />
The run command must have some environment variables to send data to SigNoz. The run command:</p>
<pre><code>SERVICE_NAME=goGinApp INSECURE_MODE=true OTEL_EXPORTER_OTLP_ENDPOINT=localhost:4317 go run main.go
</code></pre>
<p>If you want to update your <code>service_name</code>, you can modify the <code>SERVICE_NAME</code> variable.<br />
<code>SERVICE_NAME</code>: goGinApp (you can name it whatever you want)</p>
</li>
<li>
<p>You can validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<ol>
<li>
<p><strong>Install Dependencies</strong><br />
Dependencies related to OpenTelemetry exporter and SDK have to be installed first. Note that we are assuming you are using <code>gin</code> request router. If you are using other request routers, check out the <a href="#request-routers">corresponding package</a>
.</p>
<p>Run the below commands after navigating to the application source folder:</p>
<pre><code>go get go.opentelemetry.io/otel \
  go.opentelemetry.io/otel/trace \
  go.opentelemetry.io/otel/sdk \
  go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc
</code></pre>
</li>
<li>
<p><strong>Declare environment variables for configuring OpenTelemetry</strong><br />
Declare the following global variables in¬†<code>main.go</code> which we will use to configure OpenTelemetry:</p>
<pre><code> var (
     serviceName  = os.Getenv(&quot;SERVICE_NAME&quot;)
     collectorURL = os.Getenv(&quot;OTEL_EXPORTER_OTLP_ENDPOINT&quot;)
     insecure     = os.Getenv(&quot;INSECURE_MODE&quot;)
 )
</code></pre>
</li>
<li>
<p><strong>Instrument your Go application with OpenTelemetry</strong><br />
To configure your application to send data we will need a function to initialize OpenTelemetry. Add the following snippet of code in your¬†<code>main.go</code> file.</p>
<pre><code> import (
     .....

     &quot;google.golang.org/grpc/credentials&quot;
     &quot;github.com/gin-gonic/gin&quot;
     &quot;go.opentelemetry.io/otel&quot;
     &quot;go.opentelemetry.io/otel/attribute&quot;
     &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace&quot;
     &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc&quot;

     &quot;go.opentelemetry.io/otel/sdk/resource&quot;
     sdktrace &quot;go.opentelemetry.io/otel/sdk/trace&quot;
 )

 func initTracer() func(context.Context) error {

     var secureOption otlptracegrpc.Option

     if strings.ToLower(insecure) == &quot;false&quot; || insecure == &quot;0&quot; || strings.ToLower(insecure) == &quot;f&quot; {
         secureOption = otlptracegrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, &quot;&quot;))
     } else {
         secureOption = otlptracegrpc.WithInsecure()
     }

     exporter, err := otlptrace.New(
         context.Background(),
         otlptracegrpc.NewClient(
             secureOption,
             otlptracegrpc.WithEndpoint(collectorURL),
         ),
     )

     if err != nil {
         log.Fatalf(&quot;Failed to create exporter: %v&quot;, err)
     }
     resources, err := resource.New(
         context.Background(),
         resource.WithAttributes(
             attribute.String(&quot;service.name&quot;, serviceName),
             attribute.String(&quot;library.language&quot;, &quot;go&quot;),
         ),
     )
     if err != nil {
         log.Fatalf(&quot;Could not set resources: %v&quot;, err)
     }

     otel.SetTracerProvider(
         sdktrace.NewTracerProvider(
             sdktrace.WithSampler(sdktrace.AlwaysSample()),
             sdktrace.WithBatcher(exporter),
             sdktrace.WithResource(resources),
         ),
     )
     return exporter.Shutdown
 }
</code></pre>
</li>
<li>
<p><strong>Initialize the tracer in main.go</strong><br />
Modify the main function to initialise the tracer in¬†<code>main.go</code>. Initiate the tracer at the very beginning of our main function.</p>
<pre><code>func main() {
    cleanup := initTracer()
    defer cleanup(context.Background())

    ......
}
</code></pre>
</li>
<li>
<p><strong>Add the OpenTelemetry Gin middleware</strong><br />
Configure Gin to use the middleware by adding the following lines in¬†<code>main.go</code>.</p>
<pre><code>import (
    ....
  &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin&quot;
)

func main() {
    ......
    r := gin.Default()
    r.Use(otelgin.Middleware(serviceName))
    ......
}
</code></pre>
</li>
<li>
<p><strong>Set environment variables and run your Go Gin application</strong><br />
The run command must have some environment variables to send data to SigNoz. The run command:</p>
<pre><code>SERVICE_NAME=&lt;service_name&gt; INSECURE_MODE=true OTEL_EXPORTER_OTLP_ENDPOINT=&lt;IP of SigNoz backend:4317&gt; go run main.go
</code></pre>
<p>We can replace the placeholders based on our environment.</p>
<p><code>SERVICE_NAME</code>: goGinApp (you can name it whatever you want)</p>
<p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code>: localhost:4317</p>
<p>Since, we have installed SigNoz on our local machine, we use the above IP. If you install SigNoz on a different machine, you can update it with the relevant IP.</p>
<p>Do not use <code>http</code> or <code>https</code> in the IP address. For example, if the IP is <code>http://test.com</code> then the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> will be <code>test.com:4317</code>.</p>
<p>Here‚Äôs a handy¬†<a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> to figure out which address to use to send data to SigNoz.</p>
<p>Hence, the final run command looks like this:</p>
<pre><code>SERVICE_NAME=goGinApp INSECURE_MODE=true OTEL_EXPORTER_OTLP_ENDPOINT=localhost:4317 go run main.go
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, golang
image_urls: https://signoz.io/img/blog/2022/04/goginapp_signoz_dashboard.webp
tracking_id: docs-instrumentation-golang-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/blog/2022/04/goginapp_signoz_dashboard.webp" alt="Go Application in the list of services being monitored in SigNoz" /></p>
<p><em>Go Application in the list of services being monitored in SigNoz</em></p>
<p>If you don't see your application reported in the list of services, try our <a href="https://signoz.io/docs/install/troubleshooting/">troubleshooting</a> guide.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#request-routers
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-request-routers
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Request Routers</h2>
<h3>## OpenTelemetry gin/gonic instrumentation</h3>
<pre><code># Add one line to your import() stanza depending upon your request router:
middleware &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin&quot;
</code></pre>
<p>and then inject OpenTelemetry middleware:</p>
<pre><code>router.Use(middleware.Middleware(serviceName))
</code></pre>
<h3>## OpenTelemetry gorillamux instrumentation</h3>
<pre><code># Add one line to your import() stanza depending upon your request router:
middleware &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gorilla/mux/otelmux&quot;
</code></pre>
<p>and then inject OpenTelemetry middleware:</p>
<pre><code>router.Use(middleware.Middleware(serviceName))
</code></pre>
<h3>## OpenTelemetry echo instrumentation</h3>
<pre><code># Add one line to your import() stanza depending upon your request router:
middleware &quot;go.opentelemetry.io/contrib/instrumentation/github.com/labstack/echo/otelecho&quot;
</code></pre>
<p>and then inject OpenTelemetry middleware:</p>
<pre><code>router.Use(middleware.Middleware(serviceName))
</code></pre>
<h3>## If you don‚Äôt use a request router</h3>
<pre><code>import (
  &quot;go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp&quot;
)
</code></pre>
<p>In each place where you pass an http.Handler to a ServeMux, you‚Äôll wrap the handler function. For instance, you‚Äôll make the following replacements:</p>
<pre><code>- mux.Handle(&quot;/path&quot;, h)
+ mux.Handle(&quot;/path&quot;, otelhttp.NewHandler(h, &quot;description of path&quot;))


- mux.Handle(&quot;/path&quot;, http.HandlerFunc(f))
+ mux.Handle(&quot;/path&quot;, otelhttp.NewHandler(http.HandlerFunc(f), &quot;description of path&quot;))
</code></pre>
<p>In this fashion, you can ensure that every function you wrap with othttp will automatically have its metadata collected and a corresponding trace started.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#adding-custom-attributes-and-custom-events-to-spans
tag_set: instrumentation, golang
image_urls: https://signoz.io/img/docs/opentelemetry_go_custom_attributes.webp, https://signoz.io/img/docs/opentelemetry_go_events.webp
tracking_id: docs-instrumentation-golang-adding-custom-attributes-and-custom-events-to-spans
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Adding custom attributes and custom events to spans</h2>
<p>It‚Äôs also possible to set custom attributes or tags to a span. To add custom attributes and events follow the below steps:</p>
<ol>
<li>
<p><strong>Import trace and attribute libraries</strong></p>
<pre><code>import (
    ...
    &quot;go.opentelemetry.io/otel/attribute&quot;
    &quot;go.opentelemetry.io/otel/trace&quot;
)
</code></pre>
</li>
<li>
<p><strong>Fetch current span from context</strong></p>
<pre><code>span := trace.SpanFromContext(c.Request.Context())
</code></pre>
</li>
<li>
<p><strong>Set attribute on current</strong></p>
<pre><code>span.SetAttributes(attribute.String(&quot;controller&quot;, &quot;books&quot;))
</code></pre>
</li>
</ol>
<p>SigNoz dashboards can be used to track these custom attributes.</p>
<p><img src="https://signoz.io/img/docs/opentelemetry_go_custom_attributes.webp" alt="Custom attributes under 'Tags' section on SigNoz trace detail page" /></p>
<p><em>Custom attributes can be seen under <code>Tags</code> section on SigNoz trace detail page</em></p>
<p>We can also set custom events on the span with its own attribute.</p>
<pre><code>span.AddEvent(&quot;This is a sample event&quot;, trace.WithAttributes(attribute.Int(&quot;pid&quot;, 4328), attribute.String(&quot;sampleAttribute&quot;, &quot;Test&quot;)))
</code></pre>
<p><img src="https://signoz.io/img/docs/opentelemetry_go_events.webp" alt="Events can be seen under  section on SigNoz trace detail page" /></p>
<p><em>Events can be seen under <code>Events</code> section on SigNoz trace detail page</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#grpc-instrumentation-with-opentelemetry
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-grpc-instrumentation-with-opentelemetry
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: gRPC Instrumentation with OpenTelemetry</h2>
<p>OpenTelemetry can also help you automatically instrument gRPC requests. To instrument any gRPC servers you have.</p>
<pre><code>import (
    &quot;go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc&quot;
)

func main() {
  [...]

    // add StatsHandler to gRPC server initialization
	s := grpc.NewServer(grpc.StatsHandler(otelgrpc.NewServerHandler()))

}
</code></pre>
<p>Similarly, instrument your gRPC client as well by adding otelgrpc when initializing gRPC client</p>
<pre><code>import (
    &quot;go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc&quot;
)

func main() {
  [...]

    // add StatsHandler to gRPC client initialization
	cc, err := grpc.NewClient(serverUrl, grpc.WithTransportCredentials(insecure.NewCredentials()),
		grpc.WithStatsHandler(otelgrpc.NewClientHandler()),
	)

}
</code></pre>
<p>We have a blog <a href="https://signoz.io/blog/opentelemetry-grpc-golang/">Monitor gRPC calls with OpenTelemetry - explained with a Golang example</a>
, do refer to that in case you need a helping hand to work with gRPC server.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#recording-errors-and-exceptions
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-recording-errors-and-exceptions
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Recording Errors and Exceptions</h2>
<pre><code>import &quot;go.opentelemetry.io/otel/codes&quot;

// Get the current span from the tracer
span := trace.SpanFromContext(ctx)

// RecordError converts an error into a span event.
span.RecordError(err)

// Mark span as failed.
span.SetStatus(codes.Error, &quot;internal error&quot;)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#sample-golang-application
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-sample-golang-application
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Sample Golang application</h2>
<p>We have included a sample gin/gonic application with <code>README.md</code> at <a href="https://github.com/SigNoz/sample-golang-app">https://github.com/SigNoz/sample-golang-app</a>.</p>
<p>Feel free to use this repo to test out OpenTelemetry instrumentation and how to send telemetry data to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#library-and-framework-support
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-library-and-framework-support
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Library and framework support</h2>
<p>Besides OpenTelemetry core modules, it is important to install instrumentation packages for every important library and framework which your service depends upon. Beyond the critical telemetry data these components emit, library and framework integrations are often required to ensure that the <a href="https://signoz.io/blog/context-propagation-in-distributed-tracing/">trace context</a> is properly propagated.</p>
<p>OpenTelemetry automatically provides instrumentation for a large number of libraries and frameworks, right out of the box.</p>
<p>The full list of supported instrumentation can be found in the <a href="https://github.com/open-telemetry/opentelemetry-go-contrib/tree/master/instrumentation">README</a>.</p>
<p>You can also find libraries, plugins, integrations, and other useful tools for extending OpenTelemetry from the OpenTelemetry <a href="https://opentelemetry.io/registry/?language=go">registry</a>.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation</h2>
<p>This doc contains instructions about how to set up OpenTelemetry(OTel) instrumentation in your PHP application. OpenTelemetry, also known as OTel for short, is an open-source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your PHP application.</p>
<p>Once the telemetry data is generated, you can configure an exporter to send the data to SigNoz for monitoring and visualization.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your PHP application with OpenTelemetry</li>
<li>Configuring the exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p>In this documentation, we will instrument a PHP application for traces and send it to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#requirements
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-requirements
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Requirements</h2>
<ul>
<li>
<p><a href="https://www.php.net/">PHP 8.0+</a></p>
</li>
<li>
<p><a href="https://pecl.php.net/">PECL</a></p>
</li>
<li>
<p><a href="https://getcomposer.org/">Composer</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#send-traces-to-signoz-cloud
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-php
<p>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud</p>
<p>Here we will be sending traces to SigNoz cloud in 4 easy steps, if you want to send traces to self hosted SigNoz , you can refer to <a href="https://signoz.io/docs/instrumentation/PHP/#send-traces-to-self-hosted-signoz">this</a>. We will be using Zero-code configuration for Automatic Instrumentation.</p>
<p><strong>Step 1: Setup Development Environment</strong></p>
<p>To configure our PHP application to send data, you need to use OpenTelemetry PHP extension. Since the extension is built from the source, you need to have the build tools, which can be installed using the following command:</p>
<p>Linux (apt)Mac (Homebrew)</p>
<pre><code>sudo apt-get install gcc make autoconf
</code></pre>
<p><strong>Step 2: Build the extension</strong></p>
<p>With our environment set up we can install the extension using <a href="https://pecl.php.net/">PECL</a>
:</p>
<pre><code>pecl install opentelemetry               
pecl install protobuf
</code></pre>
<p>After successfully installing the OpenTelemetry extension, add the extension to <code>php.ini</code> file of your project:</p>
<pre><code>[opentelemetry]
extension=opentelemetry.so
</code></pre>
<p>Verify that the extension is enabled by running:</p>
<pre><code>php -m | grep opentelemetry
</code></pre>
<p>This should output:</p>
<pre><code>opentelemetry
</code></pre>
<p><strong>Step 3: Add the dependencies</strong></p>
<p>Add dependencies required for OpenTelemetry SDK for PHP to perform automatic instrumentation using this command :</p>
<pre><code>composer config allow-plugins.php-http/discovery false
composer require \
  open-telemetry/sdk \
  open-telemetry/exporter-otlp \
  php-http/guzzle7-adapter \
  open-telemetry/opentelemetry-auto-slim
</code></pre>
<p>‚úÖ Info</p>
<p>You can install the additional dependencies provided by OpenTelemetry for different PHP frameworks from <a href="https://packagist.org/explore/?query=open-telemetry">here</a>.</p>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<p>We are passing the environment variables on run time and this way we don't have to change anything in code. Run your application using:</p>
<pre><code>env OTEL_PHP_AUTOLOAD_ENABLED=true \
    OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; \
    OTEL_TRACES_EXPORTER=otlp \
    OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf \
    OTEL_EXPORTER_OTLP_ENDPOINT=&lt;SIGNOZ_ENDPOINT&gt; \
    OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;INGESTION_KEY&gt; \
    OTEL_PROPAGATORS=baggage,tracecontext \
    php -S localhost:8080 app.php
</code></pre>
<p>You can change the env vars value by referencing values from the following lookup table</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_SERVICE_NAME</td>
<td><code>&lt;SERVICE_NAME&gt;</code> replace it with name of your app</td>
</tr>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td><code>&lt;SIGNOZ_ENDPOINT&gt;</code> replace this with SigNoz cloud endpoint</td>
</tr>
<tr>
<td>OTEL_EXPORTER_OTLP_HEADERS</td>
<td>signoz-access-token=<code>&lt;INGESTION_KEY&gt;</code> replace this with the ingestion key which you must have received in mail</td>
</tr>
<tr>
<td>php -S localhost:8080 app.php</td>
<td>you can replace this with the run command of your PHP application</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#send-traces-via-otel-collector-binary
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-php
<p>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p><strong>Step 1: Install OTel Collector</strong></p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your PHP application.</p>
<p><strong>Step 2: Setup Development Environment</strong></p>
<p>To configure our PHP application to send data, you need to use OpenTelemetry PHP extension. Since the extension is built from the source, you need to have the build tools, which can be installed using the following command:</p>
<p>Linux (apt)Mac (Homebrew)</p>
<pre><code>sudo apt-get install gcc make autoconf
</code></pre>
<p><strong>Step 3: Build the extension</strong></p>
<p>With our environment set up we can install the extension using <a href="https://pecl.php.net/">PECL</a>
:</p>
<pre><code>pecl install opentelemetry
pecl install protobuf
</code></pre>
<p>After successfully installing the OpenTelemetry extension, add the extension to <code>php.ini</code> file of your project:</p>
<pre><code>[opentelemetry]
extension=opentelemetry.so
</code></pre>
<p>Verify that the extension is enabled by running:</p>
<pre><code>php -m | grep opentelemetry
</code></pre>
<p>This should output:</p>
<pre><code>opentelemetry
</code></pre>
<p><strong>Step 4: Add the dependencies</strong></p>
<p>Add dependencies required for OpenTelemetry SDK for PHP to perform automatic instrumentation using this command :</p>
<pre><code>composer config allow-plugins.php-http/discovery false
composer require \
  open-telemetry/sdk \
  open-telemetry/exporter-otlp \
  php-http/guzzle7-adapter \
  open-telemetry/opentelemetry-auto-slim
</code></pre>
<p>‚úÖ Info</p>
<p>You can install the additional dependencies provided by OpenTelemetry for different PHP frameworks from <a href="https://packagist.org/explore/?query=open-telemetry">here</a>.</p>
<p><strong>Step 5: Set environment variables and run app</strong></p>
<p>We are passing the environment variables on run time and this way we don't have to change anything in code. Run your application using:</p>
<pre><code>env OTEL_PHP_AUTOLOAD_ENABLED=true \
    OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; \
    OTEL_TRACES_EXPORTER=otlp \
    OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf \
    OTEL_EXPORTER_OTLP_ENDPOINT=&lt;COLLECTOR_ENDPOINT&gt; \
    OTEL_PROPAGATORS=baggage,tracecontext \
    php -S localhost:8080 app.php
</code></pre>
<p>You can change the env vars value by referencing values from the following lookup table</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_SERVICE_NAME</td>
<td><code>&lt;SERVICE_NAME&gt;</code> replace it with name of your app</td>
</tr>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td><code>&lt;COLLECTOR_ENDPOINT&gt;</code> replace this with the Otel Collector Endpoint. If you have hosted it somewhere, provide the URL. Otherwise, the default is <code>http://localhost:4317</code>, if you have followed our guide.</td>
</tr>
<tr>
<td>php -S localhost:8080 app.php</td>
<td>you can replace this with the run command of your PHP application</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<p><strong>Step 1: Setup Development Environment</strong></p>
<p>To configure our PHP application to send data, you need to use OpenTelemetry PHP extension. Since the extension is built from the source, you need to have the build tools, which can be installed using the following command:</p>
<p>Linux (apt)Mac (Homebrew)</p>
<pre><code>sudo apt-get install gcc make autoconf
</code></pre>
<p><strong>Step 2: Build the extension</strong></p>
<p>With our environment set up we can install the extension using <a href="https://pecl.php.net/">PECL</a>
:</p>
<pre><code>pecl install opentelemetry
pecl install protobuf
</code></pre>
<p>After successfully installing the OpenTelemetry extension, add the extension to <code>php.ini</code> file of your project:</p>
<pre><code>[opentelemetry]
extension=opentelemetry.so
</code></pre>
<p>Verify that the extension is enabled by running:</p>
<pre><code>php -m | grep opentelemetry
</code></pre>
<p>This should output:</p>
<pre><code>opentelemetry
</code></pre>
<p><strong>Step 3: Add the dependencies</strong></p>
<p>Add dependencies required for OpenTelemetry SDK for PHP to perform automatic instrumentation using this command :</p>
<pre><code>composer config allow-plugins.php-http/discovery false
composer require \
  open-telemetry/sdk \
  open-telemetry/opentelemetry-auto-slim \
  php-http/guzzle7-adapter \
  open-telemetry/exporter-otlp
</code></pre>
<p>‚úÖ Info</p>
<p>You can install the additional dependencies provided by OpenTelemetry for different PHP frameworks from <a href="https://packagist.org/explore/?query=open-telemetry">here</a>.</p>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<p>We are passing the environment variables on run time and this way we don't have to change anything in code. Run your application using:</p>
<pre><code>env OTEL_PHP_AUTOLOAD_ENABLED=true \
    OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; \
    OTEL_TRACES_EXPORTER=otlp \
    OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf \
    OTEL_EXPORTER_OTLP_ENDPOINT=&lt;SIGNOZ_ENDPOINT&gt; \
    OTEL_PROPAGATORS=baggage,tracecontext \
    php -S localhost:8080 app.php
</code></pre>
<p>You can change the env vars value by referencing values from the following lookup table</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_SERVICE_NAME</td>
<td><code>&lt;SERVICE_NAME&gt;</code> replace it with name of your app</td>
</tr>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td><code>&lt;SIGNOZ_ENDPOINT&gt;</code> replace this with the url where you have hosted SigNoz</td>
</tr>
<tr>
<td>php -S localhost:8080 app.php</td>
<td>you can replace this with the run command of your PHP application</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#sample-php-application
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-sample-php-application
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Sample PHP Application</h2>
<p>We have included a sample PHP application at <a href="https://github.com/SigNoz/OpenTelemetry-PHP-example">Sample PHP App Github Repo</a>
,</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#tutorial
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-tutorial
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Tutorial</h2>
<p>Here's a <a href="https://signoz.io/blog/opentelemetry-PHP/">tutorial</a> with step by step guide on how to install SigNoz and start monitoring a sample PHP app.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry(OTel) instrumentation in your .NET application. OpenTelemetry, also known as OTel for short, is an open-source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your .NET application.</p>
<p>OpenTelemetry .NET is the language-specific implementation of OpenTelemetry in .NET.</p>
<p>Once the telemetry data is generated, you can configure an exporter to send the data to SigNoz for monitoring and visualization.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your .NET application with OpenTelemetry</li>
<li>Configuring the exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p>In this tutorial, we will instrument a .NET application for traces and send it to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#requirements
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-requirements
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation: Requirements</h2>
<p><a href="https://dotnet.microsoft.com/en-us/download">.NET SDK</a> (.NET 5.0 or Later)</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-to-signoz-cloud
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>Tthere are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
<ul>
<li><a href="#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
<ul>
<li><a href="#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended
group_tracking_ids: docs-instrumentation-dotnet
<p>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1: Create a shell script with the following content</strong> You can either create a bash script with the following content or paste this directly into your terminal after replacing <code>SERVICE_NAME</code>, <code>SIGNOZ_ENDPOINT</code> and <code>SIGNOZ_INGESTION_KEY</code> .</p>
<pre><code># Download the bash script
curl -sSfL https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/releases/latest/download/otel-dotnet-auto-install.sh -O

# Install core files
sh ./otel-dotnet-auto-install.sh

# Enable execution for the instrumentation script
chmod +x $HOME/.otel-dotnet-auto/instrument.sh

# Setup the instrumentation for the current shell session
. $HOME/.otel-dotnet-auto/instrument.sh

# Run your application with instrumentation
OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; OTEL_TRACES_EXPORTER=otlp OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_RESOURCE_ATTRIBUTES=deployment.environment=staging,service.version=1.0.0 OTEL_EXPORTER_OTLP_ENDPOINT=&lt;SIGNOZ_ENDPOINT&gt; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; ./MyNetApp
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>SERVICE_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>If you are doing it on mac os , you will need to install <code>coreutils</code>, you can do it by using the following command</p>
<pre><code>brew install coreutils
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation
tag_set: instrumentation, dotnet
image_urls: https://signoz.io/img/docs/ingestion_key_details.webp, https://signoz.io/img/docs/sample_net_app.webp
tracking_id: docs-instrumentation-dotnet-send-traces-directly-to-signoz-cloud--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-dotnet
<p>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation</p>
<p><strong>Step 1: Installing the OpenTelemetry dependency packages:</strong></p>
<pre><code>dotnet add package OpenTelemetry
dotnet add package OpenTelemetry.Exporter.OpenTelemetryProtocol 
dotnet add package OpenTelemetry.Extensions.Hosting
dotnet add package OpenTelemetry.Instrumentation.Runtime
dotnet add package OpenTelemetry.Instrumentation.AspNetCore 
dotnet add package OpenTelemetry.AutoInstrumentation
</code></pre>
<p><strong>Step 2: Adding OpenTelemetry as a service and configuring exporter options in <code>Program.cs</code>:</strong></p>
<p>In your <code>Program.cs</code> file, add OpenTelemetry as a service. Here, we are configuring these variables:</p>
<ul>
<li><code>serviceName</code> - It is the name of your service.</li>
<li><code>otlpOptions.Endpoint</code> - It is the endpoint for SigNoz Cloud.</li>
<li><code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> - You will get your ingestion key when you <a href="https://signoz.io/teams/">sign up</a> for SigNoz cloud.</li>
</ul>
<p>Here‚Äôs a sample <code>Program.cs</code> file with the configured variables.</p>
<pre><code>using System.Diagnostics;
using OpenTelemetry.Exporter;
using OpenTelemetry.Resources;
using OpenTelemetry.Trace;

var builder = WebApplication.CreateBuilder(args);

// Configure OpenTelemetry with tracing and auto-start.
builder.Services.AddOpenTelemetry()
    .ConfigureResource(resource =&gt; 
		resource.AddService(serviceName: &quot;sample-net-app&quot;))
    .WithTracing(tracing =&gt; tracing
        .AddAspNetCoreInstrumentation()
        .AddOtlpExporter(otlpOptions =&gt;
        {
						//SigNoz Cloud Endpoint 
            otlpOptions.Endpoint = new Uri(&quot;https://ingest.{region}.signoz.cloud:443&quot;);

            otlpOptions.Protocol = OtlpExportProtocol.Grpc;
						
						//SigNoz Cloud account Ingestion key
            string headerKey = &quot;signoz-access-token&quot;;
            string headerValue = &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;;

            string formattedHeader = $&quot;{headerKey}={headerValue}&quot;;
            otlpOptions.Headers = formattedHeader;
        }));

var app = builder.Build();

// The index route (&quot;/&quot;) is set up to write out the OpenTelemetry trace information on the response:
app.MapGet(&quot;/&quot;, () =&gt; $&quot;Hello World! OpenTelemetry Trace: {Activity.Current?.Id}&quot;);

app.Run();
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>The program uses the¬†<a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/src/OpenTelemetry.Instrumentation.AspNetCore/README.md">OpenTelemetry.Instrumentation.AspNetCore</a> package to automatically create traces for incoming ASP.NET Core requests.</p>
<p>The <code>OpenTelemetry.Exporter.Options</code> get or set the target to which the exporter is going to send traces. Here, we‚Äôre configuring it to send traces to the SigNoz cloud. The target must be a valid Uri with the scheme (<code>http</code> or <code>https</code>) and host and may contain a port and a path.</p>
<p>This is done by configuring an OpenTelemetry¬†<a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/docs/trace/customizing-the-sdk/README.MD#tracerprovider">TracerProvider</a> using extension methods and setting it to auto-start when the host is started.</p>
<p>üìù Note</p>
<p>You can find your Signoz cloud address and ingestion key under the settings of your Signoz cloud account.</p>
<p><img src="https://signoz.io/img/docs/ingestion_key_details.webp" alt="Access the ingestion key details in SigNoz UI" /></p>
<p><em>Access the ingestion key details in SigNoz UI</em></p>
<p><strong>Step 3. Running the .NET application:</strong></p>
<pre><code>dotnet build
dotnet run
</code></pre>
<p><strong>Step 4: Generating some load data and checking your application in SigNoz UI</strong></p>
<p>Once your application is running, generate some traffic by interacting with it.</p>
<p>In the SigNoz account, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>. Ensure that you're checking data for the¬†<code>time range filter</code>¬†applied in the top right corner. You might have to wait for a few seconds before the data appears on SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/sample_net_app.webp" alt="The sample .NET application is being monitored in the SigNoz ‚ÄòServices‚Äô tab" /></p>
<p><em>The sample .NET application is being monitored in the SigNoz ‚ÄòServices‚Äô tab</em></p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-send-traces-via-otel-collector-binary--no-code-automatic-instrumentation
group_tracking_ids: docs-instrumentation-dotnet
<p>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation: ### ## Send traces via OTel Collector binary - Send traces via OTel Collector binary - No Code Automatic Instrumentation</p>
<p><strong>Step 1: Create a shell script with the following content</strong> You can either create a bash script with the following content or paste this directly into your terminal after replacing <code>SERVICE_NAME</code>, <code>SIGNOZ_ENDPOINT</code> and <code>SIGNOZ_INGESTION_KEY</code> .</p>
<pre><code># Download the bash script
curl -sSfL https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/releases/latest/download/otel-dotnet-auto-install.sh -O

# Install core files
sh ./otel-dotnet-auto-install.sh

# Enable execution for the instrumentation script
chmod +x $HOME/.otel-dotnet-auto/instrument.sh

# Setup the instrumentation for the current shell session
. $HOME/.otel-dotnet-auto/instrument.sh

# Run your application with instrumentation
OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; OTEL_TRACES_EXPORTER=otlp OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_RESOURCE_ATTRIBUTES=deployment.environment=staging,service.version=1.0.0 OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/traces ./MyNetApp
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>SERVICE_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>If you are doing it on mac os , you will need to install <code>coreutils</code>, you can do it by using the following command</p>
<pre><code>brew install coreutils
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation
tag_set: instrumentation, dotnet
image_urls: https://signoz.io/img/docs/sample_net_app.webp
tracking_id: docs-instrumentation-dotnet-send-traces-via-otel-collector-binary--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-dotnet
<p>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation: Send traces via OTel Collector binary - Code Level Automatic Instrumentation</p>
<p><strong>Step 1: Setting up OpenTelemetry Collector binary as an agent in your machine</strong></p>
<p>OpenTelemetry Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install OTel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>Go to <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">setup OTel Collector binary</a> to install Otel Collector as agent that will collect telemetry data from your sample dotnet app and send it to SigNoz cloud.</p>
<p><strong>Step 2: Installing the OpenTelemetry dependency packages:</strong></p>
<p>Install the following dependencies in your application.</p>
<pre><code>dotnet add package OpenTelemetry
dotnet add package OpenTelemetry.Exporter.OpenTelemetryProtocol 
dotnet add package OpenTelemetry.Extensions.Hosting
dotnet add package OpenTelemetry.Instrumentation.Runtime
dotnet add package OpenTelemetry.Instrumentation.AspNetCore 
dotnet add package OpenTelemetry.AutoInstrumentation
</code></pre>
<p><strong>Step 2: Adding OpenTelemetry as a service and configuring exporter options in <code>Program.cs</code>:</strong></p>
<p>In your <code>Program.cs</code> file, add OpenTelemetry as a service. Here, we are configuring these variables:</p>
<ul>
<li><code>serviceName</code> - It is the name of your service.</li>
<li><code>otlpOptions.Endpoint</code> - It is the endpoint for your OTel Collector binary agent.</li>
</ul>
<p>Here‚Äôs a sample <code>Program.cs</code> file with the configured variables.</p>
<pre><code>using System.Diagnostics;
using OpenTelemetry.Exporter;
using OpenTelemetry.Resources;
using OpenTelemetry.Trace;

var builder = WebApplication.CreateBuilder(args);

// Configure OpenTelemetry with tracing and auto-start.
builder.Services.AddOpenTelemetry()
    .ConfigureResource(resource =&gt; 
		resource.AddService(serviceName: &quot;sample-net-app&quot;))
    .WithTracing(tracing =&gt; tracing
        .AddAspNetCoreInstrumentation()
        .AddOtlpExporter(otlpOptions =&gt;
        {
            otlpOptions.Endpoint = new Uri(&quot;http://localhost:4317&quot;);

            otlpOptions.Protocol = OtlpExportProtocol.Grpc;
        }));

var app = builder.Build();

//The index route (&quot;/&quot;) is set up to write out the OpenTelemetry trace information on the response:
app.MapGet(&quot;/&quot;, () =&gt; $&quot;Hello World! OpenTelemetry Trace: {Activity.Current?.Id}&quot;);

app.Run();
</code></pre>
<p>The program uses the¬†<a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/src/OpenTelemetry.Instrumentation.AspNetCore/README.md">OpenTelemetry.Instrumentation.AspNetCore</a> package to automatically create traces for incoming ASP.NET Core requests.</p>
<p>The <code>OpenTelemetry.Exporter.Options</code> get or set the target to which the exporter is going to send traces. Here, we‚Äôre configuring it to send traces to the OTel collector agent. The target must be a valid Uri with the scheme (<code>http</code> or <code>https</code>) and host and may contain a port and a path.</p>
<p>This is done by configuring an OpenTelemetry¬†<a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/docs/trace/customizing-the-sdk/README.MD#tracerprovider">TracerProvider</a> using extension methods and setting it to auto-start when the host is started.</p>
<p><strong>Step 4. Running the .NET application:</strong></p>
<pre><code>dotnet build
dotnet run
</code></pre>
<p><strong>Step 5: Generating some load data and checking your application in SigNoz UI</strong></p>
<p>After the Otel collector is all set and running, and your too application is running, generate some traffic by interacting with it.</p>
<p>In the SigNoz account, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>. Ensure that you're checking data for the¬†<code>time range filter</code>¬†applied in the top right corner. You might have to wait for a few seconds before the data appears on SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/sample_net_app.webp" alt="The sample .NET application is being monitored in the SigNoz ‚ÄòServices‚Äô tab" /></p>
<p><em>The sample .NET application is being monitored in the SigNoz ‚ÄòServices‚Äô tab</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#troubleshooting
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-troubleshooting
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Troubleshooting</h2>
<p>The console exporter prints data to the Console window. You can use it to verify if the instrumentation is properly set up or not.</p>
<p>Below are the steps on how to use the console exporter:</p>
<p><strong>Step 1. Adding the OpenTelemetry console exporter package:</strong></p>
<pre><code>dotnet add package OpenTelemetry.Exporter.Console 
</code></pre>
<p><strong>Step 2. Adding the console exporter method:</strong></p>
<pre><code>using System.Diagnostics;
using OpenTelemetry.Exporter;
using OpenTelemetry.Resources;
using OpenTelemetry.Trace;

var builder = WebApplication.CreateBuilder(args);

// Configure OpenTelemetry with tracing and auto-start.
builder.Services.AddOpenTelemetry()
    .ConfigureResource(resource =&gt; 
		resource.AddService(serviceName: &quot;sample-net-app&quot;))
    .WithTracing(tracing =&gt; tracing
        .AddAspNetCoreInstrumentation()
        .AddOtlpExporter(otlpOptions =&gt;
        {
						//SigNoz Cloud Endpoint 
            otlpOptions.Endpoint = new Uri(&quot;https://ingest.{region}.signoz.cloud:443&quot;);

            otlpOptions.Protocol = OtlpExportProtocol.Grpc;
						
						//SigNoz Cloud account Ingestion key
            string headerKey = &quot;signoz-access-token&quot;;
            string headerValue = &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;;

            string formattedHeader = $&quot;{headerKey}={headerValue}&quot;;
            otlpOptions.Headers = formattedHeader;
        })
				.AddConsoleExporter());

var app = builder.Build();

//The index route (&quot;/&quot;) is set up to write out the OpenTelemetry trace information on the response:
app.MapGet(&quot;/&quot;, () =&gt; $&quot;Hello World! OpenTelemetry Trace: {Activity.Current?.Id}&quot;);

app.Run();
</code></pre>
<p>Monitor the application on the console. You will be able to see the trace output as below:</p>
<pre><code>info: Microsoft.Hosting.Lifetime[14]
      Now listening on: https://localhost:7062
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://localhost:5017
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
info: Microsoft.Hosting.Lifetime[0]
      Hosting environment: Development
info: Microsoft.Hosting.Lifetime[0]
      Content root path: C:\sample-app2
Activity.TraceId:            e1c2b70e9f39c6cc15d5d94b75412b70
Activity.SpanId:             17da84c0833e0075
Activity.TraceFlags:         Recorded
Activity.ActivitySourceName: Microsoft.AspNetCore
Activity.DisplayName:        /
Activity.Kind:               Server
Activity.StartTime:          2023-11-05T19:59:39.7875151Z
Activity.Duration:           00:00:00.2548901
Activity.Tags:
    net.host.name: localhost
    net.host.port: 7062
    http.method: GET
    http.scheme: https
    http.target: /
    http.url: https://localhost:7062/
    http.flavor: 2.0
    http.user_agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36
    http.status_code: 200
Resource associated with Activity:
    service.name: sample-app2
    service.instance.id: 44a34277-d46e-4758-b4f0-91b5a9435a4c
    telemetry.sdk.name: opentelemetry
    telemetry.sdk.language: dotnet
    telemetry.sdk.version: 1.6.0
</code></pre>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#send-traces-to-signoz-cloud
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: Ruby on Rails OpenTelemetry Instrumentation - Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h3>## Send traces directly to SigNoz Cloud</h3>
<p><strong>Step 1. Install dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry SDK and exporter using gem.</p>
<pre><code>gem install opentelemetry-sdk
gem install opentelemetry-exporter-otlp
gem install opentelemetry-instrumentation-all
</code></pre>
<p>Include the required packages into your gemfile.</p>
<pre><code>gem 'opentelemetry-sdk'
gem 'opentelemetry-exporter-otlp'
gem 'opentelemetry-instrumentation-all'
</code></pre>
<p>Run the bundle install command:</p>
<pre><code>bundle install
</code></pre>
<p><strong>Step 2. Initialize the OpenTelemetry SDK</strong></p>
<p>Initialize the otel sdk by adding below lines to <code>config/environment.rb</code> of your Ruby on Rails application.</p>
<pre><code>require 'opentelemetry/sdk'
require_relative 'application'

OpenTelemetry::SDK.configure do |c|
  c.use_all
end

Rails.application.initialize!
</code></pre>
<p><strong>Step 3. Running your Ruby application</strong></p>
<p>Run the application using the below:</p>
<pre><code>OTEL_EXPORTER=otlp \
OTEL_SERVICE_NAME= \
OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.{region}.signoz.cloud:443 \
OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=SIGNOZ_INGESTION_KEY \
rails server
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of service. For example, <code>sampleRailsApp</code></li>
<li><code>SIGNOZ_INGESTION_KEY</code> : The ingestion key sent by SigNoz over email. It can also be found in the <code>settings</code> section of your SigNoz Cloud UI.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<hr />
<h3>## Send traces via OTel Collector binary</h3>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Ruby on Rails application.</p>
<p><strong>Step 1. Install dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry SDK and exporter using gem.</p>
<pre><code>gem install opentelemetry-sdk
gem install opentelemetry-exporter-otlp
gem install opentelemetry-instrumentation-all
</code></pre>
<p>Include the required packages into your gemfile.</p>
<pre><code>gem 'opentelemetry-sdk'
gem 'opentelemetry-exporter-otlp'
gem 'opentelemetry-instrumentation-all'
</code></pre>
<p>Run the bundle install command:</p>
<pre><code>bundle install
</code></pre>
<p><strong>Step 2. Initialize the OpenTelemetry SDK</strong></p>
<p>Initialize the otel sdk by adding below lines to <code>config/environment.rb</code> of your Ruby on Rails application.</p>
<pre><code>require 'opentelemetry/sdk'
require_relative 'application'

OpenTelemetry::SDK.configure do |c|
  c.use_all
end

Rails.application.initialize!
</code></pre>
<p><strong>Step 3. Running your Ruby application</strong></p>
<p>Run the application using the below:</p>
<pre><code>OTEL_EXPORTER=otlp \
OTEL_SERVICE_NAME= \
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 \
rails server
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of service. For example, <code>sampleRailsApp</code></li>
</ul>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4318 with <code>&lt;IP Address of the VM&gt;:4318</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can follow these steps to send your traces directly to your Self-Host SigNoz instance.</p>
<p><strong>Step 1. Install dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry SDK and exporter using gem.</p>
<pre><code>gem install opentelemetry-sdk
gem install opentelemetry-exporter-otlp
gem install opentelemetry-instrumentation-all
</code></pre>
<p>Include the required packages into your gemfile.</p>
<pre><code>gem 'opentelemetry-sdk'
gem 'opentelemetry-exporter-otlp'
gem 'opentelemetry-instrumentation-all'
</code></pre>
<p>Run the bundle install command:</p>
<pre><code>bundle install
</code></pre>
<p><strong>Step 2. Initialize the OpenTelemetry SDK</strong></p>
<p>Initialize the otel sdk by adding below lines to <code>config/environment.rb</code> of your Ruby on Rails application.</p>
<pre><code>require 'opentelemetry/sdk'
require_relative 'application'

OpenTelemetry::SDK.configure do |c|
  c.use_all
end

Rails.application.initialize!
</code></pre>
<p><strong>Step 3. Running your Ruby application</strong></p>
<p>Run the application using the below:</p>
<pre><code>OTEL_EXPORTER=otlp \
OTEL_SERVICE_NAME= \
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 \
rails server
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of service. For example, <code>sampleRailsApp</code></li>
</ul>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4318 with <code>&lt;IP Address of the VM&gt;:4318</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#tutorials
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-tutorials
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: Tutorials</h2>
<p>Here's a <a href="https://signoz.io/blog/opentelemetry-ruby/">tutorial</a> with step by step guide on how to install SigNoz and start monitoring a sample Ruby on Rails app.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#sample-ruby-on-rails-application
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-sample-ruby-on-rails-application
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: Sample Ruby on Rails application</h2>
<p>We have included a sample Ruby on Rails application with README.md at <a href="https://github.com/SigNoz/sample-rails-app">Sample Rails App Github Repo.</a></p>
<p>Feel free to use this repo to test out OpenTelemetry instrumentation and how to send telemetry data to SigNoz.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation</h2>
<p>This document contains OpenTelemetry instrumentation instructions for Elixir Phoenix + Ecto framework.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#send-traces-to-signoz-cloud
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-elixir
<p>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud</p>
<p><strong>Step 1. Add dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry by adding them to <code>mix.exs</code> file</p>
<pre><code>    {:opentelemetry_exporter, &quot;~&gt; 1.6&quot;},
    {:opentelemetry_api, &quot;~&gt; 1.2&quot;},
    {:opentelemetry, &quot;~&gt; 1.3&quot;},
    {:opentelemetry_semantic_conventions, &quot;~&gt; 0.2&quot;},
    {:opentelemetry_cowboy, &quot;~&gt; 0.2.1&quot;},
    {:opentelemetry_phoenix, &quot;~&gt; 1.1&quot;},
    {:opentelemetry_ecto, &quot;~&gt; 1.1&quot;}
</code></pre>
<p>In your application start, usually the <code>application.ex</code> file, setup the telemetry handlers</p>
<pre><code>    :opentelemetry_cowboy.setup()
    OpentelemetryPhoenix.setup(adapter: :cowboy2)
    OpentelemetryEcto.setup([:YOUR_APP_NAME, :repo])
</code></pre>
<p><code>YOUR_APP_NAME</code> - Name of your application or service.</p>
<p>As an example, this is how you can setup the handlers in your <code>application.ex</code> file for an application called <code>demo</code> :</p>
<pre><code># application.ex
@impl true
def start(_type, _args) do
  :opentelemetry_cowboy.setup()
  OpentelemetryPhoenix.setup(adapter: :cowboy2)
  OpentelemetryEcto.setup([:demo, :repo])

end
</code></pre>
<p><strong>Step 2. Configure Application</strong></p>
<p>You need to configure your application to send telemtry data by adding the follwing config to your <code>runtime.exs</code> file:</p>
<pre><code>config :opentelemetry, :resource, service: %{name: &quot;YOUR_APP_NAME&quot;}

config :opentelemetry, :processors,
  otel_batch_processor: %{
    exporter: {
      :opentelemetry_exporter,
      %{
        endpoints: [&quot;https://ingest.{region}.signoz.cloud:443&quot;],
        headers: [\
          {&quot;signoz-access-token&quot;, SIGNOZ_ACCESS_TOKEN}\
        ]
      }
    }
  }
</code></pre>
<p><code>YOUR_APP_NAME</code>: Your application or service name.</p>
<p><code>SIGNOZ_INGESTION_KEY</code> : The ingestion key sent by SigNoz over email. It can also be found in the <code>settings</code> section of your SigNoz Cloud UI.</p>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#send-traces-via-otel-collector-binary
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-elixir
<p>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Elixir (Phoenix + Ecto) application.</p>
<p><strong>Step 1. Add dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry by adding them to <code>mix.exs</code> file</p>
<pre><code>    {:opentelemetry_exporter, &quot;~&gt; 1.6&quot;},
    {:opentelemetry_api, &quot;~&gt; 1.2&quot;},
    {:opentelemetry, &quot;~&gt; 1.3&quot;},
    {:opentelemetry_semantic_conventions, &quot;~&gt; 0.2&quot;},
    {:opentelemetry_cowboy, &quot;~&gt; 0.2.1&quot;},
    {:opentelemetry_phoenix, &quot;~&gt; 1.1&quot;},
    {:opentelemetry_ecto, &quot;~&gt; 1.1&quot;}
</code></pre>
<p>In your application start, usually the <code>application.ex</code> file, setup the telemetry handlers</p>
<pre><code>    :opentelemetry_cowboy.setup()
    OpentelemetryPhoenix.setup(adapter: :cowboy2)
    OpentelemetryEcto.setup([:YOUR_APP_NAME, :repo])
</code></pre>
<p>As an example, this is how you can setup the handlers in your <code>application.ex</code> file for an application called <code>demo</code> :</p>
<pre><code># application.ex
@impl true
def start(_type, _args) do
  :opentelemetry_cowboy.setup()
  OpentelemetryPhoenix.setup(adapter: :cowboy2)
  OpentelemetryEcto.setup([:demo, :repo])

end
</code></pre>
<p><strong>Step 2. Configure Application</strong></p>
<p>You need to configure your application to send telemtry data by adding the follwing config to your <code>runtime.exs</code> file:</p>
<pre><code>config :opentelemetry, :resource, service: %{name: &quot;YOUR_APP_NAME&quot;}

config :opentelemetry, :processors,
    otel_batch_processor: %{
      exporter: 
      {:opentelemetry_exporter, 
      %{endpoints: [&quot;http://localhost:4318&quot;]}
      }
  }
</code></pre>
<p><code>YOUR_APP_NAME</code>: Your application or service name.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<p>We‚Äôll focus on instrumenting one of the most common combos of the Elixir world: <code>Phoenix + Ecto</code>.</p>
<h3>## Step 1: Add dependencies</h3>
<p>The first step to instrument your Elixir application with OpenTelemetry is to add the required dependencies to your <code>mix.exs</code> file and fetch them with <code>mix deps.get</code></p>
<pre><code>{:opentelemetry, &quot;~&gt; 1.0.3&quot;},
{:opentelemetry_exporter, &quot;~&gt; 1.0.3&quot;},
{:opentelemetry_phoenix, &quot;~&gt; 1.0.0&quot;},
{:opentelemetry_ecto, &quot;~&gt; 1.0.0&quot;}
</code></pre>
<h3>## Step 2: Configure Elixir application</h3>
<p>Then we need to configure our application to export telemetry data. There are two things that you need to set:</p>
<ul>
<li>
<p><code>YOUR_APP_NAME</code><br />
You can put your application or service name here for identification.</p>
</li>
<li>
<p><code>OTEL Collector endpoint</code><br />
The OTEL collector comes bundled with SigNoz installation. Since, we installed SigNoz on our local machine, the endpoint is <code>http://localhost:4318</code>.</p>
<p>config :opentelemetry, :resource, service: %{name: &quot;YOUR_APP_NAME&quot;}</p>
<p>config :opentelemetry, :processors,
otel_batch_processor: %{
exporter: {
:opentelemetry_exporter,
%{endpoints: [&quot;http://localhost:4318&quot;]}
}
}</p>
</li>
</ul>
<h3>## Step 3: Initialize telemetry handlers</h3>
<p>As it is documented in the <code>opentelemetry_phoenix</code> and <code>opentelemetry_ecto</code> <a href="http://hexdocs.pm">hexdocs.pm</a> pages, we need to initialize both telemetry handlers.</p>
<pre><code>OpentelemetryPhoenix.setup()
OpentelemetryEcto.setup([:your_app_name, :repo])
</code></pre>
<p><code>:your_app_name</code> should be replaced by your app name and congratulations, you have instrumented your application with OpenTelemetry.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#sample-examples
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-sample-examples
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Sample Examples</h2>
<p>Here's a <a href="https://signoz.io/blog/opentelemetry-elixir/">tutorial</a> with step by step guide on how to install SigNoz and start monitoring a sample Elixir app.</p>
<p>Thanks to our community member <a href="https://github.com/ricardoccpaiva">Ricardo</a> for creating this guide.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation</h2>
<p>This doc contains instructions on how to set up OpenTelemetry(OTel) instrumentation in your Rust application. OpenTelemetry, also known as OTel for short, is an open-source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Rust application.</p>
<p>Once the telemetry data is generated, you can configure an exporter to send the data to SigNoz for monitoring and visualization.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Rust application with OpenTelemetry</li>
<li>Configuring the exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p>In this tutorial, we will instrument a Rust application for traces and send it to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#requirements
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-requirements
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Requirements</h2>
<p><a href="https://www.rust-lang.org/tools/install">Rust</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#send-traces-to-signoz-cloud
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-rust
<p>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud: Send traces directly to SigNoz cloud: Send traces directly to SigNoz cloud</p>
<p>Here we will be sending traces to SigNoz cloud in 4 easy steps, if you want to send traces to self hosted SigNoz , you can refer to <a href="https://signoz.io/docs/instrumentation/rust/#send-traces-to-self-hosted-signoz">this</a>.</p>
<p>‚úÖ Info</p>
<p>If you are using the <a href="https://github.com/SigNoz/sample-rust-app">sample Rust application</a>
, then you just need to update .env file and you are good to go!</p>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure our Rust application to send data we need to initialize OpenTelemetry, Otel has already created some crates which you need to add into your <code>Cargo.toml</code> file, just below <code>[dependencies]</code> section.</p>
<pre><code>opentelemetry = { version = &quot;0.18.0&quot;, features = [&quot;rt-tokio&quot;, &quot;metrics&quot;, &quot;trace&quot;] }
opentelemetry-otlp = { version = &quot;0.11.0&quot;, features = [&quot;trace&quot;, &quot;metrics&quot;] }
opentelemetry-semantic-conventions = { version = &quot;0.10.0&quot; }
opentelemetry-proto = { version = &quot;0.1.0&quot;}
tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
tonic = { version = &quot;0.8.2&quot;, features = [&quot;tls-roots&quot;] }
</code></pre>
<p>after adding these in <code>Cargo.toml</code> , you need to use these in entry point of your Rust application , which is <code>main.rs</code> file in majority of applications.</p>
<pre><code>use opentelemetry::global::shutdown_tracer_provider;
use opentelemetry::sdk::Resource;
use opentelemetry::trace::TraceError;
use opentelemetry::{
    global, sdk::trace as sdktrace,
    trace::{TraceContextExt, Tracer},
    Context, Key, KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tonic::metadata::{MetadataMap, MetadataValue};
</code></pre>
<p><strong>Step 2: Initialize the tracer and create env file</strong></p>
<p>Add this function in main.rs file, <code>init_tracer</code> is initializing an OpenTelemetry tracer with the OpenTelemetry OTLP exporter which is sending data to SigNoz Cloud.</p>
<pre><code>fn init_tracer() -&gt; Result&lt;sdktrace::Tracer, TraceError&gt; {
    let signoz_access_token = std::env::var(&quot;SIGNOZ_ACCESS_TOKEN&quot;).expect(&quot;SIGNOZ_ACCESS_TOKEN not set&quot;);
    let mut metadata = MetadataMap::new();
    metadata.insert(
        &quot;signoz-access-token&quot;,
        MetadataValue::from_str(&amp;signoz_access_token).unwrap(),
    );
    opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_metadata(metadata)
                .with_endpoint(std::env::var(&quot;SIGNOZ_ENDPOINT&quot;).expect(&quot;SIGNOZ_ENDPOINT not set&quot;)),
        )
        .with_trace_config(
            sdktrace::config().with_resource(Resource::new(vec![\
                KeyValue::new(\
                    opentelemetry_semantic_conventions::resource::SERVICE_NAME,\
                    std::env::var(&quot;APP_NAME&quot;).expect(&quot;APP_NAME not set&quot;),\
                ),\
            ])),
        )
        .install_batch(opentelemetry::runtime::Tokio)
}
</code></pre>
<p>After adding this function, you need to create a <code>.env</code> file in root of project , the structure should look like this.</p>
<pre><code>project_root/
|-- Cargo.toml
|-- src/
|   |-- main.rs
|-- .env
</code></pre>
<p>Paste these in <code>.env</code> file</p>
<pre><code>PORT=3000
APP_NAME=rust-sample
SIGNOZ_ENDPOINT=https://ingest.[region].signoz.cloud:443/v1/traces
SIGNOZ_ACCESS_TOKEN=XXXXXXXXXX
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PORT (Optional)</td>
<td>If it is a web app pass port or else you can ignore this variable</td>
</tr>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p><strong>Step 3: Add the OpenTelemetry instrumentation for your Rust app</strong></p>
<p>Open your Cargo.toml file and paste these below <code>[dependencies]</code></p>
<pre><code>dotenv = &quot;0.15.0&quot;
</code></pre>
<p>Import these at top, so you can use variables from <code>.env</code> file</p>
<pre><code>use dotenv::dotenv;
</code></pre>
<p>After importing , just call these functions inside <code>main()</code> function by pasting this at starting of <code>main()</code> function</p>
<pre><code>dotenv().ok();
let _ = init_tracer();
</code></pre>
<p>also change</p>
<pre><code>fn main(){
    //rest of the code
}
</code></pre>
<p>to</p>
<pre><code>#[tokio::main]
async fn main() {
    //rest of the code
}
</code></pre>
<p>Now comes the most interesting part, Sending data to SigNoz to get sense of your traces. After adding the below block you can send data to SigNoz cloud</p>
<pre><code>  let tracer = global::tracer(&quot;global_tracer&quot;);
    let _cx = Context::new();
  
    tracer.in_span(&quot;operation&quot;, |cx| {
        let span = cx.span();
        span.set_attribute(Key::new(&quot;KEY&quot;).string(&quot;value&quot;));

        span.add_event(
            format!(&quot;Operations&quot;),
            vec![\
                Key::new(&quot;SigNoz is&quot;).string(&quot;Awesome&quot;),\
            ],
        );
    });
    shutdown_tracer_provider()
</code></pre>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<p>Go to your <code>.env</code> file and update the value of required variables i.e <code>APP_NAME</code>, <code>SIGNOZ_ENDPOINT</code> and <code>SIGNOZ_ACCESS_TOKEN</code></p>
<p>Now run <code>cargo run</code> in terminal to start the application!</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#send-traces-via-otel-collector-binary
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-rust
<p>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud: Send traces via OTel Collector binary: Send traces via OTel Collector binary: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Rust application.</p>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure our Rust application to send traces we need to initialize OpenTelemetry, Otel has already created some crates which you need to add into your <code>Cargo.toml</code> file, just below <code>[dependencies]</code> section.</p>
<pre><code>opentelemetry = { version = &quot;0.18.0&quot;, features = [&quot;rt-tokio&quot;, &quot;metrics&quot;, &quot;trace&quot;] }
opentelemetry-otlp = { version = &quot;0.11.0&quot;, features = [&quot;trace&quot;, &quot;metrics&quot;] }
opentelemetry-semantic-conventions = { version = &quot;0.10.0&quot; }
opentelemetry-proto = { version = &quot;0.1.0&quot;}
tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
tonic = { version = &quot;0.8.2&quot;, features = [&quot;tls-roots&quot;] }
</code></pre>
<p>after adding these in <code>Cargo.toml</code> , you need to use these in entry point of your Rust application , which is <code>main.rs</code> file in majority of applications.</p>
<pre><code>use opentelemetry::global::shutdown_tracer_provider;
use opentelemetry::sdk::Resource;
use opentelemetry::trace::TraceError;
use opentelemetry::{
    global, sdk::trace as sdktrace,
    trace::{TraceContextExt, Tracer},
    Context, Key, KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tonic::metadata::{MetadataMap, MetadataValue};
</code></pre>
<p><strong>Step 2: Initialize the tracer and create env file</strong></p>
<p>Add this function in main.rs file, <code>init_tracer</code> is initializing an OpenTelemetry tracer with the OpenTelemetry OTLP exporter which is sending data to SigNoz Cloud.</p>
<p>This tracer initializes the connection with the OTel collector from the system variables passed while starting the app.</p>
<pre><code>fn init_tracer() -&gt; Result&lt;sdktrace::Tracer, TraceError&gt; {
    opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(opentelemetry_otlp::new_exporter().tonic().with_env())
        .with_trace_config(
            sdktrace::config().with_resource(Resource::default()),
        )
        .install_batch(opentelemetry::runtime::Tokio)
}
</code></pre>
<p><strong>Step 3: Add the OpenTelemetry instrumentation for your Rust app</strong></p>
<p>You need call init_tracer function inside <code>main()</code> at starting so that as soon as your rust application starts, tracer will be available globally.</p>
<pre><code>let _ = init_tracer();
</code></pre>
<p>also change</p>
<pre><code>fn main(){
    //rest of the code
}
</code></pre>
<p>to</p>
<pre><code>#[tokio::main]
async fn main() {
    //rest of the code
}
</code></pre>
<p>Now comes the most interesting part, Sending data to SigNoz to get sense of your traces. After adding the below block you can send traces to SigNoz cloud</p>
<pre><code>  let tracer = global::tracer(&quot;global_tracer&quot;);
    let _cx = Context::new();
  
    tracer.in_span(&quot;operation&quot;, |cx| {
        let span = cx.span();
        span.set_attribute(Key::new(&quot;KEY&quot;).string(&quot;value&quot;));

        span.add_event(
            format!(&quot;Operations&quot;),
            vec![\
                Key::new(&quot;SigNoz is&quot;).string(&quot;Awesome&quot;),\
            ],
        );
    });
    shutdown_tracer_provider()
</code></pre>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td>This is the endpoint of your OTel Collector. If you have hosted it somewhere, provide the URL. Otherwise, the default is <code>http://localhost:4317</code> if you have followed our guide.</td>
</tr>
<tr>
<td>OTEL_RESOURCE_ATTRIBUTES=service.name</td>
<td>Specify as the value. This will be the name of your Rust application on SigNoz services page, allowing you to uniquely identify the application traces.</td>
</tr>
</tbody>
</table>
<p>Now run</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 OTEL_RESOURCE_ATTRIBUTES=service.name=sample-rust-app cargo run
</code></pre>
<p>in terminal to start the application!</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure our Rust application to send traces we need to initialize OpenTelemetry, Otel has already created some crates which you need to add into your <code>Cargo.toml</code> file, just below <code>[dependencies]</code> section.</p>
<pre><code>opentelemetry = { version = &quot;0.18.0&quot;, features = [&quot;rt-tokio&quot;, &quot;metrics&quot;, &quot;trace&quot;] }
opentelemetry-otlp = { version = &quot;0.11.0&quot;, features = [&quot;trace&quot;, &quot;metrics&quot;] }
opentelemetry-semantic-conventions = { version = &quot;0.10.0&quot; }
opentelemetry-proto = { version = &quot;0.1.0&quot;}
tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
tonic = { version = &quot;0.8.2&quot;, features = [&quot;tls-roots&quot;] }
</code></pre>
<p>after adding these in <code>Cargo.toml</code> , you need to use these in entry point of your Rust application , which is <code>main.rs</code> file in majority of applications.</p>
<pre><code>use opentelemetry::global::shutdown_tracer_provider;
use opentelemetry::sdk::Resource;
use opentelemetry::trace::TraceError;
use opentelemetry::{
    global, sdk::trace as sdktrace,
    trace::{TraceContextExt, Tracer},
    Context, Key, KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tonic::metadata::{MetadataMap, MetadataValue};
</code></pre>
<p><strong>Step 2: Initialize the tracer and create env file</strong></p>
<p>Add this function in main.rs file, <code>init_tracer</code> is initializing an OpenTelemetry tracer with the OpenTelemetry OTLP exporter which is sending data to your Self-Hosted SigNoz.</p>
<pre><code>fn init_tracer() -&gt; Result&lt;sdktrace::Tracer, TraceError&gt; {
    opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(opentelemetry_otlp::new_exporter().tonic().with_env())
        .with_trace_config(
            sdktrace::config().with_resource(Resource::default()),
        )
        .install_batch(opentelemetry::runtime::Tokio)
}
</code></pre>
<p><strong>Step 3: Add the OpenTelemetry instrumentation for your Rust app</strong></p>
<p>You need call init_tracer function inside <code>main()</code> at starting so that as soon as your rust application starts, tracer will be available globally.</p>
<pre><code>let _ = init_tracer();
</code></pre>
<p>also change</p>
<pre><code>fn main(){
    //rest of the code
}
</code></pre>
<p>to</p>
<pre><code>#[tokio::main]
async fn main() {
    //rest of the code
}
</code></pre>
<p>Now comes the most interesting part, Sending data to SigNoz to get sense of your traces. After adding the below block you can send traces to Self-Hosted SigNoz</p>
<pre><code>  let tracer = global::tracer(&quot;global_tracer&quot;);
    let _cx = Context::new();
  
    tracer.in_span(&quot;operation&quot;, |cx| {
        let span = cx.span();
        span.set_attribute(Key::new(&quot;KEY&quot;).string(&quot;value&quot;));

        span.add_event(
            format!(&quot;Operations&quot;),
            vec![\
                Key::new(&quot;SigNoz is&quot;).string(&quot;Awesome&quot;),\
            ],
        );
    });
    shutdown_tracer_provider()
</code></pre>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td>This is the url where you have hosted SigNoz</td>
</tr>
<tr>
<td>OTEL_RESOURCE_ATTRIBUTES=service.name</td>
<td>Specify as the value. This will be the name of your Rust application on SigNoz services page, allowing you to uniquely identify the application traces.</td>
</tr>
</tbody>
</table>
<p>Now run</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 OTEL_RESOURCE_ATTRIBUTES=service.name=sample-rust-app cargo run
</code></pre>
<p>in terminal to start the application!</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#sample-rust-application
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-sample-rust-application
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Sample Rust Application</h2>
<p>We have included a sample Rust application at <a href="https://github.com/SigNoz/sample-rust-app">Sample Rust App Github Repo</a>
,</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#tutorial
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-tutorial
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Tutorial</h2>
<p>Here's a <a href="https://signoz.io/blog/opentelemetry-rust/">tutorial</a> with step by step guide on how to install SigNoz and start monitoring a sample Rust app.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation</h2>
<p>This documentation contains instructions on how to set up OpenTelemetry(OTel) instrumentation in your Swift application. OpenTelemetry, also known as OTel for short, is an open-source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Swift application.</p>
<p>Once the telemetry data is generated, you can configure an exporter to send the data to SigNoz for monitoring and visualization.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Swift application with OpenTelemetry</li>
<li>Configuring the exporter to send data to SigNoz</li>
<li>Validating the configuration to ensure that data is being sent as expected.</li>
</ul>
<p>In this tutorial, we will instrument a Swift application for traces and send it to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#requirements
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-requirements
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Requirements</h2>
<p><a href="https://www.swift.org/getting-started/#installing-swift">Swift</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#send-traces-to-signoz-cloud
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetes</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-swift
<p>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud: Send traces directly to SigNoz cloud: Send traces directly to SigNoz cloud</p>
<p>Here we will be sending traces to SigNoz cloud in 4 easy steps, if you want to send traces to self hosted SigNoz , you can refer to <a href="https://signoz.io/docs/instrumentation/swift/#send-traces-to-self-hosted-signoz">this</a>.</p>
<p>‚úÖ Info</p>
<p>If you are using the <a href="https://github.com/SigNoz/OpenTelemetry-swift-example/">sample swift application</a>
, then you just need to update the ingestion key and SigNoz endpoint after that you are good to go!</p>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure your Swift application to send traces to OpenTelemetry you need to install <a href="https://github.com/open-telemetry/opentelemetry-swift.git">opentelemetry-swift</a> and <a href="https://github.com/grpc/grpc-swift">grpc-swift</a> as dependency in your project.</p>
<p>For that, paste the following inside <code>Package.swift</code> of your project.</p>
<pre><code> dependencies: [\
        .package(url: &quot;https://github.com/open-telemetry/opentelemetry-swift.git&quot;, .upToNextMajor(from: &quot;1.9.1&quot;)),\
        .package(url: &quot;https://github.com/grpc/grpc-swift&quot;, from: &quot;1.15.0&quot;),\
    ],
    targets: [\
        .executableTarget(\
            name: &quot;&lt;NAME_OF_APP&gt;&quot;,\
            dependencies: [\
                .product(name: &quot;OpenTelemetryApi&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetrySdk&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;StdoutExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ResourceExtension&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ZipkinExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetryProtocolExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;SignPostIntegration&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;GRPC&quot;, package: &quot;grpc-swift&quot;)\
            ],\
            path: &quot;.&quot;\
        )\
    ]
</code></pre>
<p>Replace <code>NAME_OF_APP</code> with the name of app, you will see this name in SigNoz Services section.</p>
<p>You also need to add the following imports to use the methods and functions from the packages/dependencies which you just imported in <code>Package.swift</code> file.</p>
<pre><code>import Foundation
import GRPC
import NIO
import NIOSSL
import OpenTelemetryApi
import OpenTelemetryProtocolExporterCommon
import OpenTelemetryProtocolExporterGrpc
import OpenTelemetrySdk
import ResourceExtension
import SignPostIntegration
import StdoutExporter
import ZipkinExporter
</code></pre>
<p><strong>Step 2: Initialize the tracer</strong></p>
<p>To enable tracing and send traces to the SigNoz cloud, you need to initialize the tracer, to do that insert the following code snippet into your <code>main.swift</code> file</p>
<pre><code>var resources = DefaultResources().get()

let instrumentationScopeName = &quot;SwiftExample&quot;
let instrumentationScopeVersion = &quot;semver:0.1.0&quot;

let otlpConfiguration: OtlpConfiguration = OtlpConfiguration(timeout: TimeInterval(10), headers: [(&quot;signoz-access-token&quot;, &lt;SIGNOZ_INGESTION_KEY&gt;)])

let grpcChannel = ClientConnection.usingPlatformAppropriateTLS(for: MultiThreadedEventLoopGroup(numberOfThreads:1)).connect(host: &lt;SIGNOZ_INGESTION_URL&gt;, port: &lt;PORT&gt;)

let otlpTraceExporter = OtlpTraceExporter(channel: grpcChannel,
                                      config: otlpConfiguration)
let stdoutExporter = StdoutExporter()

let spanExporter = MultiSpanExporter(spanExporters: [otlpTraceExporter, stdoutExporter])

let spanProcessor = SimpleSpanProcessor(spanExporter: spanExporter)
OpenTelemetry.registerTracerProvider(tracerProvider:
    TracerProviderBuilder()
        .add(spanProcessor: spanProcessor)
        .build()
)
</code></pre>
<p>Ensure to replace <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code>, <code>&lt;PORT&gt;</code>, and <code>&lt;SIGNOZ_INGESTION_URL&gt;</code> with your actual SigNoz ingestion key, Port, and URL, respectively.</p>
<p>You can find your Ingestion Key in the following places:</p>
<ul>
<li>mail which you recieved from SigNoz after signing up.</li>
<li>inside settings on SigNoz dashboard</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443/v1/traces</td>
</tr>
</tbody>
</table>
<p>These are the variables you need to replace :</p>
<table>
<thead>
<tr>
<th>Placeholder</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&lt;SIGNOZ_INGESTION_KEY&gt;</code></td>
<td>Your SigNoz ingestion key</td>
</tr>
<tr>
<td><code>&lt;PORT&gt;</code></td>
<td>Port (default: 443)</td>
</tr>
<tr>
<td><code>&lt;SIGNOZ_INGESTION_URL&gt;</code></td>
<td>URL for SigNoz ingestion</td>
</tr>
</tbody>
</table>
<p><strong>Step 3: Send Telemetry data to SigNoz</strong></p>
<p>To send telemetry data to SigNoz, you can create a function to add spans and data. This is a sample function</p>
<pre><code>let sampleKey = &quot;sampleKey&quot;
let sampleValue = &quot;sampleValue&quot;

func doWork() {
    let childSpan = tracer.spanBuilder(spanName: &quot;doWork&quot;).setSpanKind(spanKind: .client).startSpan()
    childSpan.setAttribute(key: sampleKey, value: sampleValue)
    Thread.sleep(forTimeInterval: Double.random(in: 0 ..&lt; 10) / 100)
    childSpan.end()
}
</code></pre>
<p><strong>Step 4: Run app</strong></p>
<p>Execute your application by issuing the following command in terminal:</p>
<pre><code>swift run
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#send-traces-via-otel-collector-binary
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-swift
<p>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud: Send traces via OTel Collector binary</p>
<p><strong>Step 1 : Install OTel Collector</strong> OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Swift application.</p>
<p><strong>Step 2 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure your Swift application to send traces to OpenTelemetry you need to install <a href="https://github.com/open-telemetry/opentelemetry-swift.git">opentelemetry-swift</a> and <a href="https://github.com/grpc/grpc-swift">grpc-swift</a> as dependency in your project.</p>
<p>For that, paste the following inside <code>Package.swift</code> of your project.</p>
<pre><code> dependencies: [\
        .package(url: &quot;https://github.com/open-telemetry/opentelemetry-swift.git&quot;, .upToNextMajor(from: &quot;1.9.1&quot;)),\
        .package(url: &quot;https://github.com/grpc/grpc-swift&quot;, from: &quot;1.15.0&quot;),\
    ],
    targets: [\
        .executableTarget(\
            name: &quot;&lt;NAME_OF_APP&gt;&quot;,\
            dependencies: [\
                .product(name: &quot;OpenTelemetryApi&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetrySdk&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;StdoutExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ResourceExtension&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ZipkinExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetryProtocolExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;SignPostIntegration&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;GRPC&quot;, package: &quot;grpc-swift&quot;)\
            ],\
            path: &quot;.&quot;\
        )\
    ]
</code></pre>
<p>Replace <code>NAME_OF_APP</code> with the name of app, you will see this name in SigNoz Services section.</p>
<p>You also need to add the following imports to use the methods and functions from the packages/dependencies which you just imported in <code>Package.swift</code> file.</p>
<pre><code>import Foundation
import GRPC
import NIO
import NIOSSL
import OpenTelemetryApi
import OpenTelemetryProtocolExporterCommon
import OpenTelemetryProtocolExporterGrpc
import OpenTelemetrySdk
import ResourceExtension
import SignPostIntegration
import StdoutExporter
import ZipkinExporter
</code></pre>
<p><strong>Step 3: Initialize the tracer</strong></p>
<p>Add these statements to initialize tracer in the <code>main.swift</code> file inside the main function or you can create another function for initializing the tracer and call it in some other block of code.</p>
<pre><code>var resources = DefaultResources().get()

let instrumentationScopeName = &quot;SwiftExample&quot;
let instrumentationScopeVersion = &quot;semver:0.1.0&quot;

let otlpConfiguration: OtlpConfiguration = OtlpConfiguration(timeout: TimeInterval(10), headers: [(&quot;signoz-access-token&quot;, &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;)])

let grpcChannel = ClientConnection.usingPlatformAppropriateTLS(for: MultiThreadedEventLoopGroup(numberOfThreads:1)).connect(host: &quot;http://localhost&quot;, port: 4317)

let otlpTraceExporter = OtlpTraceExporter(channel: grpcChannel,
                                      config: otlpConfiguration)
let stdoutExporter = StdoutExporter()
let spanExporter = MultiSpanExporter(spanExporters: [otlpTraceExporter, stdoutExporter])

let spanProcessor = SimpleSpanProcessor(spanExporter: spanExporter)
OpenTelemetry.registerTracerProvider(tracerProvider:
    TracerProviderBuilder()
        .add(spanProcessor: spanProcessor)
        .build()
)

let tracer = OpenTelemetry.instance.tracerProvider.get(instrumentationName: instrumentationScopeName, instrumentationVersion: instrumentationScopeVersion)
</code></pre>
<p>Ensure to replace <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> with your actual SigNoz ingestion key.</p>
<p><strong>Step 4: Send Telemetry data to SigNoz</strong></p>
<p>To send telemetry data to SigNoz, you can create a function to add spans and data. This is a sample function</p>
<pre><code>let sampleKey = &quot;sampleKey&quot;
let sampleValue = &quot;sampleValue&quot;

func doWork() {
    let childSpan = tracer.spanBuilder(spanName: &quot;doWork&quot;).setSpanKind(spanKind: .client).startSpan()
    childSpan.setAttribute(key: sampleKey, value: sampleValue)
    Thread.sleep(forTimeInterval: Double.random(in: 0 ..&lt; 10) / 100)
    childSpan.end()
}
</code></pre>
<p><strong>Step 5: Run app</strong></p>
<p>Run your app using the below command to send your traces:</p>
<pre><code>swift run
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure your Swift application to send traces to OpenTelemetry you need to install <a href="https://github.com/open-telemetry/opentelemetry-swift.git">opentelemetry-swift</a> and <a href="https://github.com/grpc/grpc-swift">grpc-swift</a> as dependency in your project.</p>
<p>For that, paste the following inside <code>Package.swift</code> of your project.</p>
<pre><code> dependencies: [\
        .package(url: &quot;https://github.com/open-telemetry/opentelemetry-swift.git&quot;, .upToNextMajor(from: &quot;1.9.1&quot;)),\
        .package(url: &quot;https://github.com/grpc/grpc-swift&quot;, from: &quot;1.15.0&quot;),\
    ],
    targets: [\
        .executableTarget(\
            name: &quot;&lt;NAME_OF_APP&gt;&quot;,\
            dependencies: [\
                .product(name: &quot;OpenTelemetryApi&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetrySdk&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;StdoutExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ResourceExtension&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ZipkinExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetryProtocolExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;SignPostIntegration&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;GRPC&quot;, package: &quot;grpc-swift&quot;)\
            ],\
            path: &quot;.&quot;\
        )\
    ]
</code></pre>
<p>Replace <code>NAME_OF_APP</code> with the name of app, you will see this name in SigNoz Services section.</p>
<p>You also need to add the following imports to use the methods and functions from the packages/dependencies which you just imported in <code>Package.swift</code> file.</p>
<pre><code>import Foundation
import GRPC
import NIO
import NIOSSL
import OpenTelemetryApi
import OpenTelemetryProtocolExporterCommon
import OpenTelemetryProtocolExporterGrpc
import OpenTelemetrySdk
import ResourceExtension
import SignPostIntegration
import StdoutExporter
import ZipkinExporter
</code></pre>
<p><strong>Step 2: Initialize the tracer</strong></p>
<p>To enable tracing and send traces to the SigNoz cloud, you need to initialize the tracer, to do that insert the following code snippet into your <code>main.swift</code> file</p>
<pre><code>var resources = DefaultResources().get()

let instrumentationScopeName = &quot;SwiftExample&quot;
let instrumentationScopeVersion = &quot;semver:0.1.0&quot;

let otlpConfiguration: OtlpConfiguration = OtlpConfiguration(timeout: TimeInterval(10))

let grpcChannel = ClientConnection.usingPlatformAppropriateTLS(for: MultiThreadedEventLoopGroup(numberOfThreads:1)).connect(host: &lt;SELF_HOSTED_SIGNOZ_URL&gt;, port: &lt;PORT&gt;)

let otlpTraceExporter = OtlpTraceExporter(channel: grpcChannel,
                                      config: otlpConfiguration)
let stdoutExporter = StdoutExporter()

let spanExporter = MultiSpanExporter(spanExporters: [otlpTraceExporter, stdoutExporter])

let spanProcessor = SimpleSpanProcessor(spanExporter: spanExporter)
OpenTelemetry.registerTracerProvider(tracerProvider:
    TracerProviderBuilder()
        .add(spanProcessor: spanProcessor)
        .build()
)
</code></pre>
<p>| Placeholder | Description | |----------------------------|---------------------------------------------| | | <code>&lt;PORT&gt;</code> | Port exposed by Self-Hosted | | <code>&lt;SELF_HOSTED_SIGNOZ_URL&gt;</code> | URL for Self-Hosted SigNoz |</p>
<p><strong>Step 3: Send Telemetry data to SigNoz</strong></p>
<p>To send telemetry data to SigNoz, you can create a function to add spans and data. This is a sample function</p>
<pre><code>let sampleKey = &quot;sampleKey&quot;
let sampleValue = &quot;sampleValue&quot;

func doWork() {
    let childSpan = tracer.spanBuilder(spanName: &quot;doWork&quot;).setSpanKind(spanKind: .client).startSpan()
    childSpan.setAttribute(key: sampleKey, value: sampleValue)
    Thread.sleep(forTimeInterval: Double.random(in: 0 ..&lt; 10) / 100)
    childSpan.end()
}
</code></pre>
<p><strong>Step 4: Run app</strong></p>
<p>Execute your application by issuing the following command in terminal:</p>
<pre><code>swift run
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#sample-swift-application
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-sample-swift-application
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Sample Swift Application</h2>
<p>We have included a sample Swift application at <a href="https://github.com/SigNoz/OpenTelemetry-swift-example/">Sample swift App Github Repo</a>
,</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/invite-team-member/
tag_set: product-features, invite-team-member
image_urls: https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step1.webp, https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step2.webp, https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step3.webp, https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step4.webp, https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step5.webp
tracking_id: docs-product-features-invite-team-member
group_tracking_ids: docs-product-features-invite-team-member
<h2>Invite Team Member</h2>
<h2>Introduction</h2>
<hr />
<p>SigNoz users can invite <strong>unlimited</strong> team members with different access levels like Admin, Editor and Viewer.</p>
<p>Follow the below steps to invite a Team Member to SigNoz:</p>
<h2>Step 1: Access the Team Management Area</h2>
<hr />
<ul>
<li>Click on the <strong>Invite Team Member</strong> button at the bottom of the left-hand menu. This will open up the Organization settings tab.</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step1.webp" alt="Invite Team Member Tab in SigNoz" /></p>
<p><em>Invite Team Member Tab in SigNoz</em></p>
<h2>Step 2: Invite Team Members</h2>
<hr />
<ul>
<li>Click the <strong>+ Invite Members</strong> button located at the top right corner of the section. This will open a popup window.</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step2.webp" alt="Invite Team member to SigNoz" /></p>
<p><em>Invite Team member to SigNoz</em></p>
<h2>Step 3: Enter Team Member Details</h2>
<hr />
<ul>
<li>In the popup window, enter the email address of the person you wish to invite.</li>
<li>Optionally, you can add a name for the team member.</li>
<li>Select the appropriate role for the member from the dropdown: ADMIN, EDITOR, or VIEWER.</li>
<li>To invite multiple team members at once, click the <strong>+ Add another team member</strong> button and repeat the process for each additional team member.</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step3.webp" alt="Enter Team Member Details" /></p>
<p><em>Enter Team Member Details and invite multiple members at once</em></p>
<h3>## Roles and Access Levels</h3>
<ul>
<li><strong>ADMIN</strong>: Has full access to Invite new users, editing settings and viewing data.</li>
<li><strong>EDITOR</strong>: Can view and edit data. For example, edit dashboard panels.</li>
<li><strong>VIEWER</strong>: Has read-only access to view data.</li>
</ul>
<h2>Step 4: Send the Invites</h2>
<hr />
<ul>
<li>Once all details are correctly entered for each team member, click the <strong>Invite Members</strong> button at the bottom of the popup.</li>
<li>This will generate individual invite links and send invitation emails to each member‚Äôs email address.</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step4.webp" alt="Send the invites" /></p>
<p><em>Send Invites</em></p>
<p>‚úÖ Info</p>
<p>For SigNoz Self-Host users, you need to copy and send the invite link. The user won't receive an email invite.</p>
<h2>Step 5: Manage Invites</h2>
<hr />
<ul>
<li>After sending invites, you can view and manage all pending invites in the <strong>Pending Invites</strong> section.</li>
<li>You can revoke invites or copy the invite link using the options available beside each pending invite.</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/invite-team-member/invite-team-member-step5.webp" alt="Manage Invites" /></p>
<p><em>Revoke invite or copy invite link options</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/
tag_set: instrumentation
image_urls: 
tracking_id: docs-instrumentation
group_tracking_ids: docs-instrumentation
<h2>Instrument your Application</h2>
<p>To instrument your applications and send data to SigNoz, follow the instructions in the sections below.</p>
<ul>
<li><a href="/docs/instrumentation/python">üìÑÔ∏è Python: Send events from you Python application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/java">üìÑÔ∏è Java: Send events from you Java application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/javascript">üìÑÔ∏è Javascript: Send events from you Javascript application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/golang">üìÑÔ∏è Golang (Go): Send events from you Golang (Go) application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/php">üìÑÔ∏è PHP: Send events from you PHP application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/dotnet">üìÑÔ∏è .NET: Send events from you .NET application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/ruby-on-rails">üìÑÔ∏è Ruby on Rails: Send events from you Ruby on Rails application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/elixir">üìÑÔ∏è Elixir: Send events from you Elixir application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/rust">üìÑÔ∏è Rust: Send events from you Rust application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/swift">üìÑÔ∏è Swift: Send events from you Swift application to SigNoz.</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/production-readiness/
tag_set: production-readiness
image_urls: 
tracking_id: docs-production-readiness
group_tracking_ids: docs-production-readiness
<h2>Best Practices to follow to run SigNoz in production</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<ol>
<li>
<p>Create a separate cluster for running SigNoz. This will help in the isolation of application and APM environments and hence, reduce the impact radius of operational issues.</p>
</li>
<li>
<p>Use infra-level otel-collectors to send host metrics from VMs (should be part of default setup)</p>
<p><a href="https://signoz.io/docs/tutorial/kubernetes-infra-metrics/">K8s Infra Metrics | SigNoz</a></p>
</li>
<li>
<p>Configure TTL for disk and use move to s3 for reduced costs. Perf of s3 is 2-3x slower than EBS. Configure retention for each of metrics, traces and logs.</p>
<p><a href="https://signoz.io/docs/userguide/retention-period/">Retention Period | SigNoz</a></p>
</li>
<li>
<p>Setup alerts on important APM metrics</p>
</li>
<li>
<p>Harness the power of distributed tracing data by creating dashboards using Clickhouse queries. You can run group by and aggregates on tags(attributeMap) and events of a span. Also, filtering by more specific conditions should be possible. Let us know if you would like us to help write a few queries to plot a chart using the traces data. Same also, applies for the logs data.</p>
</li>
<li>
<p>Secure query-service and otel-collector using TLS ingress</p>
<p><a href="https://signoz.io/docs/tutorial/setting-up-tls-for-signoz/">Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager | SigNoz</a></p>
</li>
<li>
<p>Authorise client otel-collectors to send data to signoz cluster (planned)</p>
</li>
<li>
<p>Horizontally scale <code>otel-collector</code> which works on the push model and not <code>otel-collector-metrics</code> which works on the pull model of prometheus scraping. You need to add a different config to add another instance of <code>otel-collector-metrics</code> to prevent duplication</p>
</li>
<li>
<p>Use higher batch size in <code>otel-collector</code> when ingesting more than 10K events/s. The default batch size is 10K rows. Batch size upto 50K should work well.</p>
<p><a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/otel-collector-config.yaml#L60">signoz/otel-collector-config.yaml at develop ¬∑ SigNoz/signoz</a></p>
</li>
<li>
<p>Use sampling to reduce the amount of data sent to SigNoz</p>
</li>
</ol>
<pre><code>[opentelemetry-collector-contrib/processor/probabilisticsamplerprocessor at main ¬∑ open-telemetry/opentelemetry-collector-contrib](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/probabilisticsamplerprocessor)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker
group_tracking_ids: docs-install-docker
<h2>Docker Standalone</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>SigNoz can be installed on macOS or Linux computers, and there are two ways in which you can install SigNoz:</p>
<ul>
<li>You may execute a script that checks your environment, installs Docker Engine and Docker Compose on Linux, and runs the <code>docker compose up</code> command for you.</li>
<li>You may execute the <code>docker compose up</code> command yourself.</li>
</ul>
<p>Both methods are provided below.</p>
<p>‚úÖ Info</p>
<p>SigNoz recommends you to use the <a href="#install-signoz-using-the-install-script">install script</a> on macOS and the following Linux distributions:</p>
<ul>
<li>Ubuntu</li>
<li>Debian</li>
<li>OpenSuse</li>
<li>CentOS</li>
<li>SUSE Linux Enterprise Server (SLES)</li>
</ul>
<p>If you're using a different Linux distribution, see the <a href="#install-signoz-using-docker-compose">Install SigNoz Using Docker Compose</a> section.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#prerequisites
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-prerequisites
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Prerequisites</h2>
<ul>
<li>
<p>A Linux or macOS machine. Microsoft Windows is not officially supported.</p>
</li>
<li>
<p>On macOS, you must manually install <a href="https://docs.docker.com/engine/install/">Docker Engine</a> before you run the install script. The install script automatically installs Docker Engine on Linux.</p>
</li>
<li>
<p>A minimum of 4GB of memory must be allocated to Docker.</p>
</li>
<li>
<p><a href="https://desktop.github.com/">Git client</a></p>
</li>
<li>
<p>Ensure that the ports <code>3301</code>, <code>4317</code> and <code>4318</code> are open on the machine where you install SigNoz.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#install-signoz-using-the-install-script
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-install-signoz-using-the-install-script
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Install SigNoz Using the Install Script</h2>
<ol>
<li>
<p>In a directory of your choosing, clone the SigNoz repository and 'cd' into the <code>signoz/deploy</code> directory by entering the following commands:</p>
<pre><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/
</code></pre>
</li>
<li>
<p>Run the <code>install.sh</code> script:</p>
<pre><code>./install.sh
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#install-signoz-using-docker-compose
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-install-signoz-using-docker-compose
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Install SigNoz Using Docker Compose</h2>
<p>‚úÖ Info</p>
<p>Before you install Signoz, ensure that <a href="https://docs.docker.com/compose/install/">Docker Compose</a> is installed on your machine.</p>
<ol>
<li>
<p>In a directory of your choosing, clone the SigNoz repository and 'cd' into the <code>signoz/deploy</code> directory by entering the following commands:</p>
<pre><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/
</code></pre>
</li>
<li>
<p>To install SigNoz, enter the <code>docker compose up</code> command, specifying the following:</p>
<ul>
<li><code>-f</code> and the path to your configuration file</li>
<li><code>-d</code> to run containers in the background</li>
</ul>
<p>docker compose -f docker/clickhouse-setup/docker-compose.yaml up -d</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#verify-the-installation
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-verify-the-installation
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Verify the Installation</h2>
<ol>
<li>
<p>Ensure that your containers are running correctly. To view the status of your containers, run the following command:</p>
<p>docker ps</p>
</li>
</ol>
<p>The output should look similar to the following:</p>
<pre><code>CONTAINER ID   IMAGE                                          COMMAND                  CREATED          STATUS                    PORTS                                                                            NAMES
01f044c4686a   signoz/frontend:0.38.2                       &quot;nginx -g 'daemon of‚Ä¶&quot;   2 minutes ago   Up 9 seconds                  80/tcp, 0.0.0.0:3301-&gt;3301/tcp                                                     signoz-frontend
86aa5b875f9f   gliderlabs/logspout:v3.2.14                  &quot;/bin/logspout syslo‚Ä¶&quot;   2 minutes ago   Up 1 second                   80/tcp                                                                             signoz-logspout
58746f684630   signoz/alertmanager:0.23.4                   &quot;/bin/alertmanager -‚Ä¶&quot;   2 minutes ago   Up 9 seconds                  9093/tcp                                                                           signoz-alertmanager
2cf1ec96bdb3   signoz/query-service:0.38.2                  &quot;./query-service -co‚Ä¶&quot;   2 minutes ago   Up About a minute (healthy)   8080/tcp                                                                           signoz-query-service
e9f0aa66d884   signoz/signoz-otel-collector:0.88.11          &quot;/signoz-collector -‚Ä¶&quot;   2 minutes ago   Up 10 seconds                 0.0.0.0:4317-4318-&gt;4317-4318/tcp                                                   signoz-otel-collector
d3d89d7d4581   clickhouse/clickhouse-server:23.11.1-alpine   &quot;/entrypoint.sh&quot;         2 minutes ago   Up 2 minutes (healthy)        0.0.0.0:8123-&gt;8123/tcp, 0.0.0.0:9000-&gt;9000/tcp, 0.0.0.0:9181-&gt;9181/tcp, 9009/tcp   signoz-clickhouse
9db88aefb6ed   signoz/locust:1.2.3                          &quot;/docker-entrypoint.‚Ä¶&quot;   2 minutes ago   Up 2 minutes                  5557-5558/tcp, 8089/tcp                                                            load-hotrod
60bb3b77b4f7   bitnami/zookeeper:3.7.1                      &quot;/opt/bitnami/script‚Ä¶&quot;   2 minutes ago   Up 2 minutes                  0.0.0.0:2181-&gt;2181/tcp, 0.0.0.0:2888-&gt;2888/tcp, 0.0.0.0:3888-&gt;3888/tcp, 8080/tcp   signoz-zookeeper-1
98c7178b4004   jaegertracing/example-hotrod:1.30            &quot;/go/bin/hotrod-linu‚Ä¶&quot;   9 days ago      Up 2 minutes                  8080-8083/tcp                                                                      hotrod
</code></pre>
<ol start="2">
<li>Wait for all the pods to be in running state, and then point your browser to <code>http://&lt;IP-ADDRESS&gt;:3301/</code> to access the dashboard, replacing <code>&lt;IP-ADDRESS&gt;</code> with the IP address of the machine where you installed SigNoz.</li>
</ol>
<p><strong>Example</strong>:</p>
<ul>
<li>If you're running SigNoz on your local machine, you should point your browser to <code>http://localhost:3301/</code>.</li>
<li>If the IP address of the machine on which you're running SigNoz is <code>66.82.18.247</code>, you should point your browser to <code>http://66.82.18.247:3301/</code></li>
</ul>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>
<p>‚úÖ Info</p>
<p>The <code>docker-compose.yaml</code> installs a sample application named <a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a> that generates tracing data. You can explore the SigNoz dashboard with the data provided by the sample application. If you wish to remove the sample application, follow the steps in the <a href="/docs/operate/docker-standalone/#remove-the-sample-application">Remove the Sample Application</a> section.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#install-specific-version-of-signoz
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-install-specific-version-of-signoz
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Install specific version of SigNoz</h2>
<ol>
<li>
<p>In a directory of your choosing, clone the SigNoz repository and 'cd' into the <code>signoz/deploy</code> directory by entering the following commands:</p>
<pre><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/
</code></pre>
</li>
<li>
<p>Checkout to the specific version tag. For example, to install SigNoz version <code>v0.6.1</code>:</p>
<pre><code>git checkout v0.6.1
</code></pre>
</li>
<li>
<p>Run the <code>install.sh</code> script:</p>
<pre><code>./install.sh
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/docker-standalone/">Docker Standalone Operate</a> section for detailed instructions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#related-topics
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-related-topics
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Related Topics</h2>
<ul>
<li><a href="/docs/install/troubleshooting/">Troubleshoot SigNoz Installation Issues</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#next-steps
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-next-steps
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Next Steps</h2>
<ul>
<li>
<p><a href="/docs/instrumentation/overview/">Instrument Your Application</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
<li>
<p><a href="/docs/operate/docker-standalone/">Operate SigNoz on Docker Standalone</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: 
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine</h2>
<h3>## Overview</h3>
<p>This tutorial shows how you can deploy OpenTelemetry binary as an agent, which collects telemetry data. Data such as traces, metrics and logs generated by applications most likely running in the same virtual machine (VM).</p>
<p>It can also be used for collecting data from other VMs in the same cluster, data center or region, however, binary is not recommended in that scenario but container or deployment which can be easily scaled.</p>
<p>In this guide, you will also learn to set up a hostmetrics receiver to collect metrics from the VM and view in SigNoz.</p>
<p>SigNoz CloudSelf-Host</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/#setup-otel-collector-as-agent
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: https://signoz.io/img/docs/saas-docs/vm-setup-2x.webp
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine-setup-otel-collector-as-agent
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine: Setup Otel Collector as agent</h2>
<p>OpenTelemetry-instrumented applications in a VM can send data to the <code>otel-binary</code> agent running in the same VM. The OTel agent can then be configured to send data to the SigNoz cloud.</p>
<p><img src="https://signoz.io/img/docs/saas-docs/vm-setup-2x.webp" alt="Collecting data from applications deployed in VM" /></p>
<p><em>OpenTelemetry-instrumented applications in a VM can send data to otel-binary which then sends data to SigNoz cloud.</em></p>
<p>Here are the steps to set up OpenTelemetry binary as an agent.</p>
<p>Linux (amd64)Linux (arm64)MacOS (amd64)MacOS (arm64)Windows (amd64)</p>
<ol>
<li>
<p>Download otel-collector tar.gz for your architecture</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.88.0/otelcol-contrib_0.88.0_linux_amd64.tar.gz
</code></pre>
</li>
<li>
<p>Extract otel-collector tar.gz to the <code>otelcol-contrib</code> folder</p>
<pre><code>mkdir otelcol-contrib &amp;&amp; tar xvzf otelcol-contrib_0.88.0_linux_amd64.tar.gz -C otelcol-contrib
</code></pre>
</li>
<li>
<p>Create <code>config.yaml</code> in folder <code>otelcol-contrib</code> with the below content in it. Replace <code>SIGNOZ_INGESTION_KEY</code> with what is provided by SigNoz:</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  hostmetrics:
    collection_interval: 60s
    scrapers:
      cpu: {}
      disk: {}
      load: {}
      filesystem: {}
      memory: {}
      network: {}
      paging: {}
      process:
        mute_process_name_error: true
        mute_process_exe_error: true
        mute_process_io_error: true
      processes: {}
  prometheus:
    config:
      global:
        scrape_interval: 60s
      scrape_configs:
        - job_name: otel-collector-binary
          static_configs:
            - targets:
              # - localhost:8888
processors:
  batch:
    send_batch_size: 1000
    timeout: 10s
  # Ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md
  resourcedetection:
    detectors: [env, system] # Before system detector, include ec2 for AWS, gcp for GCP and azure for Azure.
    # Using OTEL_RESOURCE_ATTRIBUTES envvar, env detector adds custom labels.
    timeout: 2s
    system:
      hostname_sources: [os] # alternatively, use [dns,os] for setting FQDN as host.name and os as fallback
extensions:
  health_check: {}
  zpages: {}
exporters:
  otlp:
    endpoint: &quot;ingest.{region}.signoz.cloud:443&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
  logging:
    verbosity: normal
service:
  telemetry:
    metrics:
      address: 0.0.0.0:8888
  extensions: [health_check, zpages]
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics/internal:
      receivers: [prometheus, hostmetrics]
      processors: [resourcedetection, batch]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Once we are done with the above configurations, we can now run the collector service with the following command:</p>
<p>From the <code>otelcol-contrib</code>, run the following command:</p>
<pre><code>./otelcol-contrib --config ./config.yaml
</code></pre>
<h3>##     Run in background</h3>
<p>If you want to run otel collector process in the background:</p>
<pre><code>./otelcol-contrib --config ./config.yaml &amp;&gt; otelcol-output.log &amp; echo &quot;$!&quot; &gt; otel-pid
</code></pre>
<p>The above command sends the output of the otel-collector to <code>otelcol-output.log</code> file and prints the process id of the background running otel collector process to the otel-pid file.</p>
<p>If you want to see the output of the logs you‚Äôve just set up for the background process, you may look it up with:</p>
<pre><code>tail -f -n 50 otelcol-output.log
</code></pre>
<p>tail 50 will give the last 50 lines from the file <code>otelcol-output.log</code></p>
<p>You can stop the collector service <code>otelcol</code> when running in backgorund, with the following command:</p>
<pre><code>kill &quot;$(&lt; otel-pid)&quot;
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/#test-sending-traces
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: https://signoz.io/img/docs/telemetrygen-output.webp
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine-test-sending-traces
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine: Test Sending Traces</h2>
<p>OpenTelemetry collector binary should be able to forward all types of telemetry data recevied: traces, metrics, and logs, to SigNoz OTLP endpoint via gRPC.</p>
<p>Let's send sample traces to the <code>otelcol</code> using <code>telemetrygen</code>.</p>
<p>To install telemetrygen binary:</p>
<pre><code>go install github.com/open-telemetry/opentelemetry-collector-contrib/cmd/telemetrygen@latest
</code></pre>
<p>To send trace data using <code>telemetrygen</code>, execute the command below:</p>
<pre><code>telemetrygen traces --traces 1 --otlp-endpoint localhost:4317 --otlp-insecure
</code></pre>
<p>Output should look like this:</p>
<pre><code>...
2023-03-15T11:04:38.967+0545    INFO    channelz/funcs.go:340   [core][Channel #1] Channel Connectivity change to READY {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.968+0545    INFO    traces/traces.go:124    generation of traces isn't being throttled
2023-03-15T11:04:38.968+0545    INFO    traces/worker.go:90     traces generated        {&quot;worker&quot;: 0, &quot;traces&quot;: 1}
2023-03-15T11:04:38.969+0545    INFO    traces/traces.go:87     stop the batch span processor
2023-03-15T11:04:38.983+0545    INFO    channelz/funcs.go:340   [core][Channel #1] Channel Connectivity change to SHUTDOWN      {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.984+0545    INFO    channelz/funcs.go:340   [core][Channel #1 SubChannel #2] Subchannel Connectivity change to SHUTDOWN     {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.984+0545    INFO    channelz/funcs.go:340   [core][Channel #1 SubChannel #2] Subchannel deleted     {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.984+0545    INFO    channelz/funcs.go:340   [core][Channel #1] Channel deleted      {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.984+0545    INFO    traces/traces.go:79     stopping the exporter
</code></pre>
<p>If the SigNoz endpoint in the configuration is set correctly and accessible, you should be able to see the traces sent via OpenTelemetry collector in VM from <code>telemetrygen</code> in the SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/telemetrygen-output.webp" alt="traces generated by telemetrygen" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/#hostmetrics-dashboard
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: 
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine-hostmetrics-dashboard
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine: HostMetrics Dashboard</h2>
<p>To setup the Hostmetrics Dashboard, check the docs <a href="https://signoz.io/docs/userguide/hostmetrics/">here</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/#list-of-metrics
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: 
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine-list-of-metrics
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine: List of metrics</h2>
<h3>## Hostmetrics</h3>
<ul>
<li>system_network_connections</li>
<li>system_disk_weighted_io_time</li>
<li>system_disk_merged</li>
<li>system_disk_operation_time</li>
<li>system_disk_pending_operations</li>
<li>system_disk_io_time</li>
<li>system_disk_operations</li>
<li>system_disk_io</li>
<li>system_filesystem_inodes_usage</li>
<li>system_filesystem_usage</li>
<li>system_cpu_time</li>
<li>system_memory_usage</li>
<li>system_network_packets</li>
<li>system_network_dropped</li>
<li>system_network_io</li>
<li>system_network_errors</li>
<li>system_cpu_load_average_5m</li>
<li>system_cpu_load_average_15m</li>
<li>system_cpu_load_average_1m</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/introduction/
tag_set: introduction
image_urls: 
tracking_id: docs-introduction
group_tracking_ids: docs-introduction
<h2>Introduction</h2>
<p>SigNoz is an open-source observability tool that helps you monitor your applications and troubleshoot problems. It provides traces, metrics, and logs under a single pane of glass. It is available both as an <a href="https://github.com/SigNoz/signoz">open-source software</a> and a <a href="https://signoz.io/teams/">cloud offering</a>.</p>
<p>With SigNoz, you can do the following:</p>
<ul>
<li>Visualise Traces, Metrics, and Logs in a single pane of glass</li>
<li>Monitor application metrics like p99 latency, error rates for your services, external API calls, and individual endpoints.</li>
<li>Find the root cause of the problem by going to the exact traces which are causing the problem and see detailed flamegraphs of individual request traces.</li>
<li>Run aggregates on trace data to get business-relevant metrics</li>
<li>Filter and query logs, build dashboards and alerts based on attributes in logs</li>
<li>Monitor infrastructure metrics such as CPU utilization or memory usage</li>
<li>Record exceptions automatically in Python, Java, Ruby, and Javascript</li>
<li>Easy to set alerts with DIY query builder</li>
</ul>
<h2>Get Started</h2>
<hr />
<p>You can either self-host SigNoz or try SigNoz Cloud. Once SigNoz is up and running, you can instrument your application to send data to SigNoz.</p>
<ul>
<li><a href="/docs/cloud/">SigNoz Cloud: Easy way to get started with SigNoz</a></li>
<li><a href="/docs/install">Self-host SigNoz: Learn how to install and self-host SigNoz</a></li>
</ul>
<h2>How Does SigNoz Work?</h2>
<hr />
<p>SigNoz collects data using <a href="https://opentelemetry.io/">OpenTelemetry</a>
, an open-source observability solution. OpenTelemetry is backed by Cloud Native Computing Foundation. The project aims to standardize how we instrument our applications for generating telemetry data(traces, metrics, and logs).</p>
<p>SigNoz supports all the frameworks and languages supported by OpenTelemetry. You can find the complete list of supported languages on the <a href="https://opentelemetry.io/docs/instrumentation/">instrumentation</a> page of the OpenTelemetry documentation.</p>
<p>Once you instrument your application with OpenTelemetry, you can send the data to SigNoz for storage, analysis, and visualization.</p>
<h3>## Architecture</h3>
<p><img src="https://signoz.io/img/architecture-signoz-clickhouse.svg" alt="acrhitecture-diagram-clickhouse" /></p>
<p>SigNoz includes the following components:</p>
<ul>
<li><strong>OpenTelemetry Collector</strong>: Collects telemetry data from your services and applications.</li>
<li><strong>ClickHouse</strong>: An open-source, high performance columnar OLAP database management system.</li>
<li><strong>Query Service</strong>: The interface between the front-end and ClickHouse</li>
<li><strong>Frontend</strong>: The user interface, built in ReactJS and TypeScript.</li>
</ul>
<p>To learn more about the architecture of SigNoz, see the <a href="/docs/architecture/">Architecture</a> page.</p>
<h2>Use SigNoz</h2>
<hr />
<p>The topics in this section provide details on using SigNoz to monitor your application.</p>
<ul>
<li><a href="/docs/instrumentation/overview/">1. Instrumentation: Learn how to instrument your applications to send data to SigNoz</a></li>
<li><a href="/docs/tutorials/">2. Tutorials: Tutorials about monitoring your applications and infrastructure</a></li>
<li><a href="/docs/operate/">3. Operate: This section explains how to manage SigNoz</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/configuration/
tag_set: operate, configuration
image_urls: 
tracking_id: docs-operate-configuration
group_tracking_ids: docs-operate-configuration
<h2>Configuration</h2>
<p>You can configure for SigNoz with different options.</p>
<p>If no configuration for an option is set, then the default value as described is applied.</p>
<h2>Environment Variables for Configuration</h2>
<hr />
<p>When using SigNoz, there are various environment variables you can set to configure your SigNoz cluster.</p>
<p>Here is a comprehensive list of all of them. However, for general use, you most likely do not have to worry about most of these.</p>
<h3>## Query Service</h3>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Description</th>
<th>Default Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>STORAGE</code></td>
<td>Database for the storage (one of <code>clickhouse</code> or <code>druid</code>(deprecated)</td>
<td><code>clickhouse</code></td>
</tr>
<tr>
<td><code>GODEBUG</code></td>
<td>Go runtime name resolver (one of <code>netdns=go</code> or <code>netdns=cgo</code>). <a href="https://pkg.go.dev/net#hdr-Name_Resolution">More info</a>&lt;br&gt;.</td>
<td><code>netdns=go</code></td>
</tr>
<tr>
<td><code>ALERTMANAGER_API_PREFIX</code></td>
<td>Endpoint of alertmanager with API prefix.</td>
<td><code>http://alertmanager:9093/api/</code></td>
</tr>
<tr>
<td><code>ClickHouseUrl</code></td>
<td>ClickHouse database endpoint with TCP port</td>
<td><code>tcp://localhost:9000</code></td>
</tr>
<tr>
<td><code>SMTP_ENABLED</code></td>
<td>Enable SMTP for sending email invitations</td>
<td><code>false</code></td>
</tr>
<tr>
<td><code>SMTP_FROM</code></td>
<td>SMTP from address</td>
<td><code>nil</code></td>
</tr>
<tr>
<td><code>SMTP_HOST</code></td>
<td>SMTP server host</td>
<td><code>nil</code></td>
</tr>
<tr>
<td><code>SMTP_PORT</code></td>
<td>SMTP server port</td>
<td><code>nil</code></td>
</tr>
<tr>
<td><code>SMTP_USERNAME</code></td>
<td>SMTP username of the account</td>
<td><code>nil</code></td>
</tr>
<tr>
<td><code>SMTP_PASSWORD</code></td>
<td>SMTP password of the account</td>
<td><code>nil</code></td>
</tr>
</tbody>
</table>
<h3>## Otel Collector</h3>
<p>Otel Collector supports multiple combination of options. Using the configuration file, you can pass <code>receivers</code>, <code>processors</code>, <code>extensions</code>, <code>exporters</code> and <code>service</code> as per your requirements. You can find the complete otel collector configuration in the <a href="https://opentelemetry.io/docs/collector/configuration/">OpenTelemetry documentation</a>.</p>
<p>By default, SigNoz runs with <a href="https://github.com/SigNoz/signoz/blob/main/deploy/docker/clickhouse-setup/otel-collector-config.yaml">this default Otel Collector configuration</a>.</p>
<p>For any configuration changes in <strong>Docker</strong>, you would have to update <code>/deploy/docker/clickhouse-setup/otel-collector-config.yaml</code> and bring up Otel Collector.</p>
<h2>Kubernetes Configuration</h2>
<hr />
<p>The latest helm chart configuration settings can be found at:</p>
<ul>
<li><a href="https://github.com/signoz/charts/blob/main/charts/signoz/README.md#configuration">https://github.com/signoz/charts/blob/main/charts/signoz/README.md#configuration</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/jvm-metrics/
tag_set: tutorial, jvm-metrics
image_urls: 
tracking_id: docs-tutorial-jvm-metrics
group_tracking_ids: docs-tutorial-jvm-metrics
<h2>Spring Boot JVM Metrics</h2>
<p>This tutorial shows you how you can visualise JVM metrics from Spring Boot applications in SigNoz.</p>
<p>In this tutorial, we use Micrometer and Spring Boot actuator to expose JVM metrics in Prometheus format. Then we update OpenTelemetry collector which comes pre-installed with SigNoz to be able to scrape these metrics.</p>
<p>You can then plot the JVM metrics relevant for your team by creating custom dashboards in SigNoz.</p>
<p>You can use a sample Spring Boot application at this <a href="https://github.com/SigNoz/spring-petclinic">GitHub repo</a>.</p>
<h2>Steps to monitor JVM metrics</h2>
<hr />
<h3>## Changes required in your Spring Boot application</h3>
<ol>
<li>
<p><strong>Add the following code in <code>pom.xml</code></strong></p>
<pre><code> 		org.springframework.boot
   spring-boot-starter-actuator
 
 
   io.micrometer
   micrometer-registry-prometheus
   runtime
</code></pre>
</li>
<li>
<p><strong>Add the following code in application.properties file located at <code>src/main/resources/application.properties</code></strong></p>
<pre><code>management.endpoints.web.exposure.include=*
management.endpoints.web.exposure.include=prometheus,health,info,metric

management.health.probes.enabled=true
management.endpoint.health.show-details=always
management.endpoint.prometheus.enabled=true
</code></pre>
<p><a href="https://github.com/SigNoz/spring-petclinic/commit/5c4d041d43c5b1b0d07ea3bc9f0ad9a3a8b49526">Sample Spring Boot app with needed changes</a></p>
</li>
<li>
<p><strong>Build the Spring Boot application again</strong></p>
</li>
</ol>
<p>You can read more on how to expose Prometheus metric from <a href="https://docs.spring.io/spring-boot/docs/current/reference/html/actuator.html#actuator.metrics.export.prometheus">Spring Boot docs</a>.</p>
<h3>## Configure SigNoz otel-collector to scrape Prometheus metrics</h3>
<ol>
<li>
<p><strong>Add the following code in <code>otel-collector-config.yaml</code> file</strong></p>
<p><a href="https://github.com/SigNoz/signoz/blob/main/deploy/docker/clickhouse-setup/otel-collector-config.yaml">SigNoz Otel collector yaml file</a></p>
<p>üìù Note</p>
<p>Target should be updated to the IP and port where Spring Boot app is exposing metrics.</p>
<pre><code>prometheus:
 config:
   scrape_configs:
     - job_name: &quot;otel-collector&quot;
       scrape_interval: 60s
       static_configs:
         - targets: [&quot;otel-collector:8889&quot;]
     - job_name: &quot;jvm-metrics&quot;
       scrape_interval: 10s
       metrics_path: &quot;/actuator/prometheus&quot;
       static_configs:
         - targets: [&quot;&lt;IP of the machine:8090&gt;&quot;]
</code></pre>
<p>For e.g. if SigNoz is running on same machine as Spring Boot application, you can replace <code>IP of SigNoz</code> with <code>host.docker.internal</code>.</p>
</li>
<li>
<p><strong>Restart otel-collector metrics using the following command</strong></p>
<pre><code>sudo docker compose -f docker-compose.yaml restart otel-collector
</code></pre>
</li>
<li>
<p><strong>Go to SigNoz dashboard and plot metrics you want</strong></p>
<p><a href="https://signoz.io/docs/userguide/dashboards/">Creating metrics dashboard in SigNoz</a></p>
</li>
</ol>
<h2>Available metrics that you can monitor</h2>
<hr />
<p>Below is the list of available JVM metrics that you can monitor with the help of SigNoz:</p>
<pre><code>http_server_requests_seconds_sum
jvm_memory_committed_bytes
jdbc_connections_min
hikaricp_connections_min
jvm_threads_states_threads
jvm_classes_unloaded_classes_total
jvm_buffer_count_buffers
logback_events_total
jvm_memory_used_bytes
jvm_gc_pause_seconds_sum
jvm_memory_max_bytes
jdbc_connections_active
jvm_classes_loaded_classes
jvm_gc_pause_seconds_count
jdbc_connections_idle
jvm_threads_live_threads
jvm_gc_memory_promoted_bytes_total
jvm_gc_memory_allocated_bytes_total
cache_gets_total
jvm_buffer_memory_used_bytes
jvm_buffer_total_capacity_bytes
jvm_gc_live_data_size_bytes
tomcat_sessions_alive_max_seconds
hikaricp_connections_usage_seconds_count
jvm_threads_daemon_threads
hikaricp_connections_creation_seconds_sum
process_cpu_usage
jvm_gc_pause_seconds_max
process_start_time_seconds
tomcat_sessions_active_max_sessions
hikaricp_connections_acquire_seconds_count
hikaricp_connections_acquire_seconds_sum
system_load_average_1m
hikaricp_connections_usage_seconds_sum
system_cpu_usage
jvm_threads_peak_threads
tomcat_sessions_expired_sessions_total
cache_removals
tomcat_sessions_created_sessions_total
hikaricp_connections_idle
tomcat_sessions_active_current_sessions
process_uptime_seconds
hikaricp_connections_acquire_seconds_max
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/jmx-metrics/
tag_set: tutorial, jmx-metrics
image_urls: 
tracking_id: docs-tutorial-jmx-metrics
group_tracking_ids: docs-tutorial-jmx-metrics
<h2>JMX Metrics</h2>
<p>This tutorial shows you how you can collect JMX metrics from Java services and use them in SigNoz.</p>
<p>There are three steps to collect JMX metrics from Java services and use them in SigNoz.</p>
<ol>
<li><strong>Configure otel-collector to scrape JMX metrics</strong></li>
<li><strong>Configure your Java service to expose JMX metrics</strong></li>
<li><strong>Create a dashboard in SigNoz to plot the metrics</strong></li>
</ol>
<h2>Steps to collect JMX metrics</h2>
<hr />
<h3>## Configure your Java service to expose JMX metrics</h3>
<p>The Java service needs to be configured to expose JMX metrics. Update the <code>JAVA_OPTS</code> environment variable to include the following arguments:</p>
<pre><code>-Dcom.sun.management.jmxremote
-Dcom.sun.management.jmxremote.port=
-Dcom.sun.management.jmxremote.rmi.port=
-Dcom.sun.management.jmxremote.ssl=false
-Dcom.sun.management.jmxremote.authenticate=false
</code></pre>
<h3>## Configure otel-collector to scrape JMX metrics</h3>
<p>The OTel collector provides a JMX receiver component to scrape JMX metrics from Java services. It has the following configuration options:</p>
<ul>
<li>
<p><code>jar_path</code>: Path to the JMX metrics gathering jar file. The jar file can be downloaded from <a href="https://github.com/open-telemetry/opentelemetry-java-contrib/releases">https://github.com/open-telemetry/opentelemetry-java-contrib/releases</a></p>
</li>
<li>
<p><code>endpoint</code>: Endpoint of the Java service's JMX connection. Value must be in the form of <code>service:jmx:&lt;protocol&gt;:&lt;sap&gt;</code> or <code>host:port</code>. Values in <code>host:port</code> form will be used to create a Service URL of <code>service:jmx:rmi:///jndi/rmi://&lt;host&gt;:&lt;port&gt;/jmxrmi</code>.</p>
</li>
<li>
<p><code>target_system</code>: Target system of the Java service's JMX connection. Must be a subset of: <code>activemq</code>, <code>cassandra</code>, <code>hbase</code>, <code>hadoop</code>, <code>jetty</code>, <code>jvm</code>, <code>kafka</code>, <code>kafka-consumer</code>, <code>kafka-producer</code>, <code>solr</code>, <code>tomcat</code>, <code>wildfly</code>. Accepts comma separated values.</p>
</li>
</ul>
<p>Optional parameters:</p>
<ul>
<li><code>username</code>: Username for the Java service's JMX connection.</li>
<li><code>password</code>: Password for the Java service's JMX connection.</li>
</ul>
<p>After downloading the jar file, add the following to the otel-collector-config.yaml file:</p>
<pre><code>receivers:
  jmx:
    jar_path: /path/to/jmx_receiver.jar
    endpoint: localhost:1099
    target_system: cassandra,tomcat
</code></pre>
<p>Add the <code>jmx</code> receiver to the metrics pipeline in the otel-collector-config.yaml file:</p>
<pre><code>service:
  pipelines:
    metrics/jmx:
      receivers: [jmx]
      processors: [resourcedetection,batch]
      exporters: [otlp]
</code></pre>
<p>üìù Note</p>
<p>Make sure to update the endpoint and target_system to match your Java service's JMX connection.</p>
<p><strong>Create a dashboard in SigNoz to plot the metrics</strong></p>
<p><a href="https://signoz.io/docs/userguide/dashboards/">Creating metrics dashboard in SigNoz</a></p>
<p>Some pre-configured dashboards are available in SigNoz. You can find them <a href="https://github.com/SigNoz/dashboards/tree/main/jmx">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/docker-standalone/
tag_set: operate, docker-standalone
image_urls: 
tracking_id: docs-operate-docker-standalone
group_tracking_ids: docs-operate-docker-standalone
<h2>Docker Standalone</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>Once you have successfully installed SigNoz on Docker Standalone, the following sections provide an overview of the activities that are required to successfully operate SigNoz.</p>
<h2>Stop/Start SigNoz Cluster</h2>
<hr />
<p>To stop SigNoz cluster:</p>
<pre><code>docker compose -f docker/clickhouse-setup/docker-compose.yaml stop
</code></pre>
<p>To start/resume SigNoz cluster:</p>
<pre><code>docker compose -f docker/clickhouse-setup/docker-compose.yaml up -d
</code></pre>
<p>üìù Note</p>
<p>The stopped SigNoz cluster should resume and mount to the existing docker volumes.</p>
<h2>Upgrade SigNoz Cluster</h2>
<hr />
<p>Use the commands below to sync to the <a href="https://github.com/SigNoz/signoz/releases/latest">latest</a> release.</p>
<ol>
<li>
<p>Checkout to <code>main</code> branch:</p>
<pre><code>git checkout main
</code></pre>
</li>
<li>
<p>Pull the <code>latest</code> changes from the <a href="https://github.com/SigNoz/signoz">SigNoz GitHub repository</a>
:</p>
<pre><code>git pull origin main
</code></pre>
</li>
<li>
<p>Go to <code>deploy</code> folder and run the <code>install.sh</code> script:</p>
<pre><code>cd deploy &amp;&amp; ./install.sh
</code></pre>
</li>
</ol>
<p>In case you wish to upgrade the SigNoz cluster to a specific version, let's say <code>v0.6.2</code>, follow the steps below:</p>
<ol>
<li>
<p>Checkout to the <a href="https://github.com/SigNoz/signoz/releases/tag/v0.6.2">v0.6.2</a> tag:</p>
<pre><code>git checkout v0.6.2
</code></pre>
</li>
<li>
<p>Go to <code>deploy</code> folder and run the <code>install.sh</code> script:</p>
<pre><code>cd deploy &amp;&amp; ./install.sh
</code></pre>
</li>
</ol>
<p>‚ö†Ô∏è Warning</p>
<ul>
<li>Be careful with breaking changes across versions!</li>
<li>There might be misconfiguration caused by version mismatch.</li>
</ul>
<h2>Uninstall SigNoz Cluster</h2>
<hr />
<p>Enter the following command to uninstall SigNoz:</p>
<pre><code>docker compose -f docker/clickhouse-setup/docker-compose.yaml down -v
</code></pre>
<h2>Remove the Sample Application from SigNoz Dashboard</h2>
<hr />
<p>Follow the steps in this section to remove the sample application that comes installed with SigNoz:</p>
<ol>
<li>
<p>From the directory in which you installed SigNoz, open Docker Compose file <code>deploy/docker/clickhouse-setup/docker-compose.yaml</code> in a plain-text editor.</p>
</li>
<li>
<p>Comment out or remove the <code>services.hotrod</code> and <code>services.load-hotrod</code> sections:</p>
</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocker-standalone-remove-the-sample-application.webp&amp;w=3840&amp;q=75" alt="Remove the sample application on Docker Standalone" /></p>
<ol start="3">
<li>
<p>Move into the <code>deploy</code> directory and run the <code>install.sh</code> script again:</p>
<pre><code>cd deploy &amp;&amp; ./install.sh
</code></pre>
</li>
</ol>
<p>üìù Note</p>
<p>If you still see the HotROD services on the dashboard, just wait for a few minutes and the changes will reflect.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/mongodb-metrics/
tag_set: tutorial, mongodb-metrics
image_urls: 
tracking_id: docs-tutorial-mongodb-metrics
group_tracking_ids: docs-tutorial-mongodb-metrics
<h2>MongoDB Metrics</h2>
<p>This tutorial shows you how you can visualise MongoDB metrics in SigNoz. We take an example of a FastAPI sample app which uses MongoDB to walk through this tutorial.</p>
<p>Based on how you are using MongoDB, your exact steps may be different - but this should give you an idea of how to go about it.</p>
<h3>## Install FastAPI sample app via docker</h3>
<ol>
<li>Clone this sample <a href="https://github.com/SigNoz/sample-fastapi-with-dbs#run-using-docker-compose">FastAPI application</a></li>
</ol>
<p>Follow instructions in the README file to set up the application. Please ensure to point the FastAPI application to IP of the machine where SigNoz is installed</p>
<ol start="2">
<li>
<p>Use <code>172.17.0.1</code> - if running SigNoz also in same VM as the FastAPI application</p>
<p>http://172.17.0.1:4317</p>
</li>
</ol>
<p>This comes with Mongo Exporter built in which exposes metrics in prometheus format at port 9216</p>
<h3>## Check that MongoDB metrics are exposed at following end point</h3>
<pre><code>curl http://localhost:9216/metrics
</code></pre>
<h3>## Update Otel Collector config file to scrape MongoDb metrics</h3>
<p>Add a job name corresponding to mongodb exporter in otel-collector config file in your SigNoz install</p>
<p><a href="https://github.com/SigNoz/signoz/blob/e3c4bfce528eec2e5a6441608165baf9e1b46388/deploy/docker/clickhouse-setup/otel-collector-config.yaml">signoz/otel-collector-config.yaml</a></p>
<pre><code>- job_name: &quot;mongo-collector&quot;
          scrape_interval: 30s
          static_configs:
            - targets: [&quot;172.17.0.1:9216&quot;]
</code></pre>
<p>Job names should be aligned</p>
<p>Please ensure that the edited YAML is correctly configured. You can use tools like <a href="http://www.yamllint.com/">http://www.yamllint.com/</a> to check the correctness of the updated YAML file</p>
<p>Make changes in otel-collector file</p>
<h3>## Restart Otel Collector container</h3>
<pre><code>docker compose --env-file ./docker/clickhouse-setup/env/x86_64.env -f docker/clickhouse-setup/docker-compose.yaml restart otel-collector
</code></pre>
<p>Check that otel-collector is running by doing</p>
<pre><code>sudo docker ps
</code></pre>
<h2>Plotting Mongo metrics in SigNoz</h2>
<hr />
<p><a href="https://www.mongodb.com/basics/how-to-monitor-mongodb-and-what-metrics-to-monitor">Good guide on metrics you may want to monitor</a></p>
<p>Create new panels in <a href="/docs/userguide/manage-dashboards-and-panels/">Dashboard section</a> of SigNoz and add the following queries for example.</p>
<pre><code>PromQL query - mongodb_ss_connections
Legend Format - {{conn_type}}

PromQL query - rate(mongodb_ss_opcounters[5m])
Legend Format - {{legacy_op_type}}
</code></pre>
<p>üí° Tip</p>
<p>You can use queries from the json files here to plot more metrics</p>
<p>Grafana Dashboard for MongoDB exporter</p>
<p><a href="https://grafana.com/grafana/dashboards/2583/revisions">https://grafana.com/grafana/dashboards/2583/revisions</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts/
tag_set: alerts
image_urls: 
tracking_id: docs-alerts
group_tracking_ids: docs-alerts
<h2>Alert Management in SigNoz</h2>
<p>This documentation helps you in understanding the Alerts feature in SigNoz and how you can create different types of alerts.</p>
<ul>
<li><a href="/docs/userguide/alerts-management">Alert Management: Alerts in SigNoz can help you to define which data to monitor, set thresholds to detect potential problems...</a></li>
<li><a href="/docs/setup-alerts-notification">Setup Alerts Notifications: You can setup notification channel for sending the generated alerts to other applications. Currently, the following channels are ...</a></li>
<li><a href="/docs/alerts-management/metrics-based-alerts">üìÑÔ∏è Metrics based alerts: A Metric-based alert in SigNoz allows you to define conditions based on metric data...</a></li>
<li><a href="/docs/alerts-management/log-based-alerts">üìÑÔ∏è Log based alerts: A Log-based alert allows you to define conditions based on log data,...</a></li>
<li><a href="/docs/alerts-management/trace-based-alerts">üìÑÔ∏è Trace based alerts: A Trace-based alert in SigNoz allows you to define conditions based on trace data...</a></li>
<li><a href="/docs/alerts-management/exceptions-based-alerts">üìÑÔ∏è Exceptions based alerts: An Exceptions-based alert in SigNoz allows you to define conditions...</a></li>
<li><a href="/docs/alerts-management/planned-maintenance">üìÑÔ∏è Planned Maintenance: Planned Maintenance in SigNoz allows you to schedule maintenance windows for your application...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/instrumenting-angular-frontend/
tag_set: tutorial, instrumenting-angular-frontend
image_urls: 
tracking_id: docs-tutorial-instrumenting-angular-frontend
group_tracking_ids: docs-tutorial-instrumenting-angular-frontend
<h2>Instrumenting Angular Frontend Web App</h2>
<h3>## Why you need to instrument your frontend application</h3>
<p>We all are familiar with instrumenting backend services but have you ever thought about instrumenting frontend applications. Let us first understand why we need to instrument frontend applications and why reliability is a priority.</p>
<ul>
<li>Frontend is the first and last point of the user's interaction.</li>
<li>Unreliable frontend can block user's access to product in turn having a direct business impact.</li>
<li>Increasing devices and platforms unlocks new user genres hence unknown and multiple points of failure.</li>
<li>To examine and analyze the reliability of a new feature served as an A/B experiment.</li>
<li>It works in region X but not in region Y.</li>
<li>RCA on the user's complete journey inside the application.</li>
</ul>
<h3>## Instrumenting angular app</h3>
<h4>## Pre-requisites</h4>
<p>Enable CORS in the OTel Receiver. Inside <code>docker/clickhouse-setup/otel-collector-config.yaml</code> add the following CORS config.</p>
<pre><code>      http:
+        cors:
+          allowed_origins:
+            - https://netflix.com  # URL of your Frontend application
</code></pre>
<p>üìù Note</p>
<p>Make sure to restart the container after making the config changes</p>
<p>Now let's get back to instrumenting our Angular Application. Let's start by installing a couple of dependencies.</p>
<pre><code>npm i @jufab/opentelemetry-angular-interceptor &amp;&amp; npm i @opentelemetry/api @opentelemetry/sdk-trace-web @opentelemetry/sdk-trace-base @opentelemetry/core @opentelemetry/semantic-conventions @opentelemetry/resources @opentelemetry/exporter-trace-otlp-http @opentelemetry/exporter-zipkin @opentelemetry/propagator-b3 @opentelemetry/propagator-jaeger @opentelemetry/context-zone-peer-dep @opentelemetry/instrumentation @opentelemetry/instrumentation-document-load @opentelemetry/instrumentation-fetch @opentelemetry/instrumentation-xml-http-request @opentelemetry/propagator-aws-xray --save-dev
</code></pre>
<p>Not let's import OTel module in <code>app.module.ts</code></p>
<pre><code>import {
  OpenTelemetryInterceptorModule,
  OtelColExporterModule,
  CompositePropagatorModule,
} from '@jufab/opentelemetry-angular-interceptor';

@NgModule({
  ...
  imports: [\
    ...\
    OpenTelemetryInterceptorModule.forRoot({\
      commonConfig: {\
        console: true, // Display trace on console (only in DEV env)\
        production: false, // Send Trace with BatchSpanProcessor (true) or SimpleSpanProcessor (false)\
        serviceName: 'Angular Sample App', // Service name send in trace\
        probabilitySampler: '1',\
      },\
      otelcolConfig: {\
        url: 'http://127.0.0.1:4318/v1/traces', // URL of opentelemetry collector\
      },\
    }),\
    //Insert OtelCol exporter module\
    OtelColExporterModule,\
    //Insert propagator module\
    CompositePropagatorModule,\
  ],
  ...
})
</code></pre>
<p>This config would be enough to get you up and running. For more tweaks refer to <a href="https://github.com/jufab/opentelemetry-angular-interceptor#readme">this</a> detailed documentation of the instrumentation library.</p>
<p>üéâ Congratulations you have successfully instrumented your application.</p>
<h3>## Stuck?</h3>
<p>Facing difficulties with instrumenting your application? Check out this video tutorial üëá</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/roadmap/
tag_set: roadmap
image_urls: 
tracking_id: docs-roadmap
group_tracking_ids: docs-roadmap
<h2>Product Roadmap</h2>
<p>Our goal is to make the most advanced opentelemetry based observability platform, where you can correlate across different signals like metrics, traces and logs seamlessly and solve issues much faster.</p>
<p>The user stories we prioritize are based on the feedback we get from our community and the above product direction.</p>
<p>Check our updated Product Roadmap <a href="https://github.com/orgs/SigNoz/projects/18/views/1">here</a></p>
<p>We believe in taking feedback from our community. Feel free to jump to our <a href="https://github.com/SigNoz/signoz/discussions">Github Discussions</a> if you have any idea or feature we should build first. We are all ears üëÇüëÇ</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/docker-swarm/
tag_set: operate, docker-swarm
image_urls: 
tracking_id: docs-operate-docker-swarm
group_tracking_ids: docs-operate-docker-swarm
<h2>Docker Swarm</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>Once you have successfully installed SigNoz on Docker Swarm, the following sections provide an overview of the activities that are required to successfully operate SigNoz.</p>
<h2>Stop/Start SigNoz Cluster</h2>
<hr />
<p>To stop SigNoz cluster:</p>
<pre><code>docker stack rm signoz
</code></pre>
<p>To start/resume SigNoz cluster:</p>
<pre><code>docker stack deploy -c docker-swarm/clickhouse-setup/docker-compose.yaml signoz
</code></pre>
<p>üìù Note</p>
<p>The newly created SigNoz cluster should mount on the existing local path in the node.</p>
<h2>Upgrade SigNoz Cluster</h2>
<hr />
<p>To upgrade, you can manually update the image tag for <code>query-service</code>, <code>frontend</code> and <code>otel-collector</code>. And run the command to start the cluster:</p>
<pre><code>docker stack deploy -c docker-swarm/clickhouse-setup/docker-compose.yaml signoz
</code></pre>
<p>üìù Note</p>
<ul>
<li>Be careful! There might be configuration changes and version mismatch.</li>
<li>Before upgrading, checkout to the release tag: for example <code>git checkout v0.6.1</code> and compare the Docker Compose YAML and config files.</li>
</ul>
<h2>Uninstall SigNoz Cluster</h2>
<hr />
<p>To delete/uninstall SigNoz cluster:</p>
<pre><code>docker stack rm signoz
</code></pre>
<h2>Scale Up SigNoz Cluster</h2>
<hr />
<p>SigNoz uses the <a href="https://github.com/open-telemetry/opentelemetry-collector">OpenTelemetry Collector</a> to ingest data. By default, the instructions in the <a href="/docs/install/docker-swarm/">Install SigNoz on Docker Swarm</a> document create three replicas, and each replica can handle 50K spans per second. To handle an increased load, perform the following steps:</p>
<ol>
<li>
<p>Open the <code>deploy/docker-swarm/clickhouse-setup/docker-compose.yaml</code> file in a plain-text editor.</p>
</li>
<li>
<p>In the <code>services.otel-collector.deploy.replicas</code> field, enter the number of replicas you wish to create. The following example creates four replicas:</p>
</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fscale-up-otel.webp&amp;w=3840&amp;q=75" alt="Open Telemetry Collector - Create four replicas" /></p>
<ol start="3">
<li>Update the <code>signoz</code> stack by entering the following command:</li>
</ol>
<p>docker stack deploy -c docker-swarm/clickhouse-setup/docker-compose.yaml signoz</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/oci-bucket-cold-storage-integration/
tag_set: tutorial, oci-bucket-cold-storage-integration
image_urls: https://signoz.io/img/docs/oci-bucket-cold-storage/oci-bucket-storage.webp, https://signoz.io/img/docs/oci-bucket-cold-storage/oci-bucket-compartment.webp, https://signoz.io/img/docs/oci-bucket-cold-storage/oci-create-bucket.webp, https://signoz.io/img/docs/oci-bucket-cold-storage/oci-iam-profile.webp, https://signoz.io/img/docs/oci-bucket-cold-storage/oci-custom-secret-keys.webp
tracking_id: docs-tutorial-oci-bucket-cold-storage-integration
group_tracking_ids: docs-tutorial-oci-bucket-cold-storage-integration
<h2>OCI Bucket Cold Storage Integration</h2>
<h2>Overview</h2>
<hr />
<p>In SigNoz, it's possible to customize the retention period for traces, logs, and metrics individually. Additionally, you have the option to specify the duration after which the data will be transferred to cold storage for traces, logs, and metrics. SigNoz seamlessly integrates with AWS S3 and GCP Google Storage buckets for this purpose.</p>
<p>This document will provide a step-by-step guide on how to integrate an OCI (Oracle Cloud) bucket as SigNoz's cold storage solution. Given that <a href="https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/s3compatibleapi.htm">OCI buckets are S3-compatible</a>
, we will leverage this feature for the integration.</p>
<h2>Prerequisites</h2>
<hr />
<ul>
<li>SigNoz application up and running</li>
<li>Helm version 3.8 or above</li>
<li>SigNoz latest helm chart version</li>
<li>Oracle Cloud Login Access</li>
<li>SigNoz helm custom overwrite-values.yaml</li>
</ul>
<p>Instruction to set retention period with cold storage (S3) can be found in the <a href="https://signoz.io/docs/userguide/retention-period/">rention period userguide</a>.</p>
<h2>Steps To Integrate OCI Bucket As Cold Storage</h2>
<hr />
<ol>
<li>
<h3>##     Create OCI Bucket.</h3>
<p>a. Login to OCI cloud and Menu ‚Äì&gt; Storage‚Äì&gt;Object Storage.</p>
<p><img src="https://signoz.io/img/docs/oci-bucket-cold-storage/oci-bucket-storage.webp" alt="OCI Bucket Menu Navigation" /></p>
<p><em>OCI Bucket Menu Navigation</em></p>
<p>b. Select the OCI compartment and click on <strong>Create Bucket</strong>.</p>
<p><img src="https://signoz.io/img/docs/oci-bucket-cold-storage/oci-bucket-compartment.webp" alt="OCI Bucket Compartment" /></p>
<p><em>OCI Bucket Compartment</em></p>
<p>c. On the OCI bucket create page, give a meaningful name like signoz-archive-data. I have also selected Auto Tiering to bring down the costs of my bucket for OCI to move infrequently used objects to lower-tiered storage.</p>
<p><img src="https://signoz.io/img/docs/oci-bucket-cold-storage/oci-create-bucket.webp" alt="OCI Create Bucket" /></p>
<p><em>OCI Create Bucket</em></p>
</li>
<li>
<h4>##     Generate customer OCI IAM secret key that will be utilized for accessing the Object Storage.</h4>
<p>a. Login to OCI cloud and go to the IAM user profile.</p>
<p><img src="https://signoz.io/img/docs/oci-bucket-cold-storage/oci-iam-profile.webp" alt="OCI IAM Profile" /></p>
<p><em>OCI IAM Profile</em></p>
<p>b. Click on Customer Secret Keys and then Generate Secret Key. Copy the access key and secret key.</p>
<p><img src="https://signoz.io/img/docs/oci-bucket-cold-storage/oci-custom-secret-keys.webp" alt="OCI Customer Secret Keys" /></p>
<p><em>OCI Customer Secret Keys</em></p>
</li>
<li>
<h3>##     Update the helm <strong>overwrite-values.yaml</strong> to configure OCI bucket as cold storage</h3>
<p>clickhouse:</p>
<h1>Cold storage configuration</h1>
<p>coldStorage:
# -- Whether to enable cold storage
enabled: true
# -- Reserve free space on default disk (in bytes)
# Default value is 10MiB
defaultKeepFreeSpaceBytes: &quot;10485760&quot;
# -- Type of cold storage: s3
type: s3
# -- Endpoint
endpoint: https://&lt;<strong><em><strong><strong><strong>&gt;.compat.objectstorage.&lt;</strong></strong></strong></em>&gt;.oraclecloud.com/&lt;</strong>********&gt;/data/
# -- OCI Access Key
accessKey: ********************************
# -- OCI Secret Access Key
secretAccess: ********************************<br />
persistence:
enabled: true<br />
size: 100Gi</p>
</li>
</ol>
<p><code>endpoint</code> would look something like</p>
<pre><code>https://&lt;OCI-NAMESPACE&gt;.compat.objectstorage.&lt;OCI REGION&gt;.oraclecloud.com/&lt;OCI BUCKET NAME&gt;/data
</code></pre>
<ol start="4">
<li>
<h3>##     Upgrade the helm deployment.</h3>
<p>helm --namespace platform upgrade my-release signoz/signoz -f overwrite-values.yaml</p>
</li>
</ol>
<p>‚úÖ Info</p>
<p>Please customize the names of OCI resources, helm release, and namespaces to align with your specific requirements.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/integrations-list/
tag_set: integrations, integrations-list
image_urls: 
tracking_id: docs-integrations-integrations-list
group_tracking_ids: docs-integrations-integrations-list
<h2>Integrations available in SigNoz</h2>
<p>These are the integrations available in SigNoz</p>
<ul>
<li><a href="/docs/integrations/redis">Redis: Collect Redis Metrics and Logs</a></li>
<li><a href="/docs/integrations/postgresql">PostgreSQL: Collect PostgreSQL Metrics and Logs</a></li>
<li><a href="/docs/integrations/nginx">Nginx: Collect Nginx Logs</a></li>
<li><a href="/docs/integrations/mongodb">MongoDB: Collect MongoDB Metrics and Logs</a></li>
<li><a href="/docs/integrations/clickhouse">Clickhouse: Collect Clickhouse Metrics and Logs</a></li>
<li><a href="/docs/integrations/aws-rds-postgres">AWS RDS (PostgrSQL): Collect AWS RDS(PostgreSQL) Metrics and Logs</a></li>
<li><a href="/docs/integrations/aws-rds-mysql">AWS RDS (MySQL): Collect AWS RDS(MySQL) Metrics and Logs</a></li>
<li><a href="/docs/integrations/aws-elasticache-redis">AWS Elasticache (redis): Collect AWS Elasticache(redis) Metrics and Logs</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#overview
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-overview
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<h2>OpenTelemetry Operator Usage: OpenTelemetry Operator Usage - Overview</h2>
<p>In this tutorial, we would introduce <a href="https://github.com/open-telemetry/opentelemetry-operator">OpenTelemetry Operator</a> which makes it very easy to set up opentelemetry collector and instrument workloads deployed on Kubernetes.</p>
<p>A Kubernetes operator is a method of packaging, deploying, and managing a Kubernetes application. OpenTelemetry Operator helps a lot in managing OpenTelemetry collectors and enables auto-instrumentation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#prerequisite
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-prerequisite
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<h2>OpenTelemetry Operator Usage: Prerequisite</h2>
<ul>
<li>
<p>Must have a K8s cluster up and running</p>
</li>
<li>
<p>Must have <code>kubectl</code> access to your cluster</p>
</li>
<li>
<p>Must have SigNoz running. You can follow the <a href="/docs/install/">installation guide</a> to install SigNoz.</p>
</li>
<li>
<p>If you don‚Äôt already have a SigNoz Cloud account, you can sign up <a href="/teams/">here</a>
.</p>
</li>
<li>
<p>Make sure to install <a href="https://cert-manager.io/docs/installation"><code>cert-manager</code></a></p>
</li>
<li>
<p>Suggestion: Make sure <a href="https://go.dev/doc/install">Golang</a> is installed for <a href="https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/cmd/telemetrygen#readme-installing">telemetrygen</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#set-up-opentelemetry-operator
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-set-up-opentelemetry-operator
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<h2>OpenTelemetry Operator Usage: Set up OpenTelemetry Operator</h2>
<p>To install the operator in the existing K8s cluster:</p>
<pre><code>kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/download/v0.88.0/opentelemetry-operator.yaml
</code></pre>
<p>Once the <code>opentelemetry-operator</code> deployment is ready, we can proceed with creation of OpenTelemetry Collector (<code>otelcol</code>) instance and autoinstrumentation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#deployment-modes
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-deployment-modes
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<h2>OpenTelemetry Operator Usage: Deployment Modes: Deployment Modes</h2>
<p>There are 3 deployment modes available for OpenTelemetry Operator:</p>
<ul>
<li>Daemonset</li>
<li>Sidecar</li>
<li>Deployment (default mode)</li>
</ul>
<p>The <code>CustomResource</code> of the <code>OpenTelemetryCollector</code> kind exposes a property named <code>.Spec.Mode</code>, which can be used to specify whether the collector should run as a <code>DaemonSet</code>, <code>Sidecar</code>, or <code>Deployment</code> (default).</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#independent-deployment
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-independent-deployment
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<p>OpenTelemetry Operator Usage: Deployment Modes: Independent Deployment</p>
<p>To create simple instance of <code>otelcol</code> with <code>Deployment</code> mode, follow the instructions below:</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: simplest
spec:
  mode: deployment
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
    processors:
      batch:
    exporters:
      logging:
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [logging]
EOF
</code></pre>
<p>‚úÖ Info</p>
<p>The above simplest <code>otelcol</code> example receives OTLP traces data using gRPC and HTTP protocols, batches the data and logs it to the console.</p>
<p>Follow the steps below to set up <a href="https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/cmd/telemetrygen#readme-installing">telemetrygen</a> and send sample traces to the sample collector:</p>
<ul>
<li>
<p>To install <code>telemetrygen</code> binary:</p>
<pre><code>go install github.com/open-telemetry/opentelemetry-collector-contrib/cmd/telemetrygen@latest
</code></pre>
</li>
<li>
<p>To forward gRPC port of the OTLP service:</p>
<pre><code>kubectl port-forward service/simplest-collector 4317
</code></pre>
</li>
<li>
<p>In another terminal, execute the command below to send trace data using <code>telemetrygen</code>:</p>
<pre><code>telemetrygen traces --traces 1 --otlp-endpoint localhost:4317 --otlp-insecure
</code></pre>
</li>
</ul>
<p>To view logs of simplest collector:</p>
<pre><code>kubectl logs -l app.kubernetes.io/name=simplest-collector
</code></pre>
<p>Output should include the following:</p>
<pre><code>2022-09-05T06:25:50.178Z	INFO	loggingexporter/logging_exporter.go:43	TracesExporter	{&quot;#spans&quot;: 2}
</code></pre>
<p>At last make sure to clean up the <code>otelcol</code> instance:</p>
<pre><code>kubectl delete otelcol/simplest
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#across-the-nodes---daemonset
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-across-the-nodes--daemonset
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<p>OpenTelemetry Operator Usage: Deployment Modes: Across the Nodes - DaemonSet</p>
<p>Similarly, OpenTelemetry Collector instance can be deployed with <code>DaemonSet</code> mode, which ensures that all (or some) nodes run copy of the collector pod.</p>
<p>In case of <code>DaemonSet</code>, only <code>Spec.Mode</code> property would be updated to <code>daemonset</code>. While config from the previous example of <code>otelcol</code> YAML can either be kept as it is or updated as per the need.</p>
<p>‚úÖ Info</p>
<p><code>DaemonSet</code> is suitable for tasks such as log collection daemons, storage daemons, and node monitoring daemons.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#sidecar-injection
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-sidecar-injection
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<p>OpenTelemetry Operator Usage: Deployment Modes: Sidecar Injection</p>
<p>A sidecar with the OpenTelemetry Collector can be injected into pod-based workloads by setting the pod annotation <code>sidecar.opentelemetry.io/inject</code> to either <code>&quot;true&quot;</code>, or to the name of a concrete <code>OpenTelemetryCollector</code> from the same namespace.</p>
<p>Here is an example to create a <code>Sidecar</code> with <code>jaeger</code> as input and logs output to console:</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: my-sidecar
spec:
  mode: sidecar
  config: |
    receivers:
      jaeger:
        protocols:
          thrift_compact:
    processors:
    exporters:
      logging:
    service:
      pipelines:
        traces:
          receivers: [jaeger]
          processors: []
          exporters: [logging]
EOF
</code></pre>
<p>Next, let us create a <code>Pod</code> using <code>jaeger</code> example image and set <code>sidecar.opentelemetry.io/inject</code> annotations to <code>&quot;true&quot;</code>:</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: myapp
  annotations:
    sidecar.opentelemetry.io/inject: &quot;true&quot;
spec:
  containers:
  - name: myapp
    image: jaegertracing/vertx-create-span:operator-e2e-tests
    ports:
      - containerPort: 8080
        protocol: TCP
EOF
</code></pre>
<p>To forward port <code>8080</code> of the <code>myapp</code> pod:</p>
<pre><code>kubectl port-forward pod/myapp 8080:8080
</code></pre>
<p>In another terminal, let's send a HTTP request using <code>curl</code>:</p>
<pre><code>curl http://localhost:8080
</code></pre>
<p>To log output of the <code>Sidecar</code>:</p>
<pre><code>kubectl logs pod/myapp -c otc-container
</code></pre>
<p>Output should look something like this:</p>
<pre><code>2022-09-05T16:51:37.753Z	info	service/collector.go:128	Everything is ready. Begin running and processing data.
2022-09-05T17:07:37.319Z	INFO	loggingexporter/logging_exporter.go:43	TracesExporter	{&quot;#spans&quot;: 4}
2022-09-05T17:07:37.322Z	INFO	loggingexporter/logging_exporter.go:43	TracesExporter	{&quot;#spans&quot;: 9}
</code></pre>
<p>At last, make sure to clean up: <code>Sidecar</code> and <code>myapp</code> pod:</p>
<p>To remove <code>Sidecar</code> collector named <code>my-sidecar</code>:</p>
<pre><code>kubectl delete otelcol/my-sidecar
</code></pre>
<p>To remove <code>myapp</code> pod:</p>
<pre><code>kubectl delete pod/myapp
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-opentelemetry-auto-instrumentation-injection
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<h2>OpenTelemetry Operator Usage: Deployment Modes: OpenTelemetry Auto-instrumentation Injection: OpenTelemetry Auto-instrumentation Injection</h2>
<p>The OpenTelemetry operator can inject and configure OpenTelemetry auto-instrumentation libraries.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#instrumentation-resource-configuration
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-instrumentation-resource-configuration
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<p>OpenTelemetry Operator Usage: Deployment Modes: OpenTelemetry Auto-instrumentation Injection: Instrumentation Resource Configuration</p>
<p>At the moment the instrumentation is supported for Java, NodeJS, Python, and DotNet languages. The instrumentation is enabled when the following annotation is applied to a workload or a namespace.</p>
<ul>
<li><code>instrumentation.opentelemetry.io/inject-java: &quot;true&quot;</code> ‚Äî for Java</li>
<li><code>instrumentation.opentelemetry.io/inject-nodejs: &quot;true&quot;</code> ‚Äî for NodeJS</li>
<li><code>instrumentation.opentelemetry.io/inject-python: &quot;true&quot;</code> ‚Äî for Python</li>
<li><code>instrumentation.opentelemetry.io/inject-dotnet: &quot;true&quot;</code> ‚Äî for DotNet</li>
<li><code>instrumentation.opentelemetry.io/inject-sdk: &quot;true&quot;</code> - for OpenTelemetry SDK environment variables only</li>
</ul>
<p>The possible values for the annotation can be:</p>
<ul>
<li><code>&quot;true&quot;</code> - inject and Instrumentation resource from the namespace.</li>
<li><code>&quot;my-instrumentation&quot;</code> - name of Instrumentation CR instance in the current namespace.</li>
<li><code>&quot;my-other-namespace/my-instrumentation&quot;</code> - name and namespace of Instrumentation CR instance in another namespace.</li>
<li><code>&quot;false&quot;</code> - do not inject.</li>
</ul>
<p>Before using auto-instrumentation, we would need to configure an <code>Instrumentation</code> resource with the configuration for the SDK and instrumentation.</p>
<p><code>Instrumentation</code> consists of following properties:</p>
<ul>
<li>
<p><code>exporter.endpoint</code> - (optional) The address where telemetry data is to be sent in OTLP format.</p>
</li>
<li>
<p><code>propagators</code> - Enables all data sources to share an underlying context mechanism for storing state and accessing data across the lifespan of a transaction.</p>
</li>
<li>
<p><code>sampler</code> - Mechanism to control the noise and overhead introduced by reducing the number of samples of traces collected and sent to the backend. OpenTelemetry provides two types: <strong>StaticSampler</strong> and <strong>TraceIDRatioBased</strong>.</p>
</li>
<li>
<p>Language properties i.e. <code>java</code>, <code>nodejs</code>, <code>python</code> and <code>dotnet</code> - custom images to be used for auto-instrumentation with respect to the languages as set in the pod annotation.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#inject-opentelemetry-sdk-environment-variables-opentelemetry
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-inject-opentelemetry-sdk-environment-variables-opentelemetry
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<p>OpenTelemetry Operator Usage: Deployment Modes: OpenTelemetry Auto-instrumentation Injection: Inject OpenTelemetry SDK Environment Variables OpenTelemetry</p>
<p>You can configure the OpenTelemetry SDK for applications which can't currently be autoinstrumented by using <code>inject-sdk</code> in place of (e.g.) <code>inject-python</code> or <code>inject-java</code>.</p>
<p>This will inject environment variables like <code>OTEL_RESOURCE_ATTRIBUTES</code>, <code>OTEL_TRACES_SAMPLER</code>, and <code>OTEL_EXPORTER_OTLP_ENDPOINT</code>, that you can configure in the Instrumentation, but will not actually provide the SDK.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#using-sidecar
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-using-sidecar
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<p>OpenTelemetry Operator Usage: Deployment Modes: OpenTelemetry Auto-instrumentation Injection: Using Sidecar</p>
<p>To create a <code>Sidecar</code> which has <code>OTLP</code> receivers as input and as output send telemetry data to SigNoz Collector as well logs to console.</p>
<p>Select the type of SigNoz instance you are running: <strong>SigNoz Cloud</strong> or <strong>Self-Hosted</strong>.</p>
<p>SigNoz CloudSelf-Host</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: my-sidecar
spec:
  mode: sidecar
  config: |
    receivers:
      otlp:
        protocols:
          http:
          grpc:
    processors:
      batch:
    exporters:
      logging:
      otlp:
        endpoint: &quot;ingest.{region}.signoz.cloud:443&quot;
        tls:
          insecure: false
        headers:
          &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [logging, otlp]
        metrics:
          receivers: [otlp]
          processors: [batch]
          exporters: [logging, otlp]
EOF
</code></pre>
<p>üìù Note</p>
<ul>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the one provided by SigNoz.</li>
<li>Replace <code>{region}</code> with the region of your SigNoz Cloud instance. Refer to the table below for the region-specific endpoints:</li>
</ul>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>To create an instance of <code>Instrumentation</code>:</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: my-instrumentation
spec:
  propagators:
    - tracecontext
    - baggage
    - b3
  sampler:
    type: parentbased_always_on
  java:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-java:latest
  nodejs:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-nodejs:latest
  python:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:latest
  dotnet:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-dotnet:latest
EOF
</code></pre>
<p>Now, we would have to set the pod annotations <code>instrumentation.opentelemetry.io/inject-java</code> and <code>sidecar.opentelemetry.io/inject</code> to <code>&quot;true&quot;</code>, for setting up auto-instrumentation of workload deployed in the K8s. It would sends OTLP data to <code>Sidecar</code> which would in turn relay it to SigNoz collector.</p>
<p>Here is an example of pet clinic with auto-instrumentation:</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spring-petclinic
spec:
  selector:
    matchLabels:
      app: spring-petclinic
  replicas: 1
  template:
    metadata:
      labels:
        app: spring-petclinic
      annotations:
        sidecar.opentelemetry.io/inject: &quot;true&quot;
        instrumentation.opentelemetry.io/inject-java: &quot;true&quot;
    spec:
      containers:
      - name: app
        image: ghcr.io/pavolloffay/spring-petclinic:latest
EOF
</code></pre>
<p>‚úÖ Info</p>
<p>We can enable auto-instrumentation to the deployed workloads by simply adding <code>instrumentation.opentelemetry.io/inject-{language}</code> pod annotations.</p>
<p>To obtain name of the Pet Clinic pod:</p>
<pre><code>export POD_NAME=$(kubectl get pod -l app=spring-petclinic -o jsonpath=&quot;{.items[0].metadata.name}&quot;)
</code></pre>
<p>To forward port <code>8080</code> of the Pet Clinic pod:</p>
<pre><code>kubectl port-forward ${POD_NAME} 8080:8080
</code></pre>
<p>Now, let's use Pet Clinic UI for a while in browser to generate telemetry data: <a href="http://localhost:8080">http://localhost:8080</a>.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fotel-operator-spring-pet-clinic.webp&amp;w=3840&amp;q=75" alt="Spring Pet Clinic metrics page" /></p>
<p>At last, we can clean it all up:</p>
<p>To remove spring-petclinic deployment:</p>
<pre><code>kubectl delete deployment/spring-petclinic
</code></pre>
<p>To remove OpenTelemetry Instrumentation:</p>
<pre><code>kubectl delete instrumentation/my-instrumentation
</code></pre>
<p>To remove <code>Sidecar</code> instance of <code>otelcol</code>:</p>
<pre><code>kubectl delete otelcol/my-sidecar
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#auto-instrumentation-without-sidecar
tag_set: tutorial, opentelemetry-operator-usage
image_urls: 
tracking_id: docs-tutorial-opentelemetry-operator-usage-auto-instrumentation-without-sidecar
group_tracking_ids: docs-tutorial-opentelemetry-operator-usage
<p>OpenTelemetry Operator Usage: Deployment Modes: OpenTelemetry Auto-instrumentation Injection: Auto-instrumentation without Sidecar</p>
<p>To create an instance of <code>Instrumentation</code> which sends OTLP data to SigNoz endpoint:</p>
<p>Select the type of SigNoz instance you are running: <strong>SigNoz Cloud</strong> or <strong>Self-Hosted</strong>.</p>
<p>SigNoz CloudSelf-HostK8s-Infra Helm Chart</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: my-instrumentation
spec:
  exporter:
    endpoint: https://ingest.{region}.signoz.cloud:443
  env:
    - name: OTEL_EXPORTER_OTLP_HEADERS
      value: signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;
    - name: OTEL_EXPORTER_OTLP_INSECURE
      value: &quot;false&quot;
  propagators:
    - tracecontext
    - baggage
    - b3
  sampler:
    type: parentbased_traceidratio
    argument: &quot;0.25&quot;
  java:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-java:latest
  nodejs:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-nodejs:latest
  python:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:latest
  dotnet:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-dotnet:latest
EOF
</code></pre>
<p>üìù Note</p>
<ul>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the one provided by SigNoz.</li>
<li>Replace <code>{region}</code> with the region of your SigNoz Cloud instance. Refer to the table below for the region-specific endpoints:</li>
</ul>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>We would just have set the pod annotation <code>instrumentation.opentelemetry.io/inject-java</code> to <code>&quot;true&quot;</code> for our Java Springboot workload deployed in K8s.</p>
<p>Here is an example of pet clinic with auto-instrumentation:</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spring-petclinic
spec:
  selector:
    matchLabels:
      app: spring-petclinic
  replicas: 1
  template:
    metadata:
      labels:
        app: spring-petclinic
      annotations:
        instrumentation.opentelemetry.io/inject-java: &quot;true&quot;
    spec:
      containers:
      - name: app
        image: ghcr.io/pavolloffay/spring-petclinic:latest
EOF
</code></pre>
<p>‚úÖ Info</p>
<p>We can enable auto-instrumentation to the deployed workloads by simply adding <code>instrumentation.opentelemetry.io/inject-{language}</code> pod annotations.</p>
<p>To obtain name of the Pet Clinic pod:</p>
<pre><code>export POD_NAME=$(kubectl get pod -l app=spring-petclinic -o jsonpath=&quot;{.items[0].metadata.name}&quot;)
</code></pre>
<p>To forward port <code>8080</code> of the Pet Clinic pod:</p>
<pre><code>kubectl port-forward ${POD_NAME} 8080:8080
</code></pre>
<p>Now, let's use Pet Clinic UI for a while in browser to generate telemetry data: <a href="http://localhost:8080">http://localhost:8080</a>.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fotel-operator-spring-pet-clinic.webp&amp;w=3840&amp;q=75" alt="Spring Pet Clinic metrics page" /></p>
<p>At last, we can clean it all up:</p>
<p>To remove spring-petclinic deployment:</p>
<pre><code>kubectl delete deployment/spring-petclinic
</code></pre>
<p>To remove OpenTelemetry Instrumentation:</p>
<pre><code>kubectl delete instrumentation/my-instrumentation
</code></pre>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/kubernetes/
tag_set: operate, kubernetes
image_urls: 
tracking_id: docs-operate-kubernetes
group_tracking_ids: docs-operate-kubernetes
<h2>Kubernetes</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>Once you have successfully installed SigNoz on Kubernetes, the following sections provide an overview of the activities that are required to successfully operate SigNoz.</p>
<h2>Stop/Start SigNoz Cluster</h2>
<hr />
<p>To stop SigNoz cluster:</p>
<pre><code>helm -n platform uninstall &quot;my-release&quot;
</code></pre>
<p>To start/resume SigNoz cluster:</p>
<pre><code>helm -n platform install &quot;my-release&quot;
</code></pre>
<p>üìù Note</p>
<p>The newly created release aka SigNoz cluster should mount to the existing persistent volume as long as the <strong>namespace</strong> and the <strong>release name</strong> matches the old one.</p>
<h2>Upgrade SigNoz Cluster</h2>
<hr />
<p>Use the steps below to upgrade to the latest version:</p>
<ol>
<li>
<p>Fetch the latest chart information from the Helm repositories</p>
<pre><code>helm repo update
</code></pre>
</li>
<li>
<p>Upgrade to the latest available version of the chart</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz
</code></pre>
</li>
</ol>
<p>‚úÖ Info</p>
<p>To override values in a Helm chart, you can also use the <code>values</code>/<code>-f</code> flag. See the <a href="https://helm.sh/docs/helm/helm_upgrade/">Helm Upgrade</a> page of the Helm documentation for more details.</p>
<p>In case you wish to upgrade the SigNoz cluster to a specific version, follow the steps below:</p>
<ol>
<li>
<p>List the available SigNoz Helm charts with their version and supported app version.</p>
<pre><code>helm search repo signoz --versions
</code></pre>
<p>The output should look similar to the following:</p>
<pre><code>NAME               	CHART VERSION	APP VERSION	DESCRIPTION
signoz/signoz      	0.2.5        	0.10.2     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.2.3        	0.10.1     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.2.2        	0.10.0     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.2.1        	0.10.0     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.2.0        	0.10.0     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.1.4        	0.9.2      	SigNoz Observability Platform Helm Chart
signoz/clickhouse  	23.4.0       	22.4.5     	A Helm chart for ClickHouse
signoz/clickhouse  	23.3.3       	22.4.5     	A Helm chart for ClickHouse
</code></pre>
</li>
<li>
<p>Run the following command to install the chart version <code>0.2.5</code> running SigNoz version <code>0.10.2</code> with the release name <code>my-release</code> and namespace <code>platform</code>:</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz --version 0.2.5
</code></pre>
</li>
</ol>
<p>‚ö†Ô∏è Warning</p>
<ul>
<li>Be careful with breaking changes across versions!</li>
<li>There might be misconfiguration caused by version mismatch.</li>
</ul>
<h2>Uninstall SigNoz Cluster</h2>
<hr />
<p>To uninstall/delete the <code>my-release</code> resources:</p>
<pre><code>helm -n platform uninstall &quot;my-release&quot;
</code></pre>
<p>See the <a href="https://helm.sh/docs/helm/helm_uninstall/">Helm docs</a> for documentation on the helm uninstall command.</p>
<p>The command above removes all the Kubernetes components associated with the chart and deletes the release except for ClickHouse CRD resources due to <code>finalizers</code>.</p>
<p>To delete resources accociated to <code>ClickHouseInstallations</code> instance:</p>
<pre><code>kubectl -n platform patch \
  clickhouseinstallations.clickhouse.altinity.com/my-release-clickhouse \
  -p '{&quot;metadata&quot;:{&quot;finalizers&quot;:[]}}' --type=merge
</code></pre>
<p>Deletion of the StatefulSet doesn't cascade to deleting associated PVCs. To delete them:</p>
<pre><code>kubectl -n platform delete pvc -l app.kubernetes.io/instance=my-release
</code></pre>
<p>At last, clean up the namespace:</p>
<pre><code>kubectl delete namespace platform
</code></pre>
<p>‚úÖ Info</p>
<p>Replace <code>my-release</code> and <code>platform</code> from above instructions with appropriate release name and SigNoz namespace respectively.</p>
<h2>Remove the Sample Application from Dashboard</h2>
<hr />
<p>Use the command below to remove the sample application:</p>
<pre><code>curl -sL https://github.com/SigNoz/signoz/raw/main/sample-apps/hotrod/hotrod-delete.sh \
  | HOTROD_NAMESPACE=sample-application bash
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-tls-for-signoz/
tag_set: tutorial, setting-up-tls-for-signoz
image_urls: 
tracking_id: docs-tutorial-setting-up-tls-for-signoz
group_tracking_ids: docs-tutorial-setting-up-tls-for-signoz
<h2>Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager</h2>
<h3>## Overview</h3>
<p>Setting up SSL/TLS certificates is essential to secure traffic over the internet. In this guide, you will configure <strong>HTTPS</strong> for Kubernetes Ingress using <a href="https://github.com/kubernetes/ingress-nginx">ingress-Nginx</a> and <a href="https://github.com/cert-manager/cert-manager">cert-manager</a> to secure SigNoz UI and SigNoz OpenTelemetry Collector endpoints.</p>
<h3>## Prerequisites</h3>
<ul>
<li>Helm version 3.8 or above</li>
<li>SigNoz helm chart version 0.4.3 or above</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-tls-for-signoz/#steps-to-secure-signoz
tag_set: tutorial, setting-up-tls-for-signoz
image_urls: 
tracking_id: docs-tutorial-setting-up-tls-for-signoz-steps-to-secure-signoz
group_tracking_ids: docs-tutorial-setting-up-tls-for-signoz
<h2>Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager: Steps to Secure SigNoz: Steps to Secure SigNoz</h2>
<p>Follow the steps below to configure SSL/TLS certificates for the domain, let's say <code>signoz.domain.com</code>.</p>
<p>‚úÖ Info</p>
<p>Please update <code>domain.com</code> in the tutorial with either your company domain or something relevant.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-tls-for-signoz/#enable-cert-manager
tag_set: tutorial, setting-up-tls-for-signoz
image_urls: 
tracking_id: docs-tutorial-setting-up-tls-for-signoz-enable-cert-manager
group_tracking_ids: docs-tutorial-setting-up-tls-for-signoz
<p>Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager: Steps to Secure SigNoz: Enable Cert-Manager</p>
<p>You can enable the <code>cert-manager</code> dependency chart by setting <code>cert-manager.enabled</code> to <code>true</code>. Also, set <code>installCRDs</code> to true for the first time to install CRDs required by the <code>cert-manager</code>.</p>
<p>Let's include it in the existing <code>override-values.yaml</code> file, create one if not present:</p>
<pre><code>cert-manager:
  enabled: true
  installCRDs: true
</code></pre>
<p>(Optional) You can also include <code>namespace: security</code> in above YAML configuration to install <code>cert-manager</code> in <code>security</code> namespace instead of the Helm release namespace.</p>
<pre><code>kubectl create namespace security
</code></pre>
<p>To install or upgrade SigNoz release with the updated configurations in <code>override-values.yaml</code>:</p>
<pre><code>helm -n platform upgrade \
    --create-namespace --install \
    my-release signoz/signoz \
    -f override-values.yaml
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-tls-for-signoz/#enable-nginx-ingress-controller
tag_set: tutorial, setting-up-tls-for-signoz
image_urls: 
tracking_id: docs-tutorial-setting-up-tls-for-signoz-enable-nginx-ingress-controller
group_tracking_ids: docs-tutorial-setting-up-tls-for-signoz
<p>Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager: Steps to Secure SigNoz: Enable Nginx Ingress Controller</p>
<p>You can enable the Nginx ingress controller by setting <code>ingress-nginx.enabled</code> configuration to <code>true</code>.</p>
<p>Let's include it in the existing <code>override-values.yaml</code> file:</p>
<pre><code>ingress-nginx:
  enabled: true
</code></pre>
<p>To upgrade SigNoz release with the updated configurations in <code>override-values.yaml</code>:</p>
<pre><code>helm -n platform upgrade \
    --create-namespace --install \
    my-release signoz/signoz \
    -f override-values.yaml
</code></pre>
<p>Now, you will need the external IP of the Ingress Nginx Controller. That value will either be the IP address itself or a publicly accessible URL provided by the cloud vendor.</p>
<p>To obtain the external IP of the ingress Nginx controller:</p>
<pre><code>kubectl get services --namespace platform | grep &quot;ingress-nginx-controller&quot;
</code></pre>
<p>Output should be similar to the following:</p>
<pre><code>my-release-ingress-nginx-controller             LoadBalancer   10.100.233.79    &lt;redacted&gt;-&lt;redacted&gt;.&lt;redacted&gt;.elb.amazonaws.com   80:31050/TCP,443:30597/TCP   74m
my-release-ingress-nginx-controller-admission   ClusterIP      10.100.230.14    &lt;none&gt;                                               443/TCP                      74m
</code></pre>
<p>In your domain management website, you should create a DNS custom record of type <strong>A</strong> pointing all required domains to the external IP address.</p>
<p>In our example output, you can see <code>&lt;redacted&gt;-&lt;redacted&gt;.&lt;redacted&gt;.elb.amazonaws.com</code> which is a publicly accessible sub-domain provided by a cloud vendor. In this case, you should create a DNS custom record of type <strong>CNAME</strong> for all required domains.</p>
<p>‚úÖ Info</p>
<p>Before proceeding further, make sure that domains resolve to the ingress Nginx controller.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-tls-for-signoz/#create-cluster-issuer
tag_set: tutorial, setting-up-tls-for-signoz
image_urls: 
tracking_id: docs-tutorial-setting-up-tls-for-signoz-create-cluster-issuer
group_tracking_ids: docs-tutorial-setting-up-tls-for-signoz
<p>Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager: Steps to Secure SigNoz: Create Cluster Issuer</p>
<p><code>ClusterIssuer</code> is a Kubernetes resource that represents certificate authorities (CAs) that can generate signed certificates by honoring certificate signing requests. All cert-manager certificates require a referenced issuer that is in a ready condition to attempt to honour the request.</p>
<p>Let's include the following configuration in the existing <code>override-values.yaml</code> file:</p>
<pre><code>cert-manager:
  enabled: true
  installCRDs: false

  letsencrypt: true
  ingressClassName: nginx
  email: prashant@domain.com
</code></pre>
<p>‚úÖ Info</p>
<p>Replace <code>prashant@domain.com</code> with your company email id.</p>
<p>To upgrade SigNoz release with the updated configurations in <code>override-values.yaml</code>:</p>
<pre><code>helm -n platform upgrade \
    --create-namespace --install \
    my-release signoz/signoz \
    -f override-values.yaml
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-tls-for-signoz/#enable-signoz-ingress
tag_set: tutorial, setting-up-tls-for-signoz
image_urls: 
tracking_id: docs-tutorial-setting-up-tls-for-signoz-enable-signoz-ingress
group_tracking_ids: docs-tutorial-setting-up-tls-for-signoz
<p>Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager: Steps to Secure SigNoz: Enable SigNoz Ingress</p>
<p>Next, you can enable Kubernetes ingress for SigNoz UI by passing the <code>ingress.className</code> configuration to set up the ingress controller and use ingress annotation in the older K8s version. You can pass host information using <code>ingress.hosts</code>.</p>
<p>Cert-manager takes care of issuing certificates using the ingress annotation <code>cert-manager.io/cluster-issuer</code> which points to previously created <code>ClusterIssuer</code> <code>letsencrypt-prod</code>.</p>
<p>Let's update the existing <code>override-values.yaml</code> file accordingly:</p>
<pre><code>frontend:
  ingress:
    enabled: true
    className: nginx
    hosts:
      - host: signoz.domain.com
        paths:
          - path: /
            pathType: ImplementationSpecific
            port: 3301
    tls:
      - secretName: signoz.domain.com
        hosts:
          - signoz.domain.com
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
</code></pre>
<p>(Optional) Similarly, you can also enable Kubernetes ingress for SigNoz OtelCollector gRPC endpoint for a domain, let's say <code>signoz-ingest.domain.com</code>.</p>
<p>Update the existing <code>override-values.yaml</code> file accordingly:</p>
<pre><code>otelCollector:
  ingress:
    enabled: true
    className: nginx
    hosts:
      - host: signoz-ingest.domain.com
        paths:
          - path: /
            pathType: ImplementationSpecific
            port: 4317
    tls:
      - secretName: signoz-ingest.domain.com
        hosts:
          - signoz-ingest.domain.com
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
      nginx.ingress.kubernetes.io/backend-protocol: &quot;GRPC&quot;
</code></pre>
<p>‚ö†Ô∏è Warning</p>
<p>After enabling SigNoz OtelCollector, you can pass SSL/TLS certificate and private key to external OpenTelemetry collectors or external instrumentations which uses the secured domain endpoint.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-tls-for-signoz/#run-signoz-with-updated-values
tag_set: tutorial, setting-up-tls-for-signoz
image_urls: 
tracking_id: docs-tutorial-setting-up-tls-for-signoz-run-signoz-with-updated-values
group_tracking_ids: docs-tutorial-setting-up-tls-for-signoz
<p>Secure SigNoz in Kubernetes using Ingress-NGINX and Cert-Manager: Steps to Secure SigNoz: Run SigNoz with Updated Values</p>
<p>At last, you can run the command below to upgrade SigNoz release with the updated configurations in <code>override-values.yaml</code>:</p>
<pre><code>helm -n platform upgrade \
    --create-namespace --install \
    my-release signoz/signoz \
    -f override-values.yaml
</code></pre>
<p>You should be able to access SigNoz UI using the domain name in <code>frontend.ingress.hostname</code>.</p>
<p>In case you have set up SSL/TLS for SigNoz OtelCollector, you can test it using <a href="https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/tracegen#section-readme">tracegen</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-sso-saml-with-keycloak/
tag_set: tutorial, setting-up-sso-saml-with-keycloak
image_urls: 
tracking_id: docs-tutorial-setting-up-sso-saml-with-keycloak
group_tracking_ids: docs-tutorial-setting-up-sso-saml-with-keycloak
<h2>Setting Up SSO SAML 2.0 With Keycloak</h2>
<p>‚úÖ Info</p>
<p>SAML-based authentication is only available on Enterprise plans (both self-hosted and on SigNoz Cloud).</p>
<h3>## Overview</h3>
<p>Keycloak is an open-source identity and access management tool which makes it easy to add authentication to applications and secure with minimum effort.</p>
<p>In enterprise plan of SigNoz, you can enforce Single Sign-On (SSO) using SAML 2.0. Keycloak can be used as a user directory to save user data while acting as the identity provider (IdP) for the SSO.</p>
<h3>## Prerequisites</h3>
<ul>
<li>SigNoz application up and running</li>
<li>Helm version 3.8 or above</li>
<li>SigNoz helm chart version 0.4.3 or above</li>
<li>Enterprise plan of SigNoz</li>
<li>Set up <code>cert-manager</code> and Nginx ingress controller</li>
<li>SigNoz Frontend ingress with TLS</li>
</ul>
<p>‚úÖ Info</p>
<p>If you have not set up <code>cert-manager</code>, Nginx ingress controller, or have Signoz with TLS, you can follow the guide <a href="/docs/tutorial/setting-up-tls-for-signoz/">here</a> to set it up.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-sso-saml-with-keycloak/#set-up-single-sign-on-using-saml
tag_set: tutorial, setting-up-sso-saml-with-keycloak
image_urls: 
tracking_id: docs-tutorial-setting-up-sso-saml-with-keycloak-set-up-single-sign-on-using-saml
group_tracking_ids: docs-tutorial-setting-up-sso-saml-with-keycloak
<h2>Setting Up SSO SAML 2.0 With Keycloak: Set up Single Sign-On using SAML: Set up Single Sign-On using SAML</h2>
<p>For this tutorial, you will be required to have Keycloak up and running.</p>
<p>It is assumed that you have set up Keycloak or any other identity and access management tool with SSL/TLS certificates.</p>
<p>In case you do not have Keycloak set up and want to install Keycloak using SigNoz chart, you can follow the <a href="/docs/tutorial/setting-up-sso-saml-with-keycloak/#optional-install-keycloak-using-signoz-helm-chart">instructions in this section</a> before proceeding further.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-sso-saml-with-keycloak/#steps-to-set-up-saml
tag_set: tutorial, setting-up-sso-saml-with-keycloak
image_urls: 
tracking_id: docs-tutorial-setting-up-sso-saml-with-keycloak-steps-to-set-up-saml
group_tracking_ids: docs-tutorial-setting-up-sso-saml-with-keycloak
<p>Setting Up SSO SAML 2.0 With Keycloak: Set up Single Sign-On using SAML: Steps to Set Up SAML</p>
<p>Let's assume <code>https://signoz.domain.com</code> as the endpoint where SigNoz is publicly accessible whereas <code>https://signoz-keycloak.domain.com</code> being the secured endpoint of <strong>Keycloak</strong>.</p>
<ol>
<li>Go to the Keycloak Admin console UI and sign in as an administrator</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fadmin-login.webp&amp;w=1920&amp;q=75" alt="Admin login page" /></p>
<ol start="2">
<li>Create a new realm: <strong>SigNoz</strong></li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fcreate-realm.webp&amp;w=3840&amp;q=75" alt="Create SigNoz realm" /></p>
<ol start="3">
<li>Create new client of type <strong>SAML</strong> with client ID same as that of secured SigNoz endpoint: <code>signoz.domain.com</code></li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fcreate-client.webp&amp;w=2048&amp;q=75" alt="Create client page" /></p>
<ol start="4">
<li>
<p>Update <strong>Access settings</strong> configurations</p>
<ul>
<li>Set <strong>Home URL</strong> to <code>https://signoz.domain.com/api/v1/complete/saml</code></li>
<li>Set <strong>Valid redirect URIs</strong> to <code>https://signoz.domain.com/*</code></li>
</ul>
</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fclient-access-settings.webp&amp;w=1920&amp;q=75" alt="Client access settings" /></p>
<ol start="5">
<li>Go to Clients &gt; Client scopes, and select <code>signoz.domain.com-dedicated</code> and add predefined mappers in SigNoz client: <em>role list</em>, <em>X500 email</em>, and <em>X500 given name</em></li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fadd-predefined-mappers.webp&amp;w=3840&amp;q=75" alt="Predefined mappers" /></p>
<ol start="6">
<li>In SAML capabilities section, select <strong>email</strong> for Name ID format</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fsaml-capabilities-email-id.webp&amp;w=1920&amp;q=75" alt="SAML capabilities email id" /></p>
<ol start="7">
<li>Go to Clients &gt; Keys, and turn off the <strong>Client signature required</strong> option from signing keys config</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fclient-signature-required.webp&amp;w=1920&amp;q=75" alt="Client signature required" /></p>
<ol start="8">
<li>Create new user which is to be used for login. Be sure to include email id in <strong>username</strong> and <strong>email</strong> fields</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fcreate-user.webp&amp;w=1920&amp;q=75" alt="Create new user" /></p>
<ol start="9">
<li>Make sure to create credentials for the created user and turn off the temporary toggle</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fnew-user-credentials.webp&amp;w=1920&amp;q=75" alt="New user credentials" /></p>
<ol start="10">
<li>Go to Realm settings &gt; General, open <strong>SAML 2.0 Identity Provider Metadata</strong> endpoint</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Frealm-setting-saml.webp&amp;w=1920&amp;q=75" alt="SAML 2.0 identity provider metadata" /></p>
<p>From the XML file, make note of the followings:</p>
<ul>
<li>SAML location URL with the <code>/protocol/saml</code> suffix which is SAML ACS URL</li>
<li>SAML entity id</li>
<li>SAML X.509 certificate</li>
</ul>
<ol start="11">
<li>In SigNoz UI, go to Settings &gt; Organization Settings &gt; Authenticated Domains</li>
</ol>
<pre><code>Add an authenticated domains which will be the domain of the user email; in case of `prashant@domain.com`, it will be `domain.com`.
</code></pre>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fadd-domain.webp&amp;w=3840&amp;q=75" alt="Add authenticated domain" /></p>
<ol start="12">
<li>SAML configurations from <strong>Step 10</strong> is set in SigNoz and enable the <strong>Enforce SSO</strong> toggle</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fsignoz-saml-configuration.webp&amp;w=1920&amp;q=75" alt="SigNoz SAML configuration" /></p>
<ol start="13">
<li>Open your browser with incognito (or private) mode, and open your SigNoz URL and click <strong>Login</strong> followed by <strong>SSO Login</strong></li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fsso-login.webp&amp;w=2048&amp;q=75" alt="SSO Login" /></p>
<ol start="14">
<li>You will be redirected to Keycloak user login page, enter Keycloak user credentials from <strong>Step 8</strong> and <strong>Step 9</strong></li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fkeycloak-saml%2Fkeycloak-user-login.webp&amp;w=1920&amp;q=75" alt="Keycloak user login" /></p>
<p>Now, you should be logged in to SigNoz using Keycloak SSO.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/setting-up-sso-saml-with-keycloak/#optional-install-keycloak-using-signoz-helm-chart
tag_set: tutorial, setting-up-sso-saml-with-keycloak
image_urls: 
tracking_id: docs-tutorial-setting-up-sso-saml-with-keycloak-optional-install-keycloak-using-signoz-helm-chart
group_tracking_ids: docs-tutorial-setting-up-sso-saml-with-keycloak
<p>Setting Up SSO SAML 2.0 With Keycloak: Set up Single Sign-On using SAML: (Optional) Install Keycloak using SigNoz Helm Chart</p>
<p>You can install the <strong>Keycloak</strong> using SigNoz helm chart. We can set custom namespace, admin console credentials, postgres admin and user credentials, and ingress related configurations.</p>
<p>Let's include them in existing <code>override-values.yaml</code>:</p>
<pre><code>keycloak:
  enabled: true

  auth:
    adminUser: admin
    adminPassword: adminpass123

  postgresql:
    auth:
      postgresPassword: pgadminpass123
      username: bn_keycloak
      password: bn_keycloak123

  ingress:
    enabled: true
    ingressClassName: nginx
    pathType: ImplementationSpecific
    path: /
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hostname: signoz-keycloak.domain.com
    servicePort: http

    tls: true
    selfSigned: false
</code></pre>
<p>‚úÖ Info</p>
<p>You will need to replace <code>domain.com</code> with company domain or something relevant.</p>
<p>Make sure that you have Nginx ingress controller and Cert-manager installed and you have a <code>ClusterIssuer</code> named <code>letsencrypt-prod</code>. You can follow the instructions from the <a href="/docs/tutorial/setting-up-tls-for-signoz/">guide here</a>.</p>
<p>To install or upgrade SigNoz release with the updated configurations in <code>override-values.yaml</code>:</p>
<pre><code>helm -n platform upgrade \
    --create-namespace --install \
    my-release signoz/signoz \
    -f override-values.yaml
</code></pre>
<p>You should be able to access Keycloak UI using the domain name in <code>ingress.hostname</code>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/monitor-http-endpoints/
tag_set: monitor-http-endpoints
image_urls: 
tracking_id: docs-monitor-http-endpoints
group_tracking_ids: docs-monitor-http-endpoints
<h2>Monitor HTTP Endpoints</h2>
<p>With SigNoz, you can monitor the health of the HTTP endpoints and set up an alert in case of HTTP endpoint failure status codes.</p>
<p>SigNoz CloudSelf-Host</p>
<ul>
<li>
<p>Add the httpcheck receiver to the config and set up the collector OTLP exporter to send data to SigNoz Cloud.</p>
<pre><code>receivers:
  httpcheck:
    targets:
      - endpoint: http://example.com
        method: GET
    collection_interval: 10s
...
exporters:
  otlp:
    endpoint: &quot;ingest.{region}.signoz.cloud:443&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
...
</code></pre>
<p>Depending on the choice of your region for the SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>The HTTP Check Receiver can be used for synthetic checks against HTTP endpoints. This receiver will make a request to the specified endpoint using the configured method. This scraper generates a metric labelled for each HTTP response status class with a value of 1 if the status code matches the class.</p>
</li>
<li>
<p>Next, we will modify the pipeline to include the receiver we have enabled above.</p>
<pre><code>service:
    ....
    metrics:
      receivers: [otlp, httpcheck]
      processors: [batch]
      exporters: [otlp]
</code></pre>
</li>
<li>
<p>We can restart the otel collector container/process so that new changes are applied and see the metrics generated for synthetic checks.</p>
</li>
<li>
<p>This receiver creates a metric name <code>httpcheck_status</code> with value 1 if the check resulted in status_code matching the status_class, otherwise 0. For more info on the additional metrics and attributes available, please read the documentation <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/httpcheckreceiver/documentation.md">here</a>
.</p>
</li>
</ul>
<h4>## Monitoring the Health of Multiple Endpoints</h4>
<p>You can configure <code>targets</code> for the httpcheck receiver to monitor the health of one or more endpoints. Following is the sample config that monitors two endpoints.</p>
<pre><code>receivers:
  httpcheck:
    targets:
      - endpoint: http://example.com
        method: GET
      - endpoint: http://my-app.com
        method: GET
    collection_interval: 10s
...
exporters:
  otlp:
    endpoint: &quot;ingest.{region}.signoz.cloud:443&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
...
  service:
      ....
      metrics:
        receivers: [otlp, httpcheck]
        processors: [batch]
        exporters: [otlp]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/writing-clickhouse-queries-in-dashboard/
tag_set: tutorial, writing-clickhouse-queries-in-dashboard
image_urls: 
tracking_id: docs-tutorial-writing-clickhouse-queries-in-dashboard
group_tracking_ids: docs-tutorial-writing-clickhouse-queries-in-dashboard
<h2>ClickHouse queries for building dashboards and alerts</h2>
<p>SigNoz gives you the ability to write powerful ClickHouse queries to plot charts. You can run SQL queries supported by ClickHouse to extract immense value out of your distributed tracing data or logs data.</p>
<p>‚úÖ Info</p>
<p>The distributed tables in ClickHouse have been named by prefixing <code>distributed_</code> to existing single shard table names. If you want to use ClickHouse queries in dashboard or alerts, you should use the distributed table names. Eg, <code>signoz_index_v2</code> now corresponds to the table of a single shard. To query all the shards, query against <code>distributed_signoz_index_v2</code>.</p>
<p>‚ö†Ô∏è Warning</p>
<ul>
<li>The tagMap column of type Map(String, String) in signoz_traces.distributed_signoz_index_v2 has been divided into stringTagMap of type Map(String, String), numberTagMap of type Map(String, Float64) and boolTagMap of type Map(String, bool).
<ul>
<li>All attributes with string values will be present in stringTagMap, attributes with number values will be present in numberTagMap and attributes with boolean values will be present in boolTagMap.</li>
</ul>
</li>
<li>The gRPCMethod column has been replaced by rpcMethod column.</li>
<li>The gRPCCode and httpCode column has been replaced by responseStatusCode column.</li>
</ul>
<p>The gRPCMethod, gRPCCode, httpCode and tagMap column will be deprecated in future releases so it recommended to replace dashboard queries with new columns.</p>
<p>Sharing a few examples of some queries which might be helpful in building dashboards.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/writing-clickhouse-queries-in-dashboard/#groupby-a-tagattribute-in-distributed-tracing-data
tag_set: tutorial, writing-clickhouse-queries-in-dashboard
image_urls: 
tracking_id: docs-tutorial-writing-clickhouse-queries-in-dashboard-groupby-a-tagattribute-in-distributed-tracing-data
group_tracking_ids: docs-tutorial-writing-clickhouse-queries-in-dashboard
<p>ClickHouse queries for building dashboards and alerts: : GroupBy a tag/attribute in distributed tracing data</p>
<pre><code>SELECT toStartOfInterval(timestamp, INTERVAL 1 MINUTE) AS interval, 
stringTagMap['peer.service'] AS op_name, 
toFloat64(avg(durationNano)) AS value 
FROM signoz_traces.distributed_signoz_index_v2
WHERE stringTagMap['peer.service']!='' 
AND timestamp &gt; now() - INTERVAL 30 MINUTE  
GROUP BY (op_name, interval) order by (op_name, interval) ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/writing-clickhouse-queries-in-dashboard/#show-count-of-eachcustomer_id-which-is-present-as-attribute-of-a-span-event
tag_set: tutorial, writing-clickhouse-queries-in-dashboard
image_urls: 
tracking_id: docs-tutorial-writing-clickhouse-queries-in-dashboard-show-count-of-eachcustomer_id-which-is-present-as-attribute-of-a-span-event
group_tracking_ids: docs-tutorial-writing-clickhouse-queries-in-dashboard
<p>ClickHouse queries for building dashboards and alerts: : Show count of each¬†<code>customer_id</code> which is present as attribute of a span event</p>
<pre><code>WITH arrayFilter(x -&gt; JSONExtractString(x, 'name')='Getting customer', events) AS filteredEvents
SELECT toStartOfInterval(timestamp, INTERVAL 1 MINUTE) AS interval, toFloat64(count()) AS count, 
arrayJoin(arrayMap(x -&gt; JSONExtractString(JSONExtractString(x, 'attributeMap'), 'customer_id'), filteredEvents)) AS resultArray 
FROM signoz_traces.distributed_signoz_index_v2
WHERE not empty(filteredEvents) 
AND timestamp &gt; toUnixTimestamp(now() - INTERVAL 30 MINUTE) 
GROUP BY (resultArray, interval) order by (resultArray, interval) ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/writing-clickhouse-queries-in-dashboard/#avg-latency-between-2-spans-of-interest-part-of-the-trace-tree
tag_set: tutorial, writing-clickhouse-queries-in-dashboard
image_urls: 
tracking_id: docs-tutorial-writing-clickhouse-queries-in-dashboard-avg-latency-between-2-spans-of-interest-part-of-the-trace-tree
group_tracking_ids: docs-tutorial-writing-clickhouse-queries-in-dashboard
<p>ClickHouse queries for building dashboards and alerts: : Avg latency between 2 spans of interest (part of the trace tree)</p>
<pre><code>SELECT
    interval,
    round(avg(time_diff), 2) AS result
FROM
(
    SELECT
        interval,
        traceID,
        if(startTime1 != 0, if(startTime2 != 0, (toUnixTimestamp64Nano(startTime2) - toUnixTimestamp64Nano(startTime1)) / 1000000, nan), nan) AS time_diff
    FROM
    (
        SELECT
            toStartOfInterval(timestamp, toIntervalMinute(1)) AS interval,
            traceID,
            minIf(timestamp, if(serviceName='driver', if(name = '/driver.DriverService/FindNearest', if((stringTagMap['component']) = 'gRPC', true, false), false), false)) AS startTime1,
            minIf(timestamp, if(serviceName='route', if(name = 'HTTP GET /route', true, false), false)) AS startTime2
        FROM signoz_traces.distributed_signoz_index_v2
        WHERE (timestamp BETWEEN {{.start_datetime}} AND {{.end_datetime}}) AND (serviceName IN ('driver', 'route'))
        GROUP BY (interval, traceID)
        ORDER BY (interval, traceID) ASC
    )
)
WHERE isNaN(time_diff) = 0
GROUP BY interval
ORDER BY interval ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/writing-clickhouse-queries-in-dashboard/#show-sum-of-values--of-customer_id-which-is-present-as-attribute-of-a-span-event
tag_set: tutorial, writing-clickhouse-queries-in-dashboard
image_urls: 
tracking_id: docs-tutorial-writing-clickhouse-queries-in-dashboard-show-sum-of-values-of-customer_id-which-is-present-as-attribute-of-a-span-event
group_tracking_ids: docs-tutorial-writing-clickhouse-queries-in-dashboard
<p>ClickHouse queries for building dashboards and alerts: : Show sum of values of <code>customer_id</code> which is present as attribute of a span event</p>
<pre><code>WITH arrayFilter(x -&gt; JSONExtractString(x, 'name')='Getting customer', events) AS filteredEvents
SELECT toStartOfInterval(timestamp, INTERVAL 1 MINUTE) AS interval, toFloat64(sum(toInt32(resultArray))) AS sum, 
arrayJoin(arrayMap(x -&gt; JSONExtractString(JSONExtractString(x, 'attributeMap'), 'customer_id'), filteredEvents)) AS resultArray 
FROM signoz_traces.distributed_signoz_index_v2
WHERE not empty(filteredEvents) and timestamp &gt; toUnixTimestamp(now() - INTERVAL 30 MINUTE) 
GROUP BY (resultArray, interval) order by (resultArray, interval) ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/writing-clickhouse-queries-in-dashboard/#plotting-a-chart-on-100ms-interval
tag_set: tutorial, writing-clickhouse-queries-in-dashboard
image_urls: 
tracking_id: docs-tutorial-writing-clickhouse-queries-in-dashboard-plotting-a-chart-on-100ms-interval
group_tracking_ids: docs-tutorial-writing-clickhouse-queries-in-dashboard
<p>ClickHouse queries for building dashboards and alerts: : Plotting a chart on <code>100ms</code> interval</p>
<p>Plot a chart of 1 minute showing count of spans in <code>100ms</code> interval of service <code>frontend</code> with duration &gt; 50ms</p>
<pre><code>SELECT fromUnixTimestamp64Milli(intDiv( toUnixTimestamp64Milli ( timestamp ), 100) * 100) AS interval, 
toFloat64(count()) AS count 
FROM (
 SELECT timestamp 
 FROM signoz_traces.distributed_signoz_index_v2
 WHERE serviceName='frontend' 
 AND durationNano&gt;=50*exp10(6) 
 AND timestamp &gt; now() - INTERVAL 1 MINUTE) GROUP BY interval 
 ORDER BY interval ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/writing-clickhouse-queries-in-dashboard/#show-count-of-loglines-per-minute
tag_set: tutorial, writing-clickhouse-queries-in-dashboard
image_urls: 
tracking_id: docs-tutorial-writing-clickhouse-queries-in-dashboard-show-count-of-loglines-per-minute
group_tracking_ids: docs-tutorial-writing-clickhouse-queries-in-dashboard
<p>ClickHouse queries for building dashboards and alerts: : Show count of loglines per minute</p>
<pre><code>SELECT toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS interval, 
toFloat64(count()) AS value 
FROM signoz_logs.distributed_logs
WHERE timestamp &gt; toUnixTimestamp64Nano(now64() - INTERVAL 30 MINUTE)  
GROUP BY interval 
ORDER BY interval ASC;
</code></pre>
<p>While writing queries for logs table, if you want to use an attribute/resource in your query you will have to reference it in the following format <code>&lt;type&gt;_&lt;dataType&gt;_value[indexOf(&lt;type&gt;_&lt;dataType&gt;_key, &lt;keyname&gt;)]</code></p>
<p>where <code>type</code> can be <code>attributes/resources</code> , <code>dataType</code> can be <code>int64/float64/string</code> and <code>keyname</code> is the name of the key.</p>
<p>Eg: If your <code>keyname</code> is <code>status</code> of <code>dataType</code> <code>string</code> and <code>type</code> <code>attribute</code>, it needs to be referenced as <code>attributes_string_value[indexOf(attributes_string_key, 'status')]</code></p>
<p>üìù Note</p>
<p>In the above example, if <code>status</code> is an selected field, then it can be referenced as <code>attribute_string_status</code></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/writing-clickhouse-queries-in-dashboard/#building-alert-queries-with-clickhouse-data
tag_set: tutorial, writing-clickhouse-queries-in-dashboard
image_urls: 
tracking_id: docs-tutorial-writing-clickhouse-queries-in-dashboard-building-alert-queries-with-clickhouse-data
group_tracking_ids: docs-tutorial-writing-clickhouse-queries-in-dashboard
<h2>ClickHouse queries for building dashboards and alerts: : Building alert queries with ClickHouse data</h2>
<p>The most common use case for alerts is to send notifications when a threshold condition is met. For example, conditions like CPU usage exceeds 70% (CPU Usage &gt; 70) or failures in a service exceed your expectations (service error count &gt; 10).</p>
<p>Every alert is composed of a threshold, date range (e.g. last 5 mins) and match condition (e.g. at least once). The rule engine is responsible for evaluating these conditions and triggering notifications. SigNoz allows you a complete flexibility in choosing all three conditions to meet your business requirements.</p>
<p>The evaluation is also based on the source or metric data, the left-hand side of the threshold condition - consider CPU Usage (LHS) &gt; 70 (RHS).</p>
<p>The query editor (<code>ClickHouse</code> tab in the <code>alert form</code>) allows you to define a query to fetch the metric (source) data from ClickHouse and evaluate if the threshold condition occurs in it. The query format requires use of following reserved columns and parameters which simplify the syntax and support dynamic parts.</p>
<p><strong>Reserved Aliases (for use in SELECT, GROUP BY only)</strong>: <code>value</code>, <code>res</code>, <code>result</code>, <code>interval</code></p>
<p><strong>Reserved Parameters (for use in WHERE condition only)</strong>:</p>
<ul>
<li><code>{{.start_datetime}}</code> <code>{{.end_datetime}}</code></li>
<li><code>{{.start_timestamp}}</code> <code>{{.end_timestamp}}</code></li>
<li><code>{{.start_timestamp_ms}}</code> <code>{{.end_timestamp_ms}}</code></li>
<li><code>{{.start_timestamp_nano}}</code> <code>{{.end_timestamp_nano}}</code></li>
</ul>
<p>Let's explore sample queries to use ClickHouse data in alert evaluation.</p>
<p>The rule engine is always looking for special column aliases. When the query returns data, the rule engine looks into the result for a column with alias <code>value</code> or <code>result</code> and compares the vector of all those values with the threshold (defined in alert form).</p>
<p>For example, the following query retrieves <code>not found</code> errors of a service. When this query returns data, the rule engine will compare <code>value</code> from each row with the threshold set in alert form.</p>
<pre><code>SELECT count() AS value  
FROM signoz_traces.distributed_signoz_error_index_v2
WHERE (serviceName='api-service‚Äô) 
AND (exceptionType='Not found‚Äô);
</code></pre>
<p>If the same query is written without the column alias <code>value</code> or <code>result</code>, the rule engine will ignore the metric and fail to arrive at a threshold match.</p>
<p>Second reserved column alias is <code>interval</code>. It is useful when you want to apply threshold check over n time intervals.</p>
<pre><code>SELECT count() AS value, toStartOfInterval(timestamp, INTERVAL 5 MINUTE) AS interval 
FROM signoz_traces.distributed_signoz_error_index_v2
WHERE (serviceName = 'api-service') 
AND (exceptionType = 'Not found') 
GROUP BY interval
</code></pre>
<p>The rule engine is also capable of dynamically applying date ranges to your <code>SELECT</code> query. When you choose <code>last 15 mins</code> option in the alert conditions section, the rule engine applies timestamp values in the timeframe condition below.</p>
<pre><code>SELECT count() AS value, toStartOfInterval(timestamp, INTERVAL 5 MINUTE) AS interval 
FROM signoz_traces.distributed_signoz_error_index_v2
WHERE timestamp BETWEEN {{.start_datetime}} AND {{.end_datetime}} 
AND (serviceName = 'api-service') 
AND (exceptionType = 'Not found') 
GROUP BY interval
</code></pre>
<p>The reserved bind variables <code>{{.start_datetime}}</code> and <code>{{.end_datetime}}</code> would allow rule engine to dynamically include date range query when the alert query runs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/
tag_set: operate, migration
image_urls: 
tracking_id: docs-operate-migration
group_tracking_ids: docs-operate-migration
<h2>Migration Guides</h2>
<p>The following sections provide instructions to migrate SigNoz components across newer versions. You need to run these migration scripts step by step. For example if you are currently on <code>0.8.2</code> and want to migrate to <code>0.10.0</code> - you need to run migration script for <code>0.9</code> first and then <code>0.10</code></p>
<p><a href="/docs/operate/migration/upgrade-0.49">üìÑÔ∏è Upgrade to v0.49</a>
<a href="/docs/operate/migration/upgrade-0.45">üìÑÔ∏è Upgrade to v0.45</a>
<a href="/docs/operate/migration/upgrade-0.38">üìÑÔ∏è Upgrade to v0.38</a>
<a href="/docs/operate/migration/upgrade-0.36">üìÑÔ∏è Upgrade to v0.36</a>
<a href="/docs/operate/migration/upgrade-0.27">üìÑÔ∏è Upgrade to v0.27</a>
<a href="/docs/operate/migration/upgrade-0.23">üìÑÔ∏è Upgrade to v0.23</a>
<a href="/docs/operate/migration/upgrade-0.19">üìÑÔ∏è Upgrade to v0.19</a>
<a href="/docs/operate/migration/upgrade-0.12">üìÑÔ∏è Upgrade to v0.12</a>
<a href="/docs/operate/migration/upgrade-0.10">üìÑÔ∏è Upgrade to v0.10</a>
<a href="/docs/operate/migration/upgrade-0.9">üìÑÔ∏è Upgrade to v0.9</a>
<a href="/docs/operate/migration/upgrade-0.8.1">üìÑÔ∏è Upgrade to v0.8.1</a>
<a href="/docs/operate/migration/upgrade-0.8.0">üìÑÔ∏è Upgrade to v0.8.0</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/traefik-observability/
tag_set: tutorial, traefik-observability
image_urls: 
tracking_id: docs-tutorial-traefik-observability
group_tracking_ids: docs-tutorial-traefik-observability
<h2>Traefik Observability</h2>
<h3>## Overview</h3>
<p>In this tutorial, we will see how to export metrics and traces of Traefik to SigNoz. Visualizing Traefik metrics and traces will help you to understand the performance of services running behind Traefik and troubleshoot issues.</p>
<h3>## Prerequisites</h3>
<ul>
<li>Traefik v3.0 or above</li>
<li>Must have SigNoz running. You can follow the <a href="https://signoz.io/docs/install/">installation guide</a> to install SigNoz.</li>
<li>Must have SigNoz OtelCollector accessible from Traefik</li>
<li>If you don‚Äôt already have a SigNoz Cloud account, you can sign up <a href="https://signoz.io/teams/">here</a>
.</li>
</ul>
<h2>Export Traefik Metrics and Traces to SigNoz</h2>
<hr />
<p>Based on how you are running SigNoz (e.g. SigNoz Cloud, in an independent VM or Kubernetes cluster), you have to provide the address to send data from the above receivers.</p>
<p>SigNoz CloudSelf-Host</p>
<p>In this section, we will see how to export Traefik metrics and traces to SigNoz Cloud.</p>
<p>For metrics, we will have to set the following CLI flags in Traefik:</p>
<ul>
<li><code>--metrics.openTelemetry=true</code></li>
<li><code>--metrics.openTelemetry.grpc=true</code></li>
<li><code>--metrics.openTelemetry.address=ingest.{region}.signoz.cloud:443</code></li>
<li><code>--metrics.openTelemetry.insecure=false</code></li>
<li><code>--metrics.openTelemetry.headers.signoz-access-token=SIGNOZ_INGESTION_KEY</code></li>
</ul>
<p>For traces, we will have to set the following CLI flags in Traefik:</p>
<ul>
<li><code>--tracing.openTelemetry=true</code></li>
<li><code>--tracing.openTelemetry.grpc=true</code></li>
<li><code>--tracing.openTelemetry.address=ingest.{region}.signoz.cloud:443</code></li>
<li><code>--tracing.openTelemetry.insecure=false</code></li>
<li><code>--tracing.openTelemetry.headers.signoz-access-token=SIGNOZ_INGESTION_KEY</code></li>
</ul>
<p>We will take an example <code>docker-compose.yaml</code> with a simple <code>hello-app</code> running behind Traefik.</p>
<pre><code># docker-compose.yaml {13-14,18-19}

version: '3'
services:
  reverse-proxy:
    image: traefik:v3.0.0-beta3
    extra_hosts:
      - signoz:host-gateway
    command:
      - --api.insecure=true
      - --providers.docker
      - --metrics.openTelemetry=true
      - --metrics.openTelemetry.grpc=true
      - --metrics.openTelemetry.insecure=false
      - --metrics.openTelemetry.address=ingest.{region}.signoz.cloud:443
      - --metrics.openTelemetry.headers.signoz-access-token=SIGNOZ_INGESTION_KEY
      - --tracing.openTelemetry=true
      - --tracing.openTelemetry.grpc=true
      - --tracing.openTelemetry.insecure=false
      - --tracing.openTelemetry.address=ingest.{region}.signoz.cloud:443
      - --tracing.openTelemetry.headers.signoz-access-token=SIGNOZ_INGESTION_KEY
    ports:
      - &quot;80:80&quot;
      - &quot;8080:8080&quot;
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
  hello-app:
    image: gcr.io/google-samples/hello-app:2.0
    environment:
      - PORT=8080
    labels:
      traefik.enable: true
      traefik.http.routers.hello-app.rule: Host(`hello-app.docker.localhost`)
      traefik.http.routers.hello-app.entrypoints: http
      traefik.http.routers.hello-app.service: hello-app
</code></pre>
<p>üìù Note</p>
<ul>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the one provided by SigNoz.</li>
<li>Replace <code>{region}</code> with the region of your SigNoz Cloud instance.</li>
</ul>
<p>Refer to the table below for the region-specific endpoints:</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>To start the services, run the following command:</p>
<pre><code>docker compose up -d
</code></pre>
<p>We will visit the <code>hello-app</code> service to generate some traffic.</p>
<pre><code>curl -H Host:hello-app.docker.localhost http://127.0.0.1
</code></pre>
<p>Now, we will visit the SigNoz UI to see the traces and metrics.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Ftutorial%2Ftraefik-traces.webp&amp;w=3840&amp;q=75" alt="Traefik Traces" /></p>
<p>To plot metrics generated from <strong>Traefik</strong>, follow the instructions given in the docs <a href="https://signoz.io/docs/userguide/dashboards/">here</a>.</p>
<p>Check out the <a href="#list-of-metrics">List of metrics from Traefik</a>.</p>
<h2>List of Metrics</h2>
<hr />
<h3>## Traefik Metrics</h3>
<ul>
<li>traefik_config_last_reload_success</li>
<li>traefik_config_reloads_total</li>
<li>traefik_entrypoint_request_duration_seconds_bucket</li>
<li>traefik_entrypoint_request_duration_seconds_count</li>
<li>traefik_entrypoint_request_duration_seconds_sum</li>
<li>traefik_entrypoint_requests_bytes_total</li>
<li>traefik_entrypoint_requests_total</li>
<li>traefik_entrypoint_responses_bytes_total</li>
<li>traefik_open_connections</li>
<li>traefik_service_request_duration_seconds_bucket</li>
<li>traefik_service_request_duration_seconds_count</li>
<li>traefik_service_request_duration_seconds_sum</li>
<li>traefik_service_requests_bytes_total</li>
<li>traefik_service_requests_total</li>
<li>traefik_service_responses_bytes_total</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#recording-exceptions
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-recording-exceptions
group_tracking_ids: docs-userguide-exceptions
<h2>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Errors and Exceptions - Recording exceptions</h2>
<p>Languages that support auto instrumentations(Python, Java, Ruby, Javascript) automatically record exceptions. For other languages, we need to record exceptions manually. Also, in some cases even for an auto-instrumented app, you may want to record custom exceptions manually.</p>
<p>To manually record exceptions, we need to get the span from the tracer and then call a function called <code>recordException()</code> or <code>recordError()</code> which varies across languages. Also, you can set the status of the span to <code>error</code> if the exception means the operation results in an error state.</p>
<p>Below are examples of how to record exceptions in different languages.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#record-exceptions-in-java
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-record-exceptions-in-java
group_tracking_ids: docs-userguide-exceptions
<p>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Record Exceptions in Java:</p>
<pre><code>// Get the current span from the tracer
Span span = Span.current();

// recordException converts a Throwable into a span event.
span.recordException(new RuntimeException(&quot;Something went wrong&quot;));

// Set the status of the span to error
span.setStatus(StatusCode.ERROR, &quot;Something bad happened!&quot;);
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#record-exceptions-in-golang
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-record-exceptions-in-golang
group_tracking_ids: docs-userguide-exceptions
<p>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Record Exceptions in Golang:</p>
<pre><code>import &quot;go.opentelemetry.io/otel/codes&quot;

// Get the current span from the tracer
span := trace.SpanFromContext(ctx)

// RecordError converts an error into a span event.
span.RecordError(err)

// Mark span as failed.
span.SetStatus(codes.Error, &quot;internal error&quot;)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#record-exceptions-in-python
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-record-exceptions-in-python
group_tracking_ids: docs-userguide-exceptions
<p>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Record Exceptions in Python:</p>
<pre><code># Get the current span from the tracer
from opentelemetry import trace
from opentelemetry.trace.status import Status, StatusCode

span = trace.get_current_span()

# record_exception converts the exception into a span event. 
exception = Exception(&quot;Something went wrong&quot;)
span.record_exception(exception)

# Update the span status to failed.
span.set_status(Status(StatusCode.ERROR, &quot;internal error&quot;))
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#record-exceptions-in-javascript
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-record-exceptions-in-javascript
group_tracking_ids: docs-userguide-exceptions
<p>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Record Exceptions in JavaScript:</p>
<pre><code>// import relevant opentelemetry functions
const { trace, SpanStatusCode } = require(&quot;@opentelemetry/api&quot;);

// Get the current span from the tracer
const span = trace.getActiveSpan();
// Create a new sample error
err = new Error(&quot;This is a sample error&quot;);
// recordException converts the error into a span event. 
span.recordException(err);
// Update the span status to failed.
span.setStatus({ code: SpanStatusCode.ERROR, message: String(err) });
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#record-exceptions-in-net
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-record-exceptions-in-net
group_tracking_ids: docs-userguide-exceptions
<p>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Record Exceptions in .NET:</p>
<p>OpenTelemetry .NET provides several options to report Exceptions in Activity. It varies from the most basic option of setting Status, to fully recording the Exception itself to activity. Follow opentelemetry .NET <a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/docs/trace/reporting-exceptions/README.md">documentation</a> to learn more.</p>
<p>Below is one such example:</p>
<pre><code>using (var activity = MyActivitySource.StartActivity(&quot;Foo&quot;))
{
    try
    {
        Func();
    }
    catch (SomeException ex)
    {
        activity?.SetStatus(ActivityStatusCode.Error, ex.message);
        activity?.RecordException(ex);
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#record-exceptions-in-ruby
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-record-exceptions-in-ruby
group_tracking_ids: docs-userguide-exceptions
<p>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Record Exceptions in Ruby:</p>
<pre><code># Import otel sdk
require &quot;opentelemetry/sdk&quot;

# Get the current span from the tracer
span = OpenTelemetry::Trace.current_span

rescue Exception =&gt; e
  # Record the exception and update the span status.
  span.record_exception(e)
  span.status = OpenTelemetry::Trace::Status.error(e.to_s)
end
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#record-exceptions-in-php
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-record-exceptions-in-php
group_tracking_ids: docs-userguide-exceptions
<p>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Record Exceptions in PHP:</p>
<pre><code>//start a root span
$rootSpan = $tracer-&gt;spanBuilder('root')-&gt;startSpan();
//future spans will be parented to the currently active span
$rootScope = $rootSpan-&gt;activate();

try {
    $span1 = $tracer-&gt;spanBuilder('foo')-&gt;startSpan();
    $span1Scope = $span1-&gt;activate();

    try {
        $span2 = $tracer-&gt;spanBuilder('bar')-&gt;startSpan();
        echo 'OpenTelemetry welcomes PHP' . PHP_EOL;
        $span2-&gt;end();
    } finally {
        $span1Scope-&gt;detach();
        $span1-&gt;end();
    }
} catch (Throwable $t) {
    //The library's code shouldn't be throwing unhandled exceptions (it should emit any errors via diagnostic events)
    //This is intended to illustrate a way you can capture unhandled exceptions coming from your app code
    $rootSpan-&gt;recordException($t);
} finally {
    //ensure span ends and scope is detached
    $rootScope-&gt;detach();
    $rootSpan-&gt;end();
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#how-to-view-exceptions
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-how-to-view-exceptions
group_tracking_ids: docs-userguide-exceptions
<h2>Errors and Exceptions: Errors and Exceptions - Recording exceptions: How to View Exceptions?</h2>
<p>The Exceptions tab shows list of exceptions which applications encounter.</p>
<p>It shows the list of exceptions in the applications in a separate page so that users can access them easily. It is also linked to trace pages through which you can see where in a trace is a request facing an exception</p>
<p>You can sort exceptions by Last Seen, First Seen, Count, Exception type and Application name</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fexception-list.webp&amp;w=3840&amp;q=75" alt="exception-list" /></p>
<p>Exception detail page includes the stack trace of the exception, exception attributes and link to the span which caused the exception.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fexception-detail-1.webp&amp;w=3840&amp;q=75" alt="exception-detail-1" /></p>
<p>By clicking <code>errors in the trace page</code> you can see the exceptions in the context of the trace request in which the exception was thrown</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fexception-detail-2.webp&amp;w=3840&amp;q=75" alt="exception-detail-2" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/exceptions/#grouping-exceptions
tag_set: userguide, exceptions
image_urls: 
tracking_id: docs-userguide-exceptions-grouping-exceptions
group_tracking_ids: docs-userguide-exceptions
<h2>Errors and Exceptions: Errors and Exceptions - Recording exceptions: Grouping Exceptions</h2>
<p>By default, exceptions on <strong>Exception List page</strong> are grouped by service name, exception type and exception message. This might result in high cardinality of exception groups, especially if exception messages contains UUIDs or randomly generated IDs.</p>
<p>To reduce the cardinality of the exception grouping, set the <code>low_cardinal_exception_grouping</code> to <code>true</code> in <code>clickhousetraces</code> exporter configuration. This will result in exception grouping by name of service and exception type.</p>
<p>‚úÖ Info</p>
<p>This new grouping strategy will only be applied to new data injected after new instance of <code>otel-collector</code> with updated the environment variable.</p>
<h3>## Docker Standalone and Docker Swarm</h3>
<p>To enable exception grouping, users can set <code>LOW_CARDINAL_EXCEPTION_GROUPING=true</code> as an environment variable for otel collector service in <code>docker-compose.yaml</code>.</p>
<pre><code>services:
  otel-collector:
    environment:
      - LOW_CARDINAL_EXCEPTION_GROUPING=true
</code></pre>
<h3>## Kubernetes (Helm)</h3>
<p>To enable exception grouping, include the following in <code>override-values.yaml</code>:</p>
<pre><code>otelCollector:
  lowCardinalityExceptionGrouping: true
</code></pre>
<p>To install or upgrade SigNoz release with the updated configurations in <code>override-values.yaml</code>:</p>
<pre><code>helm -n platform upgrade \
    --create-namespace --install \
    my-release signoz/signoz \
    -f override-values.yaml
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/infinite-retention-aws-s3/
tag_set: tutorial, infinite-retention-aws-s3
image_urls: https://signoz.io/img/blog/common/join_slack_cta.webp
tracking_id: docs-tutorial-infinite-retention-aws-s3
group_tracking_ids: docs-tutorial-infinite-retention-aws-s3
<h2>Infinite Retention of OpenTelemetry Data in AWS S3</h2>
<h2>Overview</h2>
<hr />
<p>It is a common practice to backup any data for longer durations due to compliance and audit purposes. You can use <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/awss3exporter">AWS S3 Exporter</a> to retain the OpenTelemetry data as long as we need.</p>
<h2>Limitation of AWS S3 Exporter</h2>
<hr />
<ul>
<li>Data in AWS S3 is inaccessible in SigNoz UI</li>
<li>Need to use third-party tool like <strong>Amazon Athena</strong> to query data</li>
<li>Due to limitation of AWS S3 Exporter, you won't be able differentiate different signals like logs, metrics, and traces - hence the need of including different prefixes</li>
</ul>
<p>‚úÖ Info</p>
<p>If you want to query data stored in AWS S3 using <strong>SigNoz</strong> and do not have the requirement for infinite or very long retention period, then use <a href="/docs/userguide/retention-period/#configuring-cold-storage---amazon-s3">SigNoz's AWS S3 Retention</a> instead.</p>
<h2>Prerequisites</h2>
<hr />
<ul>
<li>Running instance of OpenTelemetry Collector (if not running already, see <a href="/docs/install/">Installation Page</a>
)</li>
<li>Access to AWS S3 Bucket either using AWS credentials as environment variables or IAM roles for ECS tasks or EC2 instances (for more details, <a href="/docs/userguide/retention-period/#configuring-cold-storage---amazon-s3">see here</a>
)</li>
</ul>
<h2>Adding AWS S3 Exporter</h2>
<hr />
<p>In our example, we will use <code>awss3</code> exporter for retaining logs data, where we will be using <code>us-east-1</code> region and <code>otel-data-backup</code> bucket.</p>
<pre><code>exporters:
  awss3/logs:
    s3uploader:
      region: 'us-east-1'
      s3_bucket: 'otel-data-backup'
      s3_prefix: 'logs'
      s3_partition: 'minute'
service:
  pipelines:
    logs:
      exporters: [otlp, awss3/logs]
</code></pre>
<p>Similarly, we can add it for <code>metrics</code> and <code>traces</code> pipelines as well with different prefixes to retain metrics and traces data respectively.</p>
<p>The above configuration needs to be added for your respective OtelCollector(s).</p>
<ul>
<li>In case of <strong>SigNoz Self-hosted</strong>, you can add it in SigNoz OtelCollector configuration to retain all incoming data of <code>logs</code> pipelines.</li>
<li>In case of <strong>SigNoz Cloud</strong>, you will need to update all OtelCollector configuration that is sending data.</li>
<li>For <strong>K8s-Infra</strong> helm chart, you can add it in the <code>otelAgent</code> configuration.</li>
<li>For any <strong>Standalone OtelCollector</strong> deployments that is directly sending data to SigNoz, you can add it in its respective configuration.</li>
</ul>
<p>List of all supported configuration of <code>awss3</code> exporter can be found <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/awss3exporter">here</a>.</p>
<hr />
<p>If you have any feedback or facing issues, feel free to join our slack community to get help!</p>
<p><a href="https://signoz.io/slack"><img src="https://signoz.io/img/blog/common/join_slack_cta.webp" alt="SigNoz Slack community" /></a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/
tag_set: operate, clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse
group_tracking_ids: docs-operate-clickhouse
<h2>ClickHouse</h2>
<p>The following sections provide instructions to operate ClickHouse. Based on your environment, proceed to one of the sections below.</p>
<ul>
<li><a href="/docs/operate/clickhouse/increase-clickhouse-pv/">üìÑÔ∏è Increase ClickHouse PV: You can use the following helm upgrade command to increase the size of the persistent...</a></li>
<li><a href="/docs/operate/clickhouse/connect-to-clickhouse/">üìÑÔ∏è Connect to ClickHouse: Follow the instructions below for connecting to your ClickHouse...</a></li>
<li><a href="/docs/operate/clickhouse/distributed-clickhouse/">üìÑÔ∏è Distributed ClickHouse: In v0.12, SigNoz introduces support for distributed clickhouse...</a></li>
<li><a href="/docs/operate/clickhouse/external-clickhouse/">üìÑÔ∏è External ClickHouse: It is recommended to use the ClickHouse instance that is deployed along...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/ingestion/signoz-cloud/keys/
tag_set: ingestion, signoz-cloud, keys
image_urls: https://signoz.io/img/docs/ingestion/keys.png, https://signoz.io/img/docs/ingestion/limits.png
tracking_id: docs-ingestion-signoz-cloud-keys
group_tracking_ids: docs-ingestion-signoz-cloud-keys
<h2>Ingestion Keys</h2>
<h2>Overview</h2>
<hr />
<p>Ingestion Keys are secure tokens used to authenticate your telemetry Ingestion requests. These keys are unique to your account and should be kept confidential.</p>
<h2>Add an Ingestion Key</h2>
<hr />
<p>To add a SigNoz Ingestion key:</p>
<ol>
<li>Navigate to <strong>Settings</strong> in the sidebar, then select the <strong>Ingestion Settings</strong> tab.</li>
<li>Click the <strong>New Ingestion Key</strong> button.</li>
<li>Enter a name for your key.</li>
<li>(<strong>Optional</strong>) Set an expiration date for the key.</li>
<li>(<strong>Optional</strong>) Add tags to categorize or organize your key.</li>
<li>Click <strong>Create new Ingestion key</strong>.</li>
</ol>
<p><img src="https://signoz.io/img/docs/ingestion/keys.png" alt="Ingestion Settings in SigNoz Settings" /></p>
<p><em>Ingestion Settings in SigNoz Settings</em></p>
<p>Limits and Restrictions:</p>
<ul>
<li>Only administrators can manage Ingestion keys.</li>
<li>There is a maximum limit of 50 Ingestion keys per account.</li>
<li>Key names must be unique within your account.</li>
</ul>
<h2>Modify Ingestion Keys</h2>
<hr />
<p>To modify an existing SigNoz Ingestion key, navigate to Settings &gt; Ingestion Settings and click the edit icon next to the relevant key. Please note that only the expiry date and tags can be modified.</p>
<h2>Remove Ingestion Keys</h2>
<hr />
<p>To remove a SigNoz Ingestion key, go to <strong>Settings &gt; Ingestion Settings</strong>. Users with the appropriate permissions can click the Trash icon next to the key they wish to remove. A confirmation prompt will appear before the key is deleted. Please be aware that this action is irreversible and will immediately revoke the Ingestion key.</p>
<h2>Add Limits to a Key</h2>
<hr />
<p>SigNoz offers the flexibility to set limits on your Ingestion keys. To add a limit to an existing key:</p>
<ol>
<li>Click on the target key to open its dropdown menu.</li>
<li>In the dropdown, select the <strong>signal</strong> to which you want to apply the limit.</li>
<li>Click on <strong>+ Limits</strong>.</li>
<li>Specify an appropriate limit value.</li>
<li>Click on <strong>Save</strong>.</li>
</ol>
<p><img src="https://signoz.io/img/docs/ingestion/limits.png" alt="Limits for an Ingestion Key" /></p>
<p><em>Limits for an Ingestion Key</em></p>
<h2>Modify a Limit</h2>
<hr />
<p>To modify an existing limit, navigate to the Ingestion key and click the edit option corresponding to the limit in question. It's important to note that modifying a limit does not reset the accumulated values. For example, if the initial limit was 20GB/day and you've used 15GB before increasing the limit to 25GB/day, the usage count will continue from 15GB.</p>
<h2>Remove a Limit</h2>
<hr />
<p>To remove a limit, navigate to the Ingestion key and click the delete option next to the relevant limit. Deleting a limit will reset all accumulated values. For instance, if the limit was set to 20GB/day and you've used 15GB before deleting the limit, the counter will be reset to 0. Any new limit created thereafter will start from 0.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/query-service/
tag_set: operate, query-service
image_urls: 
tracking_id: docs-operate-query-service
group_tracking_ids: docs-operate-query-service
<h2>Query Service</h2>
<p>The following sections provide instructions to operate Query Service. You can proceed to one of the sections below.</p>
<ul>
<li><a href="/docs/operate/query-service/reset-admin-password/">üìÑÔ∏è Reset Admin Password: In case you have forgotten the root admin password...</a></li>
<li><a href="/docs/operate/query-service/user-invitation-smtp/">üìÑÔ∏è Enable SMTP for User Invitations: The following environment variables need to be set for query-service to send...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/community/llm-monitoring/
tag_set: community, llm-monitoring
image_urls: 
tracking_id: docs-community-llm-monitoring
group_tracking_ids: docs-community-llm-monitoring
<h2>LLM Monitoring</h2>
<p>Welcome to the Generative AI section! Here, you'll find tutorials contributed by the SigNoz community to help you make the most out of SigNoz.</p>
<h3>## LLM Observability with OpenTelemetry and SigNoz</h3>
<p>Learn how to do manual and automatic instrumentation of LangChain LLM applications using OpenTelmetry and monitor them using dashboards in SigNoz.</p>
<p>üëâ <a href="https://signoz.io/blog/llm-observability/#opentelemetry-for-llm-observability">link</a></p>
<h3>## Integrating Langtrace with SigNoz</h3>
<p>Ship your traces from your LLM applications generated using Langtrace and visualize them in SigNoz.</p>
<p>üëâ <a href="https://docs.langtrace.ai/supported-observability/signoz">link</a></p>
<h3>## LLM Observability with SigNoz and OpenLLMetry</h3>
<p>Monitor and debug your LLM application using OpenLLMetry and SigNoz.</p>
<p>üëâ <a href="https://www.traceloop.com/docs/openllmetry/integrations/signoz">link</a></p>
<h3>## OpenTelemetry-native Observability for LLMs with SigNoz and OpenLIT</h3>
<p>Monitor your LLM applications and Vector Databases using an OTel-native collector, and start visualizing the data in SigNoz with a prebuilt <a href="https://raw.githubusercontent.com/openlit/openlit/main/assets/signoz-openlit.json">dashboard</a>.</p>
<p>üëâ <a href="https://docs.openlit.io/latest/connections/signoz">link</a></p>
<p>Want to get your tutorial added to this list? Just raise a PR on <a href="https://github.com/SigNoz/signoz-web">signoz-web GitHub</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/feature-flags/
tag_set: operate, feature-flags
image_urls: 
tracking_id: docs-operate-feature-flags
group_tracking_ids: docs-operate-feature-flags
<h2>Feature Flags</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<h2>Available Feature Flags</h2>
<hr />
<p><code>TIMESTAMP_SORT_FEATURE</code> and <code>DURATION_SORT_FEATURE</code> are enabled by default.</p>
<ul>
<li>
<p><code>TIMESTAMP_SORT_FEATURE</code>: When this feature is enabled, then SigNoz creates a materialized view table to optimize sorting spans/traces by timestamp at cost of increasing storage by 40-50% of <code>signoz_index_v2</code> table.</p>
</li>
<li>
<p><code>DURATION_SORT_FEATURE</code>: When this feature is enabled, then SigNoz creates a projection over table to optimize sorting spans/traces by duration at cost of increasing storage by 40-50% of <code>signoz_index_v2</code> table. This also speeds up loading of duration trace filters by upto 90%.</p>
</li>
</ul>
<p>üìù Note</p>
<p>To toggle features, configs have to be added on both OTel Collector and Query Service</p>
<h2>Adding configs to OTel collector</h2>
<hr />
<p>Features can be enabled or disabled via the arguments to OTel Collector with the¬†<code>--feature-gates</code> flag.</p>
<p>‚úÖ Info</p>
<p>When using the <code>--feature-gates</code> flag, feature identifiers must be presented as a comma-delimited list. Feature identifiers prefixed with¬†<code>-</code> will disable the feature, and prefixing with¬†<code>+</code> or with no prefix will enable the feature.</p>
<p>To disable both <code>DURATION_SORT_FEATURE</code> and <code>TIMESTAMP_SORT_FEATURE</code> , you need to update the <code>docker-compose.yaml</code> file of SigNoz installation.</p>
<p>Replace <a href="https://github.com/SigNoz/signoz/blob/65af8c1b98d85469da6fdb40584df24457c9dbb4/deploy/docker/clickhouse-setup/docker-compose.yaml#L85">this line</a> in your file with below line:</p>
<pre><code>command: [&quot;--config=/etc/otel-collector-config.yaml&quot;, &quot;--feature-gates=-DURATION_SORT_FEATURE,-TIMESTAMP_SORT_FEATURE&quot;]
</code></pre>
<h2>Adding configs to Query Service</h2>
<hr />
<p>We need to set environment variables to toggle features. Add new environment variables <a href="https://github.com/SigNoz/signoz/blob/65af8c1b98d85469da6fdb40584df24457c9dbb4/deploy/docker/clickhouse-setup/docker-compose.yaml#L52">here</a> to enable or disable features:</p>
<pre><code>- DURATION_SORT_FEATURE=false
- TIMESTAMP_SORT_FEATURE=false
</code></pre>
<p>On this page</p>
<p><a href="#available-feature-flags">Available Feature Flags</a></p>
<p><a href="#adding-configs-to-otel-collector">Adding configs to OTel collector</a></p>
<p><a href="#adding-configs-to-query-service">Adding configs to Query Service</a></p>
<p>Docs</p>
<p><a href="/docs/">Introduction</a>
<a href="/docs/contributing/">Contributing</a></p>
<p><a href="https://knowledgebase.signoz.io/kb">Knowledge Base</a></p>
<p><a href="/api_reference/">SigNoz API</a></p>
<p>Community</p>
<p><a href="/support/">Support</a></p>
<p><a href="https://signoz.io/slack">Slack</a></p>
<p><a href="https://twitter.com/SigNozHQ">Twitter</a></p>
<p><a href="https://community-chat.signoz.io/">Community Archive</a></p>
<p><a href="/changelog/">Changelog</a></p>
<p>More</p>
<p><a href="/product-comparison/signoz-vs-datadog/">SigNoz vs Datadog</a>
<a href="/product-comparison/signoz-vs-newrelic/">SigNoz vs New Relic</a>
<a href="/product-comparison/signoz-vs-grafana/">SigNoz vs Grafana</a>
<a href="/product-comparison/signoz-vs-dynatrace/">SigNoz vs Dynatrace</a></p>
<p><a href="https://jobs.gem.com/signoz">Careers</a></p>
<p><a href="/about-us/">About</a>
<a href="/terms-of-service/">Terms</a>
<a href="/privacy/">Privacy</a>
<a href="https://trust.signoz.io/">Security &amp; Compliance</a></p>
<p><img src="https://signoz.io/img/SigNozLogo-orange.svg" alt="" /></p>
<p>SigNoz</p>
<p>All systems operational</p>
<p><a href="https://github.com/SigNoz"></a>
<a href="https://www.linkedin.com/company/signozio/"></a>
<a href="https://signoz.io/slack"></a>
<a href="https://twitter.com/SigNozHQ"></a>
<a href="https://www.youtube.com/@signoz"></a></p>
<p><img src="https://signoz.io/svgs/icons/SOC-2.svg" alt="" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/faqs/
tag_set: faqs
image_urls: 
tracking_id: docs-faqs
group_tracking_ids: docs-faqs
<h2>FAQs</h2>
<p>Find the most commonly questions about SigNoz Installation, Instrumentation, Features, Troubleshooting, and Contributing here:</p>
<ul>
<li><a href="/docs/faqs/product">üìÑÔ∏è Product - FAQs: Frequently asked question about Product</a></li>
<li><a href="/docs/faqs/troubleshooting">üìÑÔ∏è Troubleshooting - FAQs: Frequently asked question about Troubleshooting</a></li>
<li><a href="/docs/faqs/instrumentation">üìÑÔ∏è Instrumentation - FAQs: Frequently asked question about Instrumentation</a></li>
<li><a href="/docs/faqs/installation">üìÑÔ∏è Installation - FAQs: Frequently asked question about Installation</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#overview
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: 
tracking_id: docs-gcp-monitoring-gcp-fns-logging-overview
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<h2>Cloud Functions Logging: Cloud Functions Logging - Overview</h2>
<p>This documentation provides a detailed walkthrough on how to set up a Google Cloud Function to send the logs directly to SigNoz. By the end of this guide, you will have a setup that automatically sends your Cloud Function logs to SigNoz.</p>
<p>SigNoz CloudSelf-Host</p>
<p>Using Pub/Sub (Recommended)Using HTTP</p>
<p><strong>Here's a quick summary of what we will be doing in this guide</strong></p>
<ul>
<li>Create and configure a Cloud Function</li>
<li>Create a Pub/Sub topic</li>
<li>Create a Log Router to route the Cloud Functions logs to SigNoz</li>
<li>Create a Compute Engine instance</li>
<li>Create OTel Collector to route logs from Pub/Sub topic to SigNoz Cloud</li>
<li>Invoke the Cloud Function using Trigger</li>
<li>Send and Visualize the logs in SigNoz Cloud</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#prerequisites
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: https://signoz.io/img/docs/gcp-monitoring/cloud-logs/cloud-functions-pubsub.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/create-function-pubsub.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-entrypoint-pubsub.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-complete-code-pubsub.webp
tracking_id: docs-gcp-monitoring-gcp-fns-logging-prerequisites
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<h2>Cloud Functions Logging: Prerequisites</h2>
<ul>
<li><a href="https://console.cloud.google.com/">Google Cloud account</a> with administrative privilege or Cloud Functions Admin privilege.</li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud Account</a>
(we are using SigNoz Cloud for this demonstration, we will also need ingestion details. To get your <strong>Ingestion Key</strong> and <strong>Ingestion URL,</strong> sign-in to your SigNoz Cloud Account and go to <strong>Settings</strong> &gt;&gt; <strong>Ingestion Settings</strong>)</li>
<li>Access to a project in GCP</li>
<li>Google Cloud Functions APIs enabled (follow <a href="https://support.google.com/googleapi/answer/6158841?hl=en">this</a> guide to see how to enable an API in Google Cloud)</li>
</ul>
<h3>## Get started with Cloud Function Configuration</h3>
<p>Follow these steps to create the Cloud Function:</p>
<p>Step 1: Go to your GCP console and search for Cloud Functions, go to Functions and click on <strong>CREATE FUNCTION</strong>.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/cloud-functions-pubsub.webp" alt="GCP Cloud Function" /></p>
<p>_</p>
<p>GCP Cloud Functions</p>
<p>_</p>
<p>Step 2: Fill in the following details to create a Cloud Function:</p>
<ol>
<li>Environment: 2nd gen</li>
<li>Function name: Name for the Cloud Function</li>
<li>Region: Takes the default region of the GCP account</li>
<li>Trigger: Defines how to trigger the Cloud Function
<ol>
<li>Trigger Type: HTTPS - this allows us to trigger the Cloud Function using a URL</li>
<li>Authentication: Choose whether you need authenticated or unauthenticated invocations. We have chosen unauthenticated invocation for this demonstration.</li>
</ol>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/create-function-pubsub.webp" alt="Create Cloud function" /></p>
<p>_</p>
<p>Create Cloud Function</p>
<p>_</p>
<p>Step 3: Click on the <strong>NEXT</strong> button, which will bring us to the page where we can add our code.</p>
<p>Select <code>Runtime</code> as <code>Python 3.10</code>.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-entrypoint-pubsub.webp" alt="Runtime and Entry Point of Cloud Function" /></p>
<p>_</p>
<p>Set entrypoint and source code</p>
<p>_</p>
<h3>## Add code to the Google Cloud Function</h3>
<p>Below is the comprehensive code of the <code>main.py</code> file, followed by a high level overview of what the code is doing.</p>
<pre><code>import functions_framework

@functions_framework.http
def hello_http(request):

    print(&quot;Initializing Function...&quot;)

    request_json = request.get_json(silent=True)
    request_args = request.args

    if request_json and 'name' in request_json:
        name = request_json['name']
        print(&quot;name is in request_json&quot;)

    elif request_args and 'name' in request_args:
        name = request_args['name']
        print(&quot;name is in request_args&quot;)

    else:
        print(&quot;No name found.&quot;)
        name = 'World'

    print(&quot;Sending response...&quot;)

    return 'Hello {}!'.format(name)
</code></pre>
<p>Below is a high-level overview of the above code snippet:</p>
<ul>
<li><strong>hello_http(request)</strong>: Handles incoming HTTP requests, extracts 'name' from the request, and logs various stages of execution.</li>
<li><strong>@funtions_framework.http</strong>: Decorator that defines <code>hello_http</code> as an HTTP handler for Google Cloud Functions.</li>
<li>The code print out different messages for log entries and returns a greeting message.</li>
</ul>
<p>There are no changes to requirements.txt file.</p>
<p>Once you‚Äôve finished writing your code, locate the <strong>DEPLOY</strong> button. After clicking the <strong>DEPLOY</strong> button, Google Cloud Function initiates deployment, starts provisioning the function according to the specified configuration, initializes the environment, handles dependencies, and makes the function ready to handle incoming requests.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-complete-code-pubsub.webp" alt="Complete Cloud Function Code" /></p>
<p>_</p>
<p>Complete Cloud Function Code</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#testing-your-cloud-function
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-navigate-to-trigger.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/cloud-functions-url-pubsub.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-logs-pubsub.webp
tracking_id: docs-gcp-monitoring-gcp-fns-logging-testing-your-cloud-function
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<h2>Cloud Functions Logging: Testing your cloud function: Testing your cloud function</h2>
<p>Step 1: After completing the deployment, navigate to the <strong>TRIGGER</strong> section to obtain the URL to invoke the function.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-navigate-to-trigger.webp" alt="Navigate to Trigger" /></p>
<p>_</p>
<p>Navigate to Trigger</p>
<p>_</p>
<p>Step 2: Hit the URL that you have obtained, you will see the function output.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/cloud-functions-url-pubsub.webp" alt="Cloud Function URL" /></p>
<p>_</p>
<p>Cloud Function URL</p>
<p>_</p>
<p>Step 3: Click on the <strong>LOGS</strong> section in your Cloud Function to view the logs, which will indicate that the log has been sent to SigNoz successfully.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-logs-pubsub.webp" alt="Cloud Function Logs" /></p>
<p>_</p>
<p>Viewing Cloud Function Logs</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#create-a-pubsub-topic
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: https://signoz.io/img/docs/gcp-monitoring/cloud-logs/cloud-functions-pubsub.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-create-pubsub-topic.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-pubsub-topic-created.webp
tracking_id: docs-gcp-monitoring-gcp-fns-logging-create-a-pubsub-topic
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<p>Cloud Functions Logging: Testing your cloud function: Create a Pub/Sub topic</p>
<p>Follow these steps to create the Pub/Sub topic:</p>
<p>Step 1: Go to your GCP console and search for Pub/Sub, go to Pub/Sub and click on <strong>CREATE TOPIC</strong>.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/cloud-functions-pubsub.webp" alt="Navigate to Pub/Sub" /></p>
<p>_</p>
<p>Navigate to Pub/Sub</p>
<p>_</p>
<p>Step 2: Create the Pub/Sub topic</p>
<p>Put an appropriate Pub/Sub topic ID. Select <code>Add a default subscription</code> checkbox. Under Encryption, let the default selection of <code>Google-managed encryption key</code> as is. Click on <code>Create</code>.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-create-pubsub-topic.webp" alt="Create Pub/Sub Topic" /></p>
<p>_</p>
<p>Create Pub/Sub Topic</p>
<p>_</p>
<p>Step 3: Ensure the Pub/Sub topic and its subscription is created.</p>
<p>Once you click on the <code>Create</code> button in Step 2, you will land on the newly created Pub/Sub topic's page. Ensure that the default subscription is created as well. The subscription will be of the form <code>&lt;topic-name&gt;-sub</code>.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-pubsub-topic-created.webp" alt="Created Pub/Sub Topic" /></p>
<p>_</p>
<p>Created Pub/Sub Topic</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#create-log-router-to-pubsub-topic
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-log-explorer.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-logs-filter.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-create-sink.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-sink-details.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-sink-destination.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-log-sink-filter.webp
tracking_id: docs-gcp-monitoring-gcp-fns-logging-create-log-router-to-pubsub-topic
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<p>Cloud Functions Logging: Testing your cloud function: Create Log Router to Pub/Sub Topic</p>
<p>Follow these steps to create the Log Router to the newly created Pub/Sub topic:</p>
<p>Step 1: On the GCP console, search for Logs and go to Logs Explorer. Select an appropriate relative time.</p>
<p>You will start seeing the recent logs.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-log-explorer.webp" alt="Logs Explorer" /></p>
<p>_</p>
<p>Log Explorer</p>
<p>_</p>
<p>Step 2: To ensure you see only Cloud Function logs, add the following query in the Query textbox:</p>
<pre><code>resource.type=&quot;cloud_run_revision&quot;
</code></pre>
<p>In case you want the logs only from a particular Cloud Function, you can add the following query to the Query text box:</p>
<pre><code>resource.type=&quot;cloud_run_revision&quot;
resource.labels.service_name=&quot;&lt;function-name&gt;&quot;
</code></pre>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-logs-filter.webp" alt="Filter Cloud Functions Logs" /></p>
<p>_</p>
<p>Filter Cloud Functions Logs</p>
<p>_</p>
<p>Step 3: With the log filters in place, click on <code>More actions</code> dropdown below the Query text box, and select <code>Create sink</code> option from the dropdown.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-create-sink.webp" alt="Create Log Sink" /></p>
<p>_</p>
<p>Create Log Sink</p>
<p>_</p>
<p>Step 4: Provide appropriate sink name and description, and click <code>Next</code>.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-sink-details.webp" alt="Provide Sink Details" /></p>
<p>_</p>
<p>Provide Sink Details</p>
<p>_</p>
<p>Step 5: In the &quot;Sink destination&quot; section, select <code>Cloud Pub/Sub topic</code> from the &quot;Select sink service&quot; dropdown, and in the &quot;Select a Cloud Pub/Sub topic&quot; dropdown, select the Pub/Sub topic that we had created in the earlier section. Click on <code>Next</code>.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-sink-destination.webp" alt="Provide Sink Destination" /></p>
<p>_</p>
<p>Provide Sink Destination</p>
<p>_</p>
<p>Step 6: In the &quot;Choose logs to include in sink&quot; section, ensure appropriate log filtering query is present. Click on <code>Next</code>.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-log-sink-filter.webp" alt="Provide Log Filter Rules" /></p>
<p>_</p>
<p>Provide Log Filter Rules</p>
<p>_</p>
<p>Step 7: We need not make any change in the <code>Choose logs to filter out of sink</code> section, and click on <code>Create sink</code> at the bottom.</p>
<p>Step 8: With this, the log router sink will be created. You will get the message <code>Your log sink was successfully created. Data should be available soon.</code> on the page.</p>
<p>You can also go to this sink at later point of time by navigating to the <code>Log router</code> from the left navigation menu in the GCP Logging service.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#provide-permissions-to-log-router-sink
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-log-router-metrics.webp, https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-pubsub-topic-metrics.webp
tracking_id: docs-gcp-monitoring-gcp-fns-logging-provide-permissions-to-log-router-sink
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<p>Cloud Functions Logging: Testing your cloud function: Provide permissions to Log Router Sink</p>
<ul>
<li>On the GCP console, search for Log Router, and navigate to Logs Router in the GCP Logging service.</li>
<li>Click on the 3 dots to the right of the Log Router sink that we created in the earlier steps, and select <code>View sink details</code> from the dropdown.</li>
<li>From the sink details, copy the <code>Writer identity</code> and keep it with you. We will require this in the upcoming steps.</li>
<li>In order to provide the Log Router sink with the permissions to write to the Pub/Sub topic, navigate to the Pub/Sub service, and click on the 3 dots to the right of the Pub/Sub topic where the router is going to sink the logs. Select <code>View permissions</code> from this dropdown.</li>
<li>Click on <strong>APP PRINCIPAL</strong> button on the top. In the <code>New principals</code> textbox, copy the value from the <code>Writer identity</code> (remove the <code>serviceAccount:</code> prefix from the value), and select any value from the dropdown that appears. In the <code>Select a role</code> dropdown, search for <code>Pub/Sub Publisher</code> and select it.</li>
<li>Click on <code>Save</code>. With this, the Log Router sink now has the permission to write to the Pub/Sub topic.</li>
<li>Wait for ~1 minute for the permissions to take effect, and now trigger the Cloud Function a few times. You should see the Log Router's volume increasing, as well as Pub/Sub topic's metrics showing up published requests.</li>
</ul>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-log-router-metrics.webp" alt="Log Router Metrics" /></p>
<p>_</p>
<p>Log Router Metrics</p>
<p>_</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-pubsub-topic-metrics.webp" alt="Pub/Sub Topic Metrics" /></p>
<p>_</p>
<p>Pub/Sub Topic Metrics</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#creating-otel-collector
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: 
tracking_id: docs-gcp-monitoring-gcp-fns-logging-creating-otel-collector
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<p>Cloud Functions Logging: Testing your cloud function: Creating OTel Collector</p>
<p>We will create the Compute Engine instance, and then run OTel Collector on it.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#creating-compute-engine-vm-instance
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: 
tracking_id: docs-gcp-monitoring-gcp-fns-logging-creating-compute-engine-vm-instance
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<p>Cloud Functions Logging: Testing your cloud function: Creating Compute Engine (VM) instance</p>
<ul>
<li>On the GCP Console, search for <code>Compute Engine</code>, and navigate to the <code>Compute Engine</code> GCP service. Click on <strong>CREATE INSTANCE</strong> at the top.</li>
<li>Put an appropriate name for the instance, and select an appropriate region and zone.</li>
<li>Select an appropriate machine configuration. Here, we have selected E2 machine.</li>
<li>In the machine type, we have selected <code>e2-medium</code>. You can choose a larger machine in case you want to run multiple processes on this machine.</li>
<li>Coming to the <code>Boot Disk</code> section, click on <strong>CHANGE</strong> button.</li>
<li>Select the <code>Ubuntu</code> operating system, and the version as <code>Ubuntu 22.04 LTS x86/64, amd64 image</code>. You can put the disk size as per your requirement. We are going with <code>20GB</code> disk size for this example. Click on <code>Save</code>.</li>
<li>In the <code>Identity and API Access</code> section, select the service account that has atleast <code>Pub/Sub Editor</code> permission.</li>
<li>In the <code>Firewall</code> section, select <code>Allow HTTP access</code> and <code>Allow HTTPS access</code>.</li>
<li>Click on <strong>CREATE</strong> button at the bottom, and wait for the instance to get created.</li>
<li>Once the instance is created, click on the <code>SSH</code> button to connect to the instance. Click on the <code>Authorize</code> button on the popup. With this you will enter into the terminal of the instance.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/logging/#install-otel-collector-as-agent
tag_set: gcp-monitoring, gcp-fns, logging
image_urls: https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-signoz-cloud-logs.webp
tracking_id: docs-gcp-monitoring-gcp-fns-logging-install-otel-collector-as-agent
group_tracking_ids: docs-gcp-monitoring-gcp-fns-logging
<p>Cloud Functions Logging: Testing your cloud function: Install OTel Collector as agent</p>
<p>Firstly, we will establish the authentication using the following commands:</p>
<ol>
<li>
<p>Initialize <code>gcloud</code>:</p>
<p>gcloud init</p>
</li>
<li>
<p>Authenticate into GCP:</p>
<p>gcloud auth application-default login</p>
</li>
</ol>
<p>Let us now proceed to the OTel Collector installation:</p>
<p>Step 1: Download otel-collector tar.gz for your architecture</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.88.0/otelcol-contrib_0.88.0_linux_amd64.tar.gz
</code></pre>
<p>Step 2: Extract otel-collector tar.gz to the otelcol-contrib folder</p>
<pre><code>mkdir otelcol-contrib &amp;&amp; tar xvzf otelcol-contrib_0.88.0_linux_amd64.tar.gz -C otelcol-contrib
</code></pre>
<p>Step 3: Create <code>config.yaml</code> in the folder <code>otelcol-contrib</code> with the below content in it. Replace <code>&lt;region&gt;</code> with the appropriate SigNoz Cloud region. Replace <code>SIGNOZ_INGESTION_KEY</code> with what is provided by SigNoz:</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  googlecloudpubsub:
    project: &lt;gcp-project-id&gt;
    subscription: projects/&lt;gcp-project-id&gt;/subscriptions/functions-log-sink-sub
    encoding: raw_text
processors:
  batch: {}
exporters:
  otlp:
    endpoint: &quot;ingest.&lt;region&gt;.signoz.cloud:443&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;&lt;SigNoz-Key&gt;&quot;
service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp, googlecloudpubsub]
      processors: [batch]
      exporters: [otlp]
</code></pre>
<p>Step 4: Once we are done with the above configurations, we can now run the collector service with the following command:</p>
<p>From the otelcol-contrib, run the following command:</p>
<pre><code>./otelcol-contrib --config ./config.yaml
</code></pre>
<p><strong>Run in background</strong></p>
<p>If you want to run OTel Collector process in the background:</p>
<pre><code>./otelcol-contrib --config ./config.yaml &amp;&gt; otelcol-output.log &amp; echo &quot;$!&quot; &gt; otel-pid
</code></pre>
<p>The above command sends the output of the otel-collector to <code>otelcol-output.log</code> file and prints the process id of the background running OTel Collector process to the <code>otel-pid</code> file.</p>
<p>If you want to see the output of the logs you‚Äôve just set up for the background process, you may look it up with:</p>
<pre><code>tail -f -n 50 otelcol-output.log
</code></pre>
<p>You can now trigger the Cloud Function a few times, and see the logs from the GCP Cloud Functions on SigNoz.</p>
<p><img src="https://signoz.io/img/docs/gcp-monitoring/cloud-logs/functions-signoz-cloud-logs.webp" alt="Functions Logs in SigNoz Cloud" /></p>
<p>_</p>
<p>Functions Logs in SigNoz Cloud</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/hostmetrics/
tag_set: userguide, hostmetrics
image_urls: https://signoz.io/img/hostmetrics-cloud-architecture.webp, https://signoz.io/img/hostmetrics-new-dashboard.webp, https://signoz.io/img/hostmetrics-load-json.webp, https://signoz.io/img/hostmetrics-select-hostname.webp, https://signoz.io/img/hostmetrics-configure-general-section.webp, https://signoz.io/img/hostmetrics-configure-variables-section.webp
tracking_id: docs-userguide-hostmetrics
group_tracking_ids: docs-userguide-hostmetrics
<h2>Hostmetrics Dashboard</h2>
<h2>Overview</h2>
<hr />
<p>The <strong>Host Metrics dashboard</strong> consists of charts for monitoring instance metrics and is designed as a generic dashboard with a hostname variable. You can access detailed information on common host system metrics such as:</p>
<ul>
<li>CPU Usage</li>
<li>Memory Usage</li>
<li>Disk I/O</li>
<li>Filesystem Usage</li>
<li>Network I/O</li>
</ul>
<p>A full list of supported hostmetrics can be found <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#getting-started">here</a>.</p>
<h2>Setup</h2>
<hr />
<p>You can choose from the two options below.</p>
<p>SigNoz CloudSelf-Host</p>
<p>A receiver serves as the entry point for data into the OpenTelemetry Collector. The Host Metrics receiver collects metrics about the host system from various sources, such as files located in paths like <code>/sys/block/sda/size</code> and <code>/proc/...</code> on the host machine. It reads and formats the data from these sources into an OTLP-compatible format. This receiver is intended to be used when the OpenTelemetry Collector is deployed as an agent.</p>
<p><img src="https://signoz.io/img/hostmetrics-cloud-architecture.webp" alt="The HostMetrics collection process for SigNoz Cloud" /></p>
<p><em>HostMetrics collection process for SigNoz Cloud</em></p>
<p>Follow the below steps to set up the HostMetrics Dashboard on the SigNoz UI.</p>
<p><strong>Step 1</strong> : Setup OTel Collector as an agent</p>
<p>Follow the steps in <strong>SigNoz Cloud section</strong> of this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">tutorial</a> to get your Otel Collector agent up and running.</p>
<p><strong>Step 2</strong> : Download/Copy the hostmetrics JSON file</p>
<p>Hostmetric JSON file will help you create a pre-built dashboard about all the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#getting-started">categories</a> of host metrics provided by hostmetrics receiver.</p>
<p>Use this <a href="https://github.com/SigNoz/dashboards/blob/main/hostmetrics/hostmetrics.json">GitHub Link</a> to download/copy the <code>hostmetrics.json</code> file.</p>
<p><strong>Step 3</strong> : Import hostmetrics JSON file to SigNoz Cloud</p>
<p>Open your SigNoz Cloud UI and go to the <strong>Dashboards</strong> section. In the top right corner, click on the <strong>+ New Dashboard</strong> button and select the Import JSON option from the dropdown menu.</p>
<p><img src="https://signoz.io/img/hostmetrics-new-dashboard.webp" alt="How to create a New Hostmetrics Dashboard in SigNoz UI" /></p>
<p><em>Create a New Dashboard by importing JSON</em></p>
<p>Upload/paste your <code>hostmetrics.json</code> data and click on <strong>Load JSON</strong> button</p>
<p><img src="https://signoz.io/img/hostmetrics-load-json.webp" alt="Load the Hostmetrics Dashboard JSON file in SigNoz UI" /></p>
<p><em>Load the Hostmetrics JSON file</em></p>
<p>After loading the JSON, you can find your hostname in the <strong><em>$hostname</em></strong> dropdown menu. Select it, and you'll access the Hostmetrics Dashboard for the VM where you've configured the OTel collector.</p>
<p><img src="https://signoz.io/img/hostmetrics-select-hostname.webp" alt="Select hostname from Hostmetrics Dashboard in SigNoz UI" /></p>
<p><em>Select hostname from the dropdown</em></p>
<h2>Dashboard Configurations</h2>
<hr />
<p>Click on <strong>Configure</strong> button at top right corner to change the name, add/remove tags, edit description, and add more variables to your dashboard.</p>
<p><img src="https://signoz.io/img/hostmetrics-configure-general-section.webp" alt="Select hostname from Hostmetrics Dashboard in SigNoz UI" /></p>
<p><em>Hostmetrics General Settings</em></p>
<p><img src="https://signoz.io/img/hostmetrics-configure-variables-section.webp" alt="Select hostname from Hostmetrics Dashboard in SigNoz UI" /></p>
<p><em>Hostmetrics Variables Settings</em></p>
<p>To explore more dashboard functionalities, check out the <a href="https://signoz.io/docs/userguide/manage-dashboards-and-panels/">Dashboard Section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/kubernetes-infra-metrics/
tag_set: tutorial, kubernetes-infra-metrics
image_urls: 
tracking_id: docs-tutorial-kubernetes-infra-metrics
group_tracking_ids: docs-tutorial-kubernetes-infra-metrics
<h2>Kubernetes Infra Metrics and Logs Collection</h2>
<h3>## Overview</h3>
<p>To export Kubernetes metrics, you can enable different receivers in OpenTelemetry collector which will send metrics about your Kubernetes infrastructure to SigNoz. These OpenTelemetry collectors will act as agents which send metrics about Kubernetes to SigNoz.</p>
<p>OtelCollector agent can also be used to tail and parse logs generated by container using <code>filelog</code> receiver and send it to desired receiver.</p>
<p><code>K8s-Infra</code> helm chart mainly does the following:</p>
<ul>
<li>Tails and parses logs generated by containers in Kubernetes cluster and sends to SigNoz</li>
<li>Collects kubelet metrics and host metrics from each nodes of the Kubernetes cluster</li>
<li>Collects cluster-level metrics from the Kubernetes API server</li>
<li>Acts as a gateway to send any incoming OTLP telemetry data to SigNoz OtelCollector</li>
</ul>
<hr />
<p>Based on how you are running SigNoz (e.g. SigNoz Cloud, in an independent VM or Kubernetes cluster), you have to provide the address to send data from the above receivers.</p>
<p>SigNoz CloudSelf-Host</p>
<h3>## Install K8s-Infra chart</h3>
<p>To add the SigNoz Helm repository to your helm client, run the following command:</p>
<pre><code>helm repo add signoz https://charts.signoz.io
</code></pre>
<p>If the chart is already present, update the chart to the latest version:</p>
<pre><code>helm repo update
</code></pre>
<p>GenericAWS EKSGKE (Standard)GKE (Autopilot)AKS</p>
<p>For generic Kubernetes clusters, you can use the following configuration:</p>
<p><em>override-values.yaml</em></p>
<pre><code>global:
  cloud: others
  clusterName: &lt;CLUSTER_NAME&gt;
  deploymentEnvironment: &lt;DEPLOYMENT_ENVIRONMENT&gt;
otelCollectorEndpoint: ingest.{region}.signoz.cloud:443
otelInsecure: false
signozApiKey: &lt;SIGNOZ_INGESTION_KEY&gt;
presets:
  otlpExporter:
    enabled: true
  loggingExporter:
    enabled: false
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the ingestion endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<ul>
<li>Replace <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> with the one provided by SigNoz.</li>
<li>Replace <code>&lt;CLUSTER_NAME&gt;</code> with the name of the Kubernetes cluster or a unique identifier of the cluster.</li>
<li>Replace <code>&lt;DEPLOYMENT_ENVIRONMENT&gt;</code> with the deployment environment of your application. Example: &quot;staging&quot;, &quot;production&quot;, etc.</li>
</ul>
<p>To install the <code>k8s-infra</code> chart with the above configuration, run the following command:</p>
<pre><code>helm install my-release signoz/k8s-infra -f override-values.yaml
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/kubernetes-infra-metrics/#send-data-from-instrumented-applications
tag_set: tutorial, kubernetes-infra-metrics
image_urls: https://signoz.io/img/docs/data-flow-2x.webp
tracking_id: docs-tutorial-kubernetes-infra-metrics-send-data-from-instrumented-applications
group_tracking_ids: docs-tutorial-kubernetes-infra-metrics
<h2>Kubernetes Infra Metrics and Logs Collection: Send Data from Instrumented Applications</h2>
<p><img src="https://signoz.io/img/docs/data-flow-2x.webp" alt="Data flow from your application to SigNoz" /></p>
<p>_</p>
<p>OpenTelemetry instrumented application sends data to OTelAgent Daemon deployed in your k8s infra. The OTelAgent daemon sends the collected data to SigNoz.</p>
<p>_</p>
<p>üìù Note</p>
<p>In case of <strong>GKE Autopilot</strong>, you will not be able to send data to OTelAgent Daemon via host port. You will need to use either the SigNoz ingestion endpoint directly or OtelAgent service name.</p>
<p>For OtelAgent service name, the endpoint would be something like <code>my-release-k8s-infra-otel-agent.default.svc:4317</code>. Replace <code>my-release</code> with your helm release name and <code>default</code> with your namespace.</p>
<p>To send data from your applications, you must first instrument it with OpenTelemetry. You can find instrumentation instructions for your specific language <a href="https://signoz.io/docs/instrumentation/">here</a>.</p>
<p>Once you're done instrumenting your application, add below to your application manifest files for applications to start sending data to the otel-collectors running as DaemonSet.</p>
<p>For example, you can add the below config to your application manifest file.</p>
<pre><code>env:
  - name: HOST_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: K8S_POD_IP
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: status.podIP
  - name: K8S_POD_UID
    valueFrom:
      fieldRef:
        fieldPath: metadata.uid
  - name: OTEL_EXPORTER_OTLP_INSECURE
    value: &quot;true&quot;
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: $(HOST_IP):4317
  - name: OTEL_RESOURCE_ATTRIBUTES
    value: service.name=APPLICATION_NAME,k8s.pod.ip=$(K8S_POD_IP),k8s.pod.uid=$(K8S_POD_UID)
</code></pre>
<p>üìù Note</p>
<ul>
<li>Replace <code>APPLICATION_NAME</code> with your application name that you wish to see in SigNoz.</li>
<li>In cases of some SDKs, you would need to include <code>http://</code> or <code>https://</code> prefix for <code>OTEL_EXPORTER_OTLP_ENDPOINT</code></li>
<li>You can also include <code>deployment.environment</code> as an attribute in <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable. This attribute will take precedence over <code>global.deploymentEnvironment</code> configuration of <code>k8s-infra</code> chart.</li>
</ul>
<h3>## Disable Logs Collection</h3>
<p>In case you do not want to collect logs from your Kubernetes cluster, you can disable using presets in <code>k8s-infra</code> chart.</p>
<pre><code>presets:
  logsCollection:
    enabled: false
</code></pre>
<h3>## Disable Metrics Collection</h3>
<p>In case you do not want to collect metrics from your Kubernetes cluster, you can disable using presets in <code>k8s-infra</code> chart.</p>
<pre><code>presets:
  hostMetrics:
    enabled: false
  kubeletMetrics:
    enabled: false
  clusterMetrics:
    enabled: false

otelDeployment:
  enabled: false
</code></pre>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/kubernetes-infra-metrics/#plot-metrics-in-signoz-ui
tag_set: tutorial, kubernetes-infra-metrics
image_urls: 
tracking_id: docs-tutorial-kubernetes-infra-metrics-plot-metrics-in-signoz-ui
group_tracking_ids: docs-tutorial-kubernetes-infra-metrics
<h2>Kubernetes Infra Metrics and Logs Collection: Plot Metrics in SigNoz UI</h2>
<p>To plot metrics generated from <code>k8s-infra</code> chart, follow the instructions given in the docs <a href="https://signoz.io/docs/userguide/dashboards/">here</a>.</p>
<p>Check out the <a href="#list-of-metrics">List of metrics from Kubernetes receiver</a>.</p>
<p>Here are some examples of metrics dashboard.</p>
<ol>
<li>
<p><strong>Import Dashboard with PVC Metrics</strong></p>
<p>You can import dashboard with PVC metrics of Kubernetes cluster from <a href="https://github.com/SigNoz/dashboards/blob/main/k8s-infra-metrics/kubernetes-pvc-metrics.json">here</a>
.</p>
</li>
<li>
<p><strong>Import Dashboard with Overall Kubernetes pods Metrics</strong></p>
<p>You can import dashboard with the general Kubernetes pods metrics of your K8s cluster from <a href="https://github.com/SigNoz/dashboards/blob/main/k8s-infra-metrics/kubernetes-pod-metrics-overall.json">here</a>
.</p>
</li>
<li>
<p><strong>Import Dashboard with Detailed Kubernetes pods Metrics</strong></p>
<p>You can import dashboard with more detailed granular Kubernetes pods metrics of your K8s cluster from <a href="https://github.com/SigNoz/dashboards/blob/main/k8s-infra-metrics/kubernetes-pod-metrics-detailed.json">here</a>
.</p>
</li>
<li>
<p><strong>Import Dashboard with Overall Kubernetes Node Metrics</strong></p>
<p>You can import dashboard with the general Kubernetes node metrics of your K8s cluster from <a href="https://github.com/SigNoz/dashboards/blob/main/k8s-infra-metrics/kubernetes-node-metrics-overall.json">here</a>
.</p>
</li>
<li>
<p><strong>Import Dashboard with Detailed Kubernetes Node Metrics</strong></p>
<p>You can import dashboard with more detailed granular Kubernetes node metrics of your K8s cluster from <a href="https://github.com/SigNoz/dashboards/blob/main/k8s-infra-metrics/kubernetes-node-metrics-detailed.json">here</a>
.</p>
</li>
</ol>
<p>In the Dashboard page of SigNoz UI, you can create your own widgets as per you need using metrics from the <a href="#list-of-metrics">list below</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/kubernetes-infra-metrics/#list-of-metrics
tag_set: tutorial, kubernetes-infra-metrics
image_urls: 
tracking_id: docs-tutorial-kubernetes-infra-metrics-list-of-metrics
group_tracking_ids: docs-tutorial-kubernetes-infra-metrics
<h2>Kubernetes Infra Metrics and Logs Collection: List of metrics</h2>
<h3>## Kubernetes Metrics - kubeletstats and k8s_cluster</h3>
<ul>
<li>container_cpu_time</li>
<li>container_cpu_utilization</li>
<li>container_filesystem_available</li>
<li>container_filesystem_capacity</li>
<li>container_filesystem_usage</li>
<li>container_memory_available</li>
<li>container_memory_major_page_faults</li>
<li>container_memory_page_faults</li>
<li>container_memory_rss</li>
<li>container_memory_usage</li>
<li>container_memory_working_set</li>
<li>k8s_container_cpu_limit</li>
<li>k8s_container_cpu_request</li>
<li>k8s_container_memory_limit</li>
<li>k8s_container_memory_request</li>
<li>k8s_container_ready</li>
<li>k8s_container_restarts</li>
<li>k8s_daemonset_current_scheduled_nodes</li>
<li>k8s_daemonset_desired_scheduled_nodes</li>
<li>k8s_daemonset_misscheduled_nodes</li>
<li>k8s_daemonset_ready_nodes</li>
<li>k8s_deployment_available</li>
<li>k8s_deployment_desired</li>
<li>k8s_job_active_pods</li>
<li>k8s_job_desired_successful_pods</li>
<li>k8s_job_failed_pods</li>
<li>k8s_job_max_parallel_pods</li>
<li>k8s_job_successful_pods</li>
<li>k8s_namespace_phase</li>
<li>k8s_node_condition_memory_pressure</li>
<li>k8s_node_condition_ready</li>
<li>k8s_node_cpu_time</li>
<li>k8s_node_cpu_utilization</li>
<li>k8s_node_filesystem_available</li>
<li>k8s_node_filesystem_capacity</li>
<li>k8s_node_filesystem_usage</li>
<li>k8s_node_memory_available</li>
<li>k8s_node_memory_major_page_faults</li>
<li>k8s_node_memory_page_faults</li>
<li>k8s_node_memory_rss</li>
<li>k8s_node_memory_usage</li>
<li>k8s_node_memory_working_set</li>
<li>k8s_node_network_errors</li>
<li>k8s_node_network_io</li>
<li>k8s_pod_cpu_time</li>
<li>k8s_pod_cpu_utilization</li>
<li>k8s_pod_filesystem_available</li>
<li>k8s_pod_filesystem_capacity</li>
<li>k8s_pod_filesystem_usage</li>
<li>k8s_pod_memory_available</li>
<li>k8s_pod_memory_major_page_faults</li>
<li>k8s_pod_memory_page_faults</li>
<li>k8s_pod_memory_rss</li>
<li>k8s_pod_memory_usage</li>
<li>k8s_pod_memory_working_set</li>
<li>k8s_pod_network_errors</li>
<li>k8s_pod_network_io</li>
<li>k8s_pod_phase</li>
<li>k8s_replicaset_available</li>
<li>k8s_replicaset_desired</li>
<li>k8s_statefulset_current_pods</li>
<li>k8s_statefulset_desired_pods</li>
<li>k8s_statefulset_ready_pods</li>
<li>k8s_statefulset_updated_pods</li>
<li>k8s_volume_available</li>
<li>k8s_volume_capacity</li>
<li>k8s_volume_inodes</li>
<li>k8s_volume_inodes_free</li>
<li>k8s_volume_inodes_used</li>
<li>k8s_node_allocatable_cpu</li>
<li>k8s_node_allocatable_memory</li>
</ul>
<h3>## Hostmetrics</h3>
<ul>
<li>system_network_connections</li>
<li>system_disk_weighted_io_time</li>
<li>system_disk_merged</li>
<li>system_disk_operation_time</li>
<li>system_disk_pending_operations</li>
<li>system_disk_io_time</li>
<li>system_disk_operations</li>
<li>system_disk_io</li>
<li>system_filesystem_inodes_usage</li>
<li>system_filesystem_usage</li>
<li>system_cpu_time</li>
<li>system_memory_usage</li>
<li>system_network_packets</li>
<li>system_network_dropped</li>
<li>system_network_io</li>
<li>system_network_errors</li>
<li>system_cpu_load_average_5m</li>
<li>system_cpu_load_average_15m</li>
<li>system_cpu_load_average_1m</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker
group_tracking_ids: docs-install-docker
<h2>Docker Standalone</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>SigNoz can be installed on macOS or Linux computers, and there are two ways in which you can install SigNoz:</p>
<ul>
<li>You may execute a script that checks your environment, installs Docker Engine and Docker Compose on Linux, and runs the <code>docker compose up</code> command for you.</li>
<li>You may execute the <code>docker compose up</code> command yourself.</li>
</ul>
<p>Both methods are provided below.</p>
<p>‚úÖ Info</p>
<p>SigNoz recommends you to use the <a href="#install-signoz-using-the-install-script">install script</a> on macOS and the following Linux distributions:</p>
<ul>
<li>Ubuntu</li>
<li>Debian</li>
<li>OpenSuse</li>
<li>CentOS</li>
<li>SUSE Linux Enterprise Server (SLES)</li>
</ul>
<p>If you're using a different Linux distribution, see the <a href="#install-signoz-using-docker-compose">Install SigNoz Using Docker Compose</a> section.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#prerequisites
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-prerequisites
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Prerequisites</h2>
<ul>
<li>
<p>A Linux or macOS machine. Microsoft Windows is not officially supported.</p>
</li>
<li>
<p>On macOS, you must manually install <a href="https://docs.docker.com/engine/install/">Docker Engine</a> before you run the install script. The install script automatically installs Docker Engine on Linux.</p>
</li>
<li>
<p>A minimum of 4GB of memory must be allocated to Docker.</p>
</li>
<li>
<p><a href="https://desktop.github.com/">Git client</a></p>
</li>
<li>
<p>Ensure that the ports <code>3301</code>, <code>4317</code> and <code>4318</code> are open on the machine where you install SigNoz.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#install-signoz-using-the-install-script
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-install-signoz-using-the-install-script
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Install SigNoz Using the Install Script</h2>
<ol>
<li>
<p>In a directory of your choosing, clone the SigNoz repository and 'cd' into the <code>signoz/deploy</code> directory by entering the following commands:</p>
<pre><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/
</code></pre>
</li>
<li>
<p>Run the <code>install.sh</code> script:</p>
<pre><code>./install.sh
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#install-signoz-using-docker-compose
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-install-signoz-using-docker-compose
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Install SigNoz Using Docker Compose</h2>
<p>‚úÖ Info</p>
<p>Before you install Signoz, ensure that <a href="https://docs.docker.com/compose/install/">Docker Compose</a> is installed on your machine.</p>
<ol>
<li>
<p>In a directory of your choosing, clone the SigNoz repository and 'cd' into the <code>signoz/deploy</code> directory by entering the following commands:</p>
<pre><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/
</code></pre>
</li>
<li>
<p>To install SigNoz, enter the <code>docker compose up</code> command, specifying the following:</p>
<ul>
<li><code>-f</code> and the path to your configuration file</li>
<li><code>-d</code> to run containers in the background</li>
</ul>
<p>docker compose -f docker/clickhouse-setup/docker-compose.yaml up -d</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#verify-the-installation
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-verify-the-installation
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Verify the Installation</h2>
<ol>
<li>
<p>Ensure that your containers are running correctly. To view the status of your containers, run the following command:</p>
<p>docker ps</p>
</li>
</ol>
<p>The output should look similar to the following:</p>
<pre><code>CONTAINER ID   IMAGE                                          COMMAND                  CREATED          STATUS                    PORTS                                                                            NAMES
01f044c4686a   signoz/frontend:0.38.2                       &quot;nginx -g 'daemon of‚Ä¶&quot;   2 minutes ago   Up 9 seconds                  80/tcp, 0.0.0.0:3301-&gt;3301/tcp                                                     signoz-frontend
86aa5b875f9f   gliderlabs/logspout:v3.2.14                  &quot;/bin/logspout syslo‚Ä¶&quot;   2 minutes ago   Up 1 second                   80/tcp                                                                             signoz-logspout
58746f684630   signoz/alertmanager:0.23.4                   &quot;/bin/alertmanager -‚Ä¶&quot;   2 minutes ago   Up 9 seconds                  9093/tcp                                                                           signoz-alertmanager
2cf1ec96bdb3   signoz/query-service:0.38.2                  &quot;./query-service -co‚Ä¶&quot;   2 minutes ago   Up About a minute (healthy)   8080/tcp                                                                           signoz-query-service
e9f0aa66d884   signoz/signoz-otel-collector:0.88.11          &quot;/signoz-collector -‚Ä¶&quot;   2 minutes ago   Up 10 seconds                 0.0.0.0:4317-4318-&gt;4317-4318/tcp                                                   signoz-otel-collector
d3d89d7d4581   clickhouse/clickhouse-server:23.11.1-alpine   &quot;/entrypoint.sh&quot;         2 minutes ago   Up 2 minutes (healthy)        0.0.0.0:8123-&gt;8123/tcp, 0.0.0.0:9000-&gt;9000/tcp, 0.0.0.0:9181-&gt;9181/tcp, 9009/tcp   signoz-clickhouse
9db88aefb6ed   signoz/locust:1.2.3                          &quot;/docker-entrypoint.‚Ä¶&quot;   2 minutes ago   Up 2 minutes                  5557-5558/tcp, 8089/tcp                                                            load-hotrod
60bb3b77b4f7   bitnami/zookeeper:3.7.1                      &quot;/opt/bitnami/script‚Ä¶&quot;   2 minutes ago   Up 2 minutes                  0.0.0.0:2181-&gt;2181/tcp, 0.0.0.0:2888-&gt;2888/tcp, 0.0.0.0:3888-&gt;3888/tcp, 8080/tcp   signoz-zookeeper-1
98c7178b4004   jaegertracing/example-hotrod:1.30            &quot;/go/bin/hotrod-linu‚Ä¶&quot;   9 days ago      Up 2 minutes                  8080-8083/tcp                                                                      hotrod
</code></pre>
<ol start="2">
<li>Wait for all the pods to be in running state, and then point your browser to <code>http://&lt;IP-ADDRESS&gt;:3301/</code> to access the dashboard, replacing <code>&lt;IP-ADDRESS&gt;</code> with the IP address of the machine where you installed SigNoz.</li>
</ol>
<p><strong>Example</strong>:</p>
<ul>
<li>If you're running SigNoz on your local machine, you should point your browser to <code>http://localhost:3301/</code>.</li>
<li>If the IP address of the machine on which you're running SigNoz is <code>66.82.18.247</code>, you should point your browser to <code>http://66.82.18.247:3301/</code></li>
</ul>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>
<p>‚úÖ Info</p>
<p>The <code>docker-compose.yaml</code> installs a sample application named <a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a> that generates tracing data. You can explore the SigNoz dashboard with the data provided by the sample application. If you wish to remove the sample application, follow the steps in the <a href="/docs/operate/docker-standalone/#remove-the-sample-application">Remove the Sample Application</a> section.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#install-specific-version-of-signoz
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-install-specific-version-of-signoz
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Install specific version of SigNoz</h2>
<ol>
<li>
<p>In a directory of your choosing, clone the SigNoz repository and 'cd' into the <code>signoz/deploy</code> directory by entering the following commands:</p>
<pre><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/
</code></pre>
</li>
<li>
<p>Checkout to the specific version tag. For example, to install SigNoz version <code>v0.6.1</code>:</p>
<pre><code>git checkout v0.6.1
</code></pre>
</li>
<li>
<p>Run the <code>install.sh</code> script:</p>
<pre><code>./install.sh
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/docker-standalone/">Docker Standalone Operate</a> section for detailed instructions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#related-topics
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-related-topics
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Related Topics</h2>
<ul>
<li><a href="/docs/install/troubleshooting/">Troubleshoot SigNoz Installation Issues</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker/#next-steps
tag_set: install, docker
image_urls: 
tracking_id: docs-install-docker-next-steps
group_tracking_ids: docs-install-docker
<h2>Docker Standalone: Next Steps</h2>
<ul>
<li>
<p><a href="/docs/instrumentation/overview/">Instrument Your Application</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
<li>
<p><a href="/docs/operate/docker-standalone/">Operate SigNoz on Docker Standalone</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: 
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine</h2>
<h3>## Overview</h3>
<p>This tutorial shows how you can deploy OpenTelemetry binary as an agent, which collects telemetry data. Data such as traces, metrics and logs generated by applications most likely running in the same virtual machine (VM).</p>
<p>It can also be used for collecting data from other VMs in the same cluster, data center or region, however, binary is not recommended in that scenario but container or deployment which can be easily scaled.</p>
<p>In this guide, you will also learn to set up a hostmetrics receiver to collect metrics from the VM and view in SigNoz.</p>
<p>SigNoz CloudSelf-Host</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/#setup-otel-collector-as-agent
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: https://signoz.io/img/docs/saas-docs/vm-setup-2x.webp
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine-setup-otel-collector-as-agent
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine: Setup Otel Collector as agent</h2>
<p>OpenTelemetry-instrumented applications in a VM can send data to the <code>otel-binary</code> agent running in the same VM. The OTel agent can then be configured to send data to the SigNoz cloud.</p>
<p><img src="https://signoz.io/img/docs/saas-docs/vm-setup-2x.webp" alt="Collecting data from applications deployed in VM" /></p>
<p><em>OpenTelemetry-instrumented applications in a VM can send data to otel-binary which then sends data to SigNoz cloud.</em></p>
<p>Here are the steps to set up OpenTelemetry binary as an agent.</p>
<p>Linux (amd64)Linux (arm64)MacOS (amd64)MacOS (arm64)Windows (amd64)</p>
<ol>
<li>
<p>Download otel-collector tar.gz for your architecture</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.88.0/otelcol-contrib_0.88.0_linux_amd64.tar.gz
</code></pre>
</li>
<li>
<p>Extract otel-collector tar.gz to the <code>otelcol-contrib</code> folder</p>
<pre><code>mkdir otelcol-contrib &amp;&amp; tar xvzf otelcol-contrib_0.88.0_linux_amd64.tar.gz -C otelcol-contrib
</code></pre>
</li>
<li>
<p>Create <code>config.yaml</code> in folder <code>otelcol-contrib</code> with the below content in it. Replace <code>SIGNOZ_INGESTION_KEY</code> with what is provided by SigNoz:</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  hostmetrics:
    collection_interval: 60s
    scrapers:
      cpu: {}
      disk: {}
      load: {}
      filesystem: {}
      memory: {}
      network: {}
      paging: {}
      process:
        mute_process_name_error: true
        mute_process_exe_error: true
        mute_process_io_error: true
      processes: {}
  prometheus:
    config:
      global:
        scrape_interval: 60s
      scrape_configs:
        - job_name: otel-collector-binary
          static_configs:
            - targets:
              # - localhost:8888
processors:
  batch:
    send_batch_size: 1000
    timeout: 10s
  # Ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md
  resourcedetection:
    detectors: [env, system] # Before system detector, include ec2 for AWS, gcp for GCP and azure for Azure.
    # Using OTEL_RESOURCE_ATTRIBUTES envvar, env detector adds custom labels.
    timeout: 2s
    system:
      hostname_sources: [os] # alternatively, use [dns,os] for setting FQDN as host.name and os as fallback
extensions:
  health_check: {}
  zpages: {}
exporters:
  otlp:
    endpoint: &quot;ingest.{region}.signoz.cloud:443&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
  logging:
    verbosity: normal
service:
  telemetry:
    metrics:
      address: 0.0.0.0:8888
  extensions: [health_check, zpages]
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics/internal:
      receivers: [prometheus, hostmetrics]
      processors: [resourcedetection, batch]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Once we are done with the above configurations, we can now run the collector service with the following command:</p>
<p>From the <code>otelcol-contrib</code>, run the following command:</p>
<pre><code>./otelcol-contrib --config ./config.yaml
</code></pre>
<h3>##     Run in background</h3>
<p>If you want to run otel collector process in the background:</p>
<pre><code>./otelcol-contrib --config ./config.yaml &amp;&gt; otelcol-output.log &amp; echo &quot;$!&quot; &gt; otel-pid
</code></pre>
<p>The above command sends the output of the otel-collector to <code>otelcol-output.log</code> file and prints the process id of the background running otel collector process to the otel-pid file.</p>
<p>If you want to see the output of the logs you‚Äôve just set up for the background process, you may look it up with:</p>
<pre><code>tail -f -n 50 otelcol-output.log
</code></pre>
<p>tail 50 will give the last 50 lines from the file <code>otelcol-output.log</code></p>
<p>You can stop the collector service <code>otelcol</code> when running in backgorund, with the following command:</p>
<pre><code>kill &quot;$(&lt; otel-pid)&quot;
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/#test-sending-traces
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: https://signoz.io/img/docs/telemetrygen-output.webp
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine-test-sending-traces
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine: Test Sending Traces</h2>
<p>OpenTelemetry collector binary should be able to forward all types of telemetry data recevied: traces, metrics, and logs, to SigNoz OTLP endpoint via gRPC.</p>
<p>Let's send sample traces to the <code>otelcol</code> using <code>telemetrygen</code>.</p>
<p>To install telemetrygen binary:</p>
<pre><code>go install github.com/open-telemetry/opentelemetry-collector-contrib/cmd/telemetrygen@latest
</code></pre>
<p>To send trace data using <code>telemetrygen</code>, execute the command below:</p>
<pre><code>telemetrygen traces --traces 1 --otlp-endpoint localhost:4317 --otlp-insecure
</code></pre>
<p>Output should look like this:</p>
<pre><code>...
2023-03-15T11:04:38.967+0545    INFO    channelz/funcs.go:340   [core][Channel #1] Channel Connectivity change to READY {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.968+0545    INFO    traces/traces.go:124    generation of traces isn't being throttled
2023-03-15T11:04:38.968+0545    INFO    traces/worker.go:90     traces generated        {&quot;worker&quot;: 0, &quot;traces&quot;: 1}
2023-03-15T11:04:38.969+0545    INFO    traces/traces.go:87     stop the batch span processor
2023-03-15T11:04:38.983+0545    INFO    channelz/funcs.go:340   [core][Channel #1] Channel Connectivity change to SHUTDOWN      {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.984+0545    INFO    channelz/funcs.go:340   [core][Channel #1 SubChannel #2] Subchannel Connectivity change to SHUTDOWN     {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.984+0545    INFO    channelz/funcs.go:340   [core][Channel #1 SubChannel #2] Subchannel deleted     {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.984+0545    INFO    channelz/funcs.go:340   [core][Channel #1] Channel deleted      {&quot;system&quot;: &quot;grpc&quot;, &quot;grpc_log&quot;: true}
2023-03-15T11:04:38.984+0545    INFO    traces/traces.go:79     stopping the exporter
</code></pre>
<p>If the SigNoz endpoint in the configuration is set correctly and accessible, you should be able to see the traces sent via OpenTelemetry collector in VM from <code>telemetrygen</code> in the SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/telemetrygen-output.webp" alt="traces generated by telemetrygen" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/#hostmetrics-dashboard
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: 
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine-hostmetrics-dashboard
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine: HostMetrics Dashboard</h2>
<p>To setup the Hostmetrics Dashboard, check the docs <a href="https://signoz.io/docs/userguide/hostmetrics/">here</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/#list-of-metrics
tag_set: tutorial, opentelemetry-binary-usage-in-virtual-machine
image_urls: 
tracking_id: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine-list-of-metrics
group_tracking_ids: docs-tutorial-opentelemetry-binary-usage-in-virtual-machine
<h2>OpenTelemetry Binary Usage in Virtual Machine: List of metrics</h2>
<h3>## Hostmetrics</h3>
<ul>
<li>system_network_connections</li>
<li>system_disk_weighted_io_time</li>
<li>system_disk_merged</li>
<li>system_disk_operation_time</li>
<li>system_disk_pending_operations</li>
<li>system_disk_io_time</li>
<li>system_disk_operations</li>
<li>system_disk_io</li>
<li>system_filesystem_inodes_usage</li>
<li>system_filesystem_usage</li>
<li>system_cpu_time</li>
<li>system_memory_usage</li>
<li>system_network_packets</li>
<li>system_network_dropped</li>
<li>system_network_io</li>
<li>system_network_errors</li>
<li>system_cpu_load_average_5m</li>
<li>system_cpu_load_average_15m</li>
<li>system_cpu_load_average_1m</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/
tag_set: instrumentation, troubleshoot-instrumentation
image_urls: 
tracking_id: docs-instrumentation-troubleshoot-instrumentation
group_tracking_ids: docs-instrumentation-troubleshoot-instrumentation
<h2>Troubleshoot guide</h2>
<h2>Troubleshoot Instrumenting your application and sending data to SigNoz</h2>
<p>==================================================================================================================================================</p>
<h3>## SigNoz Otel Collector Address Grid</h3>
<p>You might have specific set up for your application and SigNoz cluster. You can use the below table to figure out which address to use to send data to SigNoz.</p>
<p>Here is the SigNoz Otel Collector address grid which could be helpful:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Where SigNoz is installed?</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>VM (Docker) - Same Machine</td>
<td>VM (Docker) - Different Machine</td>
<td>K8s (Same Cluster)</td>
<td>K8s (Different Cluster)</td>
</tr>
<tr>
<td>Where your application is running?</td>
<td>VM (native/binary)</td>
<td>localhost:4317</td>
<td>&lt;otelcollector-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
</tr>
<tr>
<td>VM (Docker)</td>
<td>172.17.0.1:4317, otel-collector:4317(shared network)</td>
<td>&lt;otelcollector-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td></td>
</tr>
<tr>
<td>Kubernetes</td>
<td>&lt;otelcollector-IP&gt;:4317</td>
<td>&lt;otelcollector-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td></td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<ol>
<li>
<p>For the <code>otelcollector-IP</code>, use private IP address if the VM is in same private network. Replace <code>app-namespace</code> with your application namespace, <code>my-release</code> with SigNoz helm release name, and <code>platform</code> with SigNoz namespace.</p>
</li>
<li>
<p>In the case of k8s where the application and SigNoz are running in different k8s cluster, you will have to expose otel collector service. Set the service type to either <code>NodePort</code> or <code>LoadBalancer</code>.</p>
<pre><code>helm upgrade --install -n platform my-release signoz/signoz \
--set otelCollector.serviceType=&quot;&lt;NodePort or LoadBalancer&gt;&quot;
</code></pre>
</li>
</ol>
<h3>## Troubleshooting SigNoz installation</h3>
<p><a href="/docs/install/troubleshooting/">This guide</a> shares detailed steps on how to find if your SigNoz installation is accessible from your application to send telemetry data</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/troubleshooting/
tag_set: install, troubleshooting
image_urls: 
tracking_id: docs-install-troubleshooting
group_tracking_ids: docs-install-troubleshooting
<h2>Troubleshooting</h2>
<p>This troubleshooting guide includes step-by-step instructions that should resolve most installation issues.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/troubleshooting/#using-signoz-troubleshooting-repository
tag_set: install, troubleshooting
image_urls: 
tracking_id: docs-install-troubleshooting-using-signoz-troubleshooting-repository
group_tracking_ids: docs-install-troubleshooting
<h2>Troubleshooting: Using SigNoz Troubleshooting Repository</h2>
<p>You can use the <a href="https://github.com/SigNoz/troubleshoot">SigNoz troubleshoot</a> to test if SigNoz otel collector is accessible from where you are running your applications.</p>
<h3>## Binary installation</h3>
<p>You can run the one liner script below to download the troubleshoot binary:</p>
<pre><code>curl -sL https://github.com/SigNoz/troubleshoot/raw/main/scripts/install.sh | bash
</code></pre>
<p>Here is the syntax:</p>
<pre><code>./troubleshoot checkEndpoint --endpoint=&lt;endpoint-to-check&gt;
</code></pre>
<h4>## Binary</h4>
<p>For example, if Otel Collector should be accessible in <code>localhost:4317</code>:</p>
<pre><code>./troubleshoot checkEndpoint --endpoint=localhost:4317
</code></pre>
<h4>## Docker</h4>
<p>You can also use troubleshoot Docker image:</p>
<pre><code>docker run -it --rm signoz/troubleshoot checkEndpoint --endpoint=172.17.0.1:4317
</code></pre>
<p>üìù Note</p>
<p>SigNoz Otel Collector should be accessible in <code>172.17.0.1:4317</code> from your application even if running in different docker network.</p>
<h4>## Kubernetes</h4>
<p>Let's spin up a pod in Kubernetes with <code>platform</code> namespace to check if <code>otel collector</code> is running properly:</p>
<pre><code>kubectl -n platform run troubleshoot --image=signoz/troubleshoot \
  --restart='Never' -i --tty --rm --command -- ./troubleshoot checkEndpoint \
  --endpoint=my-release-signoz-otel-collector.platform.svc.cluster.local:4317
</code></pre>
<p>You can also spin up a pod in Kubernetes with same namespace as your application to check if <code>otel collector</code> is accessible:</p>
<pre><code>kubectl -n app-namespace run troubleshoot --image=signoz/troubleshoot \
  --restart='Never' -i --tty --rm --command -- ./troubleshoot checkEndpoint \
  --endpoint=my-release-signoz-otel-collector.platform.svc.cluster.local:4317
</code></pre>
<p>üìù Note</p>
<ol>
<li>
<p>Replace <code>app-namespace</code> with your application namespace, <code>my-release</code> with SigNoz helm release name, and <code>platform</code> with SigNoz namespace.</p>
</li>
<li>
<p>In case on multiple k8s cluster, you might have to set otel collector service type as <code>NodePort</code> or <code>LoadBalancer</code>.</p>
<p>helm upgrade --install -n platform my-release signoz/signoz <br />
--set otelCollector.serviceType=&quot;&lt;NodePort or LoadBalancer&gt;&quot;</p>
</li>
</ol>
<h3>## Troubleshooting Video</h3>
<p>If you instead prefer to watch a video, check out this video:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/troubleshooting/#troubleshooting-docker-standalone-installation-of-signoz
tag_set: install, troubleshooting
image_urls: 
tracking_id: docs-install-troubleshooting-troubleshooting-docker-standalone-installation-of-signoz
group_tracking_ids: docs-install-troubleshooting
<h2>Troubleshooting: Troubleshooting Docker Standalone Installation of SigNoz</h2>
<ol>
<li>Before you install SigNoz on Docker Standalone, ensure that all <a href="/docs/install/docker/#prerequisites">prerequisites</a> are met.</li>
<li>Run the <code>docker ps</code> command and ensure that the <code>signoz-clickhouse</code> and <code>signoz-query-service</code> containers are running. If these containers are not running, increase the memory allocated to Docker.</li>
<li>If you don't see any data in the front-end, please wait two or three minutes.</li>
<li>Run the <code>install.sh</code> script again. The script will try to reinstall the containers that failed.</li>
<li>Follow the steps for <a href="/docs/operate/docker-standalone/#uninstall-signoz">uninstalling SigNoz</a> section and then install SigNoz again by following the steps in the <a href="/docs/install/docker/">Install SigNoz on Docker Standalone</a>
section.</li>
<li>If you're still facing issues trying to install SigNoz, please reach out to us on <a href="https://signoz.io/slack">Slack</a></li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/troubleshooting/#troubleshooting-clickhouse-container-crashes
tag_set: install, troubleshooting
image_urls: 
tracking_id: docs-install-troubleshooting-troubleshooting-clickhouse-container-crashes
group_tracking_ids: docs-install-troubleshooting
<h2>Troubleshooting: Troubleshooting ClickHouse container crashes</h2>
<p>If the clickhouse container is unable to start due to certain reasons, it is recommended to take the following steps before recreating the container</p>
<ol>
<li>
<p>Run the following command to see more detailed logs for the ClickHouse container:</p>
<p>sudo docker-compose -f ./docker/clickhouse-setup/docker-compose.yaml ps -a</p>
</li>
</ol>
<p>This will show you the status of all containers, including ClickHouse.</p>
<ul>
<li>
<p><strong>Verify system resources:</strong> Ensure your system has enough resources (CPU, memory, disk space) to run the ClickHouse container.</p>
</li>
<li>
<p><strong>Check permissions:</strong> Make sure you have the necessary permissions to run Docker containers. Try running the installation script with sudo privileges.</p>
</li>
<li>
<p><strong>Review Docker configuration:</strong> Ensure your Docker installation is up-to-date and properly configured.</p>
</li>
</ul>
<ol start="2">
<li>If the logs mention about <code>Permission denied</code> error, it is likely due to the correct permissions not set for the container.</li>
<li>Use the <code>chwon</code> command to change the ownership for the files that clickhouse container is unable to access.</li>
<li>If neither of this works, run the <code>install.sh</code> script again as the root user or by using <code>sudo</code> as a prefix.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/troubleshooting/#signoz-otel-collector-address-grid
tag_set: install, troubleshooting
image_urls: 
tracking_id: docs-install-troubleshooting-signoz-otel-collector-address-grid
group_tracking_ids: docs-install-troubleshooting
<h2>Troubleshooting: SigNoz Otel Collector Address Grid</h2>
<p>You might have specific set up for your application and SigNoz cluster. It might not be very clear on which address to use to send data to SigNoz.</p>
<p>Here is the SigNoz Otel Collector address grid which could be helpful:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Where SigNoz is installed?</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>VM (Docker) - Same Machine</td>
<td>VM (Docker) - Different Machine</td>
<td>K8s (Same Cluster)</td>
<td>K8s (Different Cluster)</td>
</tr>
<tr>
<td>Where your application is running?</td>
<td>VM (native/binary)</td>
<td>localhost:4317</td>
<td>&lt;otelcollector-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
</tr>
<tr>
<td>VM (Docker)</td>
<td>172.17.0.1:4317, otel-collector:4317(shared network)</td>
<td>&lt;otelcollector-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td></td>
</tr>
<tr>
<td>Kubernetes</td>
<td>&lt;otelcollector-IP&gt;:4317</td>
<td>&lt;otelcollector-IP&gt;:4317</td>
<td>&lt;release-name&gt;-signoz-otel-collector.&lt;namespace&gt;.svc.cluster.local:4317</td>
<td>&lt;k8s-node-IP&gt;:&lt;otelcollector-node-port&gt;, &lt;k8s-loadbalancer-IP&gt;:4317</td>
<td></td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<ol>
<li>
<p>For the <code>&lt;otelcollector-IP&gt;</code>, use private IP address if the VM is in same private network.</p>
</li>
<li>
<p>Replace <code>&lt;namespace&gt;</code> with SigNoz namespace and <code>&lt;release-name&gt;</code> with SigNoz helm release name.</p>
</li>
<li>
<p>In the case of k8s where the application and SigNoz are running in different k8s cluster, you will have to expose otel collector service. Set the service type to either <code>NodePort</code> or <code>LoadBalancer</code>.</p>
<p>helm upgrade --install -n platform my-release signoz/signoz <br />
--set otelCollector.serviceType=&quot;&lt;NodePort or LoadBalancer&gt;&quot;</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/
tag_set: instrumentation, django
image_urls: https://signoz.io/img/docs/otel_python_instrumentation.webp
tracking_id: docs-instrumentation-django
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Django applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Django application.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Django application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/otel_python_instrumentation.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Python applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in Django.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#requirements
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-requirements
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Requirements</h2>
<ul>
<li>
<p>Python 3.8 or newer</p>
</li>
<li>
<p>for Django, you must define¬†<code>DJANGO_SETTINGS_MODULE</code>correctly. If your project is called¬†<code>mysite</code>, something like following should work:</p>
<pre><code>export DJANGO_SETTINGS_MODULE=mysite.settings
</code></pre>
</li>
</ul>
<p>Please refer the official¬†<a href="https://docs.djangoproject.com/en/1.10/topics/settings/#designating-the-settings">Django docs</a> for more details.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#send-traces-to-signoz-cloud
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetes</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-django
<p>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud</p>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
<p><strong>Step 4.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
opentelemetry-instrument &lt;your_run_command&gt;
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code>¬†is the name of the service you want</li>
<li><code>&lt;your_run_command&gt;</code>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>python manage.py runserver --noreload</code></li>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the api token provided by SigNoz. You can find it in the email sent by SigNoz with your cloud account details.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, you can disable the auto reload with <code>--noreload</code>.</p>
<p><strong>Step 5.</strong> Validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#send-traces-via-otel-collector-binary
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-django
<p>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Python application.</p>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p><strong>Step 4.</strong> To run your application and send data to collector in same VM:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>python manage.py runserver --noreload</code></p>
<p><em><code>http://localhost:4317</code></em> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively.</p>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4317 with <code>&lt;IP Address of the VM&gt;:4317</code>.</p>
<p><strong>Step 5.</strong> You can validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Send Traces to Self-Hosted SigNoz</h2>
<p>You can use OpenTelemetry to send your traces directly to SigNoz. OpenTelemetry provides a handy distro in Python that can help you get started with automatic instrumentation. We recommend using it to get started quickly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#steps-to-auto-instrument-django-app-for-traces
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-steps-to-auto-instrument-django-app-for-traces
group_tracking_ids: docs-instrumentation-django
<p>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Django app for traces</p>
<ol>
<li>
<p><strong>Create a virtual environment</strong></p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
</li>
<li>
<p><strong>Install the OpenTelemetry dependencies</strong></p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenient wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
</li>
<li>
<p><strong>Add automatic instrumentation</strong><br />
The below command inspects the dependencies of your application and installs the instrumentation packages relevant for your Django application.</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
</li>
<li>
<p><strong>Run your application</strong><br />
In the final run command, you can configure environment variables and flags. Flags for exporters:</p>
<p>For running your application, there are a few things that you need to keep in mind. Below are the notes:</p>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, you can disable the auto reload with <code>--noreload</code>.</p>
<p>For running applications with application servers which are based on <a href="#running-applications-with-gunicorn-uwsgi">pre fork model</a>
, like Gunicorn, uWSGI you have to add a post_fork hook or a @postfork decorator in your configuration.</p>
<p>To start sending data to SigNoz, use the following run command:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>python manage.py runserver --noreload</code></p>
<p><code>IP of SigNoz backend</code> is the IP of the machine where you installed SigNoz. If you have installed SigNoz on <code>localhost</code>, the endpoint will be <code>http://localhost:4317</code> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively. Remember to allow incoming requests to port¬†<strong>4317</strong>/<strong>4318</strong>¬†of machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, django
image_urls: https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp
tracking_id: docs-instrumentation-django-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp" alt="Python Application in the list of services being monitored in SigNoz" /></p>
<p><em>Python Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#database-instrumentation
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-database-instrumentation
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Database Instrumentation</h2>
<p>Make sure that the DB client library you are using has the corresponding instrumentation library, and the version of the DB client library is supported by OpenTelemetry.</p>
<h3>## MongoDB Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MongoDB database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MongoDB, the instrumentation library is <code>opentelemetry-instrumentation-pymongo</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Redis Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your Redis database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Redis, the instrumentation library is <code>opentelemetry-instrumentation-redis</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## MySQL Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MySQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MySQL, we have two isntrumentation libraries:</p>
<ul>
<li>opentelemetry-instrumentation-mysql</li>
<li>opentelemetry-instrumentation-pymysql</li>
</ul>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Postgres Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your PostgreSQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Postgres, the instrumentation library is <code>opentelemetry-instrumentation-psycopg2</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<p>üìù Note</p>
<p><code>psycopg2-binary</code> is not supported by opentelemetry auto instrumentation libraries as it is not recommended for production use. Please use <code>psycopg2</code> to see DB calls also in your trace data in SigNoz</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#running-applications-with-gunicorn-uwsgi
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-running-applications-with-gunicorn-uwsgi
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Running applications with Gunicorn, uWSGI</h2>
<p>For application servers which are based on pre fork model like Gunicorn, uWSGI you have to add a <code>post_fork</code> hook or a <code>@postfork</code> decorator in your configuration.</p>
<p>Check this <a href="https://opentelemetry-python.readthedocs.io/en/latest/examples/fork-process-model/README.html">documentation</a> from OpenTelemetry on how to set it up.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-python/tree/main/docs/examples/fork-process-model">Here's</a> a working example a gunicorn server is configured with <code>post_fork</code> hook.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#troubleshooting-your-installation
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Troubleshooting your installation</h2>
<h4>## Spans are not being reported</h4>
<p>If spans are not being reported to SigNoz, try enabling debug exporter which writes the JSON formatted trace data to the console by setting env var OTEL_TRACES_EXPORTER=console.</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=python_app OTEL_TRACES_EXPORTER=console opentelemetry-instrument &lt;your run command&gt;


{
    &quot;name&quot;: &quot;alice&quot;,
    &quot;context&quot;: {
        &quot;trace_id&quot;: &quot;0xedb7caf0c8b082a9578460a201759193&quot;,
        &quot;span_id&quot;: &quot;0x57cf7eee198e1fed&quot;,
        &quot;trace_state&quot;: &quot;[]&quot;
    },
    &quot;kind&quot;: &quot;SpanKind.INTERNAL&quot;,
    &quot;parent_id&quot;: null,
    &quot;start_time&quot;: &quot;2022-03-27T14:55:18.804758Z&quot;,
    &quot;end_time&quot;: &quot;2022-03-27T14:55:18.804805Z&quot;,
    &quot;status&quot;: {
        &quot;status_code&quot;: &quot;UNSET&quot;
    },
    &quot;attributes&quot;: {},
    &quot;events&quot;: [],
    &quot;links&quot;: [],
    &quot;resource&quot;: {
        &quot;telemetry.sdk.language&quot;: &quot;python&quot;,
        &quot;telemetry.sdk.name&quot;: &quot;opentelemetry&quot;,
        &quot;telemetry.sdk.version&quot;: &quot;1.10.0&quot;,
        &quot;service.name&quot;: &quot;my-service&quot;
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#sample-django-application
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-sample-django-application
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Sample django Application</h2>
<ul>
<li>
<p><a href="https://github.com/SigNoz/opentelemetry-python/tree/main/docs/examples/fork-process-model">Working example</a> where we have configured a gunicorn server with <code>post_fork</code> hook.</p>
</li>
<li>
<p>We have included a sample django application with README.md at <a href="https://github.com/SigNoz/sample-django">Sample django App Github Repo.</a></p>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/django/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, django
image_urls: 
tracking_id: docs-instrumentation-django-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-django
<h2>Django OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/
tag_set: instrumentation, jboss
image_urls: https://signoz.io/img/docs/opentelemetry_java_instrument.webp
tracking_id: docs-instrumentation-jboss
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your JBoss applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your JBoss application.</p>
<p>OpenTelemetry Java is the language-specific implementation of OpenTelemetry in Java that can be used to instrument JBoss applications.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your JBoss application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/opentelemetry_java_instrument.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from JBoss applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>There are two types of application instrumentation:</p>
<ul>
<li>
<p><strong>Auto Instrumentation</strong><br />
A completely automatic and out of box experience, with minimal code changes. For your JBoss application, we recommend getting started with auto instrumentation.</p>
</li>
<li>
<p><strong>Manual Instrumentation</strong><br />
It involves writing instrumentation using OpenTelemetry SDK and API manually. You would need to get a handle to an instance of the <code>OpenTelemetry</code> interface, acquire a tracer, and create spans manually.</p>
</li>
</ul>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in a JBoss application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#requirements
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-requirements
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Requirements</h2>
<p>Java 8 or higher</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#send-traces-to-signoz-cloud
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud</h2>
<p>OpenTelemetry provides a handy Java JAR agent that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h4>## Send traces directly to SigNoz Cloud</h4>
<p>OpenTelemetry Java agent can send traces directly to SigNoz Cloud.</p>
<p><strong>Step 1.</strong> Download otel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Open the configuration file</p>
<pre><code>vim /opt/jboss-eap-7.1/bin/standalone.conf
</code></pre>
<p><strong>Step 3.</strong> Update <code>JAVA_OPTS</code> environment variable</p>
<p>Update <code>JAVA_OPTS</code> environment variable with configurations required to send data to SigNoz cloud in your configuration file.</p>
<pre><code>JAVA_OPTS=&quot;-javaagent:/path/opentelemetry-javaagent.jar
-Dotel.exporter.otlp.endpoint=https://ingest.{region}.signoz.cloud:443
-Dotel.exporter.otlp.headers=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot;
-Dotel.resource.attributes=&quot;service.name=&lt;app_name&gt;&quot;&quot;
</code></pre>
<p>You need to replace the following things based on your environment:</p>
<ul>
<li>
<p><code>path</code> - Update it to the path of your downloaded Java JAR agent.</p>
</li>
<li>
<p><code>&lt;app_name&gt;</code> is the name for your application</p>
</li>
<li>
<p><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p><strong>Step 4.</strong> [Optional] Write the output/logs of standalone.sh script to a file nohup.out as a background thread</p>
<pre><code>/opt/jboss-eap-7.1/bin/standalone.sh &gt; /opt/jboss-eap-7.1/bin/nohup.out &amp;
</code></pre>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />
<h4>## Send traces via OTel Collector binary</h4>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Java application.</p>
<p><strong>Step 1.</strong> Download OTel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Open the configuration file</p>
<pre><code>vim /opt/jboss-eap-7.1/bin/standalone.conf
</code></pre>
<p><strong>Step 3.</strong> Update <code>JAVA_OPTS</code> environment variable</p>
<p>Update <code>JAVA_OPTS</code> environment variable with configurations required to send data to SigNoz cloud in your configuration file.</p>
<pre><code>JAVA_OPTS=&quot;-javaagent:/path/opentelemetry-javaagent.jar&quot;
</code></pre>
<p>where,</p>
<ul>
<li><code>path</code> - Update it to the path of your downloaded Java JAR agent.</li>
</ul>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can use OpenTelemetry Java to send your traces directly to SigNoz. OpenTelemetry provides a <strong>handy Java JAR agent</strong> that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<h3>## Steps to auto-instrument JBoss applications for traces</h3>
<p>OpenTelemetry Java auto-instrumentation supports collecting telemetry data from a huge number of libraries and frameworks. You can check out the full list <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/supported-libraries.md">here</a>.</p>
<ol>
<li>
<p><strong>Download the latest OpenTelemetry Java JAR agent</strong><br />
Download the latest <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar">Java JAR agent</a>
. You can also use the terminal to get the file using the following command:</p>
<pre><code> wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
</li>
<li>
<p><strong>Open the configuration file</strong></p>
<pre><code>vim /opt/jboss-eap-7.1/bin/standalone.conf
</code></pre>
</li>
<li>
<p><strong>Update <code>JAVA_OPTS</code> environment variable</strong><br />
Update <code>JAVA_OPTS</code> environment variable with configurations required for OpenTelemetry in your configuration file.</p>
<pre><code>JAVA_OPTS=&quot;-javaagent:/path/opentelemetry-javaagent.jar
-Dotel.exporter.otlp.endpoint=http://&lt;IP of SigNoz Backend&gt;:4317
-Dotel.resource.attributes=&quot;service.name=&lt;app_name&gt;&quot;&quot;
</code></pre>
<p>You need to replace the following things based on your environment:<br />
<code>path</code> - Update it to the path of your downloaded Java JAR agent.<br />
<code>IP of SigNoz Backend</code> - This is the endpoint of the machine where SigNoz is installed.<br />
<code>app_name</code> - Replace it with your app name. It will show up under the list of <code>Services</code> in SigNoz.</p>
<p>Here‚Äôs a handy <a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> to figure out which address to use to send data to SigNoz.</p>
</li>
<li>
<p><strong>[Optional] Write the output/logs of standalone.sh script to a file nohup.out as a background thread</strong></p>
<pre><code>/opt/jboss-eap-7.1/bin/standalone.sh &gt; /opt/jboss-eap-7.1/bin/nohup.out &amp;
</code></pre>
</li>
</ol>
<p>üìù Note</p>
<p>üí° Remember to allow incoming requests to port 4317 of the machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, jboss
image_urls: https://signoz.io/img/docs/java_app_services_list.webp
tracking_id: docs-instrumentation-jboss-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/java_app_services_list.webp" alt="Java Application in the list of services being monitored in SigNoz" /></p>
<p><em>Java Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#configuring-the-agent
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-configuring-the-agent
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Configuring the agent</h2>
<p>The agent is highly configurable. You can check out all the configuration options available <a href="https://opentelemetry.io/docs/instrumentation/java/automatic/agent-config/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#disabled-instrumentations
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-disabled-instrumentations
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Disabled instrumentations</h2>
<p>Some instrumentations can produce too many spans and make traces very noisy. For this reason, the following instrumentations are disabled by default:</p>
<ul>
<li><code>jdbc-datasource</code>¬†which creates spans whenever the¬†<code>java.sql.DataSource#getConnection</code>¬†method is called.</li>
<li><code>dropwizard-metrics</code>,¬†which might create very low-quality metrics data because of the lack of label/attribute support in the Dropwizard metrics API.</li>
</ul>
<p>To enable them, add the¬†<code>otel.instrumentation.&lt;name&gt;.enabled</code>¬†system property:¬†<code>-Dotel.instrumentation.jdbc-datasource.enabled=true</code></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#manual-instrumentation
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-manual-instrumentation
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Manual Instrumentation</h2>
<p>For manual instrumentation of Java application, refer to the docs <a href="https://opentelemetry.io/docs/instrumentation/java/manual/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#troubleshooting-your-installation
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Troubleshooting your installation</h2>
<p>If spans are not being reported to SigNoz, try running in debug mode by setting <code>OTEL_LOG_LEVEL=debug</code>:</p>
<p>The debug log level will print out the configuration information. It will also emit every span to the console, which should look something like:</p>
<pre><code>Span {
  attributes: {},
  links: [],
  events: [],
  status: { code: 0 },
  endTime: [ 1597810686, 885498645 ],
  _ended: true,
  _duration: [ 0, 43333 ],
  name: 'bar',
  spanContext: {
    traceId: 'eca3cc297720bd705e734f4941bca45a',
    spanId: '891016e5f8c134ad',
    traceFlags: 1,
    traceState: undefined
  },
  parentSpanId: 'cff3a2c6bfd4bbef',
  kind: 0,
  startTime: [ 1597810686, 885455312 ],
  resource: Resource { labels: [Object] },
  instrumentationLibrary: { name: 'example', version: '*' },
  _logger: ConsoleLogger {
    debug: [Function],
    info: [Function],
    warn: [Function],
    error: [Function]
  },
  _traceParams: {
    numberOfAttributesPerSpan: 32,
    numberOfLinksPerSpan: 32,
    numberOfEventsPerSpan: 128
  },
  _spanProcessor: MultiSpanProcessor { _spanProcessors: [Array] }
},
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#sample-java-application
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-sample-java-application
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: Sample Java Application</h2>
<ul>
<li>We have included a sample Java application with README.md at <a href="https://github.com/SigNoz/distributed-tracing-java-sample">Sample Java App Github Repo.</a></li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/jboss/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, jboss
image_urls: 
tracking_id: docs-instrumentation-jboss-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-jboss
<h2>JBoss OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Express applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Express application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#requirements
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-requirements
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation: Requirements</h2>
<p>Supported Versions</p>
<ul>
<li><code>^4.0.0</code></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#send-traces-to-signoz-cloud
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
<ul>
<li><a href="#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
<ul>
<li><a href="#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;&lt;SIGNOZ_ENDPOINT&gt;&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_ACCESS_TOKEN&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-send-traces-directly-to-signoz-cloud--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api@^1.6.0                                                                       
npm install --save @opentelemetry/sdk-node@^0.45.0
npm install --save @opentelemetry/auto-instrumentations-node@^0.39.4
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.45.0
</code></pre>
<p><strong>Step 2.</strong> Create tracing.js file<br />
You need to configure the endpoint for SigNoz cloud in this file. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
<pre><code>// tracing.js
'use strict'
const process = require('process');
const opentelemetry = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');

// do not set headers in exporterOptions, the OTel spec recommends setting headers through ENV variables
// https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/exporter.md#specifying-headers-via-environment-variables

// highlight-start
const exporterOptions = {
  url: 'https://ingest.{region}.signoz.cloud:443/v1/traces'
}
// highlight-end

const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({
    // highlight-next-line
    [SemanticResourceAttributes.SERVICE_NAME]: 'node_app'
  })
});

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk.shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0));
});
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443/v1/traces</td>
</tr>
</tbody>
</table>
<p><strong>Step 3.</strong> Run the application<br />
Make sure you set the <code>OTEL_EXPORTER_OTLP_HEADERS</code> env as follows</p>
<pre><code>OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;&quot; node -r ./tracing.js app.js
</code></pre>
<p><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
<p><strong>Step 4.</strong> You can validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-send-traces-via-otel-collector-binary--no-code-automatic-instrumentation
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - No Code Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended-1
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended-1
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4318/v1/traces&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-send-traces-via-otel-collector-binary--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - Code Level Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api@^1.6.0
npm install --save @opentelemetry/sdk-node@^0.45.0
npm install --save @opentelemetry/auto-instrumentations-node@^0.39.4
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.45.0
</code></pre>
<p><strong>Step 2.</strong> Create tracing.js file</p>
<pre><code>// tracing.js
'use strict'
const process = require('process');
const opentelemetry = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');

const exporterOptions = {
  url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces',
}

const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({
    // highlight-next-line
    [SemanticResourceAttributes.SERVICE_NAME]: 'node_app'
  })
});

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk.shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0));
});
</code></pre>
<p><strong>Step 3.</strong> Run the application</p>
<pre><code>node -r ./tracing.js app.js
</code></pre>
<p><strong>Step 4.</strong> You can validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, express
image_urls: https://signoz.io/img/docs/express_instrumentation.webp
tracking_id: docs-instrumentation-express-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Send Traces to Self-Hosted SigNoz</h2>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Express application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/express_instrumentation.webp" alt="All in one auto instrumentation library - identifies and instruments packages used by your NestJS application" /></p>
<p><em>All in one auto instrumentation library - identifies and instruments packages used by your Express application</em></p>
<p>You have two choices for instrumenting your Express application with OpenTelemetry.</p>
<ul>
<li>
<p><strong><a href="#using-the-all-in-one-auto-instrumentation-library">Use the all-in-one auto-instrumentation library(Recommended)</a></strong><br />
The auto-instrumentation library of OpenTelemetry is a meta package that provides a simple way to initialize multiple Nodejs instrumnetations.</p>
<p>‚úÖ Info</p>
<p>If you are on K8s, you should checkout <a href="/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">opentelemetry operators</a> which enable auto instrumenting Javascript applications very easily.</p>
</li>
<li>
<p><strong><a href="#using-a-specific-auto-instrumentation-library">Use a specific auto-instrumentation library</a></strong><br />
You can use individual auto-instrumentation libraries too for a specific component of your application. For example, you can use <code>@opentelemetry/instrumentation-express</code> for instrumenting the Express web framework.</p>
</li>
</ul>
<p>Let's see how to instrument your Express application with OpenTelemetry.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#using-the-all-in-one-auto-instrumentation-library
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-using-the-all-in-one-auto-instrumentation-library
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Using the all-in-one auto-instrumentation library</p>
<p>The recommended way to instrument your Express application is to use the all-in-one auto-instrumentation library - <code>@opentelemetry/auto-instrumentations-node</code>. It provides a simple way to initialize multiple Nodejs instrumentations.</p>
<p>Internally, it calls the specific auto-instrumentation library for components used in the application. You can see the complete list <a href="https://github.com/open-telemetry/opentelemetry-js-contrib/tree/main/metapackages/auto-instrumentations-node#supported-instrumentations">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#steps-to-auto-instrument-express-application
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-steps-to-auto-instrument-express-application
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Express application: Steps to auto-instrument Express application: Steps to auto-instrument Express application</p>
<ol>
<li>
<p>Install the dependencies<br />
We start by installing the relevant dependencies.</p>
<pre><code>npm install --save @opentelemetry/sdk-node
npm install --save @opentelemetry/auto-instrumentations-node
npm install --save @opentelemetry/exporter-trace-otlp-http
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>@opentelemetry/sdk-node</code> - This package provides the full OpenTelemetry SDK for Node.js including tracing and metrics.</p>
<p><code>@opentelemetry/auto-instrumentations-node</code> - This module provides a simple way to initialize multiple Node instrumentations.</p>
<p><code>@opentelemetry/exporter-trace-otlp-http</code> - This module provides the exporter to be used with OTLP (<code>http/json</code>) compatible receivers.</p>
<p>üìù Note</p>
<p>If you run into any error, you might want to use these pinned versions of OpenTelemetry libraries used in this <a href="https://github.com/SigNoz/sample-nodejs-app/blob/master/package.json">GitHub repo</a>
.</p>
</li>
<li>
<p><strong>Create a <code>tracing.js</code> file</strong><br />
The <code>tracing.js</code> file will contain the tracing setup code. Notice, that we have set some environment variables in the code(highlighted). You can update these variables based on your environment.</p>
<p>// tracing.js
'use strict'
const process = require('process');
const opentelemetry = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');</p>
<p>const exporterOptions = {
// highlight-next-line
url: 'http://localhost:4318/v1/traces'
}</p>
<p>const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new opentelemetry.NodeSDK({
traceExporter,
instrumentations: [getNodeAutoInstrumentations()],
// highlight-start
resource: new Resource({
[SemanticResourceAttributes.SERVICE_NAME]: 'node_app'
})
// highlight-end
});</p>
<pre><code>// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk.shutdown()
  .then(() =&gt; console.log('Tracing terminated'))
  .catch((error) =&gt; console.log('Error terminating tracing', error))
  .finally(() =&gt; process.exit(0));
  });
</code></pre>
</li>
</ol>
<p>OpenTelemetry Node SDK currently does not detect the <code>OTEL_RESOURCE_ATTRIBUTES</code> from <code>.env</code> files as of today. That‚Äôs why we need to include the variables in the <code>tracing.js</code> file itself.</p>
<p>About environment variables:</p>
<ul>
<li>
<p><code>service_name</code>¬†: node_app (you can give whatever name that suits you)</p>
</li>
<li>
<p><code>http://localhost:4318/v1/traces</code> is the default url for sending your tracing data. We are assuming you have installed SigNoz on your <code>localhost</code>. Based on your environment, you can update it accordingly. It should be in the following format:</p>
<pre><code>http://&lt;IP of SigNoz backend&gt;:4318/v1/traces
</code></pre>
<p>Here‚Äôs a handy <a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> to figure out which address to use to send data to SigNoz.</p>
<p>üìù Note</p>
<p>Remember to allow incoming requests to port 4318 of machine where SigNoz backend is hosted.</p>
</li>
</ul>
<ol start="3">
<li>
<p><strong>Run the application</strong><br />
The tracing configuration should be run before your application code. We will use the <a href="https://nodejs.org/api/cli.html#cli_r_require_module"><code>-r, ‚Äîrequire module</code></a> flag for that.</p>
<pre><code>node -r ./tracing.js app.js
</code></pre>
<p>üìù Note</p>
<p>If you're running your nodejs application in PM2 cluster mode, it doesn't support node args: <a href="https://github.com/Unitech/pm2/issues/3227">Unitech/pm2#3227</a>
. As above sample app instrumentation requires to load <code>tracing.js</code> before app load by passing node arg, so nodejs instrumentation doesn't work in PM2 cluster mode. So you need to import <code>tracing.js</code> in your main application. The <code>import ./tracing.js</code> should be the first line of your application code and initialize it before any other function. Here's the <a href="https://github.com/SigNoz/sample-nodejs-app/tree/init-tracer-main">sample github repo</a> which shows the implementation.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, express
image_urls: https://signoz.io/img/docs/express_application_services_list.webp
tracking_id: docs-instrumentation-express-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Express application: Validating instrumentation by checking for traces</p>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the <code>Services</code> tab. Hit the <code>Refresh</code> button on the top right corner, and your application should appear in the list of <code>Applications</code>.</li>
<li>Go to the <code>Traces</code> tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs <a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/express_application_services_list.webp" alt="Nestjs Application in the list of services being monitored in SigNoz" /></p>
<p><em>Express Application in the list of services being monitored in SigNoz</em></p>
<p>If you don't see your application reported in the list of services, try our <a href="https://signoz.io/docs/install/troubleshooting/">troubleshooting</a> guide.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#using-a-specific-auto-instrumentation-library
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-using-a-specific-auto-instrumentation-library
group_tracking_ids: docs-instrumentation-express
<p>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Express application: Using a specific auto-instrumentation library</p>
<p>If you want to instrument only your Express framework, then you need to use the following package:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-express
</code></pre>
<p>üìù Note</p>
<p>In the above case, you will have to install packages for all the components that you want to instrument with OpenTelemetry individually. You can find detailed instructions <a href="https://signoz.io/docs/instrumentation/javascript/#using-a-specific-auto-instrumentation-library">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#instrumentation-modules-for-databases
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-instrumentation-modules-for-databases
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Instrumentation Modules for Databases</h2>
<p>The <code>@opentelemetry/auto-instrumentations-node</code> can inititialize instrumentation for popular databases. Hence it‚Äôs recommended to <a href="#using-the-all-in-one-auto-instrumentation-library">get started</a> with it.</p>
<p>But if you are using <a href="#using-a-specific-auto-instrumentation-library">specific auto-instrumentation packages</a>
, here‚Äôs a list of packages for popular databases.</p>
<h3>## MongoDB instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>‚Ä¢ <code>&gt;=3.3 &lt;5</code></p>
<p>Module that provides automatic instrumentation for MongoDB:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-mongodb
</code></pre>
<p>üìù Note</p>
<p>Refer here for sample <a href="https://github.com/SigNoz/sample-nodejs-app/tree/mongodb">NodeJs express application with MongoDB.</a></p>
<h3>## Redis Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>This package supports¬†<code>redis@^2.6.0</code> and¬†<code>redis@^3.0.0</code> For version¬†<code>redis@^4.0.0</code>, please use¬†<code>@opentelemetry/instrumentation-redis-4</code></p>
<pre><code>npm install --save @opentelemetry/instrumentation-redis
</code></pre>
<h3>## MySQL Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>‚Ä¢ <code>2.x</code></p>
<p>Module that provides automatic instrumentation for MySQL:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-mysql
</code></pre>
<h3>## Memcached Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<ul>
<li><code>&gt;=2.2</code></li>
</ul>
<p>Module that provides automatic instrumentation for Memcached:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-memcached
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#troubleshooting-your-installation
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Troubleshooting your installation</h2>
<p>Set an environment variable to run the OpenTelemetry launcher in debug mode, where it logs details about the configuration and emitted spans:</p>
<pre><code>export OTEL_LOG_LEVEL=debug
</code></pre>
<p>The output may be very verbose with some benign errors. Early in the console output, look for logs about the configuration. Next, look for lines like the ones below, which are emitted when spans are emitted to SigNoz.</p>
<pre><code>{
  &quot;traceId&quot;: &quot;985b66d592a1299f7d12ebca56ca1fe3&quot;,
  &quot;parentId&quot;: &quot;8d62a70aa335a227&quot;,
  &quot;name&quot;: &quot;bar&quot;,
  &quot;id&quot;: &quot;17ada85c3d55376a&quot;,
  &quot;kind&quot;: 0,
  &quot;timestamp&quot;: 1685674607399000,
  &quot;duration&quot;: 299,
  &quot;attributes&quot;: {},
  &quot;status&quot;: { &quot;code&quot;: 0 },
  &quot;events&quot;: []
}
{
  &quot;traceId&quot;: &quot;985b66d592a1299f7d12ebca56ca1fe3&quot;,
  &quot;name&quot;: &quot;foo&quot;,
  &quot;id&quot;: &quot;8d62a70aa335a227&quot;,
  &quot;kind&quot;: 0,
  &quot;timestamp&quot;: 1585130342183948,
  &quot;duration&quot;: 315,
  &quot;attributes&quot;: {
    &quot;name&quot;: &quot;value&quot;
  },
  &quot;status&quot;: { &quot;code&quot;: 0 },
  &quot;events&quot;: [\
    {\
      &quot;name&quot;: &quot;event in foo&quot;,\
      &quot;time&quot;: [1585130342, 184213041]\
    }\
  ]
}
</code></pre>
<p><em>Running short applications (Lambda/Serverless/etc)</em> If your application exits quickly after startup, you may need to explicitly shutdown the tracer to ensure that all spans are flushed:</p>
<pre><code>opentelemetry.trace.getTracer('your_tracer_name').getActiveSpanProcessor().shutdown()
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#sample-express-app
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-sample-express-app
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Sample Express App</h2>
<ul>
<li>
<p>We have included a sample applications at:</p>
</li>
<li>
<p><a href="https://github.com/SigNoz/sample-nodejs-app">Sample NodeJs App Github Repo</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#further-reading
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-further-reading
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Further Reading</h2>
<ul>
<li>
<p><a href="https://signoz.io/blog/nodejs-performance-monitoring/">Nodejs Performance Monitoring</a></p>
</li>
<li>
<p><a href="https://signoz.io/blog/distributed-tracing-nodejs/">Implementing Distributed Tracing in a Nodejs application</a></p>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/express/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, express
image_urls: 
tracking_id: docs-instrumentation-express-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-express
<h2>Express OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/celery/
tag_set: instrumentation, celery
image_urls: 
tracking_id: docs-instrumentation-celery
group_tracking_ids: docs-instrumentation-celery
<h2>Celery Worker OpenTelemetry Setup</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Celery worker applications.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/celery/#background
tag_set: instrumentation, celery
image_urls: 
tracking_id: docs-instrumentation-celery-background
group_tracking_ids: docs-instrumentation-celery
<h2>Celery Worker OpenTelemetry Setup: Background</h2>
<p>Celery uses prefork worker model by <a href="https://celeryproject.readthedocs.io/zh-cn/latest/userguide/configuration.html#worker-pool">default</a>. In this model, it is important to ensure that the libraries which are not fork-safe create their own instances in the parent process and each child process. To achieve this, we use <code>worker_process_init</code> hook and instantiate the OpenTelemetry SDK in each worker process. Add the following code to the app file of your Celery application.</p>
<pre><code>from celery.signals import worker_process_init

from opentelemetry.instrumentation.celery import CeleryInstrumentor
from opentelemetry import metrics, trace
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import (
    OTLPMetricExporter,
)
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import (
    OTLPSpanExporter,
)
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

@worker_process_init.connect(weak=False)
def init_celery_tracing(*args, **kwargs):
    CeleryInstrumentor().instrument()

    resource = Resource.create({})

    trace.set_tracer_provider(TracerProvider(resource=resource))
    span_processor = BatchSpanProcessor(OTLPSpanExporter())
    trace.get_tracer_provider().add_span_processor(span_processor)

    reader = PeriodicExportingMetricReader(
        OTLPMetricExporter()
    )
    metrics.set_meter_provider(
        MeterProvider(
            resource=resource,
            metric_readers=[reader],
        )
    )
</code></pre>
<p>A brief explanation of the code above:</p>
<ul>
<li>Import the <code>worker_process_init</code> to connect to the Celery signal so that each worker process can initialize its own OpenTelemetry SDK.</li>
<li>Import the <code>CeleryInstrumentor</code> to instrument Celery.</li>
<li>Import the necessary components from the OpenTelemetry library to set up the tracer and meter providers.</li>
<li>Set up the tracer and meter providers with processors and exporters.</li>
</ul>
<p>üìù Note</p>
<p>This adjustment is required only for the worker processes, not for the producer.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/celery/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, celery
image_urls: 
tracking_id: docs-instrumentation-celery-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-celery
<h2>Celery Worker OpenTelemetry Setup: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/
tag_set: instrumentation, nextjs
image_urls: 
tracking_id: docs-instrumentation-nextjs
group_tracking_ids: docs-instrumentation-nextjs
<h2>Nextjs OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Nextjs applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Nextjs application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#send-traces-to-signoz-cloud
tag_set: instrumentation, nextjs
image_urls: 
tracking_id: docs-instrumentation-nextjs-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-nextjs
<h2>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
<ul>
<li><a href="#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
<ul>
<li><a href="#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended
tag_set: instrumentation, nextjs
image_urls: 
tracking_id: docs-instrumentation-nextjs-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended
group_tracking_ids: docs-instrumentation-nextjs
<p>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;&lt;SIGNOZ_ENDPOINT&gt;&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_ACCESS_TOKEN&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation
tag_set: instrumentation, nextjs
image_urls: 
tracking_id: docs-instrumentation-nextjs-send-traces-directly-to-signoz-cloud--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-nextjs
<p>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation</p>
<p><strong>Step 1.</strong> Install the OpenTelemetry packages</p>
<pre><code>npm i @opentelemetry/sdk-node
npm i @opentelemetry/auto-instrumentations-node
npm i @opentelemetry/exporter-trace-otlp-http
npm i @opentelemetry/resources
npm i @opentelemetry/semantic-conventions
</code></pre>
<p><strong>Step 2.</strong> Update <code>next.config.mjs</code> to include instrumentationHook</p>
<pre><code>/** @type {import('next').NextConfig} */
const nextConfig = {
    // include instrumentationHook experimental feature
    experimental: {
        instrumentationHook: true,
    },
};

export default nextConfig;
</code></pre>
<p><strong>Step 3.</strong> Create <code>instrumentation.ts</code> file</p>
<pre><code>export async function register() {
  if (process.env.NEXT_RUNTIME === 'nodejs') {
    await import('./instrumentation.node')
  }
}
</code></pre>
<p><strong>Step 4.</strong> Create <code>instrumentation.node.ts</code> file<br />
You need to configure the endpoint for SigNoz cloud in this file.</p>
<pre><code>'use strict'
import process from 'process';
import {NodeSDK} from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http'
import { Resource } from '@opentelemetry/resources'
import { SEMRESATTRS_SERVICE_NAME } from '@opentelemetry/semantic-conventions'
 
// Add otel logging when debugging
// import { diag, DiagConsoleLogger, DiagLogLevel } from '@opentelemetry/api';
// diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);

const exporterOptions = {
  url: 'https://ingest.[region].signoz.cloud:443/v1/traces', // use your own data region 
  headers: { 'signoz-access-token': 'SIGNOZ_INGESTION_KEY' }, // Use if you are using SigNoz Cloud
}

const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
    resource: new Resource({
    [SEMRESATTRS_SERVICE_NAME]: 'next-app',
  }),
});

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk.shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0));
});
</code></pre>
<ul>
<li><code>SIGNOZ_INGESTION_KEY</code> : You can find your ingestion key from SigNoz cloud account details sent on your email.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443/v1/traces</td>
</tr>
</tbody>
</table>
<p><strong>Step 5.</strong> Once you're done configuring the exporter options, try running your application and validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>You can also check the sample application at this <a href="https://github.com/SigNoz/sample-nextjs-app">GitHub repo</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation
tag_set: instrumentation, nextjs
image_urls: 
tracking_id: docs-instrumentation-nextjs-send-traces-via-otel-collector-binary--no-code-automatic-instrumentation
group_tracking_ids: docs-instrumentation-nextjs
<p>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - No Code Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended-1
tag_set: instrumentation, nextjs
image_urls: 
tracking_id: docs-instrumentation-nextjs-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended-1
group_tracking_ids: docs-instrumentation-nextjs
<p>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4318/v1/traces&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation
tag_set: instrumentation, nextjs
image_urls: 
tracking_id: docs-instrumentation-nextjs-send-traces-via-otel-collector-binary--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-nextjs
<p>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - Code Level Automatic Instrumentation</p>
<p><strong>Step 1.</strong> Install OpenTelemetry Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM.</p>
<p>While creating the <code>config.yaml</code> during the installation fo the OTel Collector Binary, you need to enable CORS under the receivers section of the config file. This is needed so that you don't get CORS error which can hinder sending your Traces to SigNoz Cloud. See the code snippet below to understand how you can enable CORS in your config file:</p>
<pre><code>      http:
+        cors:
+          allowed_origins:
+            - &lt;Frontend-application-URL&gt;  # URL of your Frontend application. Example -&gt; http://localhost:4200, https://netflix.com etc.
</code></pre>
<p><code>&lt;Frontend-application-URL&gt;</code> - URL where your frontend application is running. For Example, <a href="http://localhost:4200">http://localhost:4200</a> or <a href="https://netflix.com">https://netflix.com</a> etc.</p>
<p><strong>NOTE:</strong> Make sure to restart your collector after making the config changes</p>
<p><strong>Step 2.</strong> Install OpenTelemetry packages</p>
<pre><code>npm i @opentelemetry/sdk-node
npm i @opentelemetry/auto-instrumentations-node
npm i @opentelemetry/exporter-trace-otlp-http
npm i @opentelemetry/resources
npm i @opentelemetry/semantic-conventions
</code></pre>
<p><strong>Step 3.</strong> Update <code>next.config.mjs</code> to include instrumentationHook</p>
<pre><code>/** @type {import('next').NextConfig} */
const nextConfig = {
    // include instrumentationHook experimental feature
    experimental: {
        instrumentationHook: true,
    },
};

export default nextConfig;
</code></pre>
<p><strong>Step 4.</strong> Create <code>instrumentation.ts</code> file</p>
<pre><code>export async function register() {
  if (process.env.NEXT_RUNTIME === 'nodejs') {
    await import('./instrumentation.node')
  }
}
</code></pre>
<p><strong>Step 5.</strong> Create <code>instrumentation.node.ts</code> file<br />
You need to configure the endpoint for SigNoz cloud in this file.</p>
<pre><code>'use strict'
import process from 'process';
import {NodeSDK} from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http'
import { Resource } from '@opentelemetry/resources'
import { SEMRESATTRS_SERVICE_NAME } from '@opentelemetry/semantic-conventions'
 
// Add otel logging when debugging
// import { diag, DiagConsoleLogger, DiagLogLevel } from '@opentelemetry/api';
// diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);

const exporterOptions = {
  url: 'http://localhost:4318/v1/traces',
}

const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
    resource: new Resource({
    [SEMRESATTRS_SERVICE_NAME]: 'next-app',
  }),
});

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk.shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0));
});
</code></pre>
<p><strong>Step 6.</strong> Once we are done with the above configurations, you can run the collector service with the following command:</p>
<pre><code>./otelcol-contrib --config ./config.yaml
</code></pre>
<p><strong>Step 7.</strong> Start running your application and wait for few seconds to start receiving instrumentation data on the SigNoz Cloud.</p>
<p>You can also check the sample application at this <a href="https://github.com/SigNoz/sample-nextjs-app">GitHub repo</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#send-traces-to-signoz-self-host
tag_set: instrumentation, nextjs
image_urls: 
tracking_id: docs-instrumentation-nextjs-send-traces-to-signoz-self-host
group_tracking_ids: docs-instrumentation-nextjs
<h2>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Self-Host: Send traces to SigNoz Self-Host</h2>
<p>If you're trying to send instrumentation data to SigNoz self-hosted way, the only minor thing (apart from installing OpenTelemetry packages) that you'd be required is to change the <code>exporterOptions</code> in the <code>tracing.js</code> file</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/kubernetes-infra-metrics/">here</a> in your Kubernetes cluster.</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm i @opentelemetry/sdk-node
npm i @opentelemetry/auto-instrumentations-node
npm i @opentelemetry/exporter-trace-otlp-http
npm i @opentelemetry/resources
npm i @opentelemetry/semantic-conventions
</code></pre>
<p><strong>Step 2.</strong> Update <code>next.config.mjs</code> to include instrumentationHook</p>
<pre><code>/** @type {import('next').NextConfig} */
const nextConfig = {
    // include instrumentationHook experimental feature
    experimental: {
        instrumentationHook: true,
    },
};

export default nextConfig;
</code></pre>
<p><strong>Step 3.</strong> Create <code>instrumentation.ts</code> file</p>
<pre><code>export async function register() {
  if (process.env.NEXT_RUNTIME === 'nodejs') {
    await import('./instrumentation.node')
  }
}
</code></pre>
<p><strong>Step 4.</strong> Create <code>instrumentation.node.ts</code> file<br />
You need to configure the endpoint for SigNoz cloud in this file.</p>
<pre><code>'use strict'
import process from 'process';
import {NodeSDK} from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http'
import { Resource } from '@opentelemetry/resources'
import { SEMRESATTRS_SERVICE_NAME } from '@opentelemetry/semantic-conventions'
 
// Add otel logging when debugging
// import { diag, DiagConsoleLogger, DiagLogLevel } from '@opentelemetry/api';
// diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);

const exporterOptions = {
  url: 'http://localhost:4318/v1/traces',
}

const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
    resource: new Resource({
    [SEMRESATTRS_SERVICE_NAME]: 'next-app',
  }),
});

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk.shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0));
});
</code></pre>
<p>Again, <code>http://localhost:4318/v1/traces</code> is the default url for sending your tracing data. We are assuming you have installed SigNoz on your localhost. Based on your environment, you can update it accordingly. It should be in the following format:</p>
<p><code>http://&lt;IP of SigNoz backend&gt;:4318/v1/traces</code></p>
<p><strong>NOTE:</strong> Remember to allow incoming requests to port 4318 of machine where SigNoz backend is hosted.</p>
<p>Once you're done with this, add your required changes including receivers, exporters in the <code>config.yml</code> file which can be generally found <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/otel-collector-config.yaml">here</a>.</p>
<p><strong>Step 5.</strong> Once you're done configuring the exporter options, try running your application and validate if your application is sending traces to SigNoz <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>You can also check the sample application at this <a href="https://github.com/SigNoz/sample-nextjs-app">GitHub repo</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, nextjs
image_urls: https://signoz.io/img/docs/nextjs_application_instrumentation.webp
tracking_id: docs-instrumentation-nextjs-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-nextjs
<p>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Self-Host: Validating instrumentation by checking for traces</p>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the <code>Services</code> tab. Hit the <code>Refresh</code> button on the top right corner, and your application should appear in the list of <code>Applications</code>.</li>
<li>Go to the <code>Traces</code> tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs <a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/nextjs_application_instrumentation.webp" alt="Nextjs Application in the list of services being monitored in SigNoz" /></p>
<p><em>Nextjs Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nextjs/#conclusion
tag_set: instrumentation, nextjs
image_urls: https://signoz.io/img/blog/common/signoz_github.webp, https://signoz.io/img/blog/common/join_slack_cta.webp
tracking_id: docs-instrumentation-nextjs-conclusion
group_tracking_ids: docs-instrumentation-nextjs
<h2>Nextjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Self-Host: Conclusion</h2>
<p>OpenTelemetry is the future for setting up observability for cloud-native apps. It is backed by a huge community and covers a wide variety of technology and frameworks. Using OpenTelemetry, engineering teams can instrument polyglot and distributed applications and be assured about compatibility with a lot of technologies.</p>
<p>SigNoz is an open-source observability tool that comes with a SaaS-like experience. You can check out SigNoz by visiting its GitHub repo üëá</p>
<p><a href="https://github.com/SigNoz/signoz"><img src="https://signoz.io/img/blog/common/signoz_github.webp" alt="SigNoz GitHub repo" /></a></p>
<p>If you are someone who understands more from video, then you can watch the below video tutorial on the same with SigNoz.</p>
<p>If you face any issues while trying out SigNoz, you can reach out with your questions in #support channel üëá</p>
<p><a href="https://signoz.io/slack"><img src="https://signoz.io/img/blog/common/join_slack_cta.webp" alt="SigNoz Slack community" /></a></p>
<p><strong>Further Reading</strong></p>
<p><a href="https://signoz.io/blog/opentelemetry-angular/">Implementing OpenTelemetry in Angular application</a></p>
<p><a href="https://signoz.io/opentelemetry/nodejs/">Monitor your Nodejs application with OpenTelemetry and SigNoz</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/
tag_set: instrumentation, springboot
image_urls: https://signoz.io/img/docs/opentelemetry_java_instrument.webp
tracking_id: docs-instrumentation-springboot
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Spring Boot applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Spring Boot application.</p>
<p>OpenTelemetry Java is the language-specific implementation of OpenTelemetry in Java that can be used to instrument Spring Boot applications.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Spring Boot application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/opentelemetry_java_instrument.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Spring Boot applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>There are two types of application instrumentation:</p>
<ul>
<li>
<p><strong>Auto Instrumentation</strong><br />
A completely automatic and out of box experience, with minimal code changes. For your Spring Boot application, we recommend getting started with auto instrumentation.</p>
</li>
<li>
<p><strong>Manual Instrumentation</strong><br />
It involves writing instrumentation using OpenTelemetry SDK and API manually. You would need to get a handle to an instance of the <code>OpenTelemetry</code> interface, acquire a tracer, and create spans manually.</p>
</li>
</ul>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in a Spring Boot application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#requirements
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-requirements
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Requirements</h2>
<p>Java 8 or higher</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#send-traces-to-signoz-cloud
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud</h2>
<p>OpenTelemetry provides a handy Java JAR agent that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h4>## Send traces directly to SigNoz Cloud</h4>
<p>OpenTelemetry Java agent can send traces directly to SigNoz Cloud.</p>
<p><strong>Step 1.</strong> Download otel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.{region}.signoz.cloud:443 \
java -javaagent:$PWD/opentelemetry-javaagent.jar -jar &lt;my-app&gt;.jar
</code></pre>
<ul>
<li><code>&lt;app_name&gt;</code> is the name for your application</li>
<li><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />
<h4>## Send traces via OTel Collector binary</h4>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Java application.</p>
<p><strong>Step 1.</strong> Download OTel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Run your application</p>
<pre><code>java -javaagent:$PWD/opentelemetry-javaagent.jar -jar &lt;myapp&gt;.jar
</code></pre>
<ul>
<li><code>&lt;myapp&gt;</code> is the name of your application jar file</li>
<li>In case you download <code>opentelemetry-javaagent.jar</code> file in different directory than that of the project, replace <code>$PWD</code> with the path of the otel jar file.</li>
</ul>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can use OpenTelemetry Java to send your traces directly to SigNoz. OpenTelemetry provides a <strong>handy Java JAR agent</strong> that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<h3>## Steps to auto-instrument Spring Boot applications for traces</h3>
<p><a href="https://signoz.io/opentelemetry/java-auto-instrumentation/">OpenTelemetry Java auto-instrumentation</a> supports collecting telemetry data from a huge number of libraries and frameworks. You can check out the full list <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/supported-libraries.md">here</a>.</p>
<ol>
<li>
<p><strong>Download the latest OpenTelemetry Java JAR agent</strong><br />
Download the latest <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar">Java JAR agent</a>
. You can also use the terminal to get the file using the following command:</p>
<pre><code> wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
</li>
<li>
<p><strong>Enable the instrumentation agent and run your application</strong><br />
If you run your Spring Boot application as a JAR file, run your application using the following command:</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; java -javaagent:/path/to/opentelemetry-javaagent.jar -jar  &lt;myapp&gt;.jar
</code></pre>
<p>where <code>&lt;app_name&gt;</code> is the name you want to set for your application.¬†<code>path</code> should be updated to the path of the downloaded Java JAR agent.</p>
<p>In the above command, we are configuring the exporter to send data to SigNoz backend. By default, OpenTelemetry Java agent uses¬†<a href="https://github.com/open-telemetry/opentelemetry-java/tree/main/exporters/otlp">OTLP exporter</a> configured to send data.</p>
<p>Two things to note about the command:</p>
<p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code> - This is the endpoint of the machine where SigNoz is installed.</p>
<p><code>path/to</code> - Update it to the path of your downloaded Java JAR agent.</p>
<p>If you have installed SigNoz on your <code>localhost</code> and your Java JAR agent is saved at <code>/Users/john/Downloads/</code>, then the final command looks like:</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=javaApp java -javaagent:/Users/john/Downloads/opentelemetry-javaagent.jar -jar target/*.jar
</code></pre>
<p>Here‚Äôs a handy <a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> to figure out which address to use to send data to SigNoz.</p>
<p>You can also specify environment variables in the following way:</p>
<pre><code>java -javaagent:/path/opentelemetry-javaagent.jar \
    -Dotel.exporter.otlp.endpoint=http://&lt;IP of SigNoz Backend&gt;:4317 \
    -Dotel.resource.attributes=service.name=&lt;app_name&gt; \
    -jar &lt;myapp&gt;.jar
</code></pre>
</li>
</ol>
<p>üìù Note</p>
<p>üí° Remember to allow incoming requests to port 4317 of the machine where SigNoz backend is hosted.</p>
<p>If you want to try this instrumentation with a sample Spring Boot application, visit this <a href="https://github.com/SigNoz/spring-petclinic">GitHub repo</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, springboot
image_urls: https://signoz.io/img/docs/java_app_services_list.webp
tracking_id: docs-instrumentation-springboot-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/java_app_services_list.webp" alt="Java Application in the list of services being monitored in SigNoz" /></p>
<p><em>Java Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#configuring-the-agent
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-configuring-the-agent
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Configuring the agent</h2>
<p>The agent is highly configurable. You can check out all the configuration options available <a href="https://opentelemetry.io/docs/instrumentation/java/automatic/agent-config/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#disabled-instrumentations
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-disabled-instrumentations
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Disabled instrumentations</h2>
<p>Some instrumentations can produce too many spans and make traces very noisy. For this reason, the following instrumentations are disabled by default:</p>
<ul>
<li><code>jdbc-datasource</code>¬†which creates spans whenever the¬†<code>java.sql.DataSource#getConnection</code>¬†method is called.</li>
<li><code>dropwizard-metrics</code>,¬†which might create very low-quality metrics data because of the lack of label/attribute support in the Dropwizard metrics API.</li>
</ul>
<p>To enable them, add the¬†<code>otel.instrumentation.&lt;name&gt;.enabled</code>¬†system property:¬†<code>-Dotel.instrumentation.jdbc-datasource.enabled=true</code></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#manual-instrumentation
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-manual-instrumentation
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Manual Instrumentation</h2>
<p>For manual instrumentation of Java application, refer to the docs <a href="https://opentelemetry.io/docs/instrumentation/java/manual/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#troubleshooting-your-installation
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Troubleshooting your installation</h2>
<p>If spans are not being reported to SigNoz, try running in debug mode by setting <code>OTEL_LOG_LEVEL=debug</code>:</p>
<p>The debug log level will print out the configuration information. It will also emit every span to the console, which should look something like:</p>
<pre><code>Span {
  attributes: {},
  links: [],
  events: [],
  status: { code: 0 },
  endTime: [ 1597810686, 885498645 ],
  _ended: true,
  _duration: [ 0, 43333 ],
  name: 'bar',
  spanContext: {
    traceId: 'eca3cc297720bd705e734f4941bca45a',
    spanId: '891016e5f8c134ad',
    traceFlags: 1,
    traceState: undefined
  },
  parentSpanId: 'cff3a2c6bfd4bbef',
  kind: 0,
  startTime: [ 1597810686, 885455312 ],
  resource: Resource { labels: [Object] },
  instrumentationLibrary: { name: 'example', version: '*' },
  _logger: ConsoleLogger {
    debug: [Function],
    info: [Function],
    warn: [Function],
    error: [Function]
  },
  _traceParams: {
    numberOfAttributesPerSpan: 32,
    numberOfLinksPerSpan: 32,
    numberOfEventsPerSpan: 128
  },
  _spanProcessor: MultiSpanProcessor { _spanProcessors: [Array] }
},
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#sample-application
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-sample-application
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: Sample Application</h2>
<ul>
<li>We have included a sample application at:
<ul>
<li>
<p><a href="https://github.com/SigNoz/distributed-tracing-java-sample">Sample Java App Github Repo.</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/spring-petclinic">Sample Springboot App Github Repo.</a></p>
</li>
</ul>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/springboot/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, springboot
image_urls: 
tracking_id: docs-instrumentation-springboot-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-springboot
<h2>Spring Boot OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/saved-view/#introduction
tag_set: product-features, saved-view
image_urls: 
tracking_id: docs-product-features-saved-view-introduction
group_tracking_ids: docs-product-features-saved-view
<h2>Save a view in SigNoz: Save a view in SigNoz - Introduction</h2>
<p><img src="https://signoz.io/img/docs/product-features/saved-view/saved-view.gif" alt="A gif explaining the Save View feature in SigNoz" /></p>
<p><em>Save View feature in Logs Explorer of SigNoz</em></p>
<p>This feature enables you to customize and preserve specific filter settings for the logs and traces data and save these tailored views for swift access in the future. This document provides a step-by-step process to using Saved Views in SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/saved-view/#use-cases
tag_set: product-features, saved-view
image_urls: 
tracking_id: docs-product-features-saved-view-use-cases
group_tracking_ids: docs-product-features-saved-view
<h2>Save a view in SigNoz: Use-Cases</h2>
<p>The Saved Views functionality caters to a variety of real-life scenarios across the Logs and Traces Explorers:</p>
<ul>
<li><strong>Fast Incidence Response</strong>: During an incident, time is of the essence. Saved Views can help you quickly zoom in on the anomaly by applying predefined filters, allowing you to diagnose and respond to issues faster.</li>
<li><strong>Collaborative Analysis</strong>: When your team needs to tackle a problem, Saved Views ensure everyone can access the same data perspective instantly, fostering a unified approach to resolving issues.</li>
<li><strong>Continuous Monitoring</strong>: For routine checks, a saved view can serve as a daily dashboard, showing you the health and performance metrics that you care about most, without the need to rebuild filters each time.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/saved-view/#how-to-use-saved-views
tag_set: product-features, saved-view
image_urls: 
tracking_id: docs-product-features-saved-view-how-to-use-saved-views
group_tracking_ids: docs-product-features-saved-view
<h2>Save a view in SigNoz: How to Use Saved Views</h2>
<h3>## Step 1: Apply Filters</h3>
<ol>
<li>
<p>Navigate to the <strong>Logs Explorer</strong> or <strong>Traces Explorer</strong> page within your SigNoz dashboard.</p>
</li>
<li>
<p>Utilize the <strong>Filter</strong> search bar to select from a plethora of filtering options, tailoring your data view.</p>
</li>
<li>
<p>Once you are done adding the filter, click on <strong>Stage &amp; Run Query</strong> to apply the selected filters.</p>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/product-features/saved-view/save-view-filtering.gif" alt="Filter Search Bar in the SigNoz Logs Explorer" /></p>
<p><em>Filter Search Bar in Logs Query Builder</em></p>
<h3>## Step 2: Save Your View</h3>
<ol>
<li>With your desired filters in place, click the <strong>Save this view</strong> button to preserve your current setup.</li>
<li>A dialog box will appear. Enter a distinct <strong>Label</strong> for your view for easy recall later.</li>
<li>Confirm by selecting <strong>Save this view</strong>. A popup will confirm that your view has been saved successfully.</li>
</ol>
<p><img src="https://signoz.io/img/docs/product-features/saved-view/save-view.gif" alt="How to Save a View in Query Builder" /></p>
<p><em>How to save a view</em></p>
<h3>## Step 3: Access Anytime</h3>
<p>Access your Saved Views from the <strong>Logs Explorer</strong> or <strong>Traces Explorer</strong> depending on where they were created. Locate the dropdown at the bottom of the Explorer and you can select any of your saved views to apply the preset filters to your data instantly.</p>
<p><img src="https://signoz.io/img/docs/product-features/saved-view/save-view-access-view.gif" alt="Accessing a saved view in Logs or Traces Explorer" /></p>
<p><em>Accessing a saved view</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/saved-view/#updating-a-view
tag_set: product-features, saved-view
image_urls: 
tracking_id: docs-product-features-saved-view-updating-a-view
group_tracking_ids: docs-product-features-saved-view
<h2>Save a view in SigNoz: Updating a View</h2>
<ol>
<li>Select a view that you want to update, from the dropdown.</li>
<li>Make the desired changes to the view. For example, you can add more filters.</li>
<li>Click on the Update this View button that appears next to the View name after you make the changes.</li>
</ol>
<p><img src="https://signoz.io/img/docs/product-features/saved-view/save-view-update-view.gif" alt="Updating a saved view in Logs or Traces Explorer" /></p>
<p><em>Updating a Saved View</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/saved-view/#renaming-a-view
tag_set: product-features, saved-view
image_urls: 
tracking_id: docs-product-features-saved-view-renaming-a-view
group_tracking_ids: docs-product-features-saved-view
<h2>Save a view in SigNoz: Renaming a View</h2>
<ol>
<li>Head over to the Views Tab.</li>
<li>Search for the view that you want to rename.</li>
<li>Click on the edit button and change the label of the view to your desired name.</li>
<li>Click on the Save Changes button.</li>
</ol>
<p><img src="https://signoz.io/img/docs/product-features/saved-view/save-view-rename.gif" alt="Renaming a saved view in Logs or Traces Explorer" /></p>
<p><em>Renaming a Saved View</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/saved-view/#deleting-a-view
tag_set: product-features, saved-view
image_urls: 
tracking_id: docs-product-features-saved-view-deleting-a-view
group_tracking_ids: docs-product-features-saved-view
<h2>Save a view in SigNoz: Deleting a View</h2>
<ol>
<li>Head over to the Views Tab.</li>
<li>Search for the view that you want to delete.</li>
<li>Click on the delete icon and select delete button from the popup.</li>
</ol>
<p><img src="https://signoz.io/img/docs/product-features/saved-view/save-view-delete.gif" alt="Deleting a saved view in Logs or Traces Explorer" /></p>
<p><em>Deleting a Saved View</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/
tag_set: instrumentation, python
image_urls: https://signoz.io/img/docs/otel_python_instrumentation.webp
tracking_id: docs-instrumentation-python
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Python applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Python application.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Python application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/otel_python_instrumentation.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Python applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in Python.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#requirements
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-requirements
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Requirements</h2>
<ul>
<li>Python 3.8 or newer</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#send-traces-to-signoz-cloud
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindowsDocker</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h4>## Send traces directly to SigNoz Cloud</h4>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p><strong>Step 4.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
opentelemetry-instrument &lt;your_run_command&gt;
</code></pre>
<ul>
<li><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</li>
<li><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></li>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the api token provided by SigNoz. You can find it in the email sent by SigNoz with your cloud account details.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p><strong>Step 5.</strong> Validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-signoz-installation">troubleshooting section</a>.</p>
<hr />
<h4>## Send traces via OTel Collector binary</h4>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Python application.</p>
<p><strong>Step 1.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p><strong>Step 2.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p><strong>Step 3.</strong> To run your application and send data to collector in same VM:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
opentelemetry-instrument &lt;your_run_command&gt;
</code></pre>
<p>where,</p>
<ul>
<li><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</li>
<li><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></li>
</ul>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4317 with <code>&lt;IP Address of the VM&gt;:4317</code>.</p>
<p><strong>Step 4.</strong> You can validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-signoz-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can use OpenTelemetry Python to send your traces directly to SigNoz. OpenTelemetry provides a handy distro in Python that can help you get started with automatic instrumentation. We recommend using it to get started quickly.</p>
<h3>## Steps to auto-instrument Python app for traces</h3>
<p>‚úÖ Info</p>
<p>If you are on K8s, you should checkout <a href="/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">opentelemetry operators</a> which enable auto instrumenting Python applications very easily.</p>
<ol>
<li>
<p><strong>Create a virtual environment</strong></p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
</li>
<li>
<p><strong>Install the OpenTelemetry dependencies</strong></p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
</li>
<li>
<p><strong>Add automatic instrumentation</strong><br />
The below command inspects the dependencies of your application and installs the instrumentation packages relevant for your Python application.</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
</li>
<li>
<p><strong>Run your application</strong></p>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, if you use <code>export FLASK_ENV=development</code>, it enables the reloader mode which breaks OpenTelemetry instrumentation.</p>
<p>To start sending data to SigNoz, use the following run command:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></p>
<p><code>IP of SigNoz backend</code> is the IP of the machine where you installed SigNoz. If you have installed SigNoz on <code>localhost</code>, the endpoint will be <code>http://localhost:4317</code> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>Replacing these environment variables, a sample final run command will look like this:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=python_app OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument python3 app.py
</code></pre>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively. Remember to allow incoming requests to port¬†<strong>4317</strong>/<strong>4318</strong>¬†of machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-signoz-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, python
image_urls: https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp
tracking_id: docs-instrumentation-python-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp" alt="Python Application in the list of services being monitored in SigNoz" /></p>
<p><em>Python Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#database-instrumentation
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-database-instrumentation
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Database Instrumentation</h2>
<p>Make sure that the DB client library you are using has the corresponding instrumentation library, and the version of the DB client library is supported by OpenTelemetry.</p>
<h3>## MongoDB</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MongoDB database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MongoDB, the instrumentation library is <code>opentelemetry-instrumentation-pymongo</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Redis</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your Redis database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Redis, the instrumentation library is <code>opentelemetry-instrumentation-redis</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## MySQL</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MySQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MySQL, we have two instrumentation libraries:</p>
<ul>
<li>opentelemetry-instrumentation-mysql</li>
<li>opentelemetry-instrumentation-pymysql</li>
</ul>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Postgres</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your PostgreSQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Postgres, the instrumentation library is <code>opentelemetry-instrumentation-psycopg2</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<p>üìù Note</p>
<p><code>psycopg2-binary</code> is not supported by opentelemetry auto instrumentation libraries as it is not recommended for production use. Please use <code>psycopg2</code> to see DB calls also in your trace data in SigNoz</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#running-applications-with-gunicorn-uwsgi
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-running-applications-with-gunicorn-uwsgi
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Running applications with Gunicorn, uWSGI</h2>
<p>For application servers which are based on pre fork model like Gunicorn, uWSGI you have to add a <code>post_fork</code> hook or a <code>@postfork</code> decorator in your configuration.</p>
<p>Check this <a href="https://opentelemetry-python.readthedocs.io/en/latest/examples/fork-process-model/README.html">documentation</a> from OpenTelemetry on how to set it up.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-python/tree/main/docs/examples/fork-process-model">Here's</a> a working example configured with a gunicorn server with <code>post_fork</code> hook.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#troubleshooting-your-signoz-installation
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-troubleshooting-your-signoz-installation
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Troubleshooting your SigNoz installation</h2>
<h4>## Application servers such as Uvicorn, Hypercorn, etc.</h4>
<ul>
<li>Uvicorn with <code>--workers</code> flag is not supported. The work around for this is to use <code>gunicorn</code> with uvicorn as the worker class <code>gunicorn -k uvicorn.workers.UvicornWorker</code>.</li>
<li>Hypercorn is not supported. There is no workaround for this. Please follow the issue <a href="https://github.com/pgjones/hypercorn/issues/215">https://github.com/pgjones/hypercorn/issues/215</a></li>
</ul>
<h4>## Spans are not being reported</h4>
<p>If spans are not being reported to SigNoz, try enabling debug exporter which writes the JSON formatted trace data to the console by setting env var OTEL_TRACES_EXPORTER=console.</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=python_app OTEL_TRACES_EXPORTER=console opentelemetry-instrument &lt;your run command&gt;


{
    &quot;name&quot;: &quot;alice&quot;,
    &quot;context&quot;: {
        &quot;trace_id&quot;: &quot;0xedb7caf0c8b082a9578460a201759193&quot;,
        &quot;span_id&quot;: &quot;0x57cf7eee198e1fed&quot;,
        &quot;trace_state&quot;: &quot;[]&quot;
    },
    &quot;kind&quot;: &quot;SpanKind.INTERNAL&quot;,
    &quot;parent_id&quot;: null,
    &quot;start_time&quot;: &quot;2022-03-27T14:55:18.804758Z&quot;,
    &quot;end_time&quot;: &quot;2022-03-27T14:55:18.804805Z&quot;,
    &quot;status&quot;: {
        &quot;status_code&quot;: &quot;UNSET&quot;
    },
    &quot;attributes&quot;: {},
    &quot;events&quot;: [],
    &quot;links&quot;: [],
    &quot;resource&quot;: {
        &quot;telemetry.sdk.language&quot;: &quot;python&quot;,
        &quot;telemetry.sdk.name&quot;: &quot;opentelemetry&quot;,
        &quot;telemetry.sdk.version&quot;: &quot;1.10.0&quot;,
        &quot;service.name&quot;: &quot;my-service&quot;
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#sample-application
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-sample-application
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: Sample Application</h2>
<ul>
<li><a href="https://github.com/SigNoz/opentelemetry-python/tree/main/docs/examples/fork-process-model">Working example</a> where we have configured a gunicorn server with <code>post_fork</code> hook.</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/python/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, python
image_urls: 
tracking_id: docs-instrumentation-python-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-python
<h2>Python OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/
tag_set: instrumentation, java
image_urls: https://signoz.io/img/docs/opentelemetry_java_instrument.webp
tracking_id: docs-instrumentation-java
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Java applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Java application.</p>
<p>OpenTelemetry Java is the language-specific implementation of OpenTelemetry in Java.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Java application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/opentelemetry_java_instrument.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Java applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>There are two types of application instrumentation:</p>
<ul>
<li>
<p><strong>Auto Instrumentation</strong><br />
A completely automatic and out of box experience, with minimal code changes. For your Java application, we recommend getting started with auto instrumentation.</p>
<p>‚úÖ Info</p>
<p>If you are on K8s, you should checkout <a href="/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">opentelemetry operators</a> which enable auto instrumenting Java applications very easily.</p>
</li>
<li>
<p><strong>Manual Instrumentation</strong><br />
It involves writing instrumentation using OpenTelemetry SDK and API manually. You would need to get a handle to an instance of the <code>OpenTelemetry</code> interface, acquire a tracer, and create <a href="https://signoz.io/blog/distributed-tracing-span/">spans</a> manually. Manual isntrumentation might also be used along with auto instrumentation.</p>
</li>
</ul>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in Java. If you're using SigNoz cloud, refer to this <a href="#send-traces-to-signoz-cloud">section</a>. If you're using self-hosted SigNoz refer to this <a href="#send-traces-to-self-hosted-signoz">section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#requirements
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-requirements
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Requirements</h2>
<p>Java 8 or higher</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#send-traces-to-signoz-cloud
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud</h2>
<p>OpenTelemetry provides a handy Java JAR agent that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetes</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h4>## Send traces directly to SigNoz Cloud</h4>
<p>OpenTelemetry Java agent can send traces directly to SigNoz Cloud.</p>
<p><strong>Step 1.</strong> Download otel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.{region}.signoz.cloud:443 \
java -javaagent:$PWD/opentelemetry-javaagent.jar -jar &lt;my-app&gt;.jar
</code></pre>
<ul>
<li><code>&lt;app_name&gt;</code> is the name for your application</li>
<li><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />
<h4>## Send traces via OTel Collector binary</h4>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Java application.</p>
<p><strong>Step 1.</strong> Download OTel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Run your application</p>
<pre><code>java -javaagent:$PWD/opentelemetry-javaagent.jar -jar &lt;myapp&gt;.jar
</code></pre>
<ul>
<li><code>&lt;myapp&gt;</code> is the name of your application jar file</li>
<li>In case you download <code>opentelemetry-javaagent.jar</code> file in different directory than that of the project, replace <code>$PWD</code> with the path of the otel jar file.</li>
</ul>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can use OpenTelemetry Java to send your traces directly to SigNoz. OpenTelemetry provides a <strong>handy Java JAR agent</strong> that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<h3>## Steps to auto-instrument Java applications for traces</h3>
<p><a href="https://signoz.io/opentelemetry/java-auto-instrumentation/">OpenTelemetry Java auto-instrumentation</a> supports collecting telemetry data from a huge number of libraries and frameworks. You can check out the full list <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/supported-libraries.md">here</a>.</p>
<ol>
<li>
<p><strong>Download the latest OpenTelemetry Java JAR agent</strong><br />
Download the latest <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar">Java JAR agent</a>
. You can also use the terminal to get the file using the following command:</p>
<pre><code> wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
</li>
<li>
<p><strong>Enable the instrumentation agent and run your application</strong><br />
If you run your Java application as a JAR file, run your application using the following command:</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; java -javaagent:/path/to/opentelemetry-javaagent.jar -jar  &lt;myapp&gt;.jar
</code></pre>
<p>where <code>&lt;app_name&gt;</code> is the name you want to set for your application.¬†<code>path</code> should be updated to the path of the downloaded Java JAR agent.</p>
<p>In the above command, we are configuring the exporter to send data to SigNoz backend. By default, OpenTelemetry Java agent uses¬†<a href="https://github.com/open-telemetry/opentelemetry-java/tree/main/exporters/otlp">OTLP exporter</a> configured to send data.</p>
<p>Two things to note about the command:</p>
<p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code> - This is the endpoint of the machine where SigNoz is installed.</p>
<p><code>path/to</code> - Update it to the path of your downloaded Java JAR agent.</p>
<p>If you have installed SigNoz on your <code>localhost</code> and your Java JAR agent is saved at <code>/Users/john/Downloads/</code>, then the final command looks like:</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=javaApp java -javaagent:/Users/john/Downloads/opentelemetry-javaagent.jar -jar target/*.jar
</code></pre>
<p>Here‚Äôs a handy <a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> to figure out which address to use to send data to SigNoz.</p>
<p>You can also specify environment variables in the following way:</p>
<pre><code>java -javaagent:/path/opentelemetry-javaagent.jar \
    -Dotel.exporter.otlp.endpoint=http://&lt;IP of SigNoz Backend&gt;:4317 \
    -Dotel.resource.attributes=service.name=&lt;app_name&gt; \
    -jar &lt;myapp&gt;.jar
</code></pre>
</li>
</ol>
<p>üìù Note</p>
<p>üí° Remember to allow incoming requests to port 4317 of the machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, java
image_urls: https://signoz.io/img/docs/java_app_services_list.webp
tracking_id: docs-instrumentation-java-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>. Ensure that you're checking data for the <code>time range filter</code> applied in the top right corner.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using self-hosted SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/java_app_services_list.webp" alt="Java Application in the list of services being monitored in SigNoz" /></p>
<p><em>Java Application in the list of services being monitored in SigNoz</em></p>
<p>If you don't see your application reported in the list of services, try our <a href="https://signoz.io/docs/install/troubleshooting/">troubleshooting</a> guide.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#configuring-the-agent
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-configuring-the-agent
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Configuring the agent</h2>
<p>The agent is highly configurable. You can check out all the configuration options available <a href="https://opentelemetry.io/docs/instrumentation/java/automatic/agent-config/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#disabled-instrumentations
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-disabled-instrumentations
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Disabled instrumentations</h2>
<p>Some instrumentations can produce too many spans and make traces very noisy. For this reason, the following instrumentations are disabled by default:</p>
<ul>
<li><code>jdbc-datasource</code>¬†which creates spans whenever the¬†<code>java.sql.DataSource#getConnection</code>¬†method is called.</li>
<li><code>dropwizard-metrics</code>,¬†which might create very low-quality metrics data because of the lack of label/attribute support in the Dropwizard metrics API.</li>
</ul>
<p>To enable them, add the¬†<code>otel.instrumentation.&lt;name&gt;.enabled</code>¬†system property:¬†<code>-Dotel.instrumentation.jdbc-datasource.enabled=true</code></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#manual-instrumentation
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-manual-instrumentation
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Manual Instrumentation</h2>
<p>For manual instrumentation of Java application, refer to the docs <a href="https://opentelemetry.io/docs/instrumentation/java/manual/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#instrumentation-using-otel-buildpack-paketo-for-java
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-instrumentation-using-otel-buildpack-paketo-for-java
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Instrumentation using Otel buildpack (paketo) for Java</h2>
<ol>
<li>
<p><strong>Clone OTel buildpack repo:</strong></p>
<pre><code>git clone https://github.com/paketo-buildpacks/opentelemetry.git
</code></pre>
</li>
<li>
<p><strong>Switch to config-binding branch:</strong></p>
<pre><code>git checkout config-binding
</code></pre>
</li>
<li>
<p><strong>Run the following command:</strong></p>
<pre><code>scripts/build.sh
</code></pre>
</li>
<li>
<p><strong>Now run below command to build a pack:</strong></p>
<pre><code>pack build paketo-demo-app \
  --path /Users/makeavish/samples/java/maven \
  --buildpack paketo-buildpacks/java \
  --buildpack . \
  --builder paketobuildpacks/builder:base \
  --verbose --trust-builder \
  -e BP_JVM_VERSION=17 -e BP_OPENTELEMETRY_ENABLED=true
</code></pre>
</li>
<li>
<p>Pass environment variables to enable java agent <code>OTEL_JAVAAGENT_ENABLED=true</code> set exporter endpoint <code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317</code> and set service name <code>OTEL_RESOURCE_ATTRIBUTES=service.name=javaApp</code></p>
</li>
<li>
<p>Other otel configurations can be updated by passing more environment variables. Refer to <a href="https://opentelemetry.io/docs/instrumentation/java/automatic/agent-config/">official docs</a> for more such configurations.</p>
</li>
<li>
<p>Run docker command:</p>
<pre><code>docker run -d -p 8080:8080 -e PORT=8080 -e OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 -e OTEL_RESOURCE_ATTRIBUTES=service.name=javaApp -e OTEL_JAVAAGENT_ENABLED=true paketo-demo-app
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#troubleshooting-your-installation
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Troubleshooting your installation</h2>
<p>If spans are not being reported to SigNoz, try running in debug mode by setting <code>OTEL_LOG_LEVEL=debug</code>:</p>
<p>The debug log level will print out the configuration information. It will also emit every span to the console, which should look something like:</p>
<pre><code>Span {
  attributes: {},
  links: [],
  events: [],
  status: { code: 0 },
  endTime: [ 1597810686, 885498645 ],
  _ended: true,
  _duration: [ 0, 43333 ],
  name: 'bar',
  spanContext: {
    traceId: 'eca3cc297720bd705e734f4941bca45a',
    spanId: '891016e5f8c134ad',
    traceFlags: 1,
    traceState: undefined
  },
  parentSpanId: 'cff3a2c6bfd4bbef',
  kind: 0,
  startTime: [ 1597810686, 885455312 ],
  resource: Resource { labels: [Object] },
  instrumentationLibrary: { name: 'example', version: '*' },
  _logger: ConsoleLogger {
    debug: [Function],
    info: [Function],
    warn: [Function],
    error: [Function]
  },
  _traceParams: {
    numberOfAttributesPerSpan: 32,
    numberOfLinksPerSpan: 32,
    numberOfEventsPerSpan: 128
  },
  _spanProcessor: MultiSpanProcessor { _spanProcessors: [Array] }
},
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#sample-java-application
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-sample-java-application
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: Sample Java Application</h2>
<ul>
<li>We have included a sample Java application with README.md at <a href="https://github.com/SigNoz/distributed-tracing-java-sample">Sample Java App Github Repo.</a></li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/java/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, java
image_urls: 
tracking_id: docs-instrumentation-java-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-java
<h2>Java OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation</h2>
<p>This document contains OpenTelemetry instrumentation instructions for Javascript backend frameworks and modules based on Nodejs. If you're using self-hosted SigNoz refer to this <a href="#send-traces-to-self-hosted-signoz">section</a>. If you're using SigNoz cloud, refer to this <a href="#send-traces-to-signoz-cloud">section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-to-signoz-cloud
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
<ul>
<li><a href="#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
<ul>
<li><a href="#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;&lt;SIGNOZ_ENDPOINT&gt;&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_ACCESS_TOKEN&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-directly-to-signoz-cloud--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api@^1.6.0
npm install --save @opentelemetry/sdk-node@^0.45.0
npm install --save @opentelemetry/auto-instrumentations-node@^0.39.4
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.45.0
</code></pre>
<p><strong>Step 2.</strong> Create tracing.js file<br />
You need to configure the endpoint for SigNoz cloud in this file. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
<pre><code>// tracing.js
'use strict'
const process = require('process')
const opentelemetry = require('@opentelemetry/sdk-node')
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node')
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')
const { Resource } = require('@opentelemetry/resources')
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions')

// do not set headers in exporterOptions, the OTel spec recommends setting headers through ENV variables
// https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/exporter.md#specifying-headers-via-environment-variables

// highlight-start
const exporterOptions = {
  url: 'https://ingest.{region}.signoz.cloud:443/v1/traces',
}
// highlight-end

const traceExporter = new OTLPTraceExporter(exporterOptions)
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({
    // highlight-next-line
    [SemanticResourceAttributes.SERVICE_NAME]: 'node_app',
  }),
})

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk
    .shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0))
})
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443/v1/traces</td>
</tr>
</tbody>
</table>
<p><strong>Step 3.</strong> Run the application<br />
Make sure you set the <code>OTEL_EXPORTER_OTLP_HEADERS</code> env as follows</p>
<pre><code>OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;&quot; node -r ./tracing.js app.js
</code></pre>
<p><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
<p><strong>Step 4.</strong> You can validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-via-otel-collector-binary--no-code-automatic-instrumentation
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - No Code Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended-1
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended-1
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4318/v1/traces&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-via-otel-collector-binary--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - Code Level Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api@^1.6.0
npm install --save @opentelemetry/sdk-node@^0.45.0
npm install --save @opentelemetry/auto-instrumentations-node@^0.39.4
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.45.0
</code></pre>
<p><strong>Step 2.</strong> Create tracing.js file</p>
<pre><code>// tracing.js
'use strict'
const process = require('process')
const opentelemetry = require('@opentelemetry/sdk-node')
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node')
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')
const { Resource } = require('@opentelemetry/resources')
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions')

const exporterOptions = {
  url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces',
}

const traceExporter = new OTLPTraceExporter(exporterOptions)
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({
    // highlight-next-line
    [SemanticResourceAttributes.SERVICE_NAME]: 'node_app',
  }),
})

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk
    .shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0))
})
</code></pre>
<p><strong>Step 3.</strong> Run the application</p>
<pre><code>node -r ./tracing.js app.js
</code></pre>
<p><strong>Step 4.</strong> You can validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Send Traces to Self-Hosted SigNoz</h2>
<p><strong>Requirements</strong></p>
<ul>
<li>Node.js version 14 or newer (<a href="https://github.com/open-telemetry/opentelemetry-js#supported-runtimes">See here</a>
)</li>
</ul>
<p>You can use <a href="https://signoz.io/opentelemetry/nodejs/">OpenTelemetry Nodejs</a> client libraries to send your traces directly to SigNoz. You have two choices for instrumenting your Nodejs application with OpenTelemetry.</p>
<ul>
<li>
<p><strong><a href="#using-the-all-in-one-auto-instrumentation-library">Use the all-in-one auto-instrumentation library(Recommended)</a></strong><br />
The auto-instrumentation library of OpenTelemetry is a meta package that provides a simple way to initialize multiple Nodejs instrumnetations.</p>
<p>‚úÖ Info</p>
<p>If you are on K8s, you should checkout <a href="/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">opentelemetry operators</a> which enable auto instrumenting Javascript applications very easily.</p>
</li>
<li>
<p><strong><a href="#using-a-specific-auto-instrumentation-library">Use a specific auto-instrumentation library</a></strong><br />
You can use individual auto-instrumentation libraries too for a specific component of your application. For example, you can use <code>@opentelemetry/instrumentation-express</code> for instrumenting the Express web framework.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#using-the-all-in-one-auto-instrumentation-library
tag_set: instrumentation, javascript
image_urls: https://signoz.io/img/docs/all_in_one_auto_instrumentation.webp
tracking_id: docs-instrumentation-javascript-using-the-all-in-one-auto-instrumentation-library
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Using the all-in-one auto-instrumentation library</p>
<p>The recommended way to instrument your Javascript Nodejs application is to use the all-in-one auto-instrumentation library - <code>@opentelemetry/auto-instrumentations-node</code>. It provides a simple way to initialize multiple Nodejs instrumentations.</p>
<p>Internally, it calls the specific auto-instrumentation library for components used in the application. You can see the complete list <a href="https://github.com/open-telemetry/opentelemetry-js-contrib/tree/main/metapackages/auto-instrumentations-node#supported-instrumentations">here</a>.</p>
<p>The instrumentation automatically identifies the following within your application:</p>
<ul>
<li>Frameworks, such as Express, Nestjs</li>
<li>Common protocols such as HTTP, HTTPS, and gRPC</li>
<li>Databases, such as MySQL, MongoDB, Redis, etc.</li>
<li>Other libraries used in the application</li>
</ul>
<p><img src="https://signoz.io/img/docs/all_in_one_auto_instrumentation.webp" alt="All in one OpenTelemetry nodejs instrumentation " /></p>
<p>_</p>
<p>All in one auto instrumentation library - identifies and instruments packages used by your Nodejs application</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#steps-to-auto-instrument-nodejs-application
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-steps-to-auto-instrument-nodejs-application
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Nodejs application: Steps to auto-instrument Nodejs application: Steps to auto-instrument Nodejs application</p>
<ol>
<li>
<p>Install the dependencies<br />
We start by installing the relevant dependencies.</p>
<pre><code>npm install --save @opentelemetry/sdk-node
npm install --save @opentelemetry/auto-instrumentations-node
npm install --save @opentelemetry/exporter-trace-otlp-http
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>@opentelemetry/sdk-node</code> - This package provides the full OpenTelemetry SDK for Node.js including tracing and metrics.</p>
<p><code>@opentelemetry/auto-instrumentations-node</code> - This module provides a simple way to initialize multiple Node instrumentations.</p>
<p><code>@opentelemetry/exporter-trace-otlp-http</code> - This module provides the exporter to be used with OTLP (<code>http/json</code>) compatible receivers.</p>
<p>üìù Note</p>
<p>If you run into any error, you might want to use these pinned versions of OpenTelemetry libraries used in this <a href="https://github.com/SigNoz/sample-nodejs-app/blob/master/package.json">GitHub repo</a>
.</p>
</li>
<li>
<p><strong>Create a <code>tracing.js</code> file</strong><br />
The <code>tracing.js</code> file will contain the tracing setup code. Notice, that we have set some environment variables in the code(highlighted). You can update these variables based on your environment.</p>
<p>// tracing.js
'use strict'
const process = require('process')
const opentelemetry = require('@opentelemetry/sdk-node')
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node')
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')
const { Resource } = require('@opentelemetry/resources')
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions')</p>
<p>const exporterOptions = {
// highlight-next-line
url: 'http://localhost:4318/v1/traces',
}</p>
<p>const traceExporter = new OTLPTraceExporter(exporterOptions)
const sdk = new opentelemetry.NodeSDK({
traceExporter,
instrumentations: [getNodeAutoInstrumentations()],
// highlight-start
resource: new Resource({
[SemanticResourceAttributes.SERVICE_NAME]: 'node_app',
}),
// highlight-end
})</p>
<p>// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()</p>
<p>// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
sdk
.shutdown()
.then(() =&gt; console.log('Tracing terminated'))
.catch((error) =&gt; console.log('Error terminating tracing', error))
.finally(() =&gt; process.exit(0))
})</p>
</li>
</ol>
<p>OpenTelemetry Node SDK currently does not detect the <code>OTEL_RESOURCE_ATTRIBUTES</code> from <code>.env</code> files as of today. That‚Äôs why we need to include the variables in the <code>tracing.js</code> file itself.</p>
<p>About environment variables:</p>
<p><code>service_name</code>¬†: node_app (you can give whatever name that suits you)</p>
<p><code>http://localhost:4318/v1/traces</code> is the default url for sending your tracing data. We are assuming you have installed SigNoz on your <code>localhost</code>. Based on your environment, you can update it accordingly. It should be in the following format:</p>
<pre><code>http://&lt;IP of SigNoz backend&gt;:4318/v1/traces
</code></pre>
<p>üìù Note</p>
<p>Remember to allow incoming requests to port 4318 of machine where SigNoz backend is hosted.</p>
<ol start="3">
<li>
<p><strong>Run the application</strong><br />
The tracing configuration should be run before your application code. We will use the <a href="https://nodejs.org/api/cli.html#cli_r_require_module"><code>-r, ‚Äîrequire module</code></a> flag for that.</p>
<pre><code>node -r ./tracing.js app.js
</code></pre>
<p>üìù Note</p>
<p>If you're running your nodejs application in PM2 cluster mode, it doesn't support node args: <a href="https://github.com/Unitech/pm2/issues/3227">Unitech/pm2#3227</a>
. As above sample app instrumentation requires to load <code>tracing.js</code> before app load by passing node arg, so nodejs instrumentation doesn't work in PM2 cluster mode. So you need to import <code>tracing.js</code> in your main application. The <code>import ./tracing.js</code> should be the first line of your application code and initialize it before any other function. Here's the <a href="https://github.com/SigNoz/sample-nodejs-app/tree/init-tracer-main">sample github repo</a> which shows the implementation.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, javascript
image_urls: https://signoz.io/img/docs/nodejs_in_services_list.webp
tracking_id: docs-instrumentation-javascript-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the <code>Services</code> tab. Hit the <code>Refresh</code> button on the top right corner, and your application should appear in the list of <code>Applications</code>.</li>
<li>Go to the <code>Traces</code> tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs <a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/nodejs_in_services_list.webp" alt="Node Application in the list of services being monitored in SigNoz" /></p>
<p><em>Node Application in the list of services being monitored in SigNoz</em></p>
<p>If you don't see your application reported in the list of services, try our <a href="https://signoz.io/docs/install/troubleshooting/">troubleshooting</a> guide.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#using-a-specific-auto-instrumentation-library
tag_set: instrumentation, javascript
image_urls: https://signoz.io/img/docs/individual_auto_instrumentation_libraries.webp
tracking_id: docs-instrumentation-javascript-using-a-specific-auto-instrumentation-library
group_tracking_ids: docs-instrumentation-javascript
<p>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Using a specific auto-instrumentation library: Using a specific auto-instrumentation library: Using a specific auto-instrumentation library</p>
<p>If total installation size is not constrained, it is recommended to use the¬†<code>@opentelemetry/auto-instrumentations-node</code> bundle with <code>@opentelemetry/sdk-node</code> for the most seamless instrumentation experience.</p>
<p>But you can also install specific auto-instrumenation packages for the components used by your application.</p>
<p><img src="https://signoz.io/img/docs/individual_auto_instrumentation_libraries.webp" alt="All in one OpenTelemetry nodejs instrumentation " /></p>
<p>_</p>
<p>You can also choose individual auto-instrumenation libraries, but the all-in-one library is recommended to get started</p>
<p>_</p>
<p>If an application uses Express, HTTP, and MongoDB, we can instrument the application using the following modules:</p>
<ul>
<li>opentelemetry-instrumentation-express</li>
<li>opentelemetry/instrumentation-mongodb</li>
<li>opentelemetry/instrumentation-http</li>
</ul>
<p>If you are using Express, the instrumentation relies on HTTP calls to also be instrumented. That‚Äôs why we‚Äôre also including the module for http instrumentation. Let‚Äôs see the steps required.</p>
<p><strong>Steps to use specific auto-instrumentation libraries</strong></p>
<ol>
<li>
<p><strong>Install the dependencies</strong><br />
We start by installing the relevant dependencies.</p>
<pre><code>npm install --save @opentelemetry/sdk-node
npm install --save @opentelemetry/exporter-trace-otlp-http
npm install --save @opentelemetry/instrumentation-express
npm install --save @opentelemetry/instrumentation-mongodb
npm install --save @opentelemetry/instrumentation-http
</code></pre>
</li>
<li>
<p><strong>Creat a <code>tracing.js</code> file</strong><br />
The <code>tracing.js</code> file will contain the following tracing setup code.</p>
<pre><code>// tracing.js
'use strict'
const process = require('process')
//OpenTelemetry
const opentelemetry = require('@opentelemetry/sdk-node')
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http')
//instrumentations
const { ExpressInstrumentation } = require('@opentelemetry/instrumentation-express')
const { MongoDBInstrumentation } = require('@opentelemetry/instrumentation-mongodb')
const { HttpInstrumentation } = require('@opentelemetry/instrumentation-http')

const { Resource } = require('@opentelemetry/resources')
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions')

const exporterOptions = {
  url: 'http://localhost:4318/v1/traces',
}

const traceExporter = new OTLPTraceExporter(exporterOptions)
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [\
    new ExpressInstrumentation(),\
    new MongoDBInstrumentation(),\
    new HttpInstrumentation(),\
  ],
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: 'node_app',
  }),
})

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start()

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk
    .shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0))
})
</code></pre>
<p>OpenTelemetry Node SDK currently does not detect the <code>OTEL_RESOURCE_ATTRIBUTES</code> from <code>.env</code> files as of today. That‚Äôs why we need to include the variables in the <code>tracing.js</code> file itself.</p>
<p>About environment variables:</p>
<p><code>service_name</code>¬†: node_app (you can give whatever name that suits you)</p>
<p><code>http://localhost:4318/v1/traces</code> is the default url for sending your tracing data. We are assuming you have installed SigNoz on your <code>localhost</code>. Based on your environment, you can update it accordingly. It should be in the following format:</p>
<p><code>http://&lt;IP of SigNoz backend&gt;:4318/v1/traces</code></p>
<p>üìù Note</p>
<p>Remember to allow incoming requests to port 4318 of machine where SigNoz backend is hosted.</p>
</li>
<li>
<p><strong>Run the application</strong><br />
The tracing configuration should be run before your application code. We will use the <a href="https://nodejs.org/api/cli.html#cli_r_require_module"><code>-r, ‚Äîrequire module</code></a> flag for that.</p>
<pre><code>node -r ./tracing.js app.js
</code></pre>
<p>üìù Note</p>
<p>If you're running your nodejs application in PM2 cluster mode, it doesn't support node args: <a href="https://github.com/Unitech/pm2/issues/3227">Unitech/pm2#3227</a>
. As above sample app instrumentation requires to load <code>tracing.js</code> before app load by passing node arg, so nodejs instrumentation doesn't work in PM2 cluster mode. So you need to import <code>tracing.js</code> in your main application. The <code>import ./tracing.js</code> should be the first line of your application code and initialize it before any other function. Here's the <a href="https://github.com/SigNoz/sample-nodejs-app/tree/init-tracer-main">sample github repo</a> which shows the implementation.</p>
</li>
</ol>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by <a href="#validating-instrumentation-by-checking-for-traces">validating</a> if your traces are being to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#manual-instrumentation-in-javascript
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-manual-instrumentation-in-javascript
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Manual Instrumentation in JavaScript</h2>
<p>For those looking to gain deeper insights into their application's performance and behavior, manual instrumentation provides a powerful way to achieve this.</p>
<p>Refer to the documentation for <a href="/docs/instrumentation/manual-instrumentation/javascript/nodejs/">Manual Instrumentation in NodeJS</a> to delve into the step-by-step process of adding custom tracing to your Node.js applications.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#instrumentation-modules-for-databases
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-instrumentation-modules-for-databases
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Instrumentation Modules for Databases</h2>
<p>The <code>@opentelemetry/auto-instrumentations-node</code> can inititialize instrumentation for popular databases. Hence it‚Äôs recommended to <a href="#using-the-all-in-one-auto-instrumentation-library">get started</a> with it.</p>
<p>But if you are using <a href="#using-a-specific-auto-instrumentation-library">specific auto-instrumentation packages</a>
, here‚Äôs a list of packages for popular databases.</p>
<h3>## MongoDB instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>‚Ä¢ <code>&gt;=3.3 &lt;5</code></p>
<p>Module that provides automatic instrumentation for MongoDB:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-mongodb
</code></pre>
<h3>## Redis Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>This package supports¬†<code>redis@^2.6.0</code> and¬†<code>redis@^3.0.0</code> For version¬†<code>redis@^4.0.0</code>, please use¬†<code>@opentelemetry/instrumentation-redis-4</code></p>
<pre><code>npm install --save @opentelemetry/instrumentation-redis
</code></pre>
<h3>## MySQL Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>‚Ä¢ <code>2.x</code></p>
<p>Module that provides automatic instrumentation for MySQL:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-mysql
</code></pre>
<h3>## Memcached Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<ul>
<li><code>&gt;=2.2</code></li>
</ul>
<p>Module that provides automatic instrumentation for Memcached:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-memcached
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#troubleshooting-your-installation
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Troubleshooting your installation</h2>
<p>Set an environment variable to run the OpenTelemetry launcher in debug mode, where it logs details about the configuration and emitted spans:</p>
<pre><code>export OTEL_LOG_LEVEL=debug
</code></pre>
<p>The output may be very verbose with some benign errors. Early in the console output, look for logs about the configuration. Next, look for lines like the ones below, which are emitted when spans are emitted to SigNoz.</p>
<pre><code>{
  &quot;traceId&quot;: &quot;985b66d592a1299f7d12ebca56ca1fe3&quot;,
  &quot;parentId&quot;: &quot;8d62a70aa335a227&quot;,
  &quot;name&quot;: &quot;bar&quot;,
  &quot;id&quot;: &quot;17ada85c3d55376a&quot;,
  &quot;kind&quot;: 0,
  &quot;timestamp&quot;: 1685674607399000,
  &quot;duration&quot;: 299,
  &quot;attributes&quot;: {},
  &quot;status&quot;: { &quot;code&quot;: 0 },
  &quot;events&quot;: []
}
{
  &quot;traceId&quot;: &quot;985b66d592a1299f7d12ebca56ca1fe3&quot;,
  &quot;name&quot;: &quot;foo&quot;,
  &quot;id&quot;: &quot;8d62a70aa335a227&quot;,
  &quot;kind&quot;: 0,
  &quot;timestamp&quot;: 1585130342183948,
  &quot;duration&quot;: 315,
  &quot;attributes&quot;: {
    &quot;name&quot;: &quot;value&quot;
  },
  &quot;status&quot;: { &quot;code&quot;: 0 },
  &quot;events&quot;: [\
    {\
      &quot;name&quot;: &quot;event in foo&quot;,\
      &quot;time&quot;: [1585130342, 184213041]\
    }\
  ]
}
</code></pre>
<p><em>Running short applications (Lambda/Serverless/etc)</em> If your application exits quickly after startup, you may need to explicitly shutdown the tracer to ensure that all spans are flushed:</p>
<pre><code>opentelemetry.trace.getTracer('your_tracer_name').getActiveSpanProcessor().shutdown()
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#sample-javascript-app
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-sample-javascript-app
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Sample Javascript App</h2>
<ul>
<li>We have included a sample applications at:
<ul>
<li>
<p><a href="https://github.com/SigNoz/sample-reactjs-app">Sample React App Github Repo</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/sample-nodejs-app">Sample NodeJs App Github Repo</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/distributed-tracing-nodejs-sample">Sample Distributed Tracing NodeJs App Github Repo</a></p>
</li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#further-reading
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-further-reading
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: Further Reading</h2>
<ul>
<li>
<p><a href="https://signoz.io/blog/nodejs-performance-monitoring/">Nodejs Performance Monitoring</a></p>
</li>
<li>
<p><a href="https://signoz.io/blog/distributed-tracing-nodejs/">Implementing Distributed Tracing in a Nodejs application</a></p>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/javascript/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, javascript
image_urls: 
tracking_id: docs-instrumentation-javascript-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-javascript
<h2>Javascript OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Go applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Go application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#send-traces-to-signoz-cloud
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-golang
<p>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud</p>
<ol>
<li>
<p><strong>Install Dependencies</strong><br />
Dependencies related to OpenTelemetry exporter and SDK have to be installed first. Note that we are assuming you are using <code>gin</code> request router. If you are using other request routers, check out the <a href="#request-routers">corresponding package</a>
.</p>
<p>Run the below commands after navigating to the application source folder:</p>
<pre><code>go get go.opentelemetry.io/otel \
  go.opentelemetry.io/otel/trace \
  go.opentelemetry.io/otel/sdk \
  go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc
</code></pre>
</li>
<li>
<p><strong>Declare environment variables for configuring OpenTelemetry</strong><br />
Declare the following global variables in¬†<code>main.go</code> which we will use to configure OpenTelemetry:</p>
<pre><code> var (
     serviceName  = os.Getenv(&quot;SERVICE_NAME&quot;)
     collectorURL = os.Getenv(&quot;OTEL_EXPORTER_OTLP_ENDPOINT&quot;)
     insecure     = os.Getenv(&quot;INSECURE_MODE&quot;)
 )
</code></pre>
</li>
<li>
<p><strong>Instrument your Go application with OpenTelemetry</strong><br />
To configure your application to send data we will need a function to initialize OpenTelemetry. Add the following snippet of code in your¬†<code>main.go</code> file.</p>
<pre><code>import (
    .....

    &quot;google.golang.org/grpc/credentials&quot;
    &quot;github.com/gin-gonic/gin&quot;
    &quot;go.opentelemetry.io/otel&quot;
    &quot;go.opentelemetry.io/otel/attribute&quot;
    &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace&quot;
    &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc&quot;

    &quot;go.opentelemetry.io/otel/sdk/resource&quot;
    sdktrace &quot;go.opentelemetry.io/otel/sdk/trace&quot;
)

func initTracer() func(context.Context) error {

    var secureOption otlptracegrpc.Option

    if strings.ToLower(insecure) == &quot;false&quot; || insecure == &quot;0&quot; || strings.ToLower(insecure) == &quot;f&quot; {
        secureOption = otlptracegrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, &quot;&quot;))
    } else {
        secureOption = otlptracegrpc.WithInsecure()
    }

    exporter, err := otlptrace.New(
        context.Background(),
        otlptracegrpc.NewClient(
            secureOption,
            otlptracegrpc.WithEndpoint(collectorURL),
        ),
    )

    if err != nil {
        log.Fatalf(&quot;Failed to create exporter: %v&quot;, err)
    }
    resources, err := resource.New(
        context.Background(),
        resource.WithAttributes(
            attribute.String(&quot;service.name&quot;, serviceName),
            attribute.String(&quot;library.language&quot;, &quot;go&quot;),
        ),
    )
    if err != nil {
        log.Fatalf(&quot;Could not set resources: %v&quot;, err)
    }

    otel.SetTracerProvider(
        sdktrace.NewTracerProvider(
            sdktrace.WithSampler(sdktrace.AlwaysSample()),
            sdktrace.WithBatcher(exporter),
            sdktrace.WithResource(resources),
        ),
    )
    return exporter.Shutdown
}
</code></pre>
</li>
<li>
<p><strong>Initialize the tracer in main.go</strong><br />
Modify the main function to initialise the tracer in¬†<code>main.go</code>. Initiate the tracer at the very beginning of our main function.</p>
<pre><code>func main() {
    cleanup := initTracer()
    defer cleanup(context.Background())

    ......
}
</code></pre>
</li>
<li>
<p><strong>Add the OpenTelemetry Gin middleware</strong><br />
Configure Gin to use the middleware by adding the following lines in¬†<code>main.go</code>.</p>
<pre><code>import (
    ....
  &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin&quot;
)

func main() {
    ......
    r := gin.Default()
    r.Use(otelgin.Middleware(serviceName))
    ......
}
</code></pre>
</li>
<li>
<p><strong>Set environment variables and run your Go Gin application</strong><br />
The run command must have some environment variables to send data to SigNoz cloud. The run command:</p>
<pre><code>SERVICE_NAME=goApp INSECURE_MODE=false OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ-INGESTION-TOKEN&gt; OTEL_EXPORTER_OTLP_ENDPOINT=ingest.{region}.signoz.cloud:443 go run main.go
</code></pre>
<p>We can replace the placeholders based on our environment.</p>
<p><code>SERVICE_NAME</code>: goGinApp (you can name it whatever you want)</p>
<p><code>OTEL_EXPORTER_OTLP_HEADERS</code>: <code>signoz-access-token=&lt;SIGNOZ-INGESTION-TOKEN&gt;</code>. Update <code>&lt;SIGNOZ-INGESTION-TOKEN&gt;</code> with the ingestion token provided by SigNoz</p>
<p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code>: ingest.<code>{region}</code>.signoz.cloud:443. Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
</li>
</ol>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#send-traces-via-otel-collector-binary
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-golang
<p>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Golang application.</p>
<ol>
<li>
<p><strong>Install Dependencies</strong><br />
Dependencies related to OpenTelemetry exporter and SDK have to be installed first. Note that we are assuming you are using <code>gin</code> request router. If you are using other request routers, check out the <a href="#request-routers">corresponding package</a>
.</p>
<p>Run the below commands after navigating to the application source folder:</p>
<pre><code>go get go.opentelemetry.io/otel \
  go.opentelemetry.io/otel/trace \
  go.opentelemetry.io/otel/sdk \
  go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc
</code></pre>
</li>
<li>
<p><strong>Declare environment variables for configuring OpenTelemetry</strong><br />
Declare the following global variables in¬†<code>main.go</code> which we will use to configure OpenTelemetry:</p>
<pre><code> var (
     serviceName  = os.Getenv(&quot;SERVICE_NAME&quot;)
     collectorURL = os.Getenv(&quot;OTEL_EXPORTER_OTLP_ENDPOINT&quot;)
     insecure     = os.Getenv(&quot;INSECURE_MODE&quot;)
 )
</code></pre>
</li>
<li>
<p><strong>Instrument your Go application with OpenTelemetry</strong><br />
To configure your application to send data we will need a function to initialize OpenTelemetry. Add the following snippet of code in your¬†<code>main.go</code> file.</p>
<pre><code> import (
     .....

     &quot;google.golang.org/grpc/credentials&quot;
     &quot;github.com/gin-gonic/gin&quot;
     &quot;go.opentelemetry.io/otel&quot;
     &quot;go.opentelemetry.io/otel/attribute&quot;
     &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace&quot;
     &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc&quot;

     &quot;go.opentelemetry.io/otel/sdk/resource&quot;
     sdktrace &quot;go.opentelemetry.io/otel/sdk/trace&quot;
 )

 func initTracer() func(context.Context) error {

     var secureOption otlptracegrpc.Option

     if strings.ToLower(insecure) == &quot;false&quot; || insecure == &quot;0&quot; || strings.ToLower(insecure) == &quot;f&quot; {
         secureOption = otlptracegrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, &quot;&quot;))
     } else {
         secureOption = otlptracegrpc.WithInsecure()
     }

     exporter, err := otlptrace.New(
         context.Background(),
         otlptracegrpc.NewClient(
             secureOption,
             otlptracegrpc.WithEndpoint(collectorURL),
         ),
     )

     if err != nil {
         log.Fatalf(&quot;Failed to create exporter: %v&quot;, err)
     }
     resources, err := resource.New(
         context.Background(),
         resource.WithAttributes(
             attribute.String(&quot;service.name&quot;, serviceName),
             attribute.String(&quot;library.language&quot;, &quot;go&quot;),
         ),
     )
     if err != nil {
         log.Fatalf(&quot;Could not set resources: %v&quot;, err)
     }

     otel.SetTracerProvider(
         sdktrace.NewTracerProvider(
             sdktrace.WithSampler(sdktrace.AlwaysSample()),
             sdktrace.WithBatcher(exporter),
             sdktrace.WithResource(resources),
         ),
     )
     return exporter.Shutdown
 }
</code></pre>
</li>
<li>
<p><strong>Initialize the tracer in main.go</strong><br />
Modify the main function to initialise the tracer in¬†<code>main.go</code>. Initiate the tracer at the very beginning of our main function.</p>
<pre><code>func main() {
    cleanup := initTracer()
    defer cleanup(context.Background())

    ......
}
</code></pre>
</li>
<li>
<p><strong>Add the OpenTelemetry Gin middleware</strong><br />
Configure Gin to use the middleware by adding the following lines in¬†<code>main.go</code>.</p>
<pre><code>import (
    ....
  &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin&quot;
)

func main() {
    ......
    r := gin.Default()
    r.Use(otelgin.Middleware(serviceName))
    ......
}
</code></pre>
</li>
<li>
<p><strong>Set environment variables and run your Go Gin application</strong><br />
The run command must have some environment variables to send data to SigNoz. The run command:</p>
<pre><code>SERVICE_NAME=goGinApp INSECURE_MODE=true OTEL_EXPORTER_OTLP_ENDPOINT=localhost:4317 go run main.go
</code></pre>
<p>If you want to update your <code>service_name</code>, you can modify the <code>SERVICE_NAME</code> variable.<br />
<code>SERVICE_NAME</code>: goGinApp (you can name it whatever you want)</p>
</li>
<li>
<p>You can validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<ol>
<li>
<p><strong>Install Dependencies</strong><br />
Dependencies related to OpenTelemetry exporter and SDK have to be installed first. Note that we are assuming you are using <code>gin</code> request router. If you are using other request routers, check out the <a href="#request-routers">corresponding package</a>
.</p>
<p>Run the below commands after navigating to the application source folder:</p>
<pre><code>go get go.opentelemetry.io/otel \
  go.opentelemetry.io/otel/trace \
  go.opentelemetry.io/otel/sdk \
  go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace \
  go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc
</code></pre>
</li>
<li>
<p><strong>Declare environment variables for configuring OpenTelemetry</strong><br />
Declare the following global variables in¬†<code>main.go</code> which we will use to configure OpenTelemetry:</p>
<pre><code> var (
     serviceName  = os.Getenv(&quot;SERVICE_NAME&quot;)
     collectorURL = os.Getenv(&quot;OTEL_EXPORTER_OTLP_ENDPOINT&quot;)
     insecure     = os.Getenv(&quot;INSECURE_MODE&quot;)
 )
</code></pre>
</li>
<li>
<p><strong>Instrument your Go application with OpenTelemetry</strong><br />
To configure your application to send data we will need a function to initialize OpenTelemetry. Add the following snippet of code in your¬†<code>main.go</code> file.</p>
<pre><code> import (
     .....

     &quot;google.golang.org/grpc/credentials&quot;
     &quot;github.com/gin-gonic/gin&quot;
     &quot;go.opentelemetry.io/otel&quot;
     &quot;go.opentelemetry.io/otel/attribute&quot;
     &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace&quot;
     &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc&quot;

     &quot;go.opentelemetry.io/otel/sdk/resource&quot;
     sdktrace &quot;go.opentelemetry.io/otel/sdk/trace&quot;
 )

 func initTracer() func(context.Context) error {

     var secureOption otlptracegrpc.Option

     if strings.ToLower(insecure) == &quot;false&quot; || insecure == &quot;0&quot; || strings.ToLower(insecure) == &quot;f&quot; {
         secureOption = otlptracegrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, &quot;&quot;))
     } else {
         secureOption = otlptracegrpc.WithInsecure()
     }

     exporter, err := otlptrace.New(
         context.Background(),
         otlptracegrpc.NewClient(
             secureOption,
             otlptracegrpc.WithEndpoint(collectorURL),
         ),
     )

     if err != nil {
         log.Fatalf(&quot;Failed to create exporter: %v&quot;, err)
     }
     resources, err := resource.New(
         context.Background(),
         resource.WithAttributes(
             attribute.String(&quot;service.name&quot;, serviceName),
             attribute.String(&quot;library.language&quot;, &quot;go&quot;),
         ),
     )
     if err != nil {
         log.Fatalf(&quot;Could not set resources: %v&quot;, err)
     }

     otel.SetTracerProvider(
         sdktrace.NewTracerProvider(
             sdktrace.WithSampler(sdktrace.AlwaysSample()),
             sdktrace.WithBatcher(exporter),
             sdktrace.WithResource(resources),
         ),
     )
     return exporter.Shutdown
 }
</code></pre>
</li>
<li>
<p><strong>Initialize the tracer in main.go</strong><br />
Modify the main function to initialise the tracer in¬†<code>main.go</code>. Initiate the tracer at the very beginning of our main function.</p>
<pre><code>func main() {
    cleanup := initTracer()
    defer cleanup(context.Background())

    ......
}
</code></pre>
</li>
<li>
<p><strong>Add the OpenTelemetry Gin middleware</strong><br />
Configure Gin to use the middleware by adding the following lines in¬†<code>main.go</code>.</p>
<pre><code>import (
    ....
  &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin&quot;
)

func main() {
    ......
    r := gin.Default()
    r.Use(otelgin.Middleware(serviceName))
    ......
}
</code></pre>
</li>
<li>
<p><strong>Set environment variables and run your Go Gin application</strong><br />
The run command must have some environment variables to send data to SigNoz. The run command:</p>
<pre><code>SERVICE_NAME=&lt;service_name&gt; INSECURE_MODE=true OTEL_EXPORTER_OTLP_ENDPOINT=&lt;IP of SigNoz backend:4317&gt; go run main.go
</code></pre>
<p>We can replace the placeholders based on our environment.</p>
<p><code>SERVICE_NAME</code>: goGinApp (you can name it whatever you want)</p>
<p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code>: localhost:4317</p>
<p>Since, we have installed SigNoz on our local machine, we use the above IP. If you install SigNoz on a different machine, you can update it with the relevant IP.</p>
<p>Do not use <code>http</code> or <code>https</code> in the IP address. For example, if the IP is <code>http://test.com</code> then the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> will be <code>test.com:4317</code>.</p>
<p>Here‚Äôs a handy¬†<a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> to figure out which address to use to send data to SigNoz.</p>
<p>Hence, the final run command looks like this:</p>
<pre><code>SERVICE_NAME=goGinApp INSECURE_MODE=true OTEL_EXPORTER_OTLP_ENDPOINT=localhost:4317 go run main.go
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, golang
image_urls: https://signoz.io/img/blog/2022/04/goginapp_signoz_dashboard.webp
tracking_id: docs-instrumentation-golang-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/blog/2022/04/goginapp_signoz_dashboard.webp" alt="Go Application in the list of services being monitored in SigNoz" /></p>
<p><em>Go Application in the list of services being monitored in SigNoz</em></p>
<p>If you don't see your application reported in the list of services, try our <a href="https://signoz.io/docs/install/troubleshooting/">troubleshooting</a> guide.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#request-routers
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-request-routers
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Request Routers</h2>
<h3>## OpenTelemetry gin/gonic instrumentation</h3>
<pre><code># Add one line to your import() stanza depending upon your request router:
middleware &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin&quot;
</code></pre>
<p>and then inject OpenTelemetry middleware:</p>
<pre><code>router.Use(middleware.Middleware(serviceName))
</code></pre>
<h3>## OpenTelemetry gorillamux instrumentation</h3>
<pre><code># Add one line to your import() stanza depending upon your request router:
middleware &quot;go.opentelemetry.io/contrib/instrumentation/github.com/gorilla/mux/otelmux&quot;
</code></pre>
<p>and then inject OpenTelemetry middleware:</p>
<pre><code>router.Use(middleware.Middleware(serviceName))
</code></pre>
<h3>## OpenTelemetry echo instrumentation</h3>
<pre><code># Add one line to your import() stanza depending upon your request router:
middleware &quot;go.opentelemetry.io/contrib/instrumentation/github.com/labstack/echo/otelecho&quot;
</code></pre>
<p>and then inject OpenTelemetry middleware:</p>
<pre><code>router.Use(middleware.Middleware(serviceName))
</code></pre>
<h3>## If you don‚Äôt use a request router</h3>
<pre><code>import (
  &quot;go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp&quot;
)
</code></pre>
<p>In each place where you pass an http.Handler to a ServeMux, you‚Äôll wrap the handler function. For instance, you‚Äôll make the following replacements:</p>
<pre><code>- mux.Handle(&quot;/path&quot;, h)
+ mux.Handle(&quot;/path&quot;, otelhttp.NewHandler(h, &quot;description of path&quot;))


- mux.Handle(&quot;/path&quot;, http.HandlerFunc(f))
+ mux.Handle(&quot;/path&quot;, otelhttp.NewHandler(http.HandlerFunc(f), &quot;description of path&quot;))
</code></pre>
<p>In this fashion, you can ensure that every function you wrap with othttp will automatically have its metadata collected and a corresponding trace started.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#adding-custom-attributes-and-custom-events-to-spans
tag_set: instrumentation, golang
image_urls: https://signoz.io/img/docs/opentelemetry_go_custom_attributes.webp, https://signoz.io/img/docs/opentelemetry_go_events.webp
tracking_id: docs-instrumentation-golang-adding-custom-attributes-and-custom-events-to-spans
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Adding custom attributes and custom events to spans</h2>
<p>It‚Äôs also possible to set custom attributes or tags to a span. To add custom attributes and events follow the below steps:</p>
<ol>
<li>
<p><strong>Import trace and attribute libraries</strong></p>
<pre><code>import (
    ...
    &quot;go.opentelemetry.io/otel/attribute&quot;
    &quot;go.opentelemetry.io/otel/trace&quot;
)
</code></pre>
</li>
<li>
<p><strong>Fetch current span from context</strong></p>
<pre><code>span := trace.SpanFromContext(c.Request.Context())
</code></pre>
</li>
<li>
<p><strong>Set attribute on current</strong></p>
<pre><code>span.SetAttributes(attribute.String(&quot;controller&quot;, &quot;books&quot;))
</code></pre>
</li>
</ol>
<p>SigNoz dashboards can be used to track these custom attributes.</p>
<p><img src="https://signoz.io/img/docs/opentelemetry_go_custom_attributes.webp" alt="Custom attributes under 'Tags' section on SigNoz trace detail page" /></p>
<p><em>Custom attributes can be seen under <code>Tags</code> section on SigNoz trace detail page</em></p>
<p>We can also set custom events on the span with its own attribute.</p>
<pre><code>span.AddEvent(&quot;This is a sample event&quot;, trace.WithAttributes(attribute.Int(&quot;pid&quot;, 4328), attribute.String(&quot;sampleAttribute&quot;, &quot;Test&quot;)))
</code></pre>
<p><img src="https://signoz.io/img/docs/opentelemetry_go_events.webp" alt="Events can be seen under  section on SigNoz trace detail page" /></p>
<p><em>Events can be seen under <code>Events</code> section on SigNoz trace detail page</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#grpc-instrumentation-with-opentelemetry
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-grpc-instrumentation-with-opentelemetry
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: gRPC Instrumentation with OpenTelemetry</h2>
<p>OpenTelemetry can also help you automatically instrument gRPC requests. To instrument any gRPC servers you have.</p>
<pre><code>import (
    &quot;go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc&quot;
)

func main() {
  [...]

    // add StatsHandler to gRPC server initialization
	s := grpc.NewServer(grpc.StatsHandler(otelgrpc.NewServerHandler()))

}
</code></pre>
<p>Similarly, instrument your gRPC client as well by adding otelgrpc when initializing gRPC client</p>
<pre><code>import (
    &quot;go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc&quot;
)

func main() {
  [...]

    // add StatsHandler to gRPC client initialization
	cc, err := grpc.NewClient(serverUrl, grpc.WithTransportCredentials(insecure.NewCredentials()),
		grpc.WithStatsHandler(otelgrpc.NewClientHandler()),
	)

}
</code></pre>
<p>We have a blog <a href="https://signoz.io/blog/opentelemetry-grpc-golang/">Monitor gRPC calls with OpenTelemetry - explained with a Golang example</a>
, do refer to that in case you need a helping hand to work with gRPC server.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#recording-errors-and-exceptions
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-recording-errors-and-exceptions
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Recording Errors and Exceptions</h2>
<pre><code>import &quot;go.opentelemetry.io/otel/codes&quot;

// Get the current span from the tracer
span := trace.SpanFromContext(ctx)

// RecordError converts an error into a span event.
span.RecordError(err)

// Mark span as failed.
span.SetStatus(codes.Error, &quot;internal error&quot;)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#sample-golang-application
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-sample-golang-application
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Sample Golang application</h2>
<p>We have included a sample gin/gonic application with <code>README.md</code> at <a href="https://github.com/SigNoz/sample-golang-app">https://github.com/SigNoz/sample-golang-app</a>.</p>
<p>Feel free to use this repo to test out OpenTelemetry instrumentation and how to send telemetry data to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#library-and-framework-support
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-library-and-framework-support
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Library and framework support</h2>
<p>Besides OpenTelemetry core modules, it is important to install instrumentation packages for every important library and framework which your service depends upon. Beyond the critical telemetry data these components emit, library and framework integrations are often required to ensure that the <a href="https://signoz.io/blog/context-propagation-in-distributed-tracing/">trace context</a> is properly propagated.</p>
<p>OpenTelemetry automatically provides instrumentation for a large number of libraries and frameworks, right out of the box.</p>
<p>The full list of supported instrumentation can be found in the <a href="https://github.com/open-telemetry/opentelemetry-go-contrib/tree/master/instrumentation">README</a>.</p>
<p>You can also find libraries, plugins, integrations, and other useful tools for extending OpenTelemetry from the OpenTelemetry <a href="https://opentelemetry.io/registry/?language=go">registry</a>.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/golang/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, golang
image_urls: 
tracking_id: docs-instrumentation-golang-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-golang
<h2>Go OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation</h2>
<p>This doc contains instructions about how to set up OpenTelemetry(OTel) instrumentation in your PHP application. OpenTelemetry, also known as OTel for short, is an open-source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your PHP application.</p>
<p>Once the telemetry data is generated, you can configure an exporter to send the data to SigNoz for monitoring and visualization.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your PHP application with OpenTelemetry</li>
<li>Configuring the exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p>In this documentation, we will instrument a PHP application for traces and send it to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#requirements
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-requirements
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Requirements</h2>
<ul>
<li>
<p><a href="https://www.php.net/">PHP 8.0+</a></p>
</li>
<li>
<p><a href="https://pecl.php.net/">PECL</a></p>
</li>
<li>
<p><a href="https://getcomposer.org/">Composer</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#send-traces-to-signoz-cloud
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-php
<p>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud</p>
<p>Here we will be sending traces to SigNoz cloud in 4 easy steps, if you want to send traces to self hosted SigNoz , you can refer to <a href="https://signoz.io/docs/instrumentation/PHP/#send-traces-to-self-hosted-signoz">this</a>. We will be using Zero-code configuration for Automatic Instrumentation.</p>
<p><strong>Step 1: Setup Development Environment</strong></p>
<p>To configure our PHP application to send data, you need to use OpenTelemetry PHP extension. Since the extension is built from the source, you need to have the build tools, which can be installed using the following command:</p>
<p>Linux (apt)Mac (Homebrew)</p>
<pre><code>sudo apt-get install gcc make autoconf
</code></pre>
<p><strong>Step 2: Build the extension</strong></p>
<p>With our environment set up we can install the extension using <a href="https://pecl.php.net/">PECL</a>
:</p>
<pre><code>pecl install opentelemetry               
pecl install protobuf
</code></pre>
<p>After successfully installing the OpenTelemetry extension, add the extension to <code>php.ini</code> file of your project:</p>
<pre><code>[opentelemetry]
extension=opentelemetry.so
</code></pre>
<p>Verify that the extension is enabled by running:</p>
<pre><code>php -m | grep opentelemetry
</code></pre>
<p>This should output:</p>
<pre><code>opentelemetry
</code></pre>
<p><strong>Step 3: Add the dependencies</strong></p>
<p>Add dependencies required for OpenTelemetry SDK for PHP to perform automatic instrumentation using this command :</p>
<pre><code>composer config allow-plugins.php-http/discovery false
composer require \
  open-telemetry/sdk \
  open-telemetry/exporter-otlp \
  php-http/guzzle7-adapter \
  open-telemetry/opentelemetry-auto-slim
</code></pre>
<p>‚úÖ Info</p>
<p>You can install the additional dependencies provided by OpenTelemetry for different PHP frameworks from <a href="https://packagist.org/explore/?query=open-telemetry">here</a>.</p>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<p>We are passing the environment variables on run time and this way we don't have to change anything in code. Run your application using:</p>
<pre><code>env OTEL_PHP_AUTOLOAD_ENABLED=true \
    OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; \
    OTEL_TRACES_EXPORTER=otlp \
    OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf \
    OTEL_EXPORTER_OTLP_ENDPOINT=&lt;SIGNOZ_ENDPOINT&gt; \
    OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;INGESTION_KEY&gt; \
    OTEL_PROPAGATORS=baggage,tracecontext \
    php -S localhost:8080 app.php
</code></pre>
<p>You can change the env vars value by referencing values from the following lookup table</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_SERVICE_NAME</td>
<td><code>&lt;SERVICE_NAME&gt;</code> replace it with name of your app</td>
</tr>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td><code>&lt;SIGNOZ_ENDPOINT&gt;</code> replace this with SigNoz cloud endpoint</td>
</tr>
<tr>
<td>OTEL_EXPORTER_OTLP_HEADERS</td>
<td>signoz-access-token=<code>&lt;INGESTION_KEY&gt;</code> replace this with the ingestion key which you must have received in mail</td>
</tr>
<tr>
<td>php -S localhost:8080 app.php</td>
<td>you can replace this with the run command of your PHP application</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#send-traces-via-otel-collector-binary
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-php
<p>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p><strong>Step 1: Install OTel Collector</strong></p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your PHP application.</p>
<p><strong>Step 2: Setup Development Environment</strong></p>
<p>To configure our PHP application to send data, you need to use OpenTelemetry PHP extension. Since the extension is built from the source, you need to have the build tools, which can be installed using the following command:</p>
<p>Linux (apt)Mac (Homebrew)</p>
<pre><code>sudo apt-get install gcc make autoconf
</code></pre>
<p><strong>Step 3: Build the extension</strong></p>
<p>With our environment set up we can install the extension using <a href="https://pecl.php.net/">PECL</a>
:</p>
<pre><code>pecl install opentelemetry
pecl install protobuf
</code></pre>
<p>After successfully installing the OpenTelemetry extension, add the extension to <code>php.ini</code> file of your project:</p>
<pre><code>[opentelemetry]
extension=opentelemetry.so
</code></pre>
<p>Verify that the extension is enabled by running:</p>
<pre><code>php -m | grep opentelemetry
</code></pre>
<p>This should output:</p>
<pre><code>opentelemetry
</code></pre>
<p><strong>Step 4: Add the dependencies</strong></p>
<p>Add dependencies required for OpenTelemetry SDK for PHP to perform automatic instrumentation using this command :</p>
<pre><code>composer config allow-plugins.php-http/discovery false
composer require \
  open-telemetry/sdk \
  open-telemetry/exporter-otlp \
  php-http/guzzle7-adapter \
  open-telemetry/opentelemetry-auto-slim
</code></pre>
<p>‚úÖ Info</p>
<p>You can install the additional dependencies provided by OpenTelemetry for different PHP frameworks from <a href="https://packagist.org/explore/?query=open-telemetry">here</a>.</p>
<p><strong>Step 5: Set environment variables and run app</strong></p>
<p>We are passing the environment variables on run time and this way we don't have to change anything in code. Run your application using:</p>
<pre><code>env OTEL_PHP_AUTOLOAD_ENABLED=true \
    OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; \
    OTEL_TRACES_EXPORTER=otlp \
    OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf \
    OTEL_EXPORTER_OTLP_ENDPOINT=&lt;COLLECTOR_ENDPOINT&gt; \
    OTEL_PROPAGATORS=baggage,tracecontext \
    php -S localhost:8080 app.php
</code></pre>
<p>You can change the env vars value by referencing values from the following lookup table</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_SERVICE_NAME</td>
<td><code>&lt;SERVICE_NAME&gt;</code> replace it with name of your app</td>
</tr>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td><code>&lt;COLLECTOR_ENDPOINT&gt;</code> replace this with the Otel Collector Endpoint. If you have hosted it somewhere, provide the URL. Otherwise, the default is <code>http://localhost:4317</code>, if you have followed our guide.</td>
</tr>
<tr>
<td>php -S localhost:8080 app.php</td>
<td>you can replace this with the run command of your PHP application</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<p><strong>Step 1: Setup Development Environment</strong></p>
<p>To configure our PHP application to send data, you need to use OpenTelemetry PHP extension. Since the extension is built from the source, you need to have the build tools, which can be installed using the following command:</p>
<p>Linux (apt)Mac (Homebrew)</p>
<pre><code>sudo apt-get install gcc make autoconf
</code></pre>
<p><strong>Step 2: Build the extension</strong></p>
<p>With our environment set up we can install the extension using <a href="https://pecl.php.net/">PECL</a>
:</p>
<pre><code>pecl install opentelemetry
pecl install protobuf
</code></pre>
<p>After successfully installing the OpenTelemetry extension, add the extension to <code>php.ini</code> file of your project:</p>
<pre><code>[opentelemetry]
extension=opentelemetry.so
</code></pre>
<p>Verify that the extension is enabled by running:</p>
<pre><code>php -m | grep opentelemetry
</code></pre>
<p>This should output:</p>
<pre><code>opentelemetry
</code></pre>
<p><strong>Step 3: Add the dependencies</strong></p>
<p>Add dependencies required for OpenTelemetry SDK for PHP to perform automatic instrumentation using this command :</p>
<pre><code>composer config allow-plugins.php-http/discovery false
composer require \
  open-telemetry/sdk \
  open-telemetry/opentelemetry-auto-slim \
  php-http/guzzle7-adapter \
  open-telemetry/exporter-otlp
</code></pre>
<p>‚úÖ Info</p>
<p>You can install the additional dependencies provided by OpenTelemetry for different PHP frameworks from <a href="https://packagist.org/explore/?query=open-telemetry">here</a>.</p>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<p>We are passing the environment variables on run time and this way we don't have to change anything in code. Run your application using:</p>
<pre><code>env OTEL_PHP_AUTOLOAD_ENABLED=true \
    OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; \
    OTEL_TRACES_EXPORTER=otlp \
    OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf \
    OTEL_EXPORTER_OTLP_ENDPOINT=&lt;SIGNOZ_ENDPOINT&gt; \
    OTEL_PROPAGATORS=baggage,tracecontext \
    php -S localhost:8080 app.php
</code></pre>
<p>You can change the env vars value by referencing values from the following lookup table</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_SERVICE_NAME</td>
<td><code>&lt;SERVICE_NAME&gt;</code> replace it with name of your app</td>
</tr>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td><code>&lt;SIGNOZ_ENDPOINT&gt;</code> replace this with the url where you have hosted SigNoz</td>
</tr>
<tr>
<td>php -S localhost:8080 app.php</td>
<td>you can replace this with the run command of your PHP application</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#sample-php-application
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-sample-php-application
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Sample PHP Application</h2>
<p>We have included a sample PHP application at <a href="https://github.com/SigNoz/OpenTelemetry-PHP-example">Sample PHP App Github Repo</a>
,</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#tutorial
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-tutorial
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Tutorial</h2>
<p>Here's a <a href="https://signoz.io/blog/opentelemetry-PHP/">tutorial</a> with step by step guide on how to install SigNoz and start monitoring a sample PHP app.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/php/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, php
image_urls: 
tracking_id: docs-instrumentation-php-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-php
<h2>PHP Opentelemetry Instrumentation: Send traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry(OTel) instrumentation in your .NET application. OpenTelemetry, also known as OTel for short, is an open-source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your .NET application.</p>
<p>OpenTelemetry .NET is the language-specific implementation of OpenTelemetry in .NET.</p>
<p>Once the telemetry data is generated, you can configure an exporter to send the data to SigNoz for monitoring and visualization.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your .NET application with OpenTelemetry</li>
<li>Configuring the exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p>In this tutorial, we will instrument a .NET application for traces and send it to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#requirements
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-requirements
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation: Requirements</h2>
<p><a href="https://dotnet.microsoft.com/en-us/download">.NET SDK</a> (.NET 5.0 or Later)</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-to-signoz-cloud
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>Tthere are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
<ul>
<li><a href="#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
<ul>
<li><a href="#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended
group_tracking_ids: docs-instrumentation-dotnet
<p>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1: Create a shell script with the following content</strong> You can either create a bash script with the following content or paste this directly into your terminal after replacing <code>SERVICE_NAME</code>, <code>SIGNOZ_ENDPOINT</code> and <code>SIGNOZ_INGESTION_KEY</code> .</p>
<pre><code># Download the bash script
curl -sSfL https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/releases/latest/download/otel-dotnet-auto-install.sh -O

# Install core files
sh ./otel-dotnet-auto-install.sh

# Enable execution for the instrumentation script
chmod +x $HOME/.otel-dotnet-auto/instrument.sh

# Setup the instrumentation for the current shell session
. $HOME/.otel-dotnet-auto/instrument.sh

# Run your application with instrumentation
OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; OTEL_TRACES_EXPORTER=otlp OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_RESOURCE_ATTRIBUTES=deployment.environment=staging,service.version=1.0.0 OTEL_EXPORTER_OTLP_ENDPOINT=&lt;SIGNOZ_ENDPOINT&gt; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; ./MyNetApp
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>SERVICE_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>If you are doing it on mac os , you will need to install <code>coreutils</code>, you can do it by using the following command</p>
<pre><code>brew install coreutils
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation
tag_set: instrumentation, dotnet
image_urls: https://signoz.io/img/docs/ingestion_key_details.webp, https://signoz.io/img/docs/sample_net_app.webp
tracking_id: docs-instrumentation-dotnet-send-traces-directly-to-signoz-cloud--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-dotnet
<p>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation</p>
<p><strong>Step 1: Installing the OpenTelemetry dependency packages:</strong></p>
<pre><code>dotnet add package OpenTelemetry
dotnet add package OpenTelemetry.Exporter.OpenTelemetryProtocol 
dotnet add package OpenTelemetry.Extensions.Hosting
dotnet add package OpenTelemetry.Instrumentation.Runtime
dotnet add package OpenTelemetry.Instrumentation.AspNetCore 
dotnet add package OpenTelemetry.AutoInstrumentation
</code></pre>
<p><strong>Step 2: Adding OpenTelemetry as a service and configuring exporter options in <code>Program.cs</code>:</strong></p>
<p>In your <code>Program.cs</code> file, add OpenTelemetry as a service. Here, we are configuring these variables:</p>
<ul>
<li><code>serviceName</code> - It is the name of your service.</li>
<li><code>otlpOptions.Endpoint</code> - It is the endpoint for SigNoz Cloud.</li>
<li><code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> - You will get your ingestion key when you <a href="https://signoz.io/teams/">sign up</a> for SigNoz cloud.</li>
</ul>
<p>Here‚Äôs a sample <code>Program.cs</code> file with the configured variables.</p>
<pre><code>using System.Diagnostics;
using OpenTelemetry.Exporter;
using OpenTelemetry.Resources;
using OpenTelemetry.Trace;

var builder = WebApplication.CreateBuilder(args);

// Configure OpenTelemetry with tracing and auto-start.
builder.Services.AddOpenTelemetry()
    .ConfigureResource(resource =&gt; 
		resource.AddService(serviceName: &quot;sample-net-app&quot;))
    .WithTracing(tracing =&gt; tracing
        .AddAspNetCoreInstrumentation()
        .AddOtlpExporter(otlpOptions =&gt;
        {
						//SigNoz Cloud Endpoint 
            otlpOptions.Endpoint = new Uri(&quot;https://ingest.{region}.signoz.cloud:443&quot;);

            otlpOptions.Protocol = OtlpExportProtocol.Grpc;
						
						//SigNoz Cloud account Ingestion key
            string headerKey = &quot;signoz-access-token&quot;;
            string headerValue = &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;;

            string formattedHeader = $&quot;{headerKey}={headerValue}&quot;;
            otlpOptions.Headers = formattedHeader;
        }));

var app = builder.Build();

// The index route (&quot;/&quot;) is set up to write out the OpenTelemetry trace information on the response:
app.MapGet(&quot;/&quot;, () =&gt; $&quot;Hello World! OpenTelemetry Trace: {Activity.Current?.Id}&quot;);

app.Run();
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>The program uses the¬†<a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/src/OpenTelemetry.Instrumentation.AspNetCore/README.md">OpenTelemetry.Instrumentation.AspNetCore</a> package to automatically create traces for incoming ASP.NET Core requests.</p>
<p>The <code>OpenTelemetry.Exporter.Options</code> get or set the target to which the exporter is going to send traces. Here, we‚Äôre configuring it to send traces to the SigNoz cloud. The target must be a valid Uri with the scheme (<code>http</code> or <code>https</code>) and host and may contain a port and a path.</p>
<p>This is done by configuring an OpenTelemetry¬†<a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/docs/trace/customizing-the-sdk/README.MD#tracerprovider">TracerProvider</a> using extension methods and setting it to auto-start when the host is started.</p>
<p>üìù Note</p>
<p>You can find your Signoz cloud address and ingestion key under the settings of your Signoz cloud account.</p>
<p><img src="https://signoz.io/img/docs/ingestion_key_details.webp" alt="Access the ingestion key details in SigNoz UI" /></p>
<p><em>Access the ingestion key details in SigNoz UI</em></p>
<p><strong>Step 3. Running the .NET application:</strong></p>
<pre><code>dotnet build
dotnet run
</code></pre>
<p><strong>Step 4: Generating some load data and checking your application in SigNoz UI</strong></p>
<p>Once your application is running, generate some traffic by interacting with it.</p>
<p>In the SigNoz account, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>. Ensure that you're checking data for the¬†<code>time range filter</code>¬†applied in the top right corner. You might have to wait for a few seconds before the data appears on SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/sample_net_app.webp" alt="The sample .NET application is being monitored in the SigNoz ‚ÄòServices‚Äô tab" /></p>
<p><em>The sample .NET application is being monitored in the SigNoz ‚ÄòServices‚Äô tab</em></p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-send-traces-via-otel-collector-binary--no-code-automatic-instrumentation
group_tracking_ids: docs-instrumentation-dotnet
<p>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation: ### ## Send traces via OTel Collector binary - Send traces via OTel Collector binary - No Code Automatic Instrumentation</p>
<p><strong>Step 1: Create a shell script with the following content</strong> You can either create a bash script with the following content or paste this directly into your terminal after replacing <code>SERVICE_NAME</code>, <code>SIGNOZ_ENDPOINT</code> and <code>SIGNOZ_INGESTION_KEY</code> .</p>
<pre><code># Download the bash script
curl -sSfL https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/releases/latest/download/otel-dotnet-auto-install.sh -O

# Install core files
sh ./otel-dotnet-auto-install.sh

# Enable execution for the instrumentation script
chmod +x $HOME/.otel-dotnet-auto/instrument.sh

# Setup the instrumentation for the current shell session
. $HOME/.otel-dotnet-auto/instrument.sh

# Run your application with instrumentation
OTEL_SERVICE_NAME=&lt;SERVICE_NAME&gt; OTEL_TRACES_EXPORTER=otlp OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_RESOURCE_ATTRIBUTES=deployment.environment=staging,service.version=1.0.0 OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/traces ./MyNetApp
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>SERVICE_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>If you are doing it on mac os , you will need to install <code>coreutils</code>, you can do it by using the following command</p>
<pre><code>brew install coreutils
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation
tag_set: instrumentation, dotnet
image_urls: https://signoz.io/img/docs/sample_net_app.webp
tracking_id: docs-instrumentation-dotnet-send-traces-via-otel-collector-binary--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-dotnet
<p>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation: Send traces via OTel Collector binary - Code Level Automatic Instrumentation</p>
<p><strong>Step 1: Setting up OpenTelemetry Collector binary as an agent in your machine</strong></p>
<p>OpenTelemetry Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install OTel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>Go to <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">setup OTel Collector binary</a> to install Otel Collector as agent that will collect telemetry data from your sample dotnet app and send it to SigNoz cloud.</p>
<p><strong>Step 2: Installing the OpenTelemetry dependency packages:</strong></p>
<p>Install the following dependencies in your application.</p>
<pre><code>dotnet add package OpenTelemetry
dotnet add package OpenTelemetry.Exporter.OpenTelemetryProtocol 
dotnet add package OpenTelemetry.Extensions.Hosting
dotnet add package OpenTelemetry.Instrumentation.Runtime
dotnet add package OpenTelemetry.Instrumentation.AspNetCore 
dotnet add package OpenTelemetry.AutoInstrumentation
</code></pre>
<p><strong>Step 2: Adding OpenTelemetry as a service and configuring exporter options in <code>Program.cs</code>:</strong></p>
<p>In your <code>Program.cs</code> file, add OpenTelemetry as a service. Here, we are configuring these variables:</p>
<ul>
<li><code>serviceName</code> - It is the name of your service.</li>
<li><code>otlpOptions.Endpoint</code> - It is the endpoint for your OTel Collector binary agent.</li>
</ul>
<p>Here‚Äôs a sample <code>Program.cs</code> file with the configured variables.</p>
<pre><code>using System.Diagnostics;
using OpenTelemetry.Exporter;
using OpenTelemetry.Resources;
using OpenTelemetry.Trace;

var builder = WebApplication.CreateBuilder(args);

// Configure OpenTelemetry with tracing and auto-start.
builder.Services.AddOpenTelemetry()
    .ConfigureResource(resource =&gt; 
		resource.AddService(serviceName: &quot;sample-net-app&quot;))
    .WithTracing(tracing =&gt; tracing
        .AddAspNetCoreInstrumentation()
        .AddOtlpExporter(otlpOptions =&gt;
        {
            otlpOptions.Endpoint = new Uri(&quot;http://localhost:4317&quot;);

            otlpOptions.Protocol = OtlpExportProtocol.Grpc;
        }));

var app = builder.Build();

//The index route (&quot;/&quot;) is set up to write out the OpenTelemetry trace information on the response:
app.MapGet(&quot;/&quot;, () =&gt; $&quot;Hello World! OpenTelemetry Trace: {Activity.Current?.Id}&quot;);

app.Run();
</code></pre>
<p>The program uses the¬†<a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/src/OpenTelemetry.Instrumentation.AspNetCore/README.md">OpenTelemetry.Instrumentation.AspNetCore</a> package to automatically create traces for incoming ASP.NET Core requests.</p>
<p>The <code>OpenTelemetry.Exporter.Options</code> get or set the target to which the exporter is going to send traces. Here, we‚Äôre configuring it to send traces to the OTel collector agent. The target must be a valid Uri with the scheme (<code>http</code> or <code>https</code>) and host and may contain a port and a path.</p>
<p>This is done by configuring an OpenTelemetry¬†<a href="https://github.com/open-telemetry/opentelemetry-dotnet/blob/main/docs/trace/customizing-the-sdk/README.MD#tracerprovider">TracerProvider</a> using extension methods and setting it to auto-start when the host is started.</p>
<p><strong>Step 4. Running the .NET application:</strong></p>
<pre><code>dotnet build
dotnet run
</code></pre>
<p><strong>Step 5: Generating some load data and checking your application in SigNoz UI</strong></p>
<p>After the Otel collector is all set and running, and your too application is running, generate some traffic by interacting with it.</p>
<p>In the SigNoz account, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>. Ensure that you're checking data for the¬†<code>time range filter</code>¬†applied in the top right corner. You might have to wait for a few seconds before the data appears on SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/sample_net_app.webp" alt="The sample .NET application is being monitored in the SigNoz ‚ÄòServices‚Äô tab" /></p>
<p><em>The sample .NET application is being monitored in the SigNoz ‚ÄòServices‚Äô tab</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#troubleshooting
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-troubleshooting
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Troubleshooting</h2>
<p>The console exporter prints data to the Console window. You can use it to verify if the instrumentation is properly set up or not.</p>
<p>Below are the steps on how to use the console exporter:</p>
<p><strong>Step 1. Adding the OpenTelemetry console exporter package:</strong></p>
<pre><code>dotnet add package OpenTelemetry.Exporter.Console 
</code></pre>
<p><strong>Step 2. Adding the console exporter method:</strong></p>
<pre><code>using System.Diagnostics;
using OpenTelemetry.Exporter;
using OpenTelemetry.Resources;
using OpenTelemetry.Trace;

var builder = WebApplication.CreateBuilder(args);

// Configure OpenTelemetry with tracing and auto-start.
builder.Services.AddOpenTelemetry()
    .ConfigureResource(resource =&gt; 
		resource.AddService(serviceName: &quot;sample-net-app&quot;))
    .WithTracing(tracing =&gt; tracing
        .AddAspNetCoreInstrumentation()
        .AddOtlpExporter(otlpOptions =&gt;
        {
						//SigNoz Cloud Endpoint 
            otlpOptions.Endpoint = new Uri(&quot;https://ingest.{region}.signoz.cloud:443&quot;);

            otlpOptions.Protocol = OtlpExportProtocol.Grpc;
						
						//SigNoz Cloud account Ingestion key
            string headerKey = &quot;signoz-access-token&quot;;
            string headerValue = &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;;

            string formattedHeader = $&quot;{headerKey}={headerValue}&quot;;
            otlpOptions.Headers = formattedHeader;
        })
				.AddConsoleExporter());

var app = builder.Build();

//The index route (&quot;/&quot;) is set up to write out the OpenTelemetry trace information on the response:
app.MapGet(&quot;/&quot;, () =&gt; $&quot;Hello World! OpenTelemetry Trace: {Activity.Current?.Id}&quot;);

app.Run();
</code></pre>
<p>Monitor the application on the console. You will be able to see the trace output as below:</p>
<pre><code>info: Microsoft.Hosting.Lifetime[14]
      Now listening on: https://localhost:7062
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://localhost:5017
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
info: Microsoft.Hosting.Lifetime[0]
      Hosting environment: Development
info: Microsoft.Hosting.Lifetime[0]
      Content root path: C:\sample-app2
Activity.TraceId:            e1c2b70e9f39c6cc15d5d94b75412b70
Activity.SpanId:             17da84c0833e0075
Activity.TraceFlags:         Recorded
Activity.ActivitySourceName: Microsoft.AspNetCore
Activity.DisplayName:        /
Activity.Kind:               Server
Activity.StartTime:          2023-11-05T19:59:39.7875151Z
Activity.Duration:           00:00:00.2548901
Activity.Tags:
    net.host.name: localhost
    net.host.port: 7062
    http.method: GET
    http.scheme: https
    http.target: /
    http.url: https://localhost:7062/
    http.flavor: 2.0
    http.user_agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36
    http.status_code: 200
Resource associated with Activity:
    service.name: sample-app2
    service.instance.id: 44a34277-d46e-4758-b4f0-91b5a9435a4c
    telemetry.sdk.name: opentelemetry
    telemetry.sdk.language: dotnet
    telemetry.sdk.version: 1.6.0
</code></pre>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/dotnet/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, dotnet
image_urls: 
tracking_id: docs-instrumentation-dotnet-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-dotnet
<h2>.NET OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#send-traces-to-signoz-cloud
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: Ruby on Rails OpenTelemetry Instrumentation - Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h3>## Send traces directly to SigNoz Cloud</h3>
<p><strong>Step 1. Install dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry SDK and exporter using gem.</p>
<pre><code>gem install opentelemetry-sdk
gem install opentelemetry-exporter-otlp
gem install opentelemetry-instrumentation-all
</code></pre>
<p>Include the required packages into your gemfile.</p>
<pre><code>gem 'opentelemetry-sdk'
gem 'opentelemetry-exporter-otlp'
gem 'opentelemetry-instrumentation-all'
</code></pre>
<p>Run the bundle install command:</p>
<pre><code>bundle install
</code></pre>
<p><strong>Step 2. Initialize the OpenTelemetry SDK</strong></p>
<p>Initialize the otel sdk by adding below lines to <code>config/environment.rb</code> of your Ruby on Rails application.</p>
<pre><code>require 'opentelemetry/sdk'
require_relative 'application'

OpenTelemetry::SDK.configure do |c|
  c.use_all
end

Rails.application.initialize!
</code></pre>
<p><strong>Step 3. Running your Ruby application</strong></p>
<p>Run the application using the below:</p>
<pre><code>OTEL_EXPORTER=otlp \
OTEL_SERVICE_NAME= \
OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.{region}.signoz.cloud:443 \
OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=SIGNOZ_INGESTION_KEY \
rails server
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of service. For example, <code>sampleRailsApp</code></li>
<li><code>SIGNOZ_INGESTION_KEY</code> : The ingestion key sent by SigNoz over email. It can also be found in the <code>settings</code> section of your SigNoz Cloud UI.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<hr />
<h3>## Send traces via OTel Collector binary</h3>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Ruby on Rails application.</p>
<p><strong>Step 1. Install dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry SDK and exporter using gem.</p>
<pre><code>gem install opentelemetry-sdk
gem install opentelemetry-exporter-otlp
gem install opentelemetry-instrumentation-all
</code></pre>
<p>Include the required packages into your gemfile.</p>
<pre><code>gem 'opentelemetry-sdk'
gem 'opentelemetry-exporter-otlp'
gem 'opentelemetry-instrumentation-all'
</code></pre>
<p>Run the bundle install command:</p>
<pre><code>bundle install
</code></pre>
<p><strong>Step 2. Initialize the OpenTelemetry SDK</strong></p>
<p>Initialize the otel sdk by adding below lines to <code>config/environment.rb</code> of your Ruby on Rails application.</p>
<pre><code>require 'opentelemetry/sdk'
require_relative 'application'

OpenTelemetry::SDK.configure do |c|
  c.use_all
end

Rails.application.initialize!
</code></pre>
<p><strong>Step 3. Running your Ruby application</strong></p>
<p>Run the application using the below:</p>
<pre><code>OTEL_EXPORTER=otlp \
OTEL_SERVICE_NAME= \
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 \
rails server
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of service. For example, <code>sampleRailsApp</code></li>
</ul>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4318 with <code>&lt;IP Address of the VM&gt;:4318</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can follow these steps to send your traces directly to your Self-Host SigNoz instance.</p>
<p><strong>Step 1. Install dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry SDK and exporter using gem.</p>
<pre><code>gem install opentelemetry-sdk
gem install opentelemetry-exporter-otlp
gem install opentelemetry-instrumentation-all
</code></pre>
<p>Include the required packages into your gemfile.</p>
<pre><code>gem 'opentelemetry-sdk'
gem 'opentelemetry-exporter-otlp'
gem 'opentelemetry-instrumentation-all'
</code></pre>
<p>Run the bundle install command:</p>
<pre><code>bundle install
</code></pre>
<p><strong>Step 2. Initialize the OpenTelemetry SDK</strong></p>
<p>Initialize the otel sdk by adding below lines to <code>config/environment.rb</code> of your Ruby on Rails application.</p>
<pre><code>require 'opentelemetry/sdk'
require_relative 'application'

OpenTelemetry::SDK.configure do |c|
  c.use_all
end

Rails.application.initialize!
</code></pre>
<p><strong>Step 3. Running your Ruby application</strong></p>
<p>Run the application using the below:</p>
<pre><code>OTEL_EXPORTER=otlp \
OTEL_SERVICE_NAME= \
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 \
rails server
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of service. For example, <code>sampleRailsApp</code></li>
</ul>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4318 with <code>&lt;IP Address of the VM&gt;:4318</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#tutorials
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-tutorials
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: Tutorials</h2>
<p>Here's a <a href="https://signoz.io/blog/opentelemetry-ruby/">tutorial</a> with step by step guide on how to install SigNoz and start monitoring a sample Ruby on Rails app.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#sample-ruby-on-rails-application
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-sample-ruby-on-rails-application
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: Sample Ruby on Rails application</h2>
<p>We have included a sample Ruby on Rails application with README.md at <a href="https://github.com/SigNoz/sample-rails-app">Sample Rails App Github Repo.</a></p>
<p>Feel free to use this repo to test out OpenTelemetry instrumentation and how to send telemetry data to SigNoz.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/ruby-on-rails/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, ruby-on-rails
image_urls: 
tracking_id: docs-instrumentation-ruby-on-rails-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-ruby-on-rails
<h2>Ruby on Rails OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation</h2>
<p>This document contains OpenTelemetry instrumentation instructions for Elixir Phoenix + Ecto framework.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#send-traces-to-signoz-cloud
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-elixir
<p>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud</p>
<p><strong>Step 1. Add dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry by adding them to <code>mix.exs</code> file</p>
<pre><code>    {:opentelemetry_exporter, &quot;~&gt; 1.6&quot;},
    {:opentelemetry_api, &quot;~&gt; 1.2&quot;},
    {:opentelemetry, &quot;~&gt; 1.3&quot;},
    {:opentelemetry_semantic_conventions, &quot;~&gt; 0.2&quot;},
    {:opentelemetry_cowboy, &quot;~&gt; 0.2.1&quot;},
    {:opentelemetry_phoenix, &quot;~&gt; 1.1&quot;},
    {:opentelemetry_ecto, &quot;~&gt; 1.1&quot;}
</code></pre>
<p>In your application start, usually the <code>application.ex</code> file, setup the telemetry handlers</p>
<pre><code>    :opentelemetry_cowboy.setup()
    OpentelemetryPhoenix.setup(adapter: :cowboy2)
    OpentelemetryEcto.setup([:YOUR_APP_NAME, :repo])
</code></pre>
<p><code>YOUR_APP_NAME</code> - Name of your application or service.</p>
<p>As an example, this is how you can setup the handlers in your <code>application.ex</code> file for an application called <code>demo</code> :</p>
<pre><code># application.ex
@impl true
def start(_type, _args) do
  :opentelemetry_cowboy.setup()
  OpentelemetryPhoenix.setup(adapter: :cowboy2)
  OpentelemetryEcto.setup([:demo, :repo])

end
</code></pre>
<p><strong>Step 2. Configure Application</strong></p>
<p>You need to configure your application to send telemtry data by adding the follwing config to your <code>runtime.exs</code> file:</p>
<pre><code>config :opentelemetry, :resource, service: %{name: &quot;YOUR_APP_NAME&quot;}

config :opentelemetry, :processors,
  otel_batch_processor: %{
    exporter: {
      :opentelemetry_exporter,
      %{
        endpoints: [&quot;https://ingest.{region}.signoz.cloud:443&quot;],
        headers: [\
          {&quot;signoz-access-token&quot;, SIGNOZ_ACCESS_TOKEN}\
        ]
      }
    }
  }
</code></pre>
<p><code>YOUR_APP_NAME</code>: Your application or service name.</p>
<p><code>SIGNOZ_INGESTION_KEY</code> : The ingestion key sent by SigNoz over email. It can also be found in the <code>settings</code> section of your SigNoz Cloud UI.</p>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#send-traces-via-otel-collector-binary
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-elixir
<p>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Elixir (Phoenix + Ecto) application.</p>
<p><strong>Step 1. Add dependencies</strong></p>
<p>Install dependencies related to OpenTelemetry by adding them to <code>mix.exs</code> file</p>
<pre><code>    {:opentelemetry_exporter, &quot;~&gt; 1.6&quot;},
    {:opentelemetry_api, &quot;~&gt; 1.2&quot;},
    {:opentelemetry, &quot;~&gt; 1.3&quot;},
    {:opentelemetry_semantic_conventions, &quot;~&gt; 0.2&quot;},
    {:opentelemetry_cowboy, &quot;~&gt; 0.2.1&quot;},
    {:opentelemetry_phoenix, &quot;~&gt; 1.1&quot;},
    {:opentelemetry_ecto, &quot;~&gt; 1.1&quot;}
</code></pre>
<p>In your application start, usually the <code>application.ex</code> file, setup the telemetry handlers</p>
<pre><code>    :opentelemetry_cowboy.setup()
    OpentelemetryPhoenix.setup(adapter: :cowboy2)
    OpentelemetryEcto.setup([:YOUR_APP_NAME, :repo])
</code></pre>
<p>As an example, this is how you can setup the handlers in your <code>application.ex</code> file for an application called <code>demo</code> :</p>
<pre><code># application.ex
@impl true
def start(_type, _args) do
  :opentelemetry_cowboy.setup()
  OpentelemetryPhoenix.setup(adapter: :cowboy2)
  OpentelemetryEcto.setup([:demo, :repo])

end
</code></pre>
<p><strong>Step 2. Configure Application</strong></p>
<p>You need to configure your application to send telemtry data by adding the follwing config to your <code>runtime.exs</code> file:</p>
<pre><code>config :opentelemetry, :resource, service: %{name: &quot;YOUR_APP_NAME&quot;}

config :opentelemetry, :processors,
    otel_batch_processor: %{
      exporter: 
      {:opentelemetry_exporter, 
      %{endpoints: [&quot;http://localhost:4318&quot;]}
      }
  }
</code></pre>
<p><code>YOUR_APP_NAME</code>: Your application or service name.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<p>We‚Äôll focus on instrumenting one of the most common combos of the Elixir world: <code>Phoenix + Ecto</code>.</p>
<h3>## Step 1: Add dependencies</h3>
<p>The first step to instrument your Elixir application with OpenTelemetry is to add the required dependencies to your <code>mix.exs</code> file and fetch them with <code>mix deps.get</code></p>
<pre><code>{:opentelemetry, &quot;~&gt; 1.0.3&quot;},
{:opentelemetry_exporter, &quot;~&gt; 1.0.3&quot;},
{:opentelemetry_phoenix, &quot;~&gt; 1.0.0&quot;},
{:opentelemetry_ecto, &quot;~&gt; 1.0.0&quot;}
</code></pre>
<h3>## Step 2: Configure Elixir application</h3>
<p>Then we need to configure our application to export telemetry data. There are two things that you need to set:</p>
<ul>
<li>
<p><code>YOUR_APP_NAME</code><br />
You can put your application or service name here for identification.</p>
</li>
<li>
<p><code>OTEL Collector endpoint</code><br />
The OTEL collector comes bundled with SigNoz installation. Since, we installed SigNoz on our local machine, the endpoint is <code>http://localhost:4318</code>.</p>
<p>config :opentelemetry, :resource, service: %{name: &quot;YOUR_APP_NAME&quot;}</p>
<p>config :opentelemetry, :processors,
otel_batch_processor: %{
exporter: {
:opentelemetry_exporter,
%{endpoints: [&quot;http://localhost:4318&quot;]}
}
}</p>
</li>
</ul>
<h3>## Step 3: Initialize telemetry handlers</h3>
<p>As it is documented in the <code>opentelemetry_phoenix</code> and <code>opentelemetry_ecto</code> <a href="http://hexdocs.pm">hexdocs.pm</a> pages, we need to initialize both telemetry handlers.</p>
<pre><code>OpentelemetryPhoenix.setup()
OpentelemetryEcto.setup([:your_app_name, :repo])
</code></pre>
<p><code>:your_app_name</code> should be replaced by your app name and congratulations, you have instrumented your application with OpenTelemetry.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#sample-examples
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-sample-examples
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: Sample Examples</h2>
<p>Here's a <a href="https://signoz.io/blog/opentelemetry-elixir/">tutorial</a> with step by step guide on how to install SigNoz and start monitoring a sample Elixir app.</p>
<p>Thanks to our community member <a href="https://github.com/ricardoccpaiva">Ricardo</a> for creating this guide.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/elixir/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, elixir
image_urls: 
tracking_id: docs-instrumentation-elixir-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-elixir
<h2>Elixir Opentelemetry Instrumentation: Send Traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation</h2>
<p>This doc contains instructions on how to set up OpenTelemetry(OTel) instrumentation in your Rust application. OpenTelemetry, also known as OTel for short, is an open-source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Rust application.</p>
<p>Once the telemetry data is generated, you can configure an exporter to send the data to SigNoz for monitoring and visualization.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Rust application with OpenTelemetry</li>
<li>Configuring the exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p>In this tutorial, we will instrument a Rust application for traces and send it to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#requirements
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-requirements
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Requirements</h2>
<p><a href="https://www.rust-lang.org/tools/install">Rust</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#send-traces-to-signoz-cloud
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-rust
<p>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud: Send traces directly to SigNoz cloud: Send traces directly to SigNoz cloud</p>
<p>Here we will be sending traces to SigNoz cloud in 4 easy steps, if you want to send traces to self hosted SigNoz , you can refer to <a href="https://signoz.io/docs/instrumentation/rust/#send-traces-to-self-hosted-signoz">this</a>.</p>
<p>‚úÖ Info</p>
<p>If you are using the <a href="https://github.com/SigNoz/sample-rust-app">sample Rust application</a>
, then you just need to update .env file and you are good to go!</p>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure our Rust application to send data we need to initialize OpenTelemetry, Otel has already created some crates which you need to add into your <code>Cargo.toml</code> file, just below <code>[dependencies]</code> section.</p>
<pre><code>opentelemetry = { version = &quot;0.18.0&quot;, features = [&quot;rt-tokio&quot;, &quot;metrics&quot;, &quot;trace&quot;] }
opentelemetry-otlp = { version = &quot;0.11.0&quot;, features = [&quot;trace&quot;, &quot;metrics&quot;] }
opentelemetry-semantic-conventions = { version = &quot;0.10.0&quot; }
opentelemetry-proto = { version = &quot;0.1.0&quot;}
tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
tonic = { version = &quot;0.8.2&quot;, features = [&quot;tls-roots&quot;] }
</code></pre>
<p>after adding these in <code>Cargo.toml</code> , you need to use these in entry point of your Rust application , which is <code>main.rs</code> file in majority of applications.</p>
<pre><code>use opentelemetry::global::shutdown_tracer_provider;
use opentelemetry::sdk::Resource;
use opentelemetry::trace::TraceError;
use opentelemetry::{
    global, sdk::trace as sdktrace,
    trace::{TraceContextExt, Tracer},
    Context, Key, KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tonic::metadata::{MetadataMap, MetadataValue};
</code></pre>
<p><strong>Step 2: Initialize the tracer and create env file</strong></p>
<p>Add this function in main.rs file, <code>init_tracer</code> is initializing an OpenTelemetry tracer with the OpenTelemetry OTLP exporter which is sending data to SigNoz Cloud.</p>
<pre><code>fn init_tracer() -&gt; Result&lt;sdktrace::Tracer, TraceError&gt; {
    let signoz_access_token = std::env::var(&quot;SIGNOZ_ACCESS_TOKEN&quot;).expect(&quot;SIGNOZ_ACCESS_TOKEN not set&quot;);
    let mut metadata = MetadataMap::new();
    metadata.insert(
        &quot;signoz-access-token&quot;,
        MetadataValue::from_str(&amp;signoz_access_token).unwrap(),
    );
    opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_metadata(metadata)
                .with_endpoint(std::env::var(&quot;SIGNOZ_ENDPOINT&quot;).expect(&quot;SIGNOZ_ENDPOINT not set&quot;)),
        )
        .with_trace_config(
            sdktrace::config().with_resource(Resource::new(vec![\
                KeyValue::new(\
                    opentelemetry_semantic_conventions::resource::SERVICE_NAME,\
                    std::env::var(&quot;APP_NAME&quot;).expect(&quot;APP_NAME not set&quot;),\
                ),\
            ])),
        )
        .install_batch(opentelemetry::runtime::Tokio)
}
</code></pre>
<p>After adding this function, you need to create a <code>.env</code> file in root of project , the structure should look like this.</p>
<pre><code>project_root/
|-- Cargo.toml
|-- src/
|   |-- main.rs
|-- .env
</code></pre>
<p>Paste these in <code>.env</code> file</p>
<pre><code>PORT=3000
APP_NAME=rust-sample
SIGNOZ_ENDPOINT=https://ingest.[region].signoz.cloud:443/v1/traces
SIGNOZ_ACCESS_TOKEN=XXXXXXXXXX
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PORT (Optional)</td>
<td>If it is a web app pass port or else you can ignore this variable</td>
</tr>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p><strong>Step 3: Add the OpenTelemetry instrumentation for your Rust app</strong></p>
<p>Open your Cargo.toml file and paste these below <code>[dependencies]</code></p>
<pre><code>dotenv = &quot;0.15.0&quot;
</code></pre>
<p>Import these at top, so you can use variables from <code>.env</code> file</p>
<pre><code>use dotenv::dotenv;
</code></pre>
<p>After importing , just call these functions inside <code>main()</code> function by pasting this at starting of <code>main()</code> function</p>
<pre><code>dotenv().ok();
let _ = init_tracer();
</code></pre>
<p>also change</p>
<pre><code>fn main(){
    //rest of the code
}
</code></pre>
<p>to</p>
<pre><code>#[tokio::main]
async fn main() {
    //rest of the code
}
</code></pre>
<p>Now comes the most interesting part, Sending data to SigNoz to get sense of your traces. After adding the below block you can send data to SigNoz cloud</p>
<pre><code>  let tracer = global::tracer(&quot;global_tracer&quot;);
    let _cx = Context::new();
  
    tracer.in_span(&quot;operation&quot;, |cx| {
        let span = cx.span();
        span.set_attribute(Key::new(&quot;KEY&quot;).string(&quot;value&quot;));

        span.add_event(
            format!(&quot;Operations&quot;),
            vec![\
                Key::new(&quot;SigNoz is&quot;).string(&quot;Awesome&quot;),\
            ],
        );
    });
    shutdown_tracer_provider()
</code></pre>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<p>Go to your <code>.env</code> file and update the value of required variables i.e <code>APP_NAME</code>, <code>SIGNOZ_ENDPOINT</code> and <code>SIGNOZ_ACCESS_TOKEN</code></p>
<p>Now run <code>cargo run</code> in terminal to start the application!</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#send-traces-via-otel-collector-binary
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-rust
<p>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud: Send traces via OTel Collector binary: Send traces via OTel Collector binary: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Rust application.</p>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure our Rust application to send traces we need to initialize OpenTelemetry, Otel has already created some crates which you need to add into your <code>Cargo.toml</code> file, just below <code>[dependencies]</code> section.</p>
<pre><code>opentelemetry = { version = &quot;0.18.0&quot;, features = [&quot;rt-tokio&quot;, &quot;metrics&quot;, &quot;trace&quot;] }
opentelemetry-otlp = { version = &quot;0.11.0&quot;, features = [&quot;trace&quot;, &quot;metrics&quot;] }
opentelemetry-semantic-conventions = { version = &quot;0.10.0&quot; }
opentelemetry-proto = { version = &quot;0.1.0&quot;}
tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
tonic = { version = &quot;0.8.2&quot;, features = [&quot;tls-roots&quot;] }
</code></pre>
<p>after adding these in <code>Cargo.toml</code> , you need to use these in entry point of your Rust application , which is <code>main.rs</code> file in majority of applications.</p>
<pre><code>use opentelemetry::global::shutdown_tracer_provider;
use opentelemetry::sdk::Resource;
use opentelemetry::trace::TraceError;
use opentelemetry::{
    global, sdk::trace as sdktrace,
    trace::{TraceContextExt, Tracer},
    Context, Key, KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tonic::metadata::{MetadataMap, MetadataValue};
</code></pre>
<p><strong>Step 2: Initialize the tracer and create env file</strong></p>
<p>Add this function in main.rs file, <code>init_tracer</code> is initializing an OpenTelemetry tracer with the OpenTelemetry OTLP exporter which is sending data to SigNoz Cloud.</p>
<p>This tracer initializes the connection with the OTel collector from the system variables passed while starting the app.</p>
<pre><code>fn init_tracer() -&gt; Result&lt;sdktrace::Tracer, TraceError&gt; {
    opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(opentelemetry_otlp::new_exporter().tonic().with_env())
        .with_trace_config(
            sdktrace::config().with_resource(Resource::default()),
        )
        .install_batch(opentelemetry::runtime::Tokio)
}
</code></pre>
<p><strong>Step 3: Add the OpenTelemetry instrumentation for your Rust app</strong></p>
<p>You need call init_tracer function inside <code>main()</code> at starting so that as soon as your rust application starts, tracer will be available globally.</p>
<pre><code>let _ = init_tracer();
</code></pre>
<p>also change</p>
<pre><code>fn main(){
    //rest of the code
}
</code></pre>
<p>to</p>
<pre><code>#[tokio::main]
async fn main() {
    //rest of the code
}
</code></pre>
<p>Now comes the most interesting part, Sending data to SigNoz to get sense of your traces. After adding the below block you can send traces to SigNoz cloud</p>
<pre><code>  let tracer = global::tracer(&quot;global_tracer&quot;);
    let _cx = Context::new();
  
    tracer.in_span(&quot;operation&quot;, |cx| {
        let span = cx.span();
        span.set_attribute(Key::new(&quot;KEY&quot;).string(&quot;value&quot;));

        span.add_event(
            format!(&quot;Operations&quot;),
            vec![\
                Key::new(&quot;SigNoz is&quot;).string(&quot;Awesome&quot;),\
            ],
        );
    });
    shutdown_tracer_provider()
</code></pre>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td>This is the endpoint of your OTel Collector. If you have hosted it somewhere, provide the URL. Otherwise, the default is <code>http://localhost:4317</code> if you have followed our guide.</td>
</tr>
<tr>
<td>OTEL_RESOURCE_ATTRIBUTES=service.name</td>
<td>Specify as the value. This will be the name of your Rust application on SigNoz services page, allowing you to uniquely identify the application traces.</td>
</tr>
</tbody>
</table>
<p>Now run</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 OTEL_RESOURCE_ATTRIBUTES=service.name=sample-rust-app cargo run
</code></pre>
<p>in terminal to start the application!</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure our Rust application to send traces we need to initialize OpenTelemetry, Otel has already created some crates which you need to add into your <code>Cargo.toml</code> file, just below <code>[dependencies]</code> section.</p>
<pre><code>opentelemetry = { version = &quot;0.18.0&quot;, features = [&quot;rt-tokio&quot;, &quot;metrics&quot;, &quot;trace&quot;] }
opentelemetry-otlp = { version = &quot;0.11.0&quot;, features = [&quot;trace&quot;, &quot;metrics&quot;] }
opentelemetry-semantic-conventions = { version = &quot;0.10.0&quot; }
opentelemetry-proto = { version = &quot;0.1.0&quot;}
tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
tonic = { version = &quot;0.8.2&quot;, features = [&quot;tls-roots&quot;] }
</code></pre>
<p>after adding these in <code>Cargo.toml</code> , you need to use these in entry point of your Rust application , which is <code>main.rs</code> file in majority of applications.</p>
<pre><code>use opentelemetry::global::shutdown_tracer_provider;
use opentelemetry::sdk::Resource;
use opentelemetry::trace::TraceError;
use opentelemetry::{
    global, sdk::trace as sdktrace,
    trace::{TraceContextExt, Tracer},
    Context, Key, KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tonic::metadata::{MetadataMap, MetadataValue};
</code></pre>
<p><strong>Step 2: Initialize the tracer and create env file</strong></p>
<p>Add this function in main.rs file, <code>init_tracer</code> is initializing an OpenTelemetry tracer with the OpenTelemetry OTLP exporter which is sending data to your Self-Hosted SigNoz.</p>
<pre><code>fn init_tracer() -&gt; Result&lt;sdktrace::Tracer, TraceError&gt; {
    opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(opentelemetry_otlp::new_exporter().tonic().with_env())
        .with_trace_config(
            sdktrace::config().with_resource(Resource::default()),
        )
        .install_batch(opentelemetry::runtime::Tokio)
}
</code></pre>
<p><strong>Step 3: Add the OpenTelemetry instrumentation for your Rust app</strong></p>
<p>You need call init_tracer function inside <code>main()</code> at starting so that as soon as your rust application starts, tracer will be available globally.</p>
<pre><code>let _ = init_tracer();
</code></pre>
<p>also change</p>
<pre><code>fn main(){
    //rest of the code
}
</code></pre>
<p>to</p>
<pre><code>#[tokio::main]
async fn main() {
    //rest of the code
}
</code></pre>
<p>Now comes the most interesting part, Sending data to SigNoz to get sense of your traces. After adding the below block you can send traces to Self-Hosted SigNoz</p>
<pre><code>  let tracer = global::tracer(&quot;global_tracer&quot;);
    let _cx = Context::new();
  
    tracer.in_span(&quot;operation&quot;, |cx| {
        let span = cx.span();
        span.set_attribute(Key::new(&quot;KEY&quot;).string(&quot;value&quot;));

        span.add_event(
            format!(&quot;Operations&quot;),
            vec![\
                Key::new(&quot;SigNoz is&quot;).string(&quot;Awesome&quot;),\
            ],
        );
    });
    shutdown_tracer_provider()
</code></pre>
<p><strong>Step 4: Set environment variables and run app</strong></p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>OTEL_EXPORTER_OTLP_ENDPOINT</td>
<td>This is the url where you have hosted SigNoz</td>
</tr>
<tr>
<td>OTEL_RESOURCE_ATTRIBUTES=service.name</td>
<td>Specify as the value. This will be the name of your Rust application on SigNoz services page, allowing you to uniquely identify the application traces.</td>
</tr>
</tbody>
</table>
<p>Now run</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 OTEL_RESOURCE_ATTRIBUTES=service.name=sample-rust-app cargo run
</code></pre>
<p>in terminal to start the application!</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#sample-rust-application
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-sample-rust-application
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Sample Rust Application</h2>
<p>We have included a sample Rust application at <a href="https://github.com/SigNoz/sample-rust-app">Sample Rust App Github Repo</a>
,</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#tutorial
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-tutorial
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Tutorial</h2>
<p>Here's a <a href="https://signoz.io/blog/opentelemetry-rust/">tutorial</a> with step by step guide on how to install SigNoz and start monitoring a sample Rust app.</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/rust/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, rust
image_urls: 
tracking_id: docs-instrumentation-rust-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-rust
<h2>Rust Opentelemetry Instrumentation: Send traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation</h2>
<p>This documentation contains instructions on how to set up OpenTelemetry(OTel) instrumentation in your Swift application. OpenTelemetry, also known as OTel for short, is an open-source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Swift application.</p>
<p>Once the telemetry data is generated, you can configure an exporter to send the data to SigNoz for monitoring and visualization.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Swift application with OpenTelemetry</li>
<li>Configuring the exporter to send data to SigNoz</li>
<li>Validating the configuration to ensure that data is being sent as expected.</li>
</ul>
<p>In this tutorial, we will instrument a Swift application for traces and send it to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#requirements
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-requirements
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Requirements</h2>
<p><a href="https://www.swift.org/getting-started/#installing-swift">Swift</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#send-traces-to-signoz-cloud
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetes</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-swift
<p>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud: Send traces directly to SigNoz cloud: Send traces directly to SigNoz cloud</p>
<p>Here we will be sending traces to SigNoz cloud in 4 easy steps, if you want to send traces to self hosted SigNoz , you can refer to <a href="https://signoz.io/docs/instrumentation/swift/#send-traces-to-self-hosted-signoz">this</a>.</p>
<p>‚úÖ Info</p>
<p>If you are using the <a href="https://github.com/SigNoz/OpenTelemetry-swift-example/">sample swift application</a>
, then you just need to update the ingestion key and SigNoz endpoint after that you are good to go!</p>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure your Swift application to send traces to OpenTelemetry you need to install <a href="https://github.com/open-telemetry/opentelemetry-swift.git">opentelemetry-swift</a> and <a href="https://github.com/grpc/grpc-swift">grpc-swift</a> as dependency in your project.</p>
<p>For that, paste the following inside <code>Package.swift</code> of your project.</p>
<pre><code> dependencies: [\
        .package(url: &quot;https://github.com/open-telemetry/opentelemetry-swift.git&quot;, .upToNextMajor(from: &quot;1.9.1&quot;)),\
        .package(url: &quot;https://github.com/grpc/grpc-swift&quot;, from: &quot;1.15.0&quot;),\
    ],
    targets: [\
        .executableTarget(\
            name: &quot;&lt;NAME_OF_APP&gt;&quot;,\
            dependencies: [\
                .product(name: &quot;OpenTelemetryApi&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetrySdk&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;StdoutExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ResourceExtension&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ZipkinExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetryProtocolExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;SignPostIntegration&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;GRPC&quot;, package: &quot;grpc-swift&quot;)\
            ],\
            path: &quot;.&quot;\
        )\
    ]
</code></pre>
<p>Replace <code>NAME_OF_APP</code> with the name of app, you will see this name in SigNoz Services section.</p>
<p>You also need to add the following imports to use the methods and functions from the packages/dependencies which you just imported in <code>Package.swift</code> file.</p>
<pre><code>import Foundation
import GRPC
import NIO
import NIOSSL
import OpenTelemetryApi
import OpenTelemetryProtocolExporterCommon
import OpenTelemetryProtocolExporterGrpc
import OpenTelemetrySdk
import ResourceExtension
import SignPostIntegration
import StdoutExporter
import ZipkinExporter
</code></pre>
<p><strong>Step 2: Initialize the tracer</strong></p>
<p>To enable tracing and send traces to the SigNoz cloud, you need to initialize the tracer, to do that insert the following code snippet into your <code>main.swift</code> file</p>
<pre><code>var resources = DefaultResources().get()

let instrumentationScopeName = &quot;SwiftExample&quot;
let instrumentationScopeVersion = &quot;semver:0.1.0&quot;

let otlpConfiguration: OtlpConfiguration = OtlpConfiguration(timeout: TimeInterval(10), headers: [(&quot;signoz-access-token&quot;, &lt;SIGNOZ_INGESTION_KEY&gt;)])

let grpcChannel = ClientConnection.usingPlatformAppropriateTLS(for: MultiThreadedEventLoopGroup(numberOfThreads:1)).connect(host: &lt;SIGNOZ_INGESTION_URL&gt;, port: &lt;PORT&gt;)

let otlpTraceExporter = OtlpTraceExporter(channel: grpcChannel,
                                      config: otlpConfiguration)
let stdoutExporter = StdoutExporter()

let spanExporter = MultiSpanExporter(spanExporters: [otlpTraceExporter, stdoutExporter])

let spanProcessor = SimpleSpanProcessor(spanExporter: spanExporter)
OpenTelemetry.registerTracerProvider(tracerProvider:
    TracerProviderBuilder()
        .add(spanProcessor: spanProcessor)
        .build()
)
</code></pre>
<p>Ensure to replace <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code>, <code>&lt;PORT&gt;</code>, and <code>&lt;SIGNOZ_INGESTION_URL&gt;</code> with your actual SigNoz ingestion key, Port, and URL, respectively.</p>
<p>You can find your Ingestion Key in the following places:</p>
<ul>
<li>mail which you recieved from SigNoz after signing up.</li>
<li>inside settings on SigNoz dashboard</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443/v1/traces</td>
</tr>
</tbody>
</table>
<p>These are the variables you need to replace :</p>
<table>
<thead>
<tr>
<th>Placeholder</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&lt;SIGNOZ_INGESTION_KEY&gt;</code></td>
<td>Your SigNoz ingestion key</td>
</tr>
<tr>
<td><code>&lt;PORT&gt;</code></td>
<td>Port (default: 443)</td>
</tr>
<tr>
<td><code>&lt;SIGNOZ_INGESTION_URL&gt;</code></td>
<td>URL for SigNoz ingestion</td>
</tr>
</tbody>
</table>
<p><strong>Step 3: Send Telemetry data to SigNoz</strong></p>
<p>To send telemetry data to SigNoz, you can create a function to add spans and data. This is a sample function</p>
<pre><code>let sampleKey = &quot;sampleKey&quot;
let sampleValue = &quot;sampleValue&quot;

func doWork() {
    let childSpan = tracer.spanBuilder(spanName: &quot;doWork&quot;).setSpanKind(spanKind: .client).startSpan()
    childSpan.setAttribute(key: sampleKey, value: sampleValue)
    Thread.sleep(forTimeInterval: Double.random(in: 0 ..&lt; 10) / 100)
    childSpan.end()
}
</code></pre>
<p><strong>Step 4: Run app</strong></p>
<p>Execute your application by issuing the following command in terminal:</p>
<pre><code>swift run
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#send-traces-via-otel-collector-binary
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-swift
<p>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz cloud: Send traces via OTel Collector binary</p>
<p><strong>Step 1 : Install OTel Collector</strong> OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Swift application.</p>
<p><strong>Step 2 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure your Swift application to send traces to OpenTelemetry you need to install <a href="https://github.com/open-telemetry/opentelemetry-swift.git">opentelemetry-swift</a> and <a href="https://github.com/grpc/grpc-swift">grpc-swift</a> as dependency in your project.</p>
<p>For that, paste the following inside <code>Package.swift</code> of your project.</p>
<pre><code> dependencies: [\
        .package(url: &quot;https://github.com/open-telemetry/opentelemetry-swift.git&quot;, .upToNextMajor(from: &quot;1.9.1&quot;)),\
        .package(url: &quot;https://github.com/grpc/grpc-swift&quot;, from: &quot;1.15.0&quot;),\
    ],
    targets: [\
        .executableTarget(\
            name: &quot;&lt;NAME_OF_APP&gt;&quot;,\
            dependencies: [\
                .product(name: &quot;OpenTelemetryApi&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetrySdk&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;StdoutExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ResourceExtension&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ZipkinExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetryProtocolExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;SignPostIntegration&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;GRPC&quot;, package: &quot;grpc-swift&quot;)\
            ],\
            path: &quot;.&quot;\
        )\
    ]
</code></pre>
<p>Replace <code>NAME_OF_APP</code> with the name of app, you will see this name in SigNoz Services section.</p>
<p>You also need to add the following imports to use the methods and functions from the packages/dependencies which you just imported in <code>Package.swift</code> file.</p>
<pre><code>import Foundation
import GRPC
import NIO
import NIOSSL
import OpenTelemetryApi
import OpenTelemetryProtocolExporterCommon
import OpenTelemetryProtocolExporterGrpc
import OpenTelemetrySdk
import ResourceExtension
import SignPostIntegration
import StdoutExporter
import ZipkinExporter
</code></pre>
<p><strong>Step 3: Initialize the tracer</strong></p>
<p>Add these statements to initialize tracer in the <code>main.swift</code> file inside the main function or you can create another function for initializing the tracer and call it in some other block of code.</p>
<pre><code>var resources = DefaultResources().get()

let instrumentationScopeName = &quot;SwiftExample&quot;
let instrumentationScopeVersion = &quot;semver:0.1.0&quot;

let otlpConfiguration: OtlpConfiguration = OtlpConfiguration(timeout: TimeInterval(10), headers: [(&quot;signoz-access-token&quot;, &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;)])

let grpcChannel = ClientConnection.usingPlatformAppropriateTLS(for: MultiThreadedEventLoopGroup(numberOfThreads:1)).connect(host: &quot;http://localhost&quot;, port: 4317)

let otlpTraceExporter = OtlpTraceExporter(channel: grpcChannel,
                                      config: otlpConfiguration)
let stdoutExporter = StdoutExporter()
let spanExporter = MultiSpanExporter(spanExporters: [otlpTraceExporter, stdoutExporter])

let spanProcessor = SimpleSpanProcessor(spanExporter: spanExporter)
OpenTelemetry.registerTracerProvider(tracerProvider:
    TracerProviderBuilder()
        .add(spanProcessor: spanProcessor)
        .build()
)

let tracer = OpenTelemetry.instance.tracerProvider.get(instrumentationName: instrumentationScopeName, instrumentationVersion: instrumentationScopeVersion)
</code></pre>
<p>Ensure to replace <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> with your actual SigNoz ingestion key.</p>
<p><strong>Step 4: Send Telemetry data to SigNoz</strong></p>
<p>To send telemetry data to SigNoz, you can create a function to add spans and data. This is a sample function</p>
<pre><code>let sampleKey = &quot;sampleKey&quot;
let sampleValue = &quot;sampleValue&quot;

func doWork() {
    let childSpan = tracer.spanBuilder(spanName: &quot;doWork&quot;).setSpanKind(spanKind: .client).startSpan()
    childSpan.setAttribute(key: sampleKey, value: sampleValue)
    Thread.sleep(forTimeInterval: Double.random(in: 0 ..&lt; 10) / 100)
    childSpan.end()
}
</code></pre>
<p><strong>Step 5: Run app</strong></p>
<p>Run your app using the below command to send your traces:</p>
<pre><code>swift run
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz</h2>
<p><strong>Step 1 : Instrument your application with OpenTelemetry</strong></p>
<p>To configure your Swift application to send traces to OpenTelemetry you need to install <a href="https://github.com/open-telemetry/opentelemetry-swift.git">opentelemetry-swift</a> and <a href="https://github.com/grpc/grpc-swift">grpc-swift</a> as dependency in your project.</p>
<p>For that, paste the following inside <code>Package.swift</code> of your project.</p>
<pre><code> dependencies: [\
        .package(url: &quot;https://github.com/open-telemetry/opentelemetry-swift.git&quot;, .upToNextMajor(from: &quot;1.9.1&quot;)),\
        .package(url: &quot;https://github.com/grpc/grpc-swift&quot;, from: &quot;1.15.0&quot;),\
    ],
    targets: [\
        .executableTarget(\
            name: &quot;&lt;NAME_OF_APP&gt;&quot;,\
            dependencies: [\
                .product(name: &quot;OpenTelemetryApi&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetrySdk&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;StdoutExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ResourceExtension&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;ZipkinExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;OpenTelemetryProtocolExporter&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;SignPostIntegration&quot;, package: &quot;opentelemetry-swift&quot;),\
                .product(name: &quot;GRPC&quot;, package: &quot;grpc-swift&quot;)\
            ],\
            path: &quot;.&quot;\
        )\
    ]
</code></pre>
<p>Replace <code>NAME_OF_APP</code> with the name of app, you will see this name in SigNoz Services section.</p>
<p>You also need to add the following imports to use the methods and functions from the packages/dependencies which you just imported in <code>Package.swift</code> file.</p>
<pre><code>import Foundation
import GRPC
import NIO
import NIOSSL
import OpenTelemetryApi
import OpenTelemetryProtocolExporterCommon
import OpenTelemetryProtocolExporterGrpc
import OpenTelemetrySdk
import ResourceExtension
import SignPostIntegration
import StdoutExporter
import ZipkinExporter
</code></pre>
<p><strong>Step 2: Initialize the tracer</strong></p>
<p>To enable tracing and send traces to the SigNoz cloud, you need to initialize the tracer, to do that insert the following code snippet into your <code>main.swift</code> file</p>
<pre><code>var resources = DefaultResources().get()

let instrumentationScopeName = &quot;SwiftExample&quot;
let instrumentationScopeVersion = &quot;semver:0.1.0&quot;

let otlpConfiguration: OtlpConfiguration = OtlpConfiguration(timeout: TimeInterval(10))

let grpcChannel = ClientConnection.usingPlatformAppropriateTLS(for: MultiThreadedEventLoopGroup(numberOfThreads:1)).connect(host: &lt;SELF_HOSTED_SIGNOZ_URL&gt;, port: &lt;PORT&gt;)

let otlpTraceExporter = OtlpTraceExporter(channel: grpcChannel,
                                      config: otlpConfiguration)
let stdoutExporter = StdoutExporter()

let spanExporter = MultiSpanExporter(spanExporters: [otlpTraceExporter, stdoutExporter])

let spanProcessor = SimpleSpanProcessor(spanExporter: spanExporter)
OpenTelemetry.registerTracerProvider(tracerProvider:
    TracerProviderBuilder()
        .add(spanProcessor: spanProcessor)
        .build()
)
</code></pre>
<p>| Placeholder | Description | |----------------------------|---------------------------------------------| | | <code>&lt;PORT&gt;</code> | Port exposed by Self-Hosted | | <code>&lt;SELF_HOSTED_SIGNOZ_URL&gt;</code> | URL for Self-Hosted SigNoz |</p>
<p><strong>Step 3: Send Telemetry data to SigNoz</strong></p>
<p>To send telemetry data to SigNoz, you can create a function to add spans and data. This is a sample function</p>
<pre><code>let sampleKey = &quot;sampleKey&quot;
let sampleValue = &quot;sampleValue&quot;

func doWork() {
    let childSpan = tracer.spanBuilder(spanName: &quot;doWork&quot;).setSpanKind(spanKind: .client).startSpan()
    childSpan.setAttribute(key: sampleKey, value: sampleValue)
    Thread.sleep(forTimeInterval: Double.random(in: 0 ..&lt; 10) / 100)
    childSpan.end()
}
</code></pre>
<p><strong>Step 4: Run app</strong></p>
<p>Execute your application by issuing the following command in terminal:</p>
<pre><code>swift run
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#sample-swift-application
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-sample-swift-application
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: Sample Swift Application</h2>
<p>We have included a sample Swift application at <a href="https://github.com/SigNoz/OpenTelemetry-swift-example/">Sample swift App Github Repo</a>
,</p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/swift/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, swift
image_urls: 
tracking_id: docs-instrumentation-swift-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-swift
<h2>Swift Opentelemetry Instrumentation: Send traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/
tag_set: userguide, retention-period
image_urls: https://signoz.io/img/docs/retention_settings.webp
tracking_id: docs-userguide-retention-period
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period</h2>
<p>By default, retention period is set to 7 days for logs and traces, and 30 days for metrics. To set retention period for metrics, traces and logs, you can navigate to the <code>General</code> tab on the <code>Settings</code> page.</p>
<p><img src="https://signoz.io/img/docs/retention_settings.webp" alt="Set Retention period of metrics traces" /></p>
<ul>
<li>You can select independent retention period for metrics, traces and logs.</li>
<li>You can also set the duration after which the data will be moved to cold storage (S3/GCS) for traces, metrics and logs. This can be only set if Cold Storage (S3/GCS) is enabled from the backend.</li>
<li>Click <code>Save</code> to update the new retention periods.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/#recommendations-for-setting-retention-period
tag_set: userguide, retention-period
image_urls: 
tracking_id: docs-userguide-retention-period-recommendations-for-setting-retention-period
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period: Recommendations for setting retention period</h2>
<p>Updating retention period can be very long running operation with large data. So, here are some recommendations:</p>
<ol>
<li>It is recommended to set retention period early in the lifecycle of the platform.</li>
<li>It is recommended to update retention period when there is less traffic on the platform or increase resources for the clickhouse. This is because updating retention period when there's lot of data will require a lot of resources and can cause performance issues.</li>
<li>It is not recommended to change retention period frequently.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/#configuring-cold-storage---amazon-s3
tag_set: userguide, retention-period
image_urls: 
tracking_id: docs-userguide-retention-period-configuring-cold-storage--amazon-s3
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period: Configuring Cold Storage - Amazon S3</h2>
<h3>## Docker and Docker Swarm</h3>
<p>In case of docker, update the <code>storage_configuration</code> section from <code>clickhouse-storage.xml</code> to configure the endpoint, access key and secret. If you have configured the AWS credentials in the ClickHouse environment, set <code>use_environment_credentials</code> to <code>true</code> and you can remove <code>access_key_id</code> and <code>secret_access_key</code> sections.</p>
<p>If region is <code>us-east-1</code>, then the endpoint will be <code>https://&lt;bucket-name&gt;.s3.amazonaws.com/data/</code>. For other regions, the endpoint will be <code>https://&lt;bucket-name&gt;.s3-&lt;region-name&gt;.amazonaws.com/data/</code>.</p>
<pre><code>            10485760
        
        
            s3
            https://BUCKET-NAME-HERE.s3-REGION-NAME-HERE.amazonaws.com/data/
            ACCESS-KEY-ID-HERE
            SECRET-ACCESS-KEY-HERE
            true
        
    
    
        
            
                
                    default
                
                
                    s3
</code></pre>
<h3>## Kubernetes</h3>
<p>In case of helm charts, update the <code>clickhouse.coldStorage</code> in <code>values.yaml</code>.</p>
<pre><code>clickhouse:
  coldStorage:
    enabled: true
    # Set free space size on default disk in bytes
    defaultKeepFreeSpaceBytes: &quot;10485760&quot; # 10MiB
    type: s3
    endpoint: https://&lt;bucket-name&gt;.s3.amazonaws.com/data/
    accessKey: &lt;access_key_id&gt;
    secretAccess: &lt;secret_access_key&gt;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/#configuring-cold-storage---google-cloud-storage-gcs
tag_set: userguide, retention-period
image_urls: 
tracking_id: docs-userguide-retention-period-configuring-cold-storage--google-cloud-storage-gcs
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period: Configuring Cold Storage - Google Cloud Storage (GCS)</h2>
<h3>## Docker and Docker Swarm</h3>
<p>In case of docker, update the <code>storage_configuration</code> section from <code>clickhouse-storage.xml</code> to configure the endpoint, access key and secret.</p>
<p>For GCS, <code>support_batch_delete</code> must be set to <code>false</code> as GCS doesn't support batch delete and results in error messages in the logs.</p>
<p>The type of the disk is <code>s3</code> because S3-compatible API of GCS is used.</p>
<p>The endpoint will be <code>https://storage.googleapis.com/&lt;bucket-name&gt;/data/</code>.</p>
<pre><code>            10485760
        
        
            s3
            https://storage.googleapis.com/BUCKET-NAME-HERE/data/
            ACCESS-KEY-ID-HERE
            SECRET-ACCESS-KEY-HERE
            false
        
    
    
        
            
                
                    default
                
                
                    s3
</code></pre>
<h3>## Kubernetes</h3>
<p>In case of helm charts, update the <code>clickhouse.coldStorage</code> in <code>values.yaml</code>.</p>
<pre><code>clickhouse:
  coldStorage:
    enabled: true
    # Set free space size on default disk in bytes
    defaultKeepFreeSpaceBytes: &quot;10485760&quot; # 10MiB
    type: gcs
    endpoint: https://storage.googleapis.com/&lt;bucket-name&gt;/data/
    accessKey: &lt;access_key_id&gt;
    secretAccess: &lt;secret_access_key&gt;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/#troubleshooting
tag_set: userguide, retention-period
image_urls: https://signoz.io/img/blog/common/join_slack_cta.webp
tracking_id: docs-userguide-retention-period-troubleshooting
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period: Troubleshooting</h2>
<ol>
<li>SigNoz UI is loading slower than usual after updating retention period.</li>
</ol>
<p>This is because the retention period update is a long running operation when there's lot of data, it might require a lot of resources and can cause performance issues. So, it is recommended to update retention period when there is less traffic on the platform or increase resources for the clickhouse.</p>
<hr />
<p>If you have any feedback or facing issues, feel free to join our slack community to get help!</p>
<p><a href="https://signoz.io/slack"><img src="https://signoz.io/img/blog/common/join_slack_cta.webp" alt="SigNoz Slack community" /></a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/
tag_set: userguide, retention-period
image_urls: https://signoz.io/img/docs/retention_settings.webp
tracking_id: docs-userguide-retention-period
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period</h2>
<p>By default, retention period is set to 7 days for logs and traces, and 30 days for metrics. To set retention period for metrics, traces and logs, you can navigate to the <code>General</code> tab on the <code>Settings</code> page.</p>
<p><img src="https://signoz.io/img/docs/retention_settings.webp" alt="Set Retention period of metrics traces" /></p>
<ul>
<li>You can select independent retention period for metrics, traces and logs.</li>
<li>You can also set the duration after which the data will be moved to cold storage (S3/GCS) for traces, metrics and logs. This can be only set if Cold Storage (S3/GCS) is enabled from the backend.</li>
<li>Click <code>Save</code> to update the new retention periods.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/#recommendations-for-setting-retention-period
tag_set: userguide, retention-period
image_urls: 
tracking_id: docs-userguide-retention-period-recommendations-for-setting-retention-period
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period: Recommendations for setting retention period</h2>
<p>Updating retention period can be very long running operation with large data. So, here are some recommendations:</p>
<ol>
<li>It is recommended to set retention period early in the lifecycle of the platform.</li>
<li>It is recommended to update retention period when there is less traffic on the platform or increase resources for the clickhouse. This is because updating retention period when there's lot of data will require a lot of resources and can cause performance issues.</li>
<li>It is not recommended to change retention period frequently.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/#configuring-cold-storage---amazon-s3
tag_set: userguide, retention-period
image_urls: 
tracking_id: docs-userguide-retention-period-configuring-cold-storage--amazon-s3
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period: Configuring Cold Storage - Amazon S3</h2>
<h3>## Docker and Docker Swarm</h3>
<p>In case of docker, update the <code>storage_configuration</code> section from <code>clickhouse-storage.xml</code> to configure the endpoint, access key and secret. If you have configured the AWS credentials in the ClickHouse environment, set <code>use_environment_credentials</code> to <code>true</code> and you can remove <code>access_key_id</code> and <code>secret_access_key</code> sections.</p>
<p>If region is <code>us-east-1</code>, then the endpoint will be <code>https://&lt;bucket-name&gt;.s3.amazonaws.com/data/</code>. For other regions, the endpoint will be <code>https://&lt;bucket-name&gt;.s3-&lt;region-name&gt;.amazonaws.com/data/</code>.</p>
<pre><code>            10485760
        
        
            s3
            https://BUCKET-NAME-HERE.s3-REGION-NAME-HERE.amazonaws.com/data/
            ACCESS-KEY-ID-HERE
            SECRET-ACCESS-KEY-HERE
            true
        
    
    
        
            
                
                    default
                
                
                    s3
</code></pre>
<h3>## Kubernetes</h3>
<p>In case of helm charts, update the <code>clickhouse.coldStorage</code> in <code>values.yaml</code>.</p>
<pre><code>clickhouse:
  coldStorage:
    enabled: true
    # Set free space size on default disk in bytes
    defaultKeepFreeSpaceBytes: &quot;10485760&quot; # 10MiB
    type: s3
    endpoint: https://&lt;bucket-name&gt;.s3.amazonaws.com/data/
    accessKey: &lt;access_key_id&gt;
    secretAccess: &lt;secret_access_key&gt;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/#configuring-cold-storage---google-cloud-storage-gcs
tag_set: userguide, retention-period
image_urls: 
tracking_id: docs-userguide-retention-period-configuring-cold-storage--google-cloud-storage-gcs
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period: Configuring Cold Storage - Google Cloud Storage (GCS)</h2>
<h3>## Docker and Docker Swarm</h3>
<p>In case of docker, update the <code>storage_configuration</code> section from <code>clickhouse-storage.xml</code> to configure the endpoint, access key and secret.</p>
<p>For GCS, <code>support_batch_delete</code> must be set to <code>false</code> as GCS doesn't support batch delete and results in error messages in the logs.</p>
<p>The type of the disk is <code>s3</code> because S3-compatible API of GCS is used.</p>
<p>The endpoint will be <code>https://storage.googleapis.com/&lt;bucket-name&gt;/data/</code>.</p>
<pre><code>            10485760
        
        
            s3
            https://storage.googleapis.com/BUCKET-NAME-HERE/data/
            ACCESS-KEY-ID-HERE
            SECRET-ACCESS-KEY-HERE
            false
        
    
    
        
            
                
                    default
                
                
                    s3
</code></pre>
<h3>## Kubernetes</h3>
<p>In case of helm charts, update the <code>clickhouse.coldStorage</code> in <code>values.yaml</code>.</p>
<pre><code>clickhouse:
  coldStorage:
    enabled: true
    # Set free space size on default disk in bytes
    defaultKeepFreeSpaceBytes: &quot;10485760&quot; # 10MiB
    type: gcs
    endpoint: https://storage.googleapis.com/&lt;bucket-name&gt;/data/
    accessKey: &lt;access_key_id&gt;
    secretAccess: &lt;secret_access_key&gt;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/retention-period/#troubleshooting
tag_set: userguide, retention-period
image_urls: https://signoz.io/img/blog/common/join_slack_cta.webp
tracking_id: docs-userguide-retention-period-troubleshooting
group_tracking_ids: docs-userguide-retention-period
<h2>Retention Period: Troubleshooting</h2>
<ol>
<li>SigNoz UI is loading slower than usual after updating retention period.</li>
</ol>
<p>This is because the retention period update is a long running operation when there's lot of data, it might require a lot of resources and can cause performance issues. So, it is recommended to update retention period when there is less traffic on the platform or increase resources for the clickhouse.</p>
<hr />
<p>If you have any feedback or facing issues, feel free to join our slack community to get help!</p>
<p><a href="https://signoz.io/slack"><img src="https://signoz.io/img/blog/common/join_slack_cta.webp" alt="SigNoz Slack community" /></a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#overview
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-overview
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Redis Metrics and Logs - Overview</h2>
<p>This integration helps you to monitor key Redis metrics and logs, view them with an out-of-the-box dashboard, and parse Redis logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#prerequisites
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-prerequisites
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>A Redis server running version 3.0 or newer</li>
<li>An OpenTelemetry (OTEL) Collector with access to the Redis server
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0 or newer) if not done already</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
<li>Ensure the OTEL collector has access to the Redis server for metrics collection and can read the Redis log file for logs collection</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#collecting-redis-metrics
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-collecting-redis-metrics
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Collecting Redis Metrics</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>redis-metrics-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  redis:
    # The hostname and port of the Redis instance, separated by a colon.
    endpoint: ${env:REDIS_ENDPOINT}
    # The frequency at which to collect metrics from the Redis instance.
    collection_interval: 60s
    # # The password used to access the Redis instance; must match the password specified in the requirepass server configuration option.
    password: ${env:REDIS_PASSWORD}
    # # Defines the network to use for connecting to the server. Valid Values are `tcp` or `Unix`
    # transport: tcp
    # tls:
    #   insecure: false
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/redis.crt
    #   key_file: /etc/ssl/certs/redis.key
    metrics:
      redis.maxmemory:
        enabled: true
      redis.cmd.latency:
        enabled: true

processors:
  # enriches the data with additional host information
  # see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#resource-detection-processor
  resourcedetection/system:
    # add additional detectors if needed
    detectors: [&quot;system&quot;]
    system:
      hostname_sources: [&quot;os&quot;]

exporters:
  # export to SigNoz cloud
  otlp/redis:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/redis:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    metrics/redis:
      receivers: [redis]
      # note: remove this processor if the collector host is not running on the same host as the redis instance
      processors: [resourcedetection/system]
      exporters: [otlp/redis]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># redis endpoint reachable from the otel collector&quot;
export REDIS_ENDPOINT=&quot;localhost:6379&quot;

# password used to access the Redis instance.
# must match the password specified in the requirepass server configuration option.
# can be left empty if the redis server is not configured to require a password.
export REDIS_PASSWORD=&quot;&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config redis-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#collecting-redis-logs
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-collecting-redis-logs
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Collecting Redis Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>redis-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/redis:
    include: [&quot;${env:REDIS_LOG_FILE}&quot;]
    operators:
      # Parse default redis log format
      # pid:role timestamp log_level message
      - type: regex_parser
        if: body matches '^(?P&lt;pid&gt;\\d+):(?P&lt;role&gt;\\w+) (?P&lt;ts&gt;\\d{2} \\w+ \\d{4} \\d{2}:\\d{2}:\\d{2}\\.\\d+) (?P&lt;log_level&gt;[.\\-*#]) (?P&lt;message&gt;.*)$'
        parse_from: body
        regex: '^(?P&lt;pid&gt;\d+):(?P&lt;role&gt;\w+) (?P&lt;ts&gt;\d{2} \w+ \d{4} \d{2}:\d{2}:\d{2}\.\d+) (?P&lt;log_level&gt;[.\-*#]) (?P&lt;message&gt;.*)$'
        timestamp:
          parse_from: attributes.ts
          layout: '02 Jan 2006 15:04:05.000'
          layout_type: gotime
        severity:
          parse_from: attributes.log_level
          overwrite_text: true
          mapping:
            debug: '.'
            info:
              - '-'
              - '*'
            warn: '#'
        on_error: send
      - type: move
        if: attributes.message  nil
        from: attributes.message
        to: body
      - type: remove
        if: attributes.log_level  nil
        field: attributes.log_level
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: add
        field: attributes.source
        value: redis

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/redis-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/redis-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true


service:
  pipelines:
    logs/redis:
      receivers: [filelog/redis]
      processors: [batch]
      exporters: [otlp/redis-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># path of Redis server log file. must be accessible by the otel collector
# typically found in /usr/local/var/log/redis on macOS
# log file location can also be found in the output of `redis-cli CONFIG GET : *`
export REDIS_LOG_FILE=/var/log/redis/redis-server.log

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config redis-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags in the collector run command.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#connect-redis
tag_set: integrations, redis
image_urls: https://signoz.io/img/docs/integrations/redis/redis-integration-search.webp, https://signoz.io/img/docs/integrations/redis/redis-integration-connect.webp, https://signoz.io/img/docs/integrations/redis/redis-integration-listening.webp
tracking_id: docs-integrations-redis-connect-redis
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Connect Redis</h2>
<p>Once you're done with setting up Redis for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the Redis integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-search.webp" alt="Search for Redis in Integrations tab" /></p>
<p><em>Search for Redis in Integrations tab</em></p>
<p>Click on the <code>Connect Redis</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your Redis instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-connect.webp" alt="Connect Redis" /></p>
<p><em>Connect Redis</em></p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-listening.webp" alt="Listening for data from Redis" /></p>
<p><em>Listening for data from Redis</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#redis-dashboard
tag_set: integrations, redis
image_urls: https://signoz.io/img/docs/integrations/redis/redis-integration-dashboard.webp
tracking_id: docs-integrations-redis-redis-dashboard
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Redis dashboard</h2>
<p>Once SigNoz has started listening to your Redis data, head over to the Dashboards tab and search for Redis, this will show you a newly created dashboard which shows differnet Redis metrics.</p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-dashboard.webp" alt="Dashboard for monitoring Redis Metrics" /></p>
<p><em>Dashboard for monitoring Redis Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Redis Dashboard by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/redis/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#data-collected
tag_set: integrations, redis
image_urls: https://signoz.io/img/docs/integrations/redis/redis-integration-data-collected.webp
tracking_id: docs-integrations-redis-data-collected
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Data Collected: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your Redis Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your Redis instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-data-collected.webp" alt="Log attributes and metrics details for Redis" /></p>
<p><em>Log attributes and metrics details for Redis</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#redis-log-attributes
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-redis-log-attributes
group_tracking_ids: docs-integrations-redis
<p>Redis Metrics and Logs: Data Collected: Redis log attributes</p>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Process ID</td>
<td>attributes.pid</td>
<td>string</td>
</tr>
<tr>
<td>Process Role</td>
<td>attributes.role</td>
<td>string</td>
</tr>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#redis-metrics
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-redis-metrics
group_tracking_ids: docs-integrations-redis
<p>Redis Metrics and Logs: Data Collected: Redis metrics: Redis metrics: Redis metrics</p>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Unit</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>redis_commands_processed</td>
<td>Sum</td>
<td>number</td>
<td>Total number of commands processed by the server</td>
</tr>
<tr>
<td>redis_cpu_time</td>
<td>Sum</td>
<td>seconds</td>
<td>System CPU consumed by the Redis server in seconds since server start</td>
</tr>
<tr>
<td>redis_keys_expired</td>
<td>Sum</td>
<td>number</td>
<td>Total number of key expiration events</td>
</tr>
<tr>
<td>redis_db_expires</td>
<td>Gauge</td>
<td>number</td>
<td>Number of keyspace keys with an expiration</td>
</tr>
<tr>
<td>redis_commands</td>
<td>Gauge</td>
<td>ops/s</td>
<td>Number of commands processed per second</td>
</tr>
<tr>
<td>redis_replication_offset</td>
<td>Gauge</td>
<td>Bytes</td>
<td>The server's current replication offset</td>
</tr>
<tr>
<td>redis_net_input</td>
<td>Sum</td>
<td>Bytes</td>
<td>The total number of bytes read from the network</td>
</tr>
<tr>
<td>redis_clients_connected</td>
<td>Sum</td>
<td>number</td>
<td>Number of client connections (excluding connections from replicas)</td>
</tr>
<tr>
<td>redis_keys_evicted</td>
<td>Sum</td>
<td>number</td>
<td>Number of evicted keys due to maxmemory limit</td>
</tr>
<tr>
<td>redis_maxmemory</td>
<td>Gauge</td>
<td>Bytes</td>
<td>The value of the maxmemory configuration directive</td>
</tr>
<tr>
<td>redis_clients_max_input_buffer</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Biggest input buffer among current client connections</td>
</tr>
<tr>
<td>redis_cmd_latency</td>
<td>Gauge</td>
<td>seconds</td>
<td>Command execution latency</td>
</tr>
<tr>
<td>redis_memory_lua</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Number of bytes used by the Lua engine</td>
</tr>
<tr>
<td>redis_replication_backlog_first_byte_offset</td>
<td>Gauge</td>
<td>Bytes</td>
<td>The master offset of the replication backlog buffer</td>
</tr>
<tr>
<td>redis_keyspace_hits</td>
<td>Sum</td>
<td>number</td>
<td>Number of successful lookup of keys in the main dictionary</td>
</tr>
<tr>
<td>redis_clients_blocked</td>
<td>Sum</td>
<td>number</td>
<td>Number of clients pending on a blocking call</td>
</tr>
<tr>
<td>redis_connections_rejected</td>
<td>Sum</td>
<td>number</td>
<td>Number of connections rejected because of maxclients limit</td>
</tr>
<tr>
<td>redis_latest_fork</td>
<td>Gauge</td>
<td>microseconds</td>
<td>Duration of the latest fork operation in microseconds</td>
</tr>
<tr>
<td>redis_clients_max_output_buffer</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Longest output list among current client connections</td>
</tr>
<tr>
<td>redis_slaves_connected</td>
<td>Sum</td>
<td>number</td>
<td>Number of connected replicas</td>
</tr>
<tr>
<td>redis_db_keys</td>
<td>Gauge</td>
<td>number</td>
<td>Number of keyspace keys</td>
</tr>
<tr>
<td>redis_keyspace_misses</td>
<td>Sum</td>
<td>number</td>
<td>Number of failed lookup of keys in the main dictionary</td>
</tr>
<tr>
<td>redis_uptime</td>
<td>Sum</td>
<td>seconds</td>
<td>Number of seconds since Redis server start</td>
</tr>
<tr>
<td>redis_memory_used</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Total number of bytes allocated by Redis using its allocator</td>
</tr>
<tr>
<td>redis_net_output</td>
<td>Sum</td>
<td>Bytes</td>
<td>The total number of bytes written to the network</td>
</tr>
<tr>
<td>redis_connections_received</td>
<td>Sum</td>
<td>number</td>
<td>Total number of connections accepted by the server</td>
</tr>
<tr>
<td>redis_rdb_changes_since_last_save</td>
<td>Sum</td>
<td>number</td>
<td>Number of changes since the last dump</td>
</tr>
<tr>
<td>redis_memory_rss</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Number of bytes that Redis allocated as seen by the operating system</td>
</tr>
<tr>
<td>redis_db_avg_ttl</td>
<td>Gauge</td>
<td>milliseconds</td>
<td>Average keyspace keys TTL</td>
</tr>
<tr>
<td>redis_memory_peak</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Peak memory consumed by Redis (in bytes)</td>
</tr>
<tr>
<td>redis_memory_fragmentation_ratio</td>
<td>Gauge</td>
<td>number</td>
<td>Ratio between used_memory_rss and used_memory</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker-swarm/
tag_set: install, docker-swarm
image_urls: 
tracking_id: docs-install-docker-swarm
group_tracking_ids: docs-install-docker-swarm
<h2>Docker Swarm</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>This section provides information on installing SigNoz on Docker Swarm.</p>
<h2>Prerequisites</h2>
<hr />
<ul>
<li>
<p>A Linux or macOS machine. Microsoft Windows is not officially supported.</p>
</li>
<li>
<p><a href="https://docs.docker.com/get-docker/">Docker Engine</a>
. A minimum of 4GB of memory must be allocated to each Docker node.</p>
</li>
<li>
<p><a href="https://docs.docker.com/compose/install/">Docker Compose</a></p>
</li>
<li>
<p><a href="https://desktop.github.com/">Git client</a></p>
</li>
</ul>
<h2>Install SigNoz on Docker Swarm</h2>
<hr />
<ol>
<li>
<p>In a directory of your choosing, clone the SigNoz repository and 'cd' into the <code>signoz/deploy</code> directory by entering the following commands:</p>
<pre><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/
</code></pre>
</li>
<li>
<p>Initialize a single-node swarm by entering the following command:</p>
<pre><code>docker swarm init
</code></pre>
<p>The output should look similar as shown below:</p>
<pre><code>Swarm initialized: current node (6muco3j7jjuo6k4rbiq8yr8fw) is now a manager.

To add a worker to this swarm, run the following command:

docker swarm join --token SWMTKN-1-6ak6diq1lbrwemx17up9c1ph039h64z0dxksjxv647qnqrd290-4tt6q22dd462p4lf2n6bqbnt4 192.168.65.3:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
</code></pre>
</li>
<li>
<p><em>(Optional)</em> You can use the <code>docker swarm join</code> command to add more nodes to the swarm. Note that the node you added in the previous step is the manager. For details, see the <a href="https://docs.docker.com/engine/reference/commandline/swarm_join/">Docker Swarm Join</a> page of the Docker documentation.</p>
</li>
<li>
<p>Deploy SigNoz by entering the <code>docker stack deploy command</code> and specifying the following:</p>
<ul>
<li>
<p><code>-c</code> and the path to the Compose file (<code>docker-swarm/clickhouse-setup/docker-compose.yaml</code>)</p>
</li>
<li>
<p>The name of the stack (<code>signoz</code>)</p>
<p>docker stack deploy -c docker-swarm/clickhouse-setup/docker-compose.yaml signoz</p>
</li>
</ul>
<p>The output should look similar to the following:</p>
<pre><code>Creating network signoz_default
Creating service signoz_query-service
Creating service signoz_frontend
Creating service signoz_otel-collector
Creating service signoz_hotrod
Creating service signoz_load-hotrod
Creating service signoz_clickhouse
Creating service signoz_alertmanager
</code></pre>
</li>
<li>
<p><em>(Optional)</em> By default, the instructions in this document create three replicas, and each replica can handle 50K spans per second. To handle an increased load, perform the steps in the <a href="/docs/operate/docker-swarm/#scale-up">Scale Up</a> section of the <a href="/docs/operate/docker-swarm/">Operate on Docker Swarm</a>
page.</p>
</li>
</ol>
<h2>Verify the Installation</h2>
<hr />
<ol>
<li>
<p>Using the <code>docker stack services</code> command, monitor the SigNoz deployment process. Wait until all SigNoz services and replicas are created:</p>
<pre><code>docker stack services signoz
</code></pre>
<p>You should see the following output:</p>
<pre><code>ID             NAME                            MODE         REPLICAS   IMAGE                                          PORTS
6b67m0nuzf40   signoz_alertmanager             replicated   1/1        signoz/alertmanager:0.23.0-0.1
zgateenyifwv   signoz_clickhouse               replicated   1/1        yandex/clickhouse-server:21.12.3.32
vzc1gdx86f0w   signoz_frontend                 replicated   1/1        signoz/frontend:0.8.0                          *:3301-&gt;3301/tcp
dgisjp0vhv8m   signoz_hotrod                   replicated   1/1        jaegertracing/example-hotrod:1.30
336omtkvwukm   signoz_load-hotrod              replicated   1/1        grubykarol/locust:1.2.3-python3.9-alpine3.12
av5iggw983b5   signoz_otel-collector           replicated   3/3        signoz/otelcontribcol:0.43.0-0.1               *:4317-4318-&gt;4317-4318/tcp
hw28zb1hozu5   signoz_query-service            replicated   1/1        signoz/query-service:0.8.0                     *:8080-&gt;8080/tcp
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/docker-swarm/">Docker Swarm Operate</a> section for detailed instructions.</p>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>
<h2>Next Steps</h2>
<hr />
<ul>
<li>
<p><a href="/docs/instrumentation/overview/">Instrument Your Application</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#overview
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-overview
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: PostgreSQL Metrics and Logs - Overview</h2>
<p>This integration helps you to monitor key Postgres metrics and logs, view them with an out-of-the-box dashboard, and parse Postgres logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#prerequisites
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-prerequisites
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>
<p>A Postgres server running version 9.6 or newer</p>
<ul>
<li>You can check the server version with the SQL statement: <code>SELECT version();</code></li>
</ul>
</li>
<li>
<p>A Postgres user with required permissions for metrics collection</p>
<ul>
<li>
<p>To create a monitoring user for Postgres versions 10+, run:</p>
<pre><code>CREATE USER monitoring WITH PASSWORD '&lt;PASSWORD&gt;';
GRANT pg_monitor TO monitoring;
GRANT SELECT ON pg_stat_database TO monitoring;
</code></pre>
</li>
<li>
<p>To create a monitoring user for Postgres versions &gt;= 9.6 and &lt; 10, run:</p>
<pre><code>CREATE USER monitoring WITH PASSWORD '&lt;PASSWORD&gt;';
GRANT SELECT ON pg_stat_database TO monitoring;
</code></pre>
</li>
</ul>
</li>
<li>
<p>An OpenTelemetry (OTEL) Collector with access to the Postgres server</p>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0 or newer) if not done already</li>
<li>Ensure you can provide config files to the collector and set environment variables</li>
<li>The collector must be able to access the Postgres server as a client using the monitoring user</li>
<li>For log collection, the collector must be able to read the Postgres server log file</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#collecting-postgres-metrics
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-collecting-postgres-metrics
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Collecting Postgres Metrics</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>postgres-metrics-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  postgresql:
    # The endpoint of the postgresql server. Whether using TCP or Unix sockets, this value should be host:port. If transport is set to unix, the endpoint will internally be translated from host:port to /host.s.PGSQL.port
    endpoint: ${env:POSTGRESQL_ENDPOINT}
    # The frequency at which to collect metrics from the Postgres instance.
    collection_interval: 60s
    # The username used to access the postgres instance
    username: ${env:POSTGRESQL_USERNAME}
    # The password used to access the postgres instance
    password: ${env:POSTGRESQL_PASSWORD}
    # The list of databases for which the receiver will attempt to collect statistics. If an empty list is provided, the receiver will attempt to collect statistics for all non-template databases
    databases: []
    # # Defines the network to use for connecting to the server. Valid Values are `tcp` or `unix`
    # transport: tcp
    tls:
      # set to false if SSL is enabled on the server
      insecure: true
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/postgres.crt
    #   key_file: /etc/ssl/certs/postgres.key
    metrics:
      postgresql.database.locks:
        enabled: true
      postgresql.deadlocks:
        enabled: true
      postgresql.sequential_scans:
        enabled: true

processors:
  # enriches the data with additional host information
  # see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#resource-detection-processor
  resourcedetection/system:
    # add additional detectors if needed
    detectors: [&quot;system&quot;]
    system:
      hostname_sources: [&quot;os&quot;]

exporters:
  # export to SigNoz cloud
  otlp/postgres:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/postgres:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    metrics/postgresql:
      receivers: [postgresql]
      # note: remove this processor if the collector host is not running on the same host as the postgres instance
      processors: [resourcedetection/system]
      exporters: [otlp/postgres]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># password for Postgres monitoring user&quot;
export POSTGRESQL_USERNAME=&quot;monitoring&quot;

# password for Postgres monitoring user&quot;
export POSTGRESQL_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;

# Postgres endpoint reachable from the otel collector&quot;
export POSTGRESQL_ENDPOINT=&quot;host:port&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config postgres-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#collecting-postgres-logs
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-collecting-postgres-logs
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Collecting Postgres Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>postgres-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/postgresql:
    include: [&quot;${env:POSTGRESQL_LOG_FILE}&quot;]
    operators:
      # Parse default postgresql text log format.
      # `log_line_prefix` postgres setting defaults to '%m [%p] ' which logs the timestamp and the process ID
      # See https://www.postgresql.org/docs/current/runtime-config-logging.html#GUC-LOG-LINE-PREFIX for more details
      - type: regex_parser
        if: body matches '^(?P&lt;ts&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.?[0-9]*? [A-Z]*) \\[(?P&lt;pid&gt;[0-9]+)\\] (?P&lt;log_level&gt;[A-Z]*). (?P&lt;message&gt;.*)$'
        parse_from: body
        regex: '^(?P&lt;ts&gt;\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.?[0-9]*? [A-Z]*) \[(?P&lt;pid&gt;[0-9]+)\] (?P&lt;log_level&gt;[A-Z]*). (?P&lt;message&gt;.*)$'
        timestamp:
          parse_from: attributes.ts
          layout: '%Y-%m-%d %H:%M:%S %Z'
        severity:
          parse_from: attributes.log_level
          mapping:
            debug:
              - DEBUG1
              - DEBUG2
              - DEBUG3
              - DEBUG4
              - DEBUG5
            info:
              - INFO
              - LOG
              - NOTICE
              - DETAIL
            warn: WARNING
            error: ERROR
            fatal:
              - FATAL
              - PANIC
        on_error: send
      - type: move
        if: attributes.message  nil
        from: attributes.message
        to: body
      - type: remove
        if: attributes.log_level  nil
        field: attributes.log_level
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: add
        field: attributes.source
        value: postgres

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/postgres-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/postgres-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    logs/postgresql:
      receivers: [filelog/postgresql]
      processors: [batch]
      exporters: [otlp/postgresql-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># path of Postgres server log file. must be accessible by the otel collector
# typically found in /usr/local/var/log/postgresql on macOS
# running `SELECT pg_current_logfile();` can also give you the location of postgresql log file
export POSTGRESQL_LOG_FILE=/var/log/postgresql/postgresql.log

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config postgres-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags in the collector run command.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#connect-postgressql
tag_set: integrations, postgresql
image_urls: https://signoz.io/img/docs/integrations/postgres/postgres-integration-search.webp, https://signoz.io/img/docs/integrations/postgres/postgres-integration-connect.webp, https://signoz.io/img/docs/integrations/postgres/postgres-integration-listening.webp
tracking_id: docs-integrations-postgresql-connect-postgressql
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Connect PostgresSQL</h2>
<p>Once you're done with setting up PostgresSQL for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the PostgresSQL integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-search.webp" alt="Search for PostgresSQL in Integrations tab" /></p>
<p><em>Search for PostgresSQL in Integrations tab</em></p>
<p>Click on the <code>Connect PostgresSQL</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your PostgresSQL instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-connect.webp" alt="Connect PostgresSQL" /></p>
<p><em>Connect PostgresSQL</em></p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-listening.webp" alt="Listening for data from PostgresSQL" /></p>
<p><em>Listening for data from PostgresSQL</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#postgressql-dashboard
tag_set: integrations, postgresql
image_urls: https://signoz.io/img/docs/integrations/postgres/postgres-integration-dashboard.webp
tracking_id: docs-integrations-postgresql-postgressql-dashboard
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: PostgresSQL dashboard</h2>
<p>Once SigNoz has started listening to your PostgresSQL data, head over to the Dashboards tab and search for Postgres, this will show you a newly created dashboard which shows differnet PostgresSQL metrics.</p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-dashboard.webp" alt="Dashboard for monitoring PostgresSQL Metrics" /></p>
<p><em>Dashboard for monitoring PostgresSQL Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Redis Dashboard by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/postgres/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#data-collected
tag_set: integrations, postgresql
image_urls: https://signoz.io/img/docs/integrations/postgres/postgres-integration-data-collected.webp
tracking_id: docs-integrations-postgresql-data-collected
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Data Collected: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your PostgresSQL Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your PostgresSQL instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-data-collected.webp" alt="Log attributes and metrics details for PostgresSQL" /></p>
<p><em>Log attributes and metrics details for PostgresSQL</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#postgrsql-log-attributes
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-postgrsql-log-attributes
group_tracking_ids: docs-integrations-postgresql
<p>PostgreSQL Metrics and Logs: Data Collected: PostgrSQL log attributes</p>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Process ID</td>
<td>attributes.pid</td>
<td>string</td>
</tr>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#postgresql-metrics
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-postgresql-metrics
group_tracking_ids: docs-integrations-postgresql
<p>PostgreSQL Metrics and Logs: Data Collected: PostgreSQL metrics</p>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Unit</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>postgresql_backends</td>
<td>sum</td>
<td>number</td>
<td>The number of backends.</td>
</tr>
<tr>
<td>postgresql_bgwriter_buffers_allocated</td>
<td>sum</td>
<td>number</td>
<td>Number of buffers allocated.</td>
</tr>
<tr>
<td>postgresql_bgwriter_buffers_writes</td>
<td>sum</td>
<td>number</td>
<td>Number of buffers written.</td>
</tr>
<tr>
<td>postgresql_bgwriter_checkpoint_count</td>
<td>sum</td>
<td>number</td>
<td>The number of checkpoints performed.</td>
</tr>
<tr>
<td>postgresql_bgwriter_duration</td>
<td>sum</td>
<td>milliseconds</td>
<td>Total time spent writing and syncing files to disk by checkpoints.</td>
</tr>
<tr>
<td>postgresql_bgwriter_maxwritten</td>
<td>sum</td>
<td>number</td>
<td>Number of times the background writer stopped a cleaning scan because it had written too many buffers.</td>
</tr>
<tr>
<td>postgresql_blocks_read</td>
<td>sum</td>
<td>number</td>
<td>The number of blocks read.</td>
</tr>
<tr>
<td>postgresql_commits</td>
<td>sum</td>
<td>number</td>
<td>The number of commits.</td>
</tr>
<tr>
<td>postgresql_connection_max</td>
<td>gauge</td>
<td>number</td>
<td>Configured maximum number of client connections allowed</td>
</tr>
<tr>
<td>postgresql_database_count</td>
<td>sum</td>
<td>number</td>
<td>Number of user databases.</td>
</tr>
<tr>
<td>postgresql_database_locks</td>
<td>gauge</td>
<td>number</td>
<td>The number of database locks.</td>
</tr>
<tr>
<td>postgresql_db_size</td>
<td>sum</td>
<td>Bytes</td>
<td>The database disk usage.</td>
</tr>
<tr>
<td>postgresql_deadlocks</td>
<td>sum</td>
<td>number</td>
<td>The number of deadlocks.</td>
</tr>
<tr>
<td>postgresql_index_scans</td>
<td>sum</td>
<td>number</td>
<td>The number of index scans on a table.</td>
</tr>
<tr>
<td>postgresql_index_size</td>
<td>gauge</td>
<td>Bytes</td>
<td>The size of the index on disk.</td>
</tr>
<tr>
<td>postgresql_operations</td>
<td>sum</td>
<td>number</td>
<td>The number of db row operations.</td>
</tr>
<tr>
<td>postgresql_replication_data_delay</td>
<td>gauge</td>
<td>Bytes</td>
<td>The amount of data delayed in replication.</td>
</tr>
<tr>
<td>postgresql_rollbacks</td>
<td>sum</td>
<td>number</td>
<td>The number of rollbacks.</td>
</tr>
<tr>
<td>postgresql_rows</td>
<td>sum</td>
<td>number</td>
<td>The number of rows in the database.</td>
</tr>
<tr>
<td>postgresql_sequential_scans</td>
<td>sum</td>
<td>number</td>
<td>The number of sequential scans.</td>
</tr>
<tr>
<td>postgresql_table_count</td>
<td>sum</td>
<td>number</td>
<td>Number of user tables in a database.</td>
</tr>
<tr>
<td>postgresql_table_size</td>
<td>sum</td>
<td>Bytes</td>
<td>Disk space used by a table.</td>
</tr>
<tr>
<td>postgresql_table_vacuum_count</td>
<td>sum</td>
<td>number</td>
<td>Number of times a table has manually been vacuumed.</td>
</tr>
<tr>
<td>postgresql_temp_files</td>
<td>sum</td>
<td>number</td>
<td>The number of temp files.</td>
</tr>
<tr>
<td>postgresql_wal_age</td>
<td>gauge</td>
<td>seconds</td>
<td>Age of the oldest WAL file.</td>
</tr>
<tr>
<td>postgresql_wal_delay</td>
<td>gauge</td>
<td>seconds</td>
<td>Time between flushing recent WAL locally and receiving notification that the standby server has completed an operation with it.</td>
</tr>
<tr>
<td>postgresql_wal_lag</td>
<td>gauge</td>
<td>seconds</td>
<td>Time between flushing recent WAL locally and receiving notification that the standby server has completed an operation with it.</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#overview
tag_set: integrations, nginx
image_urls: 
tracking_id: docs-integrations-nginx-overview
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Nignx Logs - Overview</h2>
<p>This integration helps you to monitor Nginx server logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#prerequisites
tag_set: integrations, nginx
image_urls: 
tracking_id: docs-integrations-nginx-prerequisites
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>
<p>An Nginx server is running version newer than 1.0.0</p>
</li>
<li>
<p>An OpenTelemetry (OTEL) Collector with access to the Nginx server</p>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0 or newer) if not done already</li>
<li>Ensure you can provide config files to the collector and set environment variables</li>
<li>The collector must be able to read the Nginx server log files</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#collecting-nginx-logs
tag_set: integrations, nginx
image_urls: 
tracking_id: docs-integrations-nginx-collecting-nginx-logs
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Collecting Nginx Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>nginx-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/nginx-access-logs:
    include: [&quot;${env:NGINX_ACCESS_LOG_FILE}&quot;]
    operators:
      # Parse the default nginx access log format. Nginx defaults to the &quot;combined&quot; log format
      # $remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;
      # For more details, see https://nginx.org/en/docs/http/ngx_http_log_module.html
      - type: regex_parser
        if: body matches '^(?P&lt;remote_addr&gt;[0-9\\.]+) - (?P&lt;remote_user&gt;[^\\s]+) \\[(?P&lt;ts&gt;.+)\\] &quot;(?P&lt;request_method&gt;\\w+?) (?P&lt;request_path&gt;.+?)&quot; (?P&lt;status&gt;[0-9]+) (?P&lt;body_bytes_sent&gt;[0-9]+) &quot;(?P&lt;http_referrer&gt;.+?)&quot; &quot;(?P&lt;http_user_agent&gt;.+?)&quot;$'
        parse_from: body
        parse_to: attributes
        regex: '^(?P&lt;remote_addr&gt;[0-9\.]+) - (?P&lt;remote_user&gt;[^\s]+) \[(?P&lt;ts&gt;.+)\] &quot;(?P&lt;request_method&gt;\w+?) (?P&lt;request_path&gt;.+?)&quot; (?P&lt;status&gt;[0-9]+) (?P&lt;body_bytes_sent&gt;[0-9]+) &quot;(?P&lt;http_referrer&gt;.+?)&quot; &quot;(?P&lt;http_user_agent&gt;.+?)&quot;$'
        timestamp:
          parse_from: attributes.ts
          layout: &quot;02/Jan/2006:15:04:05 -0700&quot;
          layout_type: gotime
        severity:
          parse_from: attributes.status
          overwrite_text: true
          mapping:
            debug: &quot;1xx&quot;
            info:
              - &quot;2xx&quot;
              - &quot;3xx&quot;
            warn: &quot;4xx&quot;
            error: &quot;5xx&quot;
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: add
        field: attributes.source
        value: nginx

  filelog/nginx-error-logs:
    include: [&quot;${env:NGINX_ERROR_LOG_FILE}&quot;]
    operators:
      # Parse the default nginx error log format.
      # YYYY/MM/DD HH:MM:SS [LEVEL] PID#TID: *CID MESSAGE
      # For more details, see https://github.com/phusion/nginx/blob/master/src/core/ngx_log.c
      - type: regex_parser
        if: body matches '^(?P&lt;ts&gt;.+?) \\[(?P&lt;log_level&gt;\\w+)\\] (?P&lt;pid&gt;\\d+)#(?P&lt;tid&gt;\\d+). \\*(?P&lt;cid&gt;\\d+) (?P&lt;message&gt;.+)$'
        parse_from: body
        parse_to: attributes
        regex: '^(?P&lt;ts&gt;.+?) \[(?P&lt;log_level&gt;\w+)\] (?P&lt;pid&gt;\d+)#(?P&lt;tid&gt;\d+). \*(?P&lt;cid&gt;\d+) (?P&lt;message&gt;.+)$'
        timestamp:
          parse_from: attributes.ts
          layout: &quot;2006/01/02 15:04:05&quot;
          layout_type: gotime
        severity:
          parse_from: attributes.log_level
          overwrite_text: true
          mapping:
            debug: &quot;debug&quot;
            info:
              - &quot;info&quot;
              - &quot;notice&quot;
            warn: &quot;warn&quot;
            error:
              - &quot;error&quot;
              - &quot;crit&quot;
              - &quot;alert&quot;
            fatal: &quot;emerg&quot;
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: move
        if: attributes.message  nil
        from: attributes.message
        to: body
      - type: add
        field: attributes.source
        value: nginx

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/nginx-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/nginx-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    logs/nginx:
      receivers: [filelog/nginx-access-logs, filelog/nginx-error-logs]
      processors: [batch]
      exporters: [otlp/nginx-logs]
</code></pre>
<p><strong>Note:</strong> If you are using a <a href="https://docs.nginx.com/nginx/admin-guide/monitoring/logging/#setting-up-the-access-log">custom Nginx log format</a>
, adjust the regex used for parsing logs in the receivers named <code>filelog/nginx-access-logs</code> and <code>filelog/nginx-error-logs</code> in the collector config.</p>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># path of Nginx access log file. must be accessible by the otel collector
# typically found at /usr/local/var/log/nginx/access.log on macOS
export NGINX_ACCESS_LOG_FILE=/var/log/nginx/access.log

# path of Nginx error log file. must be accessible by the otel collector
# typically found at /usr/local/var/log/nginx/error.log on macOS
export NGINX_ERROR_LOG_FILE=/var/log/nginx/error.log

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config nginx-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags in the collector run command.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#connect-nginx
tag_set: integrations, nginx
image_urls: https://signoz.io/img/docs/integrations/nginx/nginx-integration-search.webp, https://signoz.io/img/docs/integrations/nginx/nginx-integration-connect.webp, https://signoz.io/img/docs/integrations/nginx/nginx-integration-listening.webp
tracking_id: docs-integrations-nginx-connect-nginx
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Connect Nginx</h2>
<p>Once you're done with setting up Nginx for collecting logs, head over to the intergrations tab in SigNoz and search for the Nginx integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/nginx/nginx-integration-search.webp" alt="Search for Nginx in Integrations tab" /></p>
<p><em>Search for Nginx in Integrations tab</em></p>
<p>Click on the <code>Connect Nginx</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your Nginx server. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/nginx/nginx-integration-connect.webp" alt="Connect Nginx" /></p>
<p><em>Connect Nginx</em></p>
<p><img src="https://signoz.io/img/docs/integrations/nginx/nginx-integration-listening.webp" alt="Listening for data from Nginx" /></p>
<p><em>Listening for data from Nginx</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#data-collected
tag_set: integrations, nginx
image_urls: https://signoz.io/img/docs/integrations/nginx/nginx-integration-data-collected.webp
tracking_id: docs-integrations-nginx-data-collected
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your Nginx Integrations, it shows you details about the different log attributes that you can monitor for Nginx. The tables below gives you a list of the different log attributes available.</p>
<p><img src="https://signoz.io/img/docs/integrations/nginx/nginx-integration-data-collected.webp" alt="Log attributes for Nginx" /></p>
<p><em>Log attributes details for Nginx</em></p>
<h3>## Nginx log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
<tr>
<td>Body Bytes Sent</td>
<td>attributes.body_bytes_sent</td>
<td>string</td>
</tr>
<tr>
<td>Referrer</td>
<td>attributes.http_referrer</td>
<td>string</td>
</tr>
<tr>
<td>User Agent</td>
<td>attributes.http_user_agent</td>
<td>string</td>
</tr>
<tr>
<td>Request Method</td>
<td>attributes.request_method</td>
<td>string</td>
</tr>
<tr>
<td>Request Path</td>
<td>attributes.request_path</td>
<td>string</td>
</tr>
<tr>
<td>Response Status Code</td>
<td>attributes.status</td>
<td>string</td>
</tr>
<tr>
<td>Remote Address</td>
<td>attributes.remote_addr</td>
<td>string</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-dashboards-and-panels/
tag_set: userguide, manage-dashboards-and-panels
image_urls: 
tracking_id: docs-userguide-manage-dashboards-and-panels
group_tracking_ids: docs-userguide-manage-dashboards-and-panels
<h2>Manage Dashboards and Panels</h2>
<p>A dashboard is a set of one or more panels. A panel is the basic visualization element that can either display a metric over a time interval or only the most recent value. On the Dashboards page, you can create custom dashboards, each combining multiple panels, which you can use to get insight into how well your application is performing.</p>
<p>In this section:</p>
<ul>
<li>
<p><a href="/docs/userguide/manage-dashboards/">Manage dashboards</a></p>
</li>
<li>
<p><a href="/docs/userguide/manage-panels/">Manage panels</a></p>
</li>
<li>
<p><a href="/docs/dashboards/panel-types/">Panel Types</a></p>
</li>
<li>
<p><a href="/docs/userguide/manage-variables/">Manage variables</a></p>
</li>
<li>
<p><a href="/docs/userguide/create-a-custom-query/">Create a custom query</a></p>
</li>
</ul>
<p>Docs</p>
<p><a href="/docs/">Introduction</a>
<a href="/docs/contributing/">Contributing</a></p>
<p><a href="https://knowledgebase.signoz.io/kb">Knowledge Base</a></p>
<p><a href="/api_reference/">SigNoz API</a></p>
<p>Community</p>
<p><a href="/support/">Support</a></p>
<p><a href="https://signoz.io/slack">Slack</a></p>
<p><a href="https://twitter.com/SigNozHQ">Twitter</a></p>
<p><a href="https://community-chat.signoz.io/">Community Archive</a></p>
<p><a href="/changelog/">Changelog</a></p>
<p>More</p>
<p><a href="/product-comparison/signoz-vs-datadog/">SigNoz vs Datadog</a>
<a href="/product-comparison/signoz-vs-newrelic/">SigNoz vs New Relic</a>
<a href="/product-comparison/signoz-vs-grafana/">SigNoz vs Grafana</a>
<a href="/product-comparison/signoz-vs-dynatrace/">SigNoz vs Dynatrace</a></p>
<p><a href="https://jobs.gem.com/signoz">Careers</a></p>
<p><a href="/about-us/">About</a>
<a href="/terms-of-service/">Terms</a>
<a href="/privacy/">Privacy</a>
<a href="https://trust.signoz.io/">Security &amp; Compliance</a></p>
<p><img src="https://signoz.io/img/SigNozLogo-orange.svg" alt="" /></p>
<p>SigNoz</p>
<p>All systems operational</p>
<p><a href="https://github.com/SigNoz"></a>
<a href="https://www.linkedin.com/company/signozio/"></a>
<a href="https://signoz.io/slack"></a>
<a href="https://twitter.com/SigNozHQ"></a>
<a href="https://www.youtube.com/@signoz"></a></p>
<p><img src="https://signoz.io/svgs/icons/SOC-2.svg" alt="" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.49/
tag_set: operate, migration, upgrade-0.49
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.49
group_tracking_ids: docs-operate-migration-upgrade-0.49
<h2>Upgrade to v0.49 from earlier versions</h2>
<p>In SigNoz version &gt;= v0.36, support for <code>.</code> in log attribute names was added. Before that, any <code>.</code> in log attribute names would get replaced by an <code>_</code><br />
For example, the attribute <code>k8s.pod.name</code> would appear as <code>k8s_pod_name</code> in SigNoz</p>
<p>SigNoz versions &lt; v0.49 have a measure in place to ensure log pipelines that were created with filters based on attributes whose <code>.</code>s had been replaced with <code>_</code>s worked fine with raw logs that had such attributes with <code>.</code>s in them instead of <code>_</code>s.<br />
So a pipeline with the filter <code>k8s_pod_name = test-pod</code> would work even though actual raw logs being sent to SigNoz only had the attribute <code>k8s.pod.name = test-pod</code></p>
<p>In SigNoz version &gt;=v0.49 i.e. SigNoz chart version &gt;=0.45.0, this measure is being removed.<br />
After this upgrade, any such pipelines that use filters based on attributes whose <code>.</code>s had been replaced with <code>_</code>s will only work if the filters are updated to use <code>.</code> based names.<br />
So after this upgrade, any pipeline that had the filter <code>k8s_pod_name = test-pod</code> will only work if the filter is updated to <code>k8s.pod.name = test-pod</code></p>
<h2>Steps to Update Pipeline Filters If Needed</h2>
<hr />
<ol>
<li>Navigate to the logs section in SigNoz by clicking on &quot;Logs&quot; in the sidebar</li>
<li>Navigate to log pipelines page by clicking on the &quot;Pipelines&quot; tab in the top bar.</li>
<li>Examine the filter expression of all of your pipelines.</li>
<li>If you have pipelines with filters that use attribute names containing <code>_</code>
<ol>
<li>Enter edit mode by clicking the &quot;Enter Edit Mode&quot; button at the top right above the pipelines list.</li>
<li>For each pipeline that has a filter that uses attribute names containing <code>_</code>
<ol>
<li>Open the edit pipeline dialog by clicking the edit action (pen icon in actions column)</li>
<li>If the &quot;Filtered Logs Preview&quot; section at the bottom of the dialog doesn't show any matched logs
<ol>
<li>For each clause that uses an attribute with <code>_</code> in its name, try updating it to its <code>.</code> based name using the autocomplete, until the &quot;Filtered Logs Preview&quot; is no longer empty
<ol>
<li>For example, replace <code>k8s_pod_name = test-pod</code> by <code>k8s.pod.name = test-pod</code></li>
</ol>
</li>
<li>Click the &quot;Update&quot; button in the Edit Pipeline dialog to close it.</li>
</ol>
</li>
<li>If the pipeline filter did not need editing, close the dialog by clicking &quot;Cancel&quot; button at the bottom of the dialog.</li>
</ol>
</li>
<li>If you updated the filter for any of the pipelines, click the &quot;Save Configuration&quot; at the bottom right of the page</li>
</ol>
</li>
</ol>
<h2>Upgrade to v0.49</h2>
<hr />
<p>Now, you can proceed with the upgrade to <code>&gt;=v0.49</code> by following the appropriate platform specific instructions</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/operate/docker-standalone/#upgrade">Docker Standalone</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/docker-swarm/#upgrade">Docker Swarm</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/kubernetes/#upgrade">Kubernetes</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#overview
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-overview
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: MongoDB Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key MongoDB metrics and logs, view them with an out-of-the-box dashboard, and parse MongoDB logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#prerequisites
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-prerequisites
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>A MongoDB server running version 4.4 or newer</li>
<li>An OpenTelemetry (OTEL) Collector with access to the MongoDB server
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0 or newer) if not done already</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
<li>Ensure the OTEL collector has access to the MongoDB server for metrics collection and can read the MongoDB log file for logs collection</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#collecting-mongodb-metrics
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-collecting-mongodb-metrics
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Collecting MongoDB Metrics</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>mongodb-metrics-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  mongodb:
    # - For standalone MongoDB deployments this is the hostname and port of the mongod instance
    # - For replica sets specify the hostnames and ports of the mongod instances that are in the replica set configuration. If the replica_set field is specified, nodes will be autodiscovered.
    # - For a sharded MongoDB deployment, please specify a list of the mongos hosts.
    hosts:
      - endpoint: ${env:MONGODB_ENDPOINT}
    # If authentication is required, the user can with clusterMonitor permissions can be provided here
    username: ${env:MONGODB_USERNAME}
    # If authentication is required, the password can be provided here.
    password: ${env:MONGODB_PASSWORD}
    collection_interval: 60s
    # If TLS is enabled, the following fields can be used to configure the connection
    tls:
      insecure: true
      insecure_skip_verify: true
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/mongodb.crt
    #   key_file: /etc/ssl/certs/mongodb.key
    metrics:
      mongodb.lock.acquire.count:
        enabled: true
      mongodb.lock.acquire.time:
        enabled: true
      mongodb.lock.acquire.wait_count:
        enabled: true
      mongodb.lock.deadlock.count:
        enabled: true
      mongodb.operation.latency.time:
        enabled: true

processors:
  # enriches the data with additional host information
  # see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#resource-detection-processor
  resourcedetection/system:
    # add additional detectors if needed
    detectors: [&quot;system&quot;]
    system:
      hostname_sources: [&quot;os&quot;]

exporters:
  # export to SigNoz cloud
  otlp/mongodb:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/mongodb:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    metrics/mongodb:
      receivers: [mongodb]
      # note: remove this processor if the collector host is not running on the same host as the mongo instance
      processors: [resourcedetection/system]
      exporters: [otlp/mongodb]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># MongoDB endpoint reachable from the otel collector
export MONGODB_ENDPOINT=&quot;host:port&quot;

# MongoDB monitoring user credentials
export MONGODB_USERNAME=&quot;monitoring&quot;
export MONGODB_PASSWORD=&quot;your_secure_password&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.us.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config mongodb-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#collecting-mongodb-logs
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-collecting-mongodb-logs
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Collecting MongoDB Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>mongodb-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/mongodb:
    include: [&quot;${env:MONGODB_LOG_FILE}&quot;]
    operators:
      # Parse structured mongodb logs
      # For more details, see https://www.mongodb.com/docs/manual/reference/log-messages/#structured-logging
      - type: json_parser
        if: body matches '^\\s*{\\s*&quot;.*}\\s*$'
        parse_from: body
        parse_to: attributes
        timestamp:
          parse_from: attributes.t.$$date
          layout: '2006-01-02T15:04:05.000-07:00'
          layout_type: gotime
        severity:
          parse_from: attributes.s
          overwrite_text: true
          mapping:
            debug:
              - D1
              - D2
              - D3
              - D4
              - D5
            info: I
            warn: W
            error: E
            fatal: F
      - type: flatten
        if: attributes.attr  nil
        field: attributes.attr
      - type: move
        if: attributes.msg  nil
        from: attributes.msg
        to: body
      - type: move
        if: attributes.c  nil
        from: attributes.c
        to: attributes.component
      - type: move
        if: attributes.id  nil
        from: attributes.id
        to: attributes.mongo_log_id
      - type: remove
        if: attributes.t  nil
        field: attributes.t
      - type: remove
        if: attributes.s  nil
        field: attributes.s
      - type: add
        field: attributes.source
        value: mongodb

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/mongodb-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/mongodb-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true


service:
  pipelines:
    logs/mongodb:
      receivers: [filelog/mongodb]
      processors: [batch]
      exporters: [otlp/mongodb-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># path of MongoDB server log file. must be accessible by the otel collector
export MONGODB_LOG_FILE=&quot;/var/log/mongodb/mongodb.log&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.us.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config mongodb-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple --config flags in the collector run command.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#connect-mongodb
tag_set: integrations, mongodb
image_urls: https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-search.webp, https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-connect.webp, https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-listening.webp
tracking_id: docs-integrations-mongodb-connect-mongodb
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Connect MongoDB</h2>
<p>Once you're done with setting up MongoDB for collecting metrics and logs, head over to the integrations tab in SigNoz and search for the MongoDB integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-search.webp" alt="Search for MongoDB in Integrations tab" /></p>
<p><em>Search for MongoDB in Integrations tab</em></p>
<p>Click on the <code>Connect Mongo</code> Button, and select I have already configured, this will start listening for data from your MongoDB instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-connect.webp" alt="Connect MongoDB" /></p>
<p><em>Connect MongoDB</em></p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-listening.webp" alt="Listening for data from MongoDB" /></p>
<p><em>Listening for data from MongoDB</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#mongodb-dashboard
tag_set: integrations, mongodb
image_urls: https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-dashboard.webp
tracking_id: docs-integrations-mongodb-mongodb-dashboard
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: MongoDB Dashboard</h2>
<p>Once SigNoz has started listening to your MongoDB data, head over to the Dashboards tab and search for Mongo. This will show you a newly created dashboard which displays various MongoDB metrics.</p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-dashboard.webp" alt="Dashboard for monitoring MongoDB Metrics" /></p>
<p><em>Dashboard for monitoring MongoDB Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Redis Dashboard by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/mongo/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#data-collected
tag_set: integrations, mongodb
image_urls: https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-dashboard.webp
tracking_id: docs-integrations-mongodb-data-collected
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Data Collected: Data Collected</h2>
<p>When you switch to the Data Collected tab of your MongoDB Integration, it shows you details about the different logs attributes and the metrics types that you can monitor for your MongoDB instance. The tables below give you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-dashboard.webp" alt="Dashboard for monitoring MongoDB Metrics" /></p>
<p><em>Dashboard for monitoring MongoDB Metrics</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#mongodb-log-attributes
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-mongodb-log-attributes
group_tracking_ids: docs-integrations-mongodb
<p>MongoDB Metrics and Logs: Data Collected: MongoDB log attributes</p>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
<tr>
<td>MongoDB Component</td>
<td>attributes.component</td>
<td>string</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#mongodb-metrics
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-mongodb-metrics
group_tracking_ids: docs-integrations-mongodb
<p>MongoDB Metrics and Logs: Data Collected: MongoDB metrics: MongoDB metrics: MongoDB metrics</p>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Unit</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>mongodb_cache_operations</td>
<td>Sum</td>
<td>number</td>
<td>The number of cache operations of the instance.</td>
</tr>
<tr>
<td>mongodb_collection_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of collections.</td>
</tr>
<tr>
<td>mongodb_data_size</td>
<td>Sum</td>
<td>Bytes</td>
<td>The size of the collection. Data compression does not affect this value.</td>
</tr>
<tr>
<td>mongodb_connection_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of connections.</td>
</tr>
<tr>
<td>mongodb_extent_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of extents.</td>
</tr>
<tr>
<td>mongodb_global_lock_time</td>
<td>Sum</td>
<td>milliseconds</td>
<td>The time the global lock has been held.</td>
</tr>
<tr>
<td>mongodb_index_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of indexes.</td>
</tr>
<tr>
<td>mongodb_index_size</td>
<td>Sum</td>
<td>Bytes</td>
<td>Sum of the space allocated to all indexes in the database, including free index space.</td>
</tr>
<tr>
<td>mongodb_memory_usage</td>
<td>Sum</td>
<td>Bytes</td>
<td>The amount of memory used.</td>
</tr>
<tr>
<td>mongodb_object_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of objects.</td>
</tr>
<tr>
<td>mongodb_operation_latency_time</td>
<td>Gauge</td>
<td>microseconds</td>
<td>The latency of operations.</td>
</tr>
<tr>
<td>mongodb_operation_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of operations executed.</td>
</tr>
<tr>
<td>mongodb_operation_repl_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of replicated operations executed.</td>
</tr>
<tr>
<td>mongodb_storage_size</td>
<td>Sum</td>
<td>Bytes</td>
<td>The total amount of storage allocated to this collection.</td>
</tr>
<tr>
<td>mongodb_database_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of existing databases.</td>
</tr>
<tr>
<td>mongodb_index_access_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of times an index has been accessed.</td>
</tr>
<tr>
<td>mongodb_document_operation_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of document operations executed.</td>
</tr>
<tr>
<td>mongodb_network_io_receive</td>
<td>Sum</td>
<td>Bytes</td>
<td>The number of bytes received.</td>
</tr>
<tr>
<td>mongodb_network_io_transmit</td>
<td>Sum</td>
<td>Bytes</td>
<td>The number of bytes transmitted.</td>
</tr>
<tr>
<td>mongodb_network_request_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of requests received by the server.</td>
</tr>
<tr>
<td>mongodb_operation_time</td>
<td>Sum</td>
<td>milliseconds</td>
<td>The total time spent performing operations.</td>
</tr>
<tr>
<td>mongodb_session_count</td>
<td>Sum</td>
<td>number</td>
<td>The total number of active sessions.</td>
</tr>
<tr>
<td>mongodb_cursor_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of open cursors maintained for clients.</td>
</tr>
<tr>
<td>mongodb_cursor_timeout_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of cursors that have timed out.</td>
</tr>
<tr>
<td>mongodb_lock_acquire_count</td>
<td>Sum</td>
<td>number</td>
<td>Number of times the lock was acquired in the specified mode.</td>
</tr>
<tr>
<td>mongodb_lock_acquire_wait_count</td>
<td>Sum</td>
<td>number</td>
<td>Number of times the lock acquisitions encountered waits because the locks were held in a conflicting mode.</td>
</tr>
<tr>
<td>mongodb_lock_acquire_time</td>
<td>Sum</td>
<td>microseconds</td>
<td>Cumulative wait time for the lock acquisitions.</td>
</tr>
<tr>
<td>mongodb_lock_deadlock_count</td>
<td>Sum</td>
<td>number</td>
<td>Number of times the lock acquisitions encountered deadlocks.</td>
</tr>
<tr>
<td>mongodb_health</td>
<td>Gauge</td>
<td>number</td>
<td>The health status of the server.</td>
</tr>
<tr>
<td>mongodb_uptime</td>
<td>Sum</td>
<td>milliseconds</td>
<td>The amount of time that the server has been running.</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#overview
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-overview
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: AWS Elasticache (redis) Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key AWS Elasticache (redis) metrics and logs, view them with an out-of-the-box dashboard, and parse MySQL logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#prerequisites
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-prerequisites
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>
<p>AWS Credentials and Permissions:</p>
<ul>
<li>Set up proper AWS credentials (e.g., <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables)</li>
<li>Required IAM permissions:
<ul>
<li><code>cloudwatch:ListMetrics</code></li>
<li><code>cloudwatch:GetMetricStatistics</code></li>
<li><code>cloudwatch:GetMetricData</code></li>
<li><code>tag:GetResources</code> (if aws_tag_select feature is used)</li>
<li><code>logs:DescribeLogGroups</code></li>
<li><code>logs:FilterLogEvents</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Java Runtime Environment (JRE) version 11 or newer for the CloudWatch Exporter (Not required if using the Docker container)</p>
</li>
<li>
<p>An OpenTelemetry (OTEL) Collector (v0.88.0+):</p>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0+) if not already done</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
</ul>
</li>
<li>
<p>To collect Redis native metrics, the collector must be able to access the Redis server as a client (optional).</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#step-1-set-up-the-prometheus-cloudwatch-exporter
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-step-1-set-up-the-prometheus-cloudwatch-exporter
group_tracking_ids: docs-integrations-aws-elasticache-redis
<p>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: ## Collecting Elasticache Metrics - Step 1: Set up the Prometheus CloudWatch Exporter</p>
<ol>
<li>
<p>Download the exporter:</p>
<pre><code>curl -sLSO https://github.com/prometheus/cloudwatch_exporter/releases/download/v0.15.5/cloudwatch_exporter-0.15.5-jar-with-dependencies.jar
</code></pre>
</li>
<li>
<p>Configure the Prometheus exporter Save the following config for collecting AWS RDS metrics in a file named <code>aws-elasticache-metrics.yaml</code> and update the region key with relevant value.</p>
<hr />
<p>region: us-east-1
metrics:</p>
<ul>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CPUUtilization
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: FreeableMemory
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: NetworkBytesIn
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: NetworkBytesOut
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: NetworkPacketsIn
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: NetworkPacketsOut
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: SwapUsage
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: BytesUsedForCache
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CacheHits
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CacheMisses
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CacheHitRate
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CurrConnections
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CurrItems
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CurrVolatileItems
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: ReplicationLag
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: ReplicationLag
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: SaveInProgress
aws_dimensions: [CacheClusterId, CacheNodeId]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: TrafficManagementActive
aws_dimensions: [CacheClusterId, CacheNodeId]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: DatabaseCapacityUsagePercentage
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: DatabaseMemoryUsagePercentage
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: EngineCPUUtilization
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: Evictions
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: GlobalDatastoreReplicationLag
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: MemoryFragmentationRatio
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: MemoryFragmentationRatio
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
</ul>
</li>
<li>
<p>Run the following command:</p>
<p>java -jar cloudwatch_exporter-0.15.5-jar-with-dependencies.jar 9106 aws-elasticache-metrics.yaml</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#step-2-create-the-otel-collector-config-file
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-step-2-create-the-otel-collector-config-file
group_tracking_ids: docs-integrations-aws-elasticache-redis
<p>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Step 2: Create the OTEL Collector Config File</p>
<p>Create <code>redis-metrics-collection-config.yaml</code>:</p>
<pre><code>receivers:
  redis:
    # The hostname and port of the Redis instance, separated by a colon.
    endpoint: ${env:REDIS_ENDPOINT}
    # The frequency at which to collect metrics from the Redis instance.
    collection_interval: 60s
    # The password used to access the Redis instance; must match the password specified in the requirepass server configuration option.
    password: ${env:REDIS_PASSWORD}
    # # Defines the network to use for connecting to the server. Valid Values are `tcp` or `Unix`
    # transport: tcp
    # tls:
    #   insecure: false
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/redis.crt
    #   key_file: /etc/ssl/certs/redis.key
    metrics:
      redis.maxmemory:
        enabled: true
      redis.cmd.latency:
        enabled: true

  prometheus:
    config:
      scrape_configs:
        - job_name: 'aws-cloudwatch-metrics'
          scrape_timeout: 120s
          scrape_interval: 120s
          static_configs:
            - targets: ['0.0.0.0:9106']

exporters:
  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true
  # export to SigNoz cloud
  otlp/signoz:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

service:
  pipelines:
    metrics/redis:
      receivers: [redis, prometheus]
      processors: []
      exporters: [otlp/signoz]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#step-3-set-environment-variables
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-step-3-set-environment-variables
group_tracking_ids: docs-integrations-aws-elasticache-redis
<p>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Step 3: Set Environment Variables</p>
<pre><code># The accessible endpoint where redis server is running.
# The hostname and port of the Redis instance, separated by a colon.
export REDIS_ENDPOINT=&quot;&lt;redis-server-endpoint&gt;&quot;

# The password to use for accessing redis instance
export REDIS_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#step-3-use-the-collector-config-file
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-step-3-use-the-collector-config-file
group_tracking_ids: docs-integrations-aws-elasticache-redis
<p>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Step 3: Use the Collector Config File</p>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config redis-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#collecting-elastcache-logs
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-collecting-elastcache-logs
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Collecting ElastCache Logs</h2>
<p>Use the <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Log_Delivery.html">log delivery</a> instructions to send redis logs to CloudWatch Logs</p>
<p>The log collection of RDS instance requires specifying the list of log group names. From the AWS CloudWatch console, please find the log group(s) relevant to the integration.</p>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create <code>redis-logs-collection-config.yaml</code>:</p>
<pre><code>receivers:
  awscloudwatch:
    region: us-east-1
    logs:
      poll_interval: 1m
      groups:
        named:
          # replace the following name with your log group for elasticache logs
          /aws/elasticache/:

processors:
  attributes/add_source:
    actions:
      - key: source
        value: &quot;elasticache_redis&quot;
        action: insert
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/redis-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true

service:
  pipelines:
    logs/redis:
      receivers: [awscloudwatch]
      processors: [attributes/add_source, batch]
      exporters: [otlp/redis-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<pre><code># region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Run the Collector</h3>
<p>Add to your collector run command:</p>
<pre><code>--config redis-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#connect-aws-elasticache-redis
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-connect-aws-elasticache-redis
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Connect AWS Elasticache (redis)</h2>
<p>Once you're done with setting up AWS Elasticache (redis) for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the AWS Elasticache (redis) integration.</p>
<p><img src="https://signoz.io/docs/integrations/aws-elasticache-redis/redis" alt="Search for AWS Elasticache (redis) in Integrations tab" /></p>
<p><em>Search for AWS Elasticache (redis) in Integrations tab</em></p>
<p>Click on the <code>Connect AWS Elasticache (redis)</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your AWS Elasticache (redis) instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/docs/integrations/aws-elasticache-redis/redis" alt="Connect AWS Elasticache (redis)" /></p>
<p><em>Connect AWS Elasticache (redis)</em></p>
<p><img src="https://signoz.io/docs/integrations/aws-elasticache-redis/redis" alt="Listening for data from RedAWS Elasticache (redis)is" /></p>
<p><em>Listening for data from AWS Elasticache (redis)</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#aws-elasticache-redis-dashboard
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-aws-elasticache-redis-dashboard
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: AWS Elasticache (redis) dashboard</h2>
<p>Once SigNoz has started listening to your AWS Elasticache (redis) data, head over to the Dashboards tab and search for redis, this will show you a newly created dashboard which shows differnet AWS Elasticache (redis) metrics.</p>
<p><img src="https://signoz.io/docs/integrations/aws-elasticache-redis/redis" alt="Dashboard for monitoring AWS Elasticache (redis) Metrics" /></p>
<p><em>Dashboards for monitoring AWS Elasticache (redis) Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Dashboards by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_elasticache/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#data-collected
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-data-collected
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your AWS Elasticache (redis) Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your AWS Elasticache (redis) instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/docs/integrations/aws-elasticache-redis/redis" alt="Log attributes and metrics details for AWS Elasticache (redis)" /></p>
<p><em>Log attributes and metrics details for AWS Elasticache (redis)</em></p>
<h3>## AWS Elasticache (redis) log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Body</td>
<td>body</td>
<td>string</td>
</tr>
</tbody>
</table>
<h3>## AWS Elasticache (redis) metrics</h3>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<p>To find a complete list of metrics you can checkout this <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_elasticache/integration.json#L57">link</a> or the <strong>Data Collected</strong> tab.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.45/
tag_set: operate, migration, upgrade-0.45
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.45
group_tracking_ids: docs-operate-migration-upgrade-0.45
<h2>Upgrade to v0.45 from earlier versions</h2>
<p>After upgrading to SigNoz version v0.45 i.e. SigNoz chart version v0.41.0, you need to run the migration script to sanitise the dashboards data.</p>
<p>This migration updates the panel height of the dashboard according to the new format.</p>
<h2>Steps to run migration script:</h2>
<hr />
<h2>First upgrade to v0.45</h2>
<hr />
<p>Follow the platform-specific instructions for upgrading to v0.45 and above.</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/operate/docker-standalone/#upgrade">Docker Standalone</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/docker-swarm/#upgrade">Docker Swarm</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/kubernetes/#upgrade">Kubernetes</a></p>
</li>
</ul>
<h3>## For Docker</h3>
<p>To change the directory in SigNoz repo and run following commands:</p>
<p>For <strong>Docker Standalone</strong>,</p>
<pre><code>cd deploy/docker/clickhouse-setup
</code></pre>
<p>For <strong>Docker Swarm</strong>,</p>
<pre><code>cd deploy/docker-swarm/clickhouse-setup
</code></pre>
<p>To run the migration script:</p>
<pre><code>docker run --name signoz-migrate -it \
  -v $PWD/data/signoz/:/var/lib/signoz/ signoz/migrate:0.45 \
  -dataSource=/var/lib/signoz/signoz.db
</code></pre>
<p>Output should be similar as below:</p>
<pre><code>Data Source path:  signoz.db
2024/05/01 15:28:22 Total Dashboard found: 2
2024/05/01 15:28:22 625fa391-d9d3-47c1-809a-1a147eea229d
2024/05/01 15:28:22 b05af383-23ec-4061-8f57-0765d45ccd51
2024/05/01 15:28:22 Dashboard 625fa391-d9d3-47c1-809a-1a147eea229d updated
2024/05/01 15:28:22 Dashboard b05af383-23ec-4061-8f57-0765d45ccd51 updated
2024/05/01 15:28:22 Dashboards migrated
</code></pre>
<p>In case of failure or when you have to run the script again, make sure to cleanup the container.</p>
<pre><code>docker stop signoz-migrate

docker rm signoz-migrate
</code></pre>
<h3>## For Kubernetes</h3>
<p>‚úÖ Info</p>
<p>Replace <code>my-release</code> with your Helm release name. And <code>platform</code> with your SigNoz namespace.</p>
<p>The steps for running the migration on kubernetes are :</p>
<ol>
<li>
<p>Update the latest chart information from the Helm repositories:</p>
<pre><code>helm repo update
</code></pre>
</li>
<li>
<p>Include the following in your custom <code>override-values.yaml</code> file:</p>
<pre><code>queryService:
  initContainers:
    migration:
      enabled: true
      image:
        registry: docker.io
        repository: signoz/migrate
        tag: 0.45
        pullPolicy: IfNotPresent
      args:
        - &quot;-dataSource=/var/lib/signoz/signoz.db&quot;
</code></pre>
</li>
<li>
<p>Run the following command to upgrade the chart:</p>
<pre><code>helm --namespace platform upgrade my-release signoz/signoz -f override-values.yaml
</code></pre>
</li>
<li>
<p>Check the logs of the migration container using:</p>
<pre><code>kubectl --namespace platform logs my-release-signoz-query-service-0 -c my-release-signoz-query-service-migration
</code></pre>
<p>Output should be similar as below:</p>
<pre><code>Data Source path:  signoz.db
2024/05/01 15:28:22 Total Dashboard found: 2
2024/05/01 15:28:22 625fa391-d9d3-47c1-809a-1a147eea229d
2024/05/01 15:28:22 b05af383-23ec-4061-8f57-0765d45ccd51
2024/05/01 15:28:22 Dashboard 625fa391-d9d3-47c1-809a-1a147eea229d updated
2024/05/01 15:28:22 Dashboard b05af383-23ec-4061-8f57-0765d45ccd51 updated
2024/05/01 15:28:22 Dashboards migrated
</code></pre>
</li>
<li>
<p>Remove the <code>migration</code> init container section added in <strong>Step 2</strong> followed by <code>helm upgrade</code>.</p>
<pre><code>helm --namespace platform upgrade my-release signoz/signoz -f override-values.yaml
</code></pre>
</li>
</ol>
<h2>In case of Upgrade Failure</h2>
<hr />
<p>Reach out to us at <a href="https://signoz.io/slack">Slack</a>.</p>
<h2>Command-Line Interface (CLI) Flags</h2>
<hr />
<p>There is a <code>-dataSource</code> flag to specify the path of SQLite database file. It defaults to <code>signoz.db</code>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#overview
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-overview
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Clickhouse Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key Clickhouse metrics and logs, view them with an out-of-the-box dashboard, and collect query logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#prerequisites
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-prerequisites
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>A Clickhouse server running version 23 or newer</li>
<li>An OpenTelemetry (OTEL) Collector with access to the Clickhouse server
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the SigNoz OTEL Collector</a>
(v0.88.0 or newer, v0.88.23+ for query log collection) if not done already</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
<li>Ensure the OTEL collector has access to the Clickhouse server for metrics collection and can read the Clickhouse server log file for logs collection</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#preparing-clickhouse-server
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-preparing-clickhouse-server
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Preparing Clickhouse Server</h2>
<h3>## Check Clickhouse Version</h3>
<p>Ensure your Clickhouse server is running a supported version (v23 or newer):</p>
<pre><code>SELECT version();
</code></pre>
<h3>## Configure Prometheus Metrics Export</h3>
<p>If collecting metrics, ensure that Clickhouse is configured to export Prometheus metrics. Follow the Clickhouse Prometheus configuration guide if needed.</p>
<h3>## Create Monitoring User</h3>
<p>If collecting query logs, create a monitoring user with required permissions:</p>
<pre><code>CREATE USER monitoring IDENTIFIED BY 'monitoring_password';
GRANT SELECT ON system.query_log to monitoring;
-- If monitoring a clustered deployment, also grant privilege for executing remote queries
GRANT REMOTE ON *.* TO 'monitoring' on CLUSTER 'cluster_name';
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#collecting-clickhouse-metrics
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-collecting-clickhouse-metrics
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Collecting Clickhouse Metrics</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>clickhouse-metrics-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  prometheus/clickhouse:
    config:
      global:
        scrape_interval: 60s
      scrape_configs:
        - job_name: clickhouse
          static_configs:
            - targets:
                - ${env:CLICKHOUSE_PROM_METRICS_ENDPOINT}
          metrics_path: ${env:CLICKHOUSE_PROM_METRICS_PATH}

processors:
  # enriches the data with additional host information
  # see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#resource-detection-processor
  resourcedetection/system:
    # add additional detectors if needed
    detectors: [&quot;system&quot;]
    system:
      hostname_sources: [&quot;os&quot;]

exporters:
  # export to SigNoz cloud
  otlp/clickhouse:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/clickhouse:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    metrics/clickhouse:
      receivers: [prometheus/clickhouse]
      # note: remove this processor if the collector host is not running on the same host as the clickhouse instance
      processors: [resourcedetection/system]
      exporters: [otlp/clickhouse]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># Prometheus metrics endpoint on the clickhouse server
export CLICKHOUSE_PROM_METRICS_ENDPOINT=&quot;clickhouse:9363&quot;

# Prometheus metrics path on the clickhouse server
export CLICKHOUSE_PROM_METRICS_PATH=&quot;/metrics&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.us.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config clickhouse-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#collecting-clickhouse-server-logs
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-collecting-clickhouse-server-logs
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Collecting Clickhouse Server Logs</h2>
<p>You can configure Clickhouse server logs collection by providing the required collector config to your collector.</p>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>clickhouse-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/clickhouse:
    include: [&quot;${env:CLICKHOUSE_LOG_FILE}&quot;]
    operators:
      # Parse default clickhouse text log format.
      # See https://github.com/ClickHouse/ClickHouse/blob/master/src/Loggers/OwnPatternFormatter.cpp
      - type: recombine
        source_identifier: attributes[&quot;log.file.name&quot;]
        is_first_entry: body matches '^\\d{4}\\.\\d{2}\\.\\d{2}\\s+'
        combine_field: body
        overwrite_with: oldest
      - type: regex_parser
        parse_from: body
        if: body matches '^(?P&lt;ts&gt;\\d{4}\\.\\d{2}\\.\\d{2} \\d{2}:\\d{2}:\\d{2}.?[0-9]*)\\s+\\[\\s+(\\x1b.*?m)?(?P&lt;thread_id&gt;\\d*)(\\x1b.*?m)?\\s+\\]\\s+{((\\x1b.*?m)?(?P&lt;query_id&gt;[0-9a-zA-Z-_]*)(\\x1b.*?m)?)?}\\s+&lt;(\\x1b.*?m)?(?P&lt;log_level&gt;\\w*)(\\x1b.*?m)?&gt;\\s+((\\x1b.*?m)?(?P&lt;clickhouse_component&gt;[a-zA-Z0-9_]+)(\\x1b.*?m)?:)?\\s+(?s)(?P&lt;message&gt;.*)$'
        regex: '^(?P&lt;ts&gt;\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}.?[0-9]*)\s+\[\s+(\x1b.*?m)?(?P&lt;thread_id&gt;\d*)(\x1b.*?m)?\s+\]\s+{((\x1b.*?m)?(?P&lt;query_id&gt;[0-9a-zA-Z-_]*)(\x1b.*?m)?)?}\s+&lt;(\x1b.*?m)?(?P&lt;log_level&gt;\w*)(\x1b.*?m)?&gt;\s+((\x1b.*?m)?(?P&lt;clickhouse_component&gt;[a-zA-Z0-9_]+)(\x1b.*?m)?:)?\s+(?s)(?P&lt;message&gt;.*)$'
      - type: time_parser
        if: attributes.ts  nil
        parse_from: attributes.ts
        layout_type: gotime
        layout: 2006.01.02 15:04:05.999999
        location: ${env:CLICKHOUSE_TIMEZONE}
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: severity_parser
        if: attributes.log_level  nil
        parse_from: attributes.log_level
        overwrite_text: true
        # For mapping details, see getPriorityName defined in https://github.com/ClickHouse/ClickHouse/blob/master/src/Interpreters/InternalTextLogsQueue.cpp
        mapping:
          trace:
            - Trace
            - Test
          debug: Debug
          info:
            - Information
            - Notice
          warn: Warning
          error: Error
          fatal:
            - Fatal
            - Critical
      - type: remove
        if: attributes.log_level  nil
        field: attributes.log_level
      - type: move
        if: attributes.message  nil
        from: attributes.message
        to: body
      - type: add
        field: attributes.source
        value: clickhouse

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/clickhouse-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/clickhouse-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    logs/clickhouse:
      receivers: [filelog/clickhouse]
      processors: [batch]
      exporters: [otlp/clickhouse-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># Path of Clickhouse server log file
export CLICKHOUSE_LOG_FILE=&quot;/var/log/clickhouse-server/server.log&quot;

# Timezone of the Clickhouse server
export CLICKHOUSE_TIMEZONE=&quot;Etc/UTC&quot;

# Region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# Your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config clickhouse-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#collecting-clickhouse-query-logs
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-collecting-clickhouse-query-logs
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Collecting Clickhouse Query Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>clickhouse-query-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  clickhousesystemtablesreceiver/query_log:
    dsn: &quot;${env:CLICKHOUSE_MONITORING_DSN}&quot;
    cluster_name: &quot;${env:CLICKHOUSE_CLUSTER_NAME}&quot;
    query_log_scrape_config:
      scrape_interval_seconds: ${env:QUERY_LOG_SCRAPE_INTERVAL_SECONDS}
      min_scrape_delay_seconds: ${env:QUERY_LOG_SCRAPE_DELAY_SECONDS}

exporters:
  # export to SigNoz cloud
  otlp/clickhouse-query-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/clickhouse-query-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    logs/clickhouse-query-logs:
      receivers: [clickhousesystemtablesreceiver/query_log]
      processors: []
      exporters: [otlp/clickhouse-query-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># DSN for connecting to clickhouse with the monitoring user
export CLICKHOUSE_MONITORING_DSN=&quot;tcp://monitoring:monitoring_password@clickhouse:9000/&quot;

# If collecting query logs from a clustered deployment, specify a non-empty cluster name
export CLICKHOUSE_CLUSTER_NAME=&quot;&quot;

# Rows from query_log table will be collected periodically based on this setting
export QUERY_LOG_SCRAPE_INTERVAL_SECONDS=20

# Must be greater than flush_interval_milliseconds setting for query_log
export QUERY_LOG_SCRAPE_DELAY_SECONDS=8

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config clickhouse-query-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>Only one collector instance should be configured to collect <code>query_logs</code>. Using multiple instances or replicas will lead to duplicate logs.</li>
<li>The collector can use multiple config files by specifying multiple <code>--config</code> flags.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#connect-clickhouse
tag_set: integrations, clickhouse
image_urls: https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-search.webp, https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-connect.webp, https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-listening.webp
tracking_id: docs-integrations-clickhouse-connect-clickhouse
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Connect Clickhouse</h2>
<p>Once you've set up Clickhouse for collecting metrics and logs, go to the integrations tab in SigNoz and search for the Clickhouse integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-search.webp" alt="Search for Clickhouse in Integrations tab" /></p>
<p><em>Search for Clickhouse in Integrations tab</em></p>
<p>Click on the <code>Connect Clickhouse</code> Button, and select <strong>I have already configured</strong>. This will start listening for data from your Clickhouse instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-connect.webp" alt="Connect Clickhouse" /></p>
<p><em>Connect Clickhouse</em></p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-listening.webp" alt="Listening for data from Clickhouse" /></p>
<p><em>Listening for data from Clickhouse</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#clickhouse-dashboard
tag_set: integrations, clickhouse
image_urls: https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-dashboard.webp
tracking_id: docs-integrations-clickhouse-clickhouse-dashboard
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Clickhouse dashboard</h2>
<p>Once SigNoz has started listening to your Clickhouse data, go to the Dashboards tab and search for Clickhouse. This will show you a newly created dashboard which displays various Clickhouse metrics and query log information.</p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-dashboard.webp" alt="Dashboard for monitoring Clickhouse Metrics" /></p>
<p><em>Dashboard for monitoring Clickhouse Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Clickhouse Dashboard by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/clickhouse/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#data-collected
tag_set: integrations, clickhouse
image_urls: https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-data-collected.webp
tracking_id: docs-integrations-clickhouse-data-collected
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your Clickhouse Integration, it shows you details about the different metrics types, server log attributes, and query log attributes that you can monitor for your Clickhouse instance.</p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-data-collected.webp" alt="Log attributes and metrics details for Clickhouse" /></p>
<p><em>Log attributes and metrics details for Clickhouse</em></p>
<h3>## Clickhouse log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
<tr>
<td>Thread ID</td>
<td>attributes.thread_id</td>
<td>string</td>
</tr>
<tr>
<td>Query ID</td>
<td>attributes.query_id</td>
<td>string</td>
</tr>
<tr>
<td>Clickhouse Component</td>
<td>attributes.clickhouse_component</td>
<td>string</td>
</tr>
</tbody>
</table>
<h3>## Clickhouse metrics</h3>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<p>To find a complete list of metrics you can checkout this <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/clickhouse/data-collected.json">link</a> or the <strong>Data Collected</strong> tab.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.38/
tag_set: operate, migration, upgrade-0.38
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.38
group_tracking_ids: docs-operate-migration-upgrade-0.38
<h2>Upgrade to v0.38 from earlier versions</h2>
<p>In the previous version of SigNoz v0.36 i.e. SigNoz chart version v0.32.0, we have added support for dot(<code>.</code>) in attribute names.</p>
<p>This migration updates the dashboards and alerts to support the new format.</p>
<p>If you are using SigNoz version prior to v0.36 please make sure that migration <a href="/docs/operate/migration/upgrade-0.36">v0.36</a> is successful before running this.</p>
<h2>Steps to run migration script:</h2>
<hr />
<h2>First upgrade to v0.38</h2>
<hr />
<p>Follow the platform specific instructions to upgrade to 0.38 and above.</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/operate/docker-standalone/#upgrade">Docker Standalone</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/docker-swarm/#upgrade">Docker Swarm</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/kubernetes/#upgrade">Kubernetes</a></p>
</li>
</ul>
<h3>## For Docker</h3>
<p>Change the directory to SigNoz repo and run following commands:</p>
<pre><code>cd deploy/docker/clickhouse-setup


docker run --name signoz-migrate-sqlite --network clickhouse-setup_default -it \
  -v $PWD/data/signoz/:/var/lib/signoz/ signoz/migrate:0.38 \
  --data_source=/var/lib/signoz/signoz.db \
  --host=clickhouse \
  --port=9000
</code></pre>
<p>Steps to check logs:</p>
<pre><code>docker logs -f signoz-migrate-sqlite
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the container before running the migration script again.</p>
<pre><code>docker stop signoz-migrate-sqlite

docker rm signoz-migrate-sqlite
</code></pre>
<h3>## For Kubernetes</h3>
<p>The steps for running the migration on kubernetes are :-</p>
<ol>
<li>
<p>Make sure you have latest chart information from the Helm repositories:</p>
<pre><code>helm repo update
</code></pre>
</li>
<li>
<p>Include the following in <code>deployment-override.yaml</code> file:</p>
<pre><code>queryService:
  initContainers:
    migration:
      enabled: true
      image:
        registry: docker.io
        repository: signoz/migrate
        tag: 0.38
        pullPolicy: IfNotPresent
      args:
        - &quot;--data_source=/var/lib/signoz/signoz.db&quot;
        - &quot;--host=$(CLICKHOUSE_HOST)&quot;
        - &quot;--user=$(CLICKHOUSE_USER)&quot;
        - &quot;--password=$(CLICKHOUSE_PASSWORD)&quot;
        - &quot;--port=$(CLICKHOUSE_PORT)&quot;
</code></pre>
</li>
</ol>
<p>If you are using external ClickHouse replace the value of <code>host</code> and <code>port</code> along with the respective value of <code>user</code> and <code>password</code></p>
<ol start="3">
<li>
<p>Run the following command to upgrade the chart:</p>
<pre><code>helm --namespace platform upgrade my-release signoz/signoz -f deployment-override.yaml
</code></pre>
</li>
<li>
<p>Check the logs of the migration container using:</p>
<pre><code>kubectl logs my-release-signoz-query-service-0 -n platform -c my-release-signoz-query-service-migration
</code></pre>
</li>
<li>
<p>Remove the init container added in step 1 if there are no errors in the step 4 and upgrade:</p>
<pre><code>helm --namespace platform upgrade my-release signoz/signoz -f deployment-override.yaml
</code></pre>
</li>
</ol>
<h2>In case of Upgrade Failure</h2>
<hr />
<p>Reach out to us at <a href="https://signoz.io/slack">Slack</a>.</p>
<h2>Command-Line Interface (CLI) Flags</h2>
<hr />
<p>There are some custom flags which can be enabled based on different use-cases. All the flags below are <code>optional</code>.</p>
<p>Flags:</p>
<ul>
<li><code>--port</code> : Specify port of clickhouse. <code>default=9000</code></li>
<li><code>--host</code> : Specify host of clickhouse. <code>default=127.0.0.1</code></li>
<li><code>--user</code> : Specify user name of clickhouse. <code>default=default</code></li>
<li><code>--password</code> : Specify password of clickhouse. <code>default=&quot;&quot;</code></li>
<li><code>--data_source</code>: Data Source path of sqlite db. <code>default=&quot;db&quot;</code></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#overview
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-overview
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: AWS RDS (PostgreSQL) Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key AWS RDS PostgreSQL metrics and logs, view them with an out-of-the-box dashboards, and parse PostgreSQL logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#prerequisites
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-prerequisites
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have the following:</p>
<ol>
<li><strong>AWS Credentials and Permissions</strong>:</li>
</ol>
<ul>
<li>Set up proper AWS credentials (e.g., <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables).</li>
<li>Required IAM permissions:
<ul>
<li><code>cloudwatch:ListMetrics</code></li>
<li><code>cloudwatch:GetMetricStatistics</code></li>
<li><code>cloudwatch:GetMetricData</code></li>
<li><code>logs:DescribeLogGroups</code></li>
<li><code>logs:FilterLogEvents</code></li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>Java Runtime Environment (JRE) 11+</strong>:</li>
</ol>
<ul>
<li>Required for the CloudWatch Exporter.</li>
<li>Alternative: Use the <a href="https://github.com/prometheus/cloudwatch_exporter#docker-images">Docker image</a>
.</li>
</ul>
<ol start="3">
<li><strong>OpenTelemetry Collector</strong>:</li>
</ol>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install an OTEL Collector</a>
(v0.88.0+) if not already done.</li>
<li>Ensure you can provide config files to the collector and set environment variables and command line flags used for running it.</li>
</ul>
<ol start="4">
<li><strong>PostgreSQL Server Access</strong>:</li>
</ol>
<ul>
<li>The OTEL collector must have client access to the Postgres server (optional if only collecting CloudWatch metrics).</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#step-1-set-up-the-prometheus-cloudwatch-exporter
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-step-1-set-up-the-prometheus-cloudwatch-exporter
group_tracking_ids: docs-integrations-aws-rds-postgres
<p>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: ## Collecting RDS PostgreSQL Metrics - Step 1: Set up the Prometheus CloudWatch Exporter</p>
<ol>
<li>
<p>Download the exporter:</p>
<pre><code>curl -sLSO https://github.com/prometheus/cloudwatch_exporter/releases/download/v0.15.5/cloudwatch_exporter-0.15.5-jar-with-dependencies.jar
</code></pre>
</li>
<li>
<p>Configure the Prometheus exporter Save the following config for collecting AWS RDS metrics in a file named <code>aws-rds-postgres-metrics.yaml</code> and update the region key with relevant value.</p>
<hr />
<p>region: us-east-1
metrics:</p>
<ul>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: BinLogDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: BurstBalance
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: CheckpointLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ConnectionAttempts
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: CPUUtilization
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DatabaseConnections
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DiskQueueDepth
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DiskQueueDepthLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: EBSByteBalance%
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: EBSIOBalance%
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeableMemory
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeStorageSpace
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeStorageSpaceLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: MaximumUsedTransactionIDs
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: NetworkReceiveThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: NetworkTransmitThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: OldestReplicationSlotLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPS
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPSLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPSLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatency
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatencyLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatencyLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadThroughputLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicaLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicationChannelLag
aws_dimensions: [DBInstanceIdentifier]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicationSlotDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: TransactionLogsDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: TransactionLogsGeneration
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteIOPS
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteLatency
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: SwapUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoad
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoadCPU
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoadNonCPU
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
</ul>
</li>
<li>
<p>Run the following command:</p>
<p>java -jar cloudwatch_exporter-0.15.5-jar-with-dependencies.jar 9106 aws-rds-postgres-metrics.yaml</p>
</li>
<li>
<p>Verify the CloudWatch metrics</p>
</li>
</ol>
<p>Visit <a href="http://localhost:9106/metrics">http://localhost:9106/metrics</a> and confirm the <code>aws_rds_*</code> metrics are avialable.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#step-2-create-the-otel-collector-config-file
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-step-2-create-the-otel-collector-config-file
group_tracking_ids: docs-integrations-aws-rds-postgres
<p>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Step 2: Create the OTEL Collector Config File</p>
<p>Create <code>postgres-metrics-collection-config.yaml</code>:</p>
<pre><code>receivers:
  postgresql:
    # The endpoint of the postgresql server. Whether using TCP or Unix sockets, this value should be host:port. If transport is set to unix, the endpoint will internally be translated from host:port to /host.s.PGSQL.port
    endpoint: ${env:POSTGRESQL_ENDPOINT}
    # The frequency at which to collect metrics from the Postgres instance.
    collection_interval: 60s
    # The username used to access the postgres instance
    username: ${env:POSTGRESQL_USERNAME}
    # The password used to access the postgres instance
    password: ${env:POSTGRESQL_PASSWORD}
    # The list of databases for which the receiver will attempt to collect statistics. If an empty list is provided, the receiver will attempt to collect statistics for all non-template databases
    databases: [&quot;pgtestdb&quot;]
    # # Defines the network to use for connecting to the server. Valid Values are `tcp` or `unix`
    # transport: tcp
    tls:
      insecure_skip_verify: true
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/postgres.crt
    #   key_file: /etc/ssl/certs/postgres.key
    metrics:
      postgresql.database.locks:
        enabled: true
      postgresql.deadlocks:
        enabled: true
      postgresql.sequential_scans:
        enabled: true

  prometheus:
    config:
      scrape_configs:
        - job_name: 'aws-cloudwatch-metrics'
          scrape_timeout: 120s
          scrape_interval: 300s
          static_configs:
            - targets: ['0.0.0.0:9106']

exporters:
  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true
  # export to SigNoz cloud
  otlp/signoz:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

service:
  pipelines:
    metrics/postgresql:
      receivers: [postgresql, prometheus]
      processors: []
      exporters: [otlp/signoz]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#step-3-set-environment-variables
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-step-3-set-environment-variables
group_tracking_ids: docs-integrations-aws-rds-postgres
<p>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Step 3: Set Environment Variables</p>
<pre><code># The accessible endpoint where PostgreSQL server is running
export POSTGRESQL_ENDPOINT=&quot;&lt;postgres-server-endpoint&gt;&quot;

export POSTGRESQL_USERNAME=&quot;&lt;username&gt;&quot;

# The password to use for accessing postgres instance
export POSTGRESQL_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#step-3-use-the-collector-config-file
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-step-3-use-the-collector-config-file
group_tracking_ids: docs-integrations-aws-rds-postgres
<p>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Step 3: Use the Collector Config File</p>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config postgres-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#collecting-rds-logs
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-collecting-rds-logs
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Collecting RDS Logs</h2>
<p>The log collection of RDS instance requires specifying the list of log group names. From the AWS CloudWatch console, please find the log group(s) relevant to the integration.</p>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create <code>postgres-logs-collection-config.yaml</code>:</p>
<pre><code>receivers:
  awscloudwatch/rds_postgres_logs:
    region: us-east-1
    logs:
      poll_interval: 1m
      groups:
        named:
          # replace with your RDS log group name
          /aws/rds/:

processors:
  attributes/add_source_postgres:
    actions:
      - key: source
        value: &quot;rds_postgres&quot;
        action: insert
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  otlp/postgres_logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

service:
  pipelines:
    logs/postgres:
      receivers: [awscloudwatch/rds_postgres_logs]
      processors: [attributes/add_source_postgres, batch]
      exporters: [otlp/postgres_logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<pre><code># region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Run the Collector</h3>
<p>Add to your collector run command:</p>
<pre><code>--config postgres-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#connect-aws-rds-postgresql
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-connect-aws-rds-postgresql
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Connect AWS RDS (PostgreSQL)</h2>
<p>Once you're done with setting up AWS RDS (PostgreSQL) for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the AWS RDS (PostgreSQL) integration.</p>
<p><img src="https://signoz.io/docs/integrations/PostgreSQL" alt="Search for AWS RDS (PostgreSQL) in Integrations tab" /></p>
<p><em>Search for AWS RDS (PostgreSQL) in Integrations tab</em></p>
<p>Click on the <code>Connect AWS RDS (PostgreSQL)</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your AWS RDS (PostgreSQL) instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/docs/integrations/PostgreSQL" alt="Connect AWS RDS (PostgreSQL)" /></p>
<p><em>Connect AWS RDS (PostgreSQL)</em></p>
<p><img src="https://signoz.io/docs/integrations/PostgreSQL" alt="Listening for data from RedAWS RDS (PostgreSQL)is" /></p>
<p><em>Listening for data from AWS RDS (PostgreSQL)</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#aws-rds-postgresql-dashboard
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-aws-rds-postgresql-dashboard
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: AWS RDS (PostgreSQL) dashboard</h2>
<p>Once SigNoz has started listening to your AWS RDS (PostgreSQL) data, head over to the Dashboards tab and search for postgres, this will show you two newly created dashboard which shows differnet AWS RDS (PostgreSQL) metrics.</p>
<p><img src="https://signoz.io/docs/integrations/PostgreSQL" alt="Dashboard for monitoring AWS RDS (PostgreSQL) Metrics" /></p>
<p><em>Dashboards for monitoring AWS RDS (PostgreSQL) Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Dashboards by importing the JSON files available <a href="https://github.com/SigNoz/signoz/tree/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_rds_postgres/assets/dashboards">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#data-collected
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-data-collected
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your AWS RDS (PostgreSQL) Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your AWS RDS (PostgreSQL) instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/docs/integrations/PostgreSQL" alt="Log attributes and metrics details for AWS RDS (PostgreSQL)" /></p>
<p><em>Log attributes and metrics details for AWS RDS (PostgreSQL)</em></p>
<h3>## AWS RDS (PostgreSQL) log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Body</td>
<td>body</td>
<td>string</td>
</tr>
</tbody>
</table>
<h3>## AWS RDS (PostgreSQL) metrics</h3>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<p>To find a complete list of metrics you can checkout this <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_rds_postgres/integration.json#L58">link</a> or the <strong>Data Collected</strong> tab.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/alerts-management/
tag_set: userguide, alerts-management
image_urls: 
tracking_id: docs-userguide-alerts-management
group_tracking_ids: docs-userguide-alerts-management
<h2>Alerts</h2>
<p>Alerts in SigNoz can help you to define which data to monitor, set thresholds to detect potential problems, and specify who should be notified and how. This can help you to identify critical issues and reduce noise. This document will help you in understanding how to set up and use alerts effectively.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/alerts-management/#managing-alerts
tag_set: userguide, alerts-management
image_urls: 
tracking_id: docs-userguide-alerts-management-managing-alerts
group_tracking_ids: docs-userguide-alerts-management
<h2>Alerts: Managing Alerts</h2>
<p><img src="https://signoz.io/img/docs/product-features/alerts/alerts-alert-rules-tab.gif" alt="A gif explaining the Alerts Rules Tab in SigNoz" /></p>
<p><em>Features of Alert Rules Tab</em></p>
<p>The Alert Rules Tab in SigNoz provides an overview of the alert defined by the user. This section allows you to view, edit, or manage alert rules, along with their associated metadata. Here's a breakdown of the features available:</p>
<h3>## Alert Rule Columns</h3>
<ul>
<li><strong>Status</strong>: Indicates whether the alert rule is enabled (OK) or disabled.</li>
<li><strong>Alert Name</strong>: The name given to the alert rule for easy identification.</li>
<li><strong>Severity</strong>: The level of severity assigned to the alert. For example, <code>warning</code>, <code>critical</code> etc.</li>
<li><strong>Labels</strong>: Displays any labels associated with the alert rule. Labels can help in categorizing alerts.</li>
</ul>
<h3>## Additional Alert Rule Options</h3>
<ul>
<li><strong>Filter by Created At, Created By, Updated At, and Updated By</strong>: The filter option in the top-right corner allows you to customize which fields are displayed. You can choose to show fields like when was the alert created who created the alert, when it was last updated, and who updated it.</li>
<li><strong>Sorting Columns</strong>: By hovering over a column name and clicking it, you can sort the list of alert rules in ascending or descending order based on that column's data.</li>
<li><strong>New Alert</strong>: At the top-right corner, the &quot;<strong>+ New Alert</strong>&quot; button lets you create a new alert rule.</li>
</ul>
<h3>## Navigation and Search</h3>
<ul>
<li><strong>Search Bar</strong>: At the top of the tab, you can search for specific alert rules by <strong>name</strong>, <strong>severity</strong>, or <strong>label</strong>.</li>
<li><strong>Pagination Controls</strong>: At the bottom-right corner, you can navigate through multiple pages of alert rules.</li>
<li><strong>Actions Menu</strong>: Found on the right side of each row, this menu allows you to perform additional actions on the alert, such as <strong>Enable</strong>, <strong>Edit</strong>, <strong>Clone</strong> and <strong>Delete</strong>.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/alerts-management/#triggered-alerts-tab
tag_set: userguide, alerts-management
image_urls: 
tracking_id: docs-userguide-alerts-management-triggered-alerts-tab
group_tracking_ids: docs-userguide-alerts-management
<h2>Alerts: Triggered Alerts Tab</h2>
<p><img src="https://signoz.io/img/docs/product-features/alerts/alerts-triggered-tab.gif" alt="A gif explaining the Triggered Alerts Tab in SigNoz" /></p>
<p><em>Features of Triggered Alerts Tab</em></p>
<p>The Triggered Alerts Tab shows the currently firing alerts. It provides a real-time view of alerts, allowing you to quickly assess which alerts are active and require attention. Here's a detailed description of the tab's features:</p>
<h3>## Triggered Alert Columns</h3>
<ul>
<li><strong>Status</strong>: Shows whether the alert is currently firing. It can have values like &quot;Firing.&quot;</li>
<li><strong>Alert Name</strong>: The name of the triggered alert.</li>
<li><strong>Severity</strong>: Indicates the severity of the triggered alert (e.g., &quot;warning&quot;).</li>
<li><strong>Tags</strong>: Displays additional information or tags related to the alert.</li>
<li><strong>Firing Since</strong>: The timestamp indicating when the alert started firing.</li>
</ul>
<h3>## Additional Triggered Alert Options</h3>
<ul>
<li><strong>Filter by Tags</strong>: You can apply filters to narrow down the list of triggered alerts based on specific tags.</li>
<li><strong>Group by</strong>: The &quot;Group by&quot; feature allows you to group alerts based on various criteria, such as alert name, severity etc.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/alerts-management/#creating-a-new-alert-in-signoz
tag_set: userguide, alerts-management
image_urls: 
tracking_id: docs-userguide-alerts-management-creating-a-new-alert-in-signoz
group_tracking_ids: docs-userguide-alerts-management
<h2>Alerts: Creating a New Alert in SigNoz</h2>
<p>After setting up a new notification channel, you can create an alert by clicking the &quot;New Alert&quot; button in the Alerts Tab. You will see four types of alerts to choose from:</p>
<ul>
<li>
<p><strong><a href="../../alerts-management/metrics-based-alerts">Metric-based Alert</a></strong>: Sends a notification when a condition occurs in metric data (e.g., CPU usage, memory utilization, request rates). You can set thresholds or rate-based conditions.</p>
</li>
<li>
<p><strong><a href="../../alerts-management/log-based-alerts">Log-based Alert</a></strong>: Sends a notification when a condition occurs in log data (e.g., specific patterns, keywords, error messages). You can set conditions based on log entries or error codes.</p>
</li>
<li>
<p><strong><a href="../../alerts-management/trace-based-alerts">Trace-based Alert</a></strong>: Sends a notification when a condition occurs in trace data (e.g., latency, errors, specific trace events). You can define conditions to trigger the alert based on distributed system traces.</p>
</li>
<li>
<p><strong><a href="../../alerts-management/exceptions-based-alerts">Exceptions-based Alert</a></strong>: Sends a notification when a condition occurs in exceptions data (e.g., application exceptions or errors). You can set conditions to trigger the alert when specific exceptions are detected.</p>
</li>
<li>
<p><strong><a href="../../alerts-management/planned-maintenance">Planned Maintenance</a></strong>: Sends a notification when a condition occurs in exceptions data (e.g., application exceptions or errors). You can set conditions to trigger the alert when specific exceptions are detected.</p>
</li>
</ul>
<p>These four types of alerts offer flexibility in monitoring different system aspects.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.36/
tag_set: operate, migration, upgrade-0.36
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.36
group_tracking_ids: docs-operate-migration-upgrade-0.36
<h2>Upgrade to v0.36 from earlier versions (Kubernetes)</h2>
<p>In the SigNoz version <code>&gt;=v0.36</code> i.e. SigNoz chart version <code>&gt;=v0.32.0</code> , we have added support for dot(<code>.</code>) in attribute names.</p>
<p>Ex:- Previously <code>service.name</code> was stored and queried as <code>service_name</code> , after this upgrade <code>service.name</code> will be supported.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.36/#steps-to-run-migration-script
tag_set: operate, migration, upgrade-0.36
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.36-steps-to-run-migration-script
group_tracking_ids: docs-operate-migration-upgrade-0.36
<h2>Upgrade to v0.36 from earlier versions (Kubernetes): Steps to run migration script:</h2>
<p>üìù Note</p>
<p>It's recommended to run the migration script 10-15 mins after upgrading to <code>v0.36</code> . You can also run this migration script multiple times.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.36/#first-upgrade-to-v036
tag_set: operate, migration, upgrade-0.36
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.36-first-upgrade-to-v036
group_tracking_ids: docs-operate-migration-upgrade-0.36
<h2>Upgrade to v0.36 from earlier versions (Kubernetes): First upgrade to v0.36</h2>
<p>Follow the platform specific instructions to upgrade to 0.36 and above.</p>
<p>Note that the past exceptions/error data will not be visible on the new application until you run the migration script.</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/operate/docker-standalone/#upgrade">Docker Standalone</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/docker-swarm/#upgrade">Docker Swarm</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/kubernetes/#upgrade">Kubernetes</a></p>
</li>
</ul>
<h3>## For Docker</h3>
<pre><code>docker run --name signoz-migrate --network clickhouse-setup_default \
  -it -d signoz/migrate:0.36 -host=clickhouse -port=9000
</code></pre>
<p>Steps to check logs:</p>
<pre><code>docker logs -f signoz-migrate
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the container before running the migration script again.</p>
<pre><code>docker stop signoz-migrate

docker rm signoz-migrate
</code></pre>
<h3>## For Docker Swarm</h3>
<p>For Swarm, you could follow similar step to that of <a href="#docker">Docker</a>. However, you would need to expose clickhouse container ports to host machine and use host machine IP i.e. <code>172.17.0.17</code> for <code>-host</code> flag instead of <code>clickhouse</code>.</p>
<p>If you do not want to change anything in the current signoz deployment or to expose clickhouse ports even temporarily, you can go through following steps.</p>
<ol>
<li>
<p>To download <code>migrate-v0.36</code> binary:</p>
<pre><code>wget https://github.com/SigNoz/signoz-db-migrations/releases/download/v0.36/migrate-v0.36-linux-amd64

chmod +x migrate-v0.36-linux-amd64
</code></pre>
</li>
<li>
<p>To copy the binary in persistent volume path <code>/var/lib/clickhouse</code> in <code>clickhouse</code> container:</p>
<pre><code>docker cp migration-v0.36-linux-amd64 $(docker ps -q -f name=signoz_clickhouse):/var/lib/clickhouse/migrate-0.36
</code></pre>
</li>
<li>
<p>To exec into the <code>clickhouse</code> container:</p>
<pre><code>docker exec -it $(docker ps -q -f name=signoz_clickhouse) bash
</code></pre>
</li>
<li>
<p>Now, change directory to the <code>/var/lib/clickhouse</code> and run the migration script:</p>
<pre><code>cd /var/lib/clickhouse

./migration-0.36
</code></pre>
</li>
<li>
<p>At last, clean up the binary:</p>
<pre><code>rm migration-0.36
</code></pre>
</li>
</ol>
<h3>## For Kubernetes</h3>
<pre><code>RELEASE=my-release
ADMIN_PASSWORD=$(
  kubectl -n platform get clickhouseinstallations.clickhouse.altinity.com $RELEASE-clickhouse \
  -o jsonpath --template '{.spec.configuration.users.admin/password}'
)

kubectl -n platform run -i -t signoz-migrate --image=signoz/migrate:0.36 --restart='Never' \
  -- -host=$RELEASE-clickhouse -port=9000 -userName=admin -password=$ADMIN_PASSWORD
</code></pre>
<p>Steps to check logs:</p>
<pre><code>kubectl -n platform logs -f signoz-migrate
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the pod before running the migration script again.</p>
<pre><code>kubectl -n platform delete pod signoz-migrate
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.36/#in-case-of-upgrade-failure
tag_set: operate, migration, upgrade-0.36
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.36-in-case-of-upgrade-failure
group_tracking_ids: docs-operate-migration-upgrade-0.36
<h2>Upgrade to v0.36 from earlier versions (Kubernetes): In case of Upgrade Failure</h2>
<ol>
<li>Note the names of fields which were not migrated. ex:- <code>telemetry.sdk.name</code></li>
<li>Exec into the clickhouse container and run <code>clickhouse client</code>.</li>
<li>Check the schema of the logs table <code>show create table signoz_logs.logs</code></li>
<li>If <code>telemetry_sdk_name</code> column or <code>telemetry_sdk_name_idx</code> index is present you can delete them</li>
</ol>
<p>For deleting index</p>
<ul>
<li><code>alter table signoz_logs.logs on cluster cluster drop index telemetry_sdk_name_idx</code></li>
</ul>
<p>For deleting column</p>
<ul>
<li><code>alter table signoz_logs.logs on cluster cluster drop column telemetry_sdk_name</code></li>
<li><code>alter table signoz_logs.distributed_logs on cluster cluster drop column telemetry_sdk_name</code></li>
</ul>
<ol start="5">
<li>Now from the UI, you can convert <code>telemetry.sdk.name</code> to selected field.</li>
<li>If you still face issue, reach out to us at <a href="https://signoz.io/slack">Slack</a>
.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.36/#command-line-interface-cli-flags
tag_set: operate, migration, upgrade-0.36
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.36-command-line-interface-cli-flags
group_tracking_ids: docs-operate-migration-upgrade-0.36
<h2>Upgrade to v0.36 from earlier versions (Kubernetes): Command-Line Interface (CLI) Flags</h2>
<p>There are some custom flags which can be enabled based on different use-cases. All the flags below are <code>optional</code>.</p>
<p>Flags:</p>
<ul>
<li><code>-port</code> : Specify port of clickhouse. <code>default=9000</code></li>
<li><code>-host</code> : Specify host of clickhouse. <code>default=127.0.0.1</code></li>
<li><code>-userName</code> : Specify user name of clickhouse. <code>default=default</code></li>
<li><code>-password</code> : Specify password of clickhouse. <code>default=&quot;&quot;</code></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.36/#updating-query-payload-dashboards-and-alerts
tag_set: operate, migration, upgrade-0.36
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.36-updating-query-payload-dashboards-and-alerts
group_tracking_ids: docs-operate-migration-upgrade-0.36
<h2>Upgrade to v0.36 from earlier versions (Kubernetes): Updating Query Payload, Dashboards and Alerts</h2>
<p>üìù Note</p>
<p>It's recommended to update Dashboards and Alerts after a few days of installing the release so that sufficient amount of newer data is ingested.</p>
<p>If you've been using the SigNoz Query API, Dashboards, or Alerts with attributes that included a dot (.) but were converted to an underscore (_), they will continue to function for the next 45 days. However, it's essential to update these attributes to reflect the new format.</p>
<p>Ex:-</p>
<ul>
<li><code>service.name</code> was previously shown and stored as <code>service_name</code> . Now you can update it to use <code>service.name</code></li>
<li><code>k8s.namespace.name</code> was previously shown and stored as <code>k8s_namespace_name</code> . Now you can update it to use <code>k8s.namespace.name</code></li>
</ul>
<p>For assistance in making these updates, please reach out to us via Intercom support or at <a href="mailto:cloud-support@signoz.io">cloud-support@signoz.io</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#overview
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-overview
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: AWS RDS (MySQL) Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key AWS RDS (MySQL) metrics and logs, view them with an out-of-the-box dashboards, and parse MySQL logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#prerequisites
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-prerequisites
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>
<p>AWS Credentials and Permissions:</p>
<ul>
<li>Set up proper AWS credentials (e.g., <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables)</li>
<li>Required IAM permissions:
<ul>
<li><code>cloudwatch:ListMetrics</code></li>
<li><code>cloudwatch:GetMetricStatistics</code></li>
<li><code>cloudwatch:GetMetricData</code></li>
<li><code>tag:GetResources</code> (if aws_tag_select feature is used)</li>
<li><code>logs:DescribeLogGroups</code></li>
<li><code>logs:FilterLogEvents</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Java Runtime Environment (JRE) version 11 or newer for the CloudWatch Exporter (Not required if using the Docker container)</p>
</li>
<li>
<p>An OpenTelemetry (OTEL) Collector (v0.88.0+):</p>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0+) if not already done</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
</ul>
</li>
<li>
<p>OTEL collector access to the MySQL server (optional, for MySQL engine metrics)</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#step-1-set-up-the-prometheus-cloudwatch-exporter
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-step-1-set-up-the-prometheus-cloudwatch-exporter
group_tracking_ids: docs-integrations-aws-rds-mysql
<p>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: ## Collecting RDS MySQL Metrics - Step 1: Set up the Prometheus CloudWatch Exporter</p>
<ol>
<li>
<p>Download the exporter:</p>
<pre><code>curl -sLSO https://github.com/prometheus/cloudwatch_exporter/releases/download/v0.15.5/cloudwatch_exporter-0.15.5-jar-with-dependencies.jar
</code></pre>
</li>
<li>
<p>Configure the Prometheus exporter Save the following config for collecting AWS RDS metrics in a file named <code>aws-rds-mysql-metrics.yaml</code> and update the region key with relevant value.</p>
<hr />
<p>region: us-east-1
metrics:</p>
<ul>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: BinLogDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: BurstBalance
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: CheckpointLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ConnectionAttempts
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: CPUUtilization
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DatabaseConnections
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DiskQueueDepth
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DiskQueueDepthLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: EBSByteBalance%
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: EBSIOBalance%
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeableMemory
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeStorageSpace
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeStorageSpaceLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: MaximumUsedTransactionIDs
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: NetworkReceiveThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: NetworkTransmitThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: OldestReplicationSlotLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPS
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPSLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPSLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatency
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatencyLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatencyLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadThroughputLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicaLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicationChannelLag
aws_dimensions: [DBInstanceIdentifier]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicationSlotDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: TransactionLogsDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: TransactionLogsGeneration
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteIOPS
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteLatency
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: SwapUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoad
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoadCPU
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoadNonCPU
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
</ul>
</li>
<li>
<p>Run the following command:</p>
<p>java -jar cloudwatch_exporter-0.15.5-jar-with-dependencies.jar 9106 aws-rds-mysql-metrics.yaml</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#step-2-create-the-otel-collector-config-file
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-step-2-create-the-otel-collector-config-file
group_tracking_ids: docs-integrations-aws-rds-mysql
<p>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Step 2: Create the OTEL Collector Config File</p>
<p>Create <code>mysql-metrics-collection-config.yaml</code>:</p>
<pre><code>receivers:
  mysql:
    # The hostname and port of the MySQL instance, separated by a colon.
    endpoint: ${env:MYSQL_ENDPOINT}
    # The username used to access the MySQL instance.
    username: ${env:MYSQL_USERNAME}
    # The password used to access the MySQL instance.
    password: ${env:MYSQL_PASSWORD}
    # The frequency at which to collect metrics from the Redis instance.
    collection_interval: 60s
    # Additional configuration for query to build mysql.statement_events.count and mysql.statement_events.wait.time metrics
    statement_events:
      digest_text_limit: 120
      time_limit: 24h
      limit: 250
    # tls:
    #   insecure: false
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/redis.crt
    #   key_file: /etc/ssl/certs/redis.key
    metrics:
      mysql.client.network.io:
        enabled: true
      mysql.commands:
        enabled: true
      mysql.connection.count:
        enabled: true
      mysql.connection.errors:
        enabled: true
      mysql.joins:
        enabled: true
      mysql.query.count:
        enabled: true
      mysql.query.slow.count:
        enabled: true
      mysql.replica.sql_delay:
        enabled: true
      mysql.replica.time_behind_source:
        enabled: true

  # Collecting cloudwatch metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: 'aws-cloudwatch-metrics'
          scrape_timeout: 120s
          scrape_interval: 300s
          static_configs:
            - targets: ['0.0.0.0:9106']

exporters:
  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true
  # export to SigNoz cloud
  otlp/signoz:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

service:
  pipelines:
    metrics/mysql:
      receivers: [mysql, prometheus]
      processors: []
      exporters: [otlp/signoz]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#step-3-set-environment-variables
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-step-3-set-environment-variables
group_tracking_ids: docs-integrations-aws-rds-mysql
<p>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Step 3: Set Environment Variables</p>
<pre><code># The accessible endpoint where MySQL server is running
export MYSQL_ENDPOINT=&quot;&lt;mysql-server-endpoint&gt;&quot;

export MYSQL_USERNAME=&quot;&lt;username&gt;&quot;

# The password to use for accessing mysql instance
export MYSQL_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#step-3-use-the-collector-config-file
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-step-3-use-the-collector-config-file
group_tracking_ids: docs-integrations-aws-rds-mysql
<p>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Step 3: Use the Collector Config File</p>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config mysql-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#collecting-rds-logs
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-collecting-rds-logs
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Collecting RDS Logs</h2>
<p>The log collection of RDS instance requires specifying the list of log group names. From the AWS CloudWatch console, please find the log group(s) relevant to the integration.</p>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create <code>mysql-logs-collection-config.yaml</code>:</p>
<pre><code>receivers:
  awscloudwatch/rds_mysql_logs:
    region: us-east-1
    logs:
      poll_interval: 1m
      groups:
        named:
          # replace the following name with your log group for RDS logs
          /aws/rds/:

processors:
  attributes/add_source_mysql:
    actions:
      - key: source
        value: &quot;rds_mysql&quot;
        action: insert
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/mysql_logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true

service:
  pipelines:
    logs/mysql:
      receivers: [awscloudwatch/rds_mysql_logs]
      processors: [attributes/add_source_mysql, batch]
      exporters: [otlp/mysql_logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<pre><code># region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Run the Collector</h3>
<p>Add to your collector run command:</p>
<pre><code>--config mysql-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#connect-aws-rds-mysql
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-connect-aws-rds-mysql
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Connect AWS RDS (MySQL)</h2>
<p>Once you're done with setting up AWS RDS (MySQL) for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the AWS RDS (MySQL) integration.</p>
<p><img src="https://signoz.io/docs/integrations/MySQL" alt="Search for AWS RDS (MySQL) in Integrations tab" /></p>
<p><em>Search for AWS RDS (MySQL) in Integrations tab</em></p>
<p>Click on the <code>Connect AWS RDS (MySQL)</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your AWS RDS (MySQL) instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/docs/integrations/MySQL" alt="Connect AWS RDS (MySQL)" /></p>
<p><em>Connect AWS RDS (MySQL)</em></p>
<p><img src="https://signoz.io/docs/integrations/MySQL" alt="Listening for data from RedAWS RDS (MySQL)is" /></p>
<p><em>Listening for data from AWS RDS (MySQL)</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#aws-rds-mysql-dashboard
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-aws-rds-mysql-dashboard
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: AWS RDS (MySQL) dashboard</h2>
<p>Once SigNoz has started listening to your AWS RDS (MySQL) data, head over to the Dashboards tab and search for mysql, this will show you two newly created dashboard which shows differnet AWS RDS (MySQL) metrics.</p>
<p><img src="https://signoz.io/docs/integrations/MySQL" alt="Dashboard for monitoring AWS RDS (MySQL) Metrics" /></p>
<p><em>Dashboards for monitoring AWS RDS (MySQL) Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Dashboards by importing the JSON files available <a href="https://github.com/SigNoz/signoz/tree/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_rds_mysql/assets/dashboards">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#data-collected
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-data-collected
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your AWS RDS (MySQL) Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your AWS RDS (MySQL) instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/docs/integrations/MySQL" alt="Log attributes and metrics details for AWS RDS (MySQL)" /></p>
<p><em>Log attributes and metrics details for AWS RDS (MySQL)</em></p>
<h3>## AWS RDS (MySQL) log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Body</td>
<td>body</td>
<td>string</td>
</tr>
</tbody>
</table>
<h3>## AWS RDS (MySQL) metrics</h3>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<p>To find a complete list of metrics you can checkout this <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_rds_mysql/integration.json#L58">link</a> or the <strong>Data Collected</strong> tab.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/community/community-integrations/
tag_set: community, community-integrations
image_urls: 
tracking_id: docs-community-community-integrations
group_tracking_ids: docs-community-community-integrations
<h2>Community Integrations</h2>
<p>There are many different platforms that have added integrations to use SigNoz as a backend. This includes:</p>
<ul>
<li>
<p><a href="https://docs.omnistrate.com/build-guides/integrations/?h=signoz#opentelemetry-providers">Omnistarate</a></p>
</li>
<li>
<p><a href="https://www.thousandeyes.com/blog/data-observability-backend-opentelemetry">ThousandEyes by Cisco</a></p>
</li>
<li>
<p><a href="https://support.squadcast.com/integrations/alert-source-integrations-native/signoz">Squadcast</a></p>
</li>
</ul>
<p>Want to get your integration added to this list? Just raise a PR on <a href="https://github.com/SigNoz/signoz-web">signoz-web GitHub</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/setup-alerts-notification/
tag_set: setup-alerts-notification
image_urls: 
tracking_id: docs-setup-alerts-notification
group_tracking_ids: docs-setup-alerts-notification
<h2>Setup Alerts Notifications Channel</h2>
<p>You can setup notification channel for sending the generated alerts to other applications. Currently, the following channels are supported.</p>
<p><a href="/docs/alerts-management/notification-channel/slack">üìÑÔ∏è Slack</a>
<a href="/docs/alerts-management/notification-channel/webhook">üìÑÔ∏è Webhook</a>
<a href="/docs/alerts-management/notification-channel/pagerduty">üìÑÔ∏è PagerDuty</a>
<a href="/docs/alerts-management/notification-channel/opsgenie">üìÑÔ∏è Opsgenie</a>
<a href="/docs/alerts-management/notification-channel/ms-teams">üìÑÔ∏è MS Teams</a>
<a href="/docs/alerts-management/notification-channel/email">üìÑÔ∏è Email</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.27/
tag_set: operate, migration, upgrade-0.27
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.27
group_tracking_ids: docs-operate-migration-upgrade-0.27
<h2>Upgrade to v0.27 from earlier versions (Kubernetes)</h2>
<p>In the SigNoz version <code>&gt;=v0.27</code> i.e. SigNoz chart version <code>&gt;=v0.23.0</code>, <a href="https://clickhouse.com/">ClickHouse</a> is upgraded from version v22.8.8 to v23.7.3.</p>
<p>This upgrade brings changes in how we index attributes in logs. From now you can have fields with same names but different dataType as selected(indexed) fields.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.27/#first-upgrade-to-v027
tag_set: operate, migration, upgrade-0.27
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.27-first-upgrade-to-v027
group_tracking_ids: docs-operate-migration-upgrade-0.27
<h2>Upgrade to v0.27 from earlier versions (Kubernetes): First upgrade to v0.27</h2>
<p>Follow the platform specific instructions to upgrade to v0.27 and above.</p>
<p>Note that the past exceptions/error data will not be visible on the new application until you run the migration script.</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/operate/docker-standalone/#upgrade">Docker Standalone</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/docker-swarm/#upgrade">Docker Swarm</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/kubernetes/#upgrade">Kubernetes</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.27/#steps-to-run-migration-script
tag_set: operate, migration, upgrade-0.27
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.27-steps-to-run-migration-script
group_tracking_ids: docs-operate-migration-upgrade-0.27
<h2>Upgrade to v0.27 from earlier versions (Kubernetes): Steps to run migration script:</h2>
<h3>## For Docker</h3>
<pre><code>docker run --name signoz-migrate --network clickhouse-setup_default \
  -it -d signoz/migrate:0.27 -host=clickhouse -port=9000
</code></pre>
<p>Steps to check logs:</p>
<pre><code>docker logs -f signoz-migrate
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the container before running the migration script again.</p>
<pre><code>docker stop signoz-migrate

docker rm signoz-migrate
</code></pre>
<h3>## For Docker Swarm</h3>
<p>For Swarm, you could follow similar step to that of <a href="#docker">Docker</a>. However, you would need to expose clickhouse container ports to host machine and use host machine IP i.e. <code>172.17.0.17</code> for <code>-host</code> flag instead of <code>clickhouse</code>.</p>
<p>If you do not want to change anything in the current signoz deployment or to expose clickhouse ports even temporarily, you can go through following steps.</p>
<ol>
<li>
<p>To download <code>migrate-v0.27</code> binary:</p>
<pre><code>wget https://github.com/SigNoz/signoz-db-migrations/releases/download/v0.27/migrate-v0.27-linux-amd64

chmod +x migrate-v0.27-linux-amd64
</code></pre>
</li>
<li>
<p>To copy the binary in persistent volume path <code>/var/lib/clickhouse</code> in <code>clickhouse</code> container:</p>
<pre><code>docker cp migration-v0.27-linux-amd64 $(docker ps -q -f name=signoz_clickhouse):/var/lib/clickhouse/migrate-0.27
</code></pre>
</li>
<li>
<p>To exec into the <code>clickhouse</code> container:</p>
<pre><code>docker exec -it $(docker ps -q -f name=signoz_clickhouse) bash
</code></pre>
</li>
<li>
<p>Now, change directory to the <code>/var/lib/clickhouse</code> and run the migration script:</p>
<pre><code>cd /var/lib/clickhouse

./migration-0.27
</code></pre>
<p>You should see output similar to this:</p>
<pre><code>2023-08-14T16:48:37.081Z	DEBUG	migrate/main.go:227	Params: clickhouse 9000 default
2023-08-14T16:48:37.130Z	INFO	migrate/main.go:142	Dropping index: method_idx
2023-08-14T16:48:37.924Z	INFO	migrate/main.go:142	Dropping index: level_idx
2023-08-14T16:48:38.645Z	INFO	migrate/main.go:142	Dropping index: container_name_idx
2023-08-14T16:48:39.368Z	INFO	migrate/main.go:142	Dropping index: telemetry_sdk_name_idx
2023-08-14T16:48:40.104Z	INFO	migrate/main.go:166	Renaming materialized column: method to attribute_string_method
2023-08-14T16:48:41.067Z	INFO	migrate/main.go:203	Create index: attribute_string_method_idx
2023-08-14T16:48:41.185Z	INFO	migrate/main.go:166	Renaming materialized column: level to attribute_string_level
2023-08-14T16:48:42.164Z	INFO	migrate/main.go:203	Create index: attribute_string_level_idx
2023-08-14T16:48:42.285Z	INFO	migrate/main.go:166	Renaming materialized column: container_name to attribute_string_container_name
2023-08-14T16:48:42.999Z	INFO	migrate/main.go:203	Create index: attribute_string_container_name_idx
2023-08-14T16:48:43.118Z	INFO	migrate/main.go:166	Renaming materialized column: telemetry_sdk_name to resource_string_telemetry_sdk_name
2023-08-14T16:48:43.823Z	INFO	migrate/main.go:203	Create index: resource_string_telemetry_sdk_name_idx
2023-08-14T16:48:43.955Z	INFO	migrate/main.go:261	Completed migration in: 6.873610753s
</code></pre>
</li>
<li>
<p>At last, clean up the binary:</p>
<pre><code>rm migration-0.27
</code></pre>
</li>
</ol>
<h3>## For Kubernetes</h3>
<pre><code>kubectl -n platform run -i -t signoz-migrate --image=signoz/migrate:0.27 --restart='Never' \
  -- -host=my-release-clickhouse -port=9000 -userName=admin -password=27ff0399-0d3a-4bd8-919d-17c2181e6fb9
</code></pre>
<p>Steps to check logs:</p>
<pre><code>kubectl -n platform logs -f signoz-migrate
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the pod before running the migration script again.</p>
<pre><code>kubectl -n platform delete pod signoz-migrate
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.27/#in-case-of-upgrade-failure
tag_set: operate, migration, upgrade-0.27
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.27-in-case-of-upgrade-failure
group_tracking_ids: docs-operate-migration-upgrade-0.27
<h2>Upgrade to v0.27 from earlier versions (Kubernetes): In case of Upgrade Failure</h2>
<ol>
<li>Note the names of fields which were not migrated. ex:- <code>telemetry_sdk_name</code></li>
<li>Exec into the clickhouse container and run <code>clickhouse client</code>.</li>
<li>Check the schema of the logs table <code>show create table signoz_logs.logs</code></li>
<li>If <code>telemetry_sdk_name</code> column or <code>telemetry_sdk_name_idx</code> index is present you can delete them</li>
</ol>
<p>For deleting index</p>
<ul>
<li><code>alter table signoz_logs.logs on cluster cluster drop index telemetry_sdk_name_idx</code></li>
</ul>
<p>For deleting column</p>
<ul>
<li><code>alter table signoz_logs.logs on cluster cluster drop column telemetry_sdk_name</code></li>
<li><code>alter table signoz_logs.distributed_logs on cluster cluster drop column telemetry_sdk_name</code></li>
</ul>
<ol start="5">
<li>Now from the UI, you can convert <code>telemetry_sdk_name</code> to selected field.</li>
<li>If you still face issue, reach out to us at <a href="https://signoz.io/slack">Slack</a>
.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.27/#command-line-interface-cli-flags
tag_set: operate, migration, upgrade-0.27
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.27-command-line-interface-cli-flags
group_tracking_ids: docs-operate-migration-upgrade-0.27
<h2>Upgrade to v0.27 from earlier versions (Kubernetes): Command-Line Interface (CLI) Flags</h2>
<p>There are some custom flags which can be enabled based on different usecases. All the flags below are <code>optional</code>.</p>
<p>Flags:</p>
<ul>
<li><code>-port</code> : Specify port of clickhouse. <code>default=9000</code></li>
<li><code>-host</code> : Specify host of clickhouse. <code>default=127.0.0.1</code></li>
<li><code>-userName</code> : Specify user name of clickhouse. <code>default=default</code></li>
<li><code>-password</code> : Specify password of clickhouse. <code>default=&quot;&quot;</code></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#overview
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-overview
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: AWS Elasticache (redis) Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key AWS Elasticache (redis) metrics and logs, view them with an out-of-the-box dashboard, and parse MySQL logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#prerequisites
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-prerequisites
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>
<p>AWS Credentials and Permissions:</p>
<ul>
<li>Set up proper AWS credentials (e.g., <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables)</li>
<li>Required IAM permissions:
<ul>
<li><code>cloudwatch:ListMetrics</code></li>
<li><code>cloudwatch:GetMetricStatistics</code></li>
<li><code>cloudwatch:GetMetricData</code></li>
<li><code>tag:GetResources</code> (if aws_tag_select feature is used)</li>
<li><code>logs:DescribeLogGroups</code></li>
<li><code>logs:FilterLogEvents</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Java Runtime Environment (JRE) version 11 or newer for the CloudWatch Exporter (Not required if using the Docker container)</p>
</li>
<li>
<p>An OpenTelemetry (OTEL) Collector (v0.88.0+):</p>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0+) if not already done</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
</ul>
</li>
<li>
<p>To collect Redis native metrics, the collector must be able to access the Redis server as a client (optional).</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#step-1-set-up-the-prometheus-cloudwatch-exporter
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-step-1-set-up-the-prometheus-cloudwatch-exporter
group_tracking_ids: docs-integrations-aws-elasticache-redis
<p>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: ## Collecting Elasticache Metrics - Step 1: Set up the Prometheus CloudWatch Exporter</p>
<ol>
<li>
<p>Download the exporter:</p>
<pre><code>curl -sLSO https://github.com/prometheus/cloudwatch_exporter/releases/download/v0.15.5/cloudwatch_exporter-0.15.5-jar-with-dependencies.jar
</code></pre>
</li>
<li>
<p>Configure the Prometheus exporter Save the following config for collecting AWS RDS metrics in a file named <code>aws-elasticache-metrics.yaml</code> and update the region key with relevant value.</p>
<hr />
<p>region: us-east-1
metrics:</p>
<ul>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CPUUtilization
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: FreeableMemory
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: NetworkBytesIn
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: NetworkBytesOut
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: NetworkPacketsIn
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: NetworkPacketsOut
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: SwapUsage
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: BytesUsedForCache
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CacheHits
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CacheMisses
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CacheHitRate
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CurrConnections
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CurrItems
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: CurrVolatileItems
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: ReplicationLag
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: ReplicationLag
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: SaveInProgress
aws_dimensions: [CacheClusterId, CacheNodeId]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: TrafficManagementActive
aws_dimensions: [CacheClusterId, CacheNodeId]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: DatabaseCapacityUsagePercentage
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: DatabaseMemoryUsagePercentage
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: EngineCPUUtilization
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: Evictions
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: GlobalDatastoreReplicationLag
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: MemoryFragmentationRatio
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/ElastiCache
aws_metric_name: MemoryFragmentationRatio
aws_dimensions: [CacheClusterId, CacheNodeId]
aws_statistics: [Sum, Average]</p>
</li>
</ul>
</li>
<li>
<p>Run the following command:</p>
<p>java -jar cloudwatch_exporter-0.15.5-jar-with-dependencies.jar 9106 aws-elasticache-metrics.yaml</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#step-2-create-the-otel-collector-config-file
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-step-2-create-the-otel-collector-config-file
group_tracking_ids: docs-integrations-aws-elasticache-redis
<p>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Step 2: Create the OTEL Collector Config File</p>
<p>Create <code>redis-metrics-collection-config.yaml</code>:</p>
<pre><code>receivers:
  redis:
    # The hostname and port of the Redis instance, separated by a colon.
    endpoint: ${env:REDIS_ENDPOINT}
    # The frequency at which to collect metrics from the Redis instance.
    collection_interval: 60s
    # The password used to access the Redis instance; must match the password specified in the requirepass server configuration option.
    password: ${env:REDIS_PASSWORD}
    # # Defines the network to use for connecting to the server. Valid Values are `tcp` or `Unix`
    # transport: tcp
    # tls:
    #   insecure: false
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/redis.crt
    #   key_file: /etc/ssl/certs/redis.key
    metrics:
      redis.maxmemory:
        enabled: true
      redis.cmd.latency:
        enabled: true

  prometheus:
    config:
      scrape_configs:
        - job_name: 'aws-cloudwatch-metrics'
          scrape_timeout: 120s
          scrape_interval: 120s
          static_configs:
            - targets: ['0.0.0.0:9106']

exporters:
  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true
  # export to SigNoz cloud
  otlp/signoz:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

service:
  pipelines:
    metrics/redis:
      receivers: [redis, prometheus]
      processors: []
      exporters: [otlp/signoz]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#step-3-set-environment-variables
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-step-3-set-environment-variables
group_tracking_ids: docs-integrations-aws-elasticache-redis
<p>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Step 3: Set Environment Variables</p>
<pre><code># The accessible endpoint where redis server is running.
# The hostname and port of the Redis instance, separated by a colon.
export REDIS_ENDPOINT=&quot;&lt;redis-server-endpoint&gt;&quot;

# The password to use for accessing redis instance
export REDIS_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#step-3-use-the-collector-config-file
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-step-3-use-the-collector-config-file
group_tracking_ids: docs-integrations-aws-elasticache-redis
<p>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Step 3: Use the Collector Config File</p>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config redis-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#collecting-elastcache-logs
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-collecting-elastcache-logs
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Collecting ElastCache Logs</h2>
<p>Use the <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Log_Delivery.html">log delivery</a> instructions to send redis logs to CloudWatch Logs</p>
<p>The log collection of RDS instance requires specifying the list of log group names. From the AWS CloudWatch console, please find the log group(s) relevant to the integration.</p>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create <code>redis-logs-collection-config.yaml</code>:</p>
<pre><code>receivers:
  awscloudwatch:
    region: us-east-1
    logs:
      poll_interval: 1m
      groups:
        named:
          # replace the following name with your log group for elasticache logs
          /aws/elasticache/:

processors:
  attributes/add_source:
    actions:
      - key: source
        value: &quot;elasticache_redis&quot;
        action: insert
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/redis-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true

service:
  pipelines:
    logs/redis:
      receivers: [awscloudwatch]
      processors: [attributes/add_source, batch]
      exporters: [otlp/redis-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<pre><code># region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Run the Collector</h3>
<p>Add to your collector run command:</p>
<pre><code>--config redis-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#connect-aws-elasticache-redis
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-connect-aws-elasticache-redis
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Connect AWS Elasticache (redis)</h2>
<p>Once you're done with setting up AWS Elasticache (redis) for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the AWS Elasticache (redis) integration.</p>
<p><img src="https://signoz.io/docs/integrations/redis" alt="Search for AWS Elasticache (redis) in Integrations tab" /></p>
<p><em>Search for AWS Elasticache (redis) in Integrations tab</em></p>
<p>Click on the <code>Connect AWS Elasticache (redis)</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your AWS Elasticache (redis) instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/docs/integrations/redis" alt="Connect AWS Elasticache (redis)" /></p>
<p><em>Connect AWS Elasticache (redis)</em></p>
<p><img src="https://signoz.io/docs/integrations/redis" alt="Listening for data from RedAWS Elasticache (redis)is" /></p>
<p><em>Listening for data from AWS Elasticache (redis)</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#aws-elasticache-redis-dashboard
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-aws-elasticache-redis-dashboard
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: AWS Elasticache (redis) dashboard</h2>
<p>Once SigNoz has started listening to your AWS Elasticache (redis) data, head over to the Dashboards tab and search for redis, this will show you a newly created dashboard which shows differnet AWS Elasticache (redis) metrics.</p>
<p><img src="https://signoz.io/docs/integrations/redis" alt="Dashboard for monitoring AWS Elasticache (redis) Metrics" /></p>
<p><em>Dashboards for monitoring AWS Elasticache (redis) Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Dashboards by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_elasticache/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-elasticache-redis/#data-collected
tag_set: integrations, aws-elasticache-redis
image_urls: 
tracking_id: docs-integrations-aws-elasticache-redis-data-collected
group_tracking_ids: docs-integrations-aws-elasticache-redis
<h2>AWS Elasticache (redis) Metrics and Logs: Collecting Elasticache Metrics: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your AWS Elasticache (redis) Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your AWS Elasticache (redis) instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/docs/integrations/redis" alt="Log attributes and metrics details for AWS Elasticache (redis)" /></p>
<p><em>Log attributes and metrics details for AWS Elasticache (redis)</em></p>
<h3>## AWS Elasticache (redis) log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Body</td>
<td>body</td>
<td>string</td>
</tr>
</tbody>
</table>
<h3>## AWS Elasticache (redis) metrics</h3>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<p>To find a complete list of metrics you can checkout this <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_elasticache/integration.json#L57">link</a> or the <strong>Data Collected</strong> tab.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/faqs/troubleshooting/
tag_set: faqs, troubleshooting
image_urls: 
tracking_id: docs-faqs-troubleshooting
group_tracking_ids: docs-faqs-troubleshooting
<h2>Troubleshooting - FAQs</h2>
<h3>## How to run SigNoz in debug mode?</h3>
<p>You might want to follow our troubleshooting docs.</p>
<p>Refer here:</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/install/troubleshooting/">SigNoz Troubleshooting Docs</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/troubleshoot">SigNoz Troubleshoot Github Repository</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=Y7OkvmuTRQ8">SigNoz YouTube Video on Troubleshooting</a></p>
</li>
</ul>
<h3>## How do I know if SigNoz is accessible from my Application?</h3>
<p>We have a troubleshooting guide to check if SigNoz is accessible from your application or not or, is the instrumentation not working or the application is not instrumented in the first place?</p>
<p>Set¬†<code>OTEL_TRACES_EXPORTER=console</code>¬†and observe. If it doesn‚Äôt output the traces to the stdout, the instrumentation is not working or your application isn‚Äôt correctly instrumented in the first place.</p>
<p>Refer here:</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/install/troubleshooting/#kubernetes">SigNoz Troubleshooting Docs</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/troubleshoot">SigNoz Troubleshoot Github Repository</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=Y7OkvmuTRQ8">SigNoz YouTube Video on Troubleshooting</a></p>
</li>
</ul>
<h3>## I have installed SigNoz on Windows Kubernetes, but I can't make it work.</h3>
<p>We don't support Microsoft Windows as of now.</p>
<h3>## I am not seeing all my services related to my application listed in the Services tab, what could be the potential reason?</h3>
<p>We rely on the semantic conventions provided by OpenTelemetry. Every unique¬†<code>service.name</code> ¬†configured and received is part of the service list. You can read more about it from our <a href="https://signoz.io/docs/userguide/metrics/#open-the-services-section">docs</a>.</p>
<h3>## My services are not showing up in the Service Map section (but present in the services and traces tab), what should I do?</h3>
<p>You might need to zoom out a bit to see your service. Also, if you‚Äôre getting started the service map shows services from the sample hotrod application. Since your services are not connected to the hotrod application, it will appear isolated.</p>
<h3>## I am trying to change the retention period of Traces but the process gets stuck everytime.</h3>
<p>The process to change the retention period is resource-intensive, especially if you have a large amount of data ingested. The TTL (Time-to-Live) status table in the Signoz SQLite database may be stuck, preventing the retention period change from completing.</p>
<p>If process is stuck then you clear TTL table to try again.</p>
<ol>
<li>You can connect to SQLite DB and clear TTL status table to allow updating retention setting. If you are using docker follow below steps:</li>
</ol>
<ul>
<li>
<p>Connect to query-service</p>
<p>docker exec -it query-service sh</p>
</li>
</ul>
<ol start="2">
<li>Run the following commands:</li>
</ol>
<ul>
<li>
<p>Install sqlite</p>
<p>apk update
apk add sqlite</p>
</li>
<li>
<p>Open sqlite with signoz.db</p>
<p>sqlite3 /var/lib/signoz/signoz.db</p>
</li>
<li>
<p>(sqlite shell) check existing ttl status</p>
<p>select * from ttl_status;</p>
</li>
<li>
<p>Delete all rows of ttl_status</p>
<p>DELETE FROM ttl_status;</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/
tag_set: alerts-management, metrics-based-alerts
image_urls: 
tracking_id: docs-alerts-management-metrics-based-alerts
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<h2>Metrics based alerts</h2>
<p>A Metric-based alert in SigNoz allows you to define conditions based on metric data and trigger alerts when these conditions are met. Here's a breakdown of the various sections and options available when configuring a Metric-based alert:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-1-define-the-metric
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-metrics-based-1.webp
tracking_id: docs-alerts-management-metrics-based-alerts-step-1-define-the-metric
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<h2>Metrics based alerts: Step 1: Define the Metric</h2>
<p>In this step, you use the <a href="https://signoz.io/docs/userguide/query-builder/#metrics-query-builder">Metrics Query Builder</a> to choose the metric to monitor. The following fields that are available in Metrics Query Builder includes:</p>
<ul>
<li>
<p><strong>Metric</strong>: A field to select the specific metric you want to monitor (e.g., CPU usage, memory utilization).</p>
</li>
<li>
<p><strong>Time aggregation</strong>: A field to select the time aggregation function to use for the metric. Learn more about <a href="../../metrics-management/types-and-aggregation/#aggregation">time aggregation</a></p>
</li>
<li>
<p><strong>WHERE</strong>: A filter field to define specific conditions for the metric. You can apply logical operators like &quot;IN,&quot; &quot;NOT IN&quot;.</p>
</li>
<li>
<p><strong>Space aggregation</strong>: A field to select the space aggregation function to use for the metric. Learn more about <a href="../../metrics-management/types-and-aggregation/#aggregation">space aggregation</a></p>
</li>
<li>
<p><strong><a href="https://signoz.io/docs/userguide/query-builder/#legend-format">Legend Format</a></strong>: An optional field to customize the legend's format in the visual representation of the alert.</p>
</li>
<li>
<p><strong>Having</strong>: Apply conditions to filter the results further based on aggregate value.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-metrics-based-1.webp" alt="Using Query Builder to the metric to monitor" /></p>
<p><em>Using Query Builder to define the metric to monitor</em></p>
<p>To know more about the functionalities of the Query Builder, checkout the <a href="https://signoz.io/docs/userguide/query-builder/">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-2-define-alert-conditions
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-metrics-based-2.webp
tracking_id: docs-alerts-management-metrics-based-alerts-step-2-define-alert-conditions
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<h2>Metrics based alerts: Step 2: Define Alert Conditions</h2>
<p>In this step, you define the specific conditions that trigger the alert and the notification frequency. The following fields are available:</p>
<ul>
<li>
<p><strong>Condition</strong>: Specify when the metric should trigger the notification</p>
<ul>
<li>Greater than (<code>&gt;</code>)</li>
<li>Less than (<code>&lt;</code>)</li>
<li>Equal to (<code>=</code>)</li>
<li>Not equal to (<code>!=</code>)</li>
</ul>
</li>
<li>
<p><strong>Occurrence</strong>: Specify how condition should be evaluated</p>
<ul>
<li>At least once</li>
<li>Every time</li>
<li>On average</li>
<li>In total</li>
</ul>
</li>
<li>
<p><strong>Evaluation window</strong>: Specify the rolling time window for the condition evaluation. The following look back options are available:</p>
<ul>
<li>Last 5 minutes</li>
<li>Last 10 minutes</li>
<li>Last 15 minutes</li>
<li>Last 1 hour</li>
<li>Last 4 hours</li>
<li>Last 1 day</li>
</ul>
</li>
<li>
<p><strong>Alert Threshold</strong>: A field to set the threshold for the alert condition.</p>
</li>
<li>
<p><strong>Threshold Unit</strong>: The unit of the threshold value. This is convenient when you want to set the threshold in a different unit than the metric. For example, the memory usage metric might be in bytes, but you might want to monitor it in MBs. Please make sure to set the Y-axis unit to indicate the unit of the metric.</p>
</li>
<li>
<p><strong>More Options</strong> :</p>
<ul>
<li>
<p><strong>Run alert every [X mins]</strong>: This option determines the frequency at which the alert is evaluated.</p>
</li>
<li>
<p><strong>Send a notification if data is missing for [X] mins</strong>: A field to specify if a notification should be sent when data is missing for a certain period.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-metrics-based-2.webp" alt="Define the alert conditions" /></p>
<p><em>Define the alert conditions</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-3-alert-configuration
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-metrics-based-3.webp
tracking_id: docs-alerts-management-metrics-based-alerts-step-3-alert-configuration
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<h2>Metrics based alerts: Step 3: Alert Configuration</h2>
<p>In this step, you set the alert's metadata, including severity, name, and description:</p>
<h3>## Severity</h3>
<p>Set the severity level for the alert (e.g., &quot;Warning&quot; or &quot;Critical&quot;).</p>
<h3>## Alert Name</h3>
<p>A field to name the alert for easy identification.</p>
<h3>## Alert Description</h3>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>You can incorporate result labels in the alert descriptions to make the alerts more informative:</p>
<p><strong>Syntax</strong>: Use <code>{{.Labels.&lt;label-name&gt;}}</code> to insert label values. Label values can be any attribute used in group by. Ensure that all <code>.</code> (dots) in attribute are converted to <code>_</code></p>
<p><strong>Example</strong>: If you have a query that has the label <code>service.name</code> then to use it in the alert description, you will use <code>{{.Labels.service_name}}</code>which creates an alert that is specific to the particular service.</p>
<h3>## Labels</h3>
<p>A field to add labels or tags for categorization. Labels should be added in key value pairs. First enter key (avoid space in key) and set value.</p>
<h3>## Notification channels</h3>
<p>A field to choose the <a href="https://signoz.io/docs/setup-alerts-notification">notification channels</a> from those configured in the Alert Channel settings.</p>
<h3>## Test Notification</h3>
<p>A button to test the alert to ensure that it works as expected.</p>
<p><img src="https://signoz.io/img/docs/alerts/alerts-metrics-based-3.webp" alt="Configure the alert" /></p>
<p><em>Setting the alert metadata</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-1-write-query-builder-query-to-define-alert-metric
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-builder.png
tracking_id: docs-alerts-management-metrics-based-alerts-step-1-write-query-builder-query-to-define-alert-metric
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: #### ## Here's a video tutorial for creating this alert: - Step 1: Write Query Builder query to define alert metric</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-builder.png" alt="metrics builder query for memory usage" /></p>
<p><em>Memory usage metric builder query</em></p>
<p>The <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver"><code>hostmetricsreceiver</code></a> creates several host system metrics, including <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/memoryscraper/documentation.md#systemmemoryusage"><code>system_memory_usage</code></a>
, which contains the memory usage for each <a href="https://www.kernel.org/doc/Documentation/filesystems/proc.txt">state</a> from <code>/proc/meminfo</code>. The states can be <code>free</code>, <code>used</code>, <code>cached</code>, etc. We want to alert when the total memory usage of a host exceeds the threshold, so the WHERE clause excludes the <code>free</code> state. We calculate the average value for each state and then sum them up by host to get the per-host memory usage.</p>
<p>‚úÖ Info</p>
<p>Remember to set the unit of the <code>y-axis</code> to bytes, as that is the unit of the mentioned metric.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-2-set-alert-conditions
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-builder-condition.png
tracking_id: docs-alerts-management-metrics-based-alerts-step-2-set-alert-conditions
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: Step 2: Set alert conditions</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-builder-condition.png" alt="metrics builder query for memory usage" /></p>
<p><em>Memory usage alert condition</em></p>
<p>The condition is set to trigger a notification if the per-minute memory usage exceeds the threshold of 400 MB at least once in the last five minutes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#2-alert-when-memory-usage-for-host-goes-above-70
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-percentage-builder.png, https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-percentage-builder-condition.png
tracking_id: docs-alerts-management-metrics-based-alerts-2-alert-when-memory-usage-for-host-goes-above-70
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: 2. Alert when memory usage for host goes above 70%</p>
<p>You might want to alert based on the percentage rather than a fixed threshold. There are two ways to get the percentage: the convenient option is when the usage percentage is reported directly by the source, or when the source only sends the exact usage in bytes and you need to derive the percentage yourself. This example demonstrates how to derive the percentage from the original bytes metric.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-percentage-builder.png" alt="metrics builder query for memory usage" /></p>
<p><em>Memory usage percentage query</em></p>
<p>We use a formula to derive the percentage value from the exact memory usage in bytes. In the example, query <code>A</code> calculates the per-host memory usage, while query <code>B</code>, as shown in the image, doesn't have any WHERE clause filter, thus providing the total memory available. The formula for <code>A</code>/<code>B</code> is interpreted as (memory usage in bytes) / (total memory available in bytes). We set the unit of the y-axis to Percent (0.0 - 1.0) to match the result of the formula.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-percentage-builder-condition.png" alt="metrics builder query for memory usage" /></p>
<p><em>Memory usage percentage condition</em></p>
<p>The condition is set to trigger a notification if the per-minute memory usage exceeds the threshold of 70% all the times in the last five minutes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#3-alert-when-the-error-percentage-for-an-endpoint-exceeds-5
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/error-percentage-builder.png, https://signoz.io/img/docs/product-features/alerts/metrics/error-percentage-builder-condition.png
tracking_id: docs-alerts-management-metrics-based-alerts-3-alert-when-the-error-percentage-for-an-endpoint-exceeds-5
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: 3. Alert when the error percentage for an endpoint exceeds 5%</p>
<p>SigNoz creates a metric <code>signoz_calls_total</code> from the trace data. The default attributes of the metric are <code>service_name</code>, <code>operation</code>, <code>span_kind</code>, <code>status_code</code>, and <code>http_status_code</code>. There is no separate metric for counting errors; instead, the <code>status_code</code> attribute is used to determine if a request counts as an error. This example demonstrates how to calculate the error percentage and alert on it.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/error-percentage-builder.png" alt="metrics builder query for error percentage" /></p>
<p><em>Error percentage query</em></p>
<p>We use a formula to derive the error percentage from the total calls metric. In the example, query <code>A</code> calculates the per-endpoint error rate, while query <code>B</code>, as shown in the image, doesn't have any WHERE clause filter for <code>status_code</code>, thus providing the per-endpoint total request rate. The formula for <code>A</code>/<code>B</code> is interpreted as (error request rate) / (total request rate), which gives the error percentage per endpoint. We set the unit of the y-axis to Percent (0.0 - 1.0) to match the result of the formula.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/error-percentage-builder-condition.png" alt="metrics builder query for error percentage" /></p>
<p><em>Error percentage condition</em></p>
<p>The condition is set to trigger a notification if the per-minute error percentage exceeds the threshold of 5% all the times in the last five minutes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#4-alert-when-p95-latency-for-an-endpoint-is-above-1200-ms
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/p95-latency-builder.png, https://signoz.io/img/docs/product-features/alerts/metrics/p95-latency-builder-condition.png
tracking_id: docs-alerts-management-metrics-based-alerts-4-alert-when-p95-latency-for-an-endpoint-is-above-1200-ms
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: 4. Alert when P95 latency for an endpoint is above 1200 ms</p>
<p>SigNoz creates a metric <code>signoz_latency_bucket</code> from the trace data. The default attributes of the metric are <code>service_name</code>, <code>operation</code>, <code>span_kind</code>, <code>status_code</code>, and <code>http_status_code</code>. This example demonstrates how to calculate the P95 latency for an endpoint and alert on it.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/p95-latency-builder.png" alt="metrics builder query for latency" /></p>
<p><em>Endpoint latency query</em></p>
<p>We use the P95 aggregation, which gives the 95th-percentile request latency per endpoint. We set the unit of the y-axis to milliseconds to match the unit of the metric.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/p95-latency-builder-condition.png" alt="metrics builder query for latency" /></p>
<p><em>Endpoint latency condition</em></p>
<p>The condition is set to trigger a notification if the per-minute P95 latency exceeds the threshold of 1200 ms at any time in the last five minutes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.23/
tag_set: operate, migration, upgrade-0.23
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.23
group_tracking_ids: docs-operate-migration-upgrade-0.23
<h2>Upgrade to v0.23 from earlier versions (Kubernetes)</h2>
<p>In the SigNoz version <code>&gt;=v0.23</code> i.e. SigNoz chart version <code>&gt;=0.19.0</code>, <a href="https://github.com/Altinity/clickhouse-operator">altinity/clickhouse-operator</a> is upgraded from version v0.19.1 to v0.21.2.</p>
<p>This upgrade brings a lot of improvements in the ClickHouse cluster management with the updated operator. It resolves the previously seen <code>Out of Sync</code> issue of ArgoCD. Henceforth, official support for ArgoCD by SigNoz is added.</p>
<p>No special steps are required for <strong>Docker Standalone</strong> and <strong>Docker Swarm</strong> deployments.</p>
<h2>Steps to Upgrade</h2>
<hr />
<p>In order for keeping the ClickHouse Operator and ClickHouse cluster in sync, please run the following commands before upgrading SigNoz chart to <code>&gt;=v0.19.0</code>.</p>
<pre><code>SIGNOZ_CHART_VERSION=&quot;0.19.0&quot;

kubectl apply \
    -f https://github.com/SigNoz/charts/raw/signoz-${SIGNOZ_CHART_VERSION}/charts/clickhouse/crds/clickhouseinstallations.clickhouse.altinity.com.yaml \
    -f https://github.com/SigNoz/charts/raw/signoz-${SIGNOZ_CHART_VERSION}/charts/clickhouse/crds/clickhouseinstallationtemplates.clickhouse.altinity.com.yaml \
    -f https://github.com/SigNoz/charts/raw/signoz-${SIGNOZ_CHART_VERSION}/charts/clickhouse/crds/clickhouseoperatorconfigurations.clickhouse.altinity.com.yaml
</code></pre>
<p>You may see a warning like below:</p>
<pre><code>Warning: resource customresourcedefinitions/clickhouseinstallations.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/clickhouseinstallations.clickhouse.altinity.com configured

Warning: resource customresourcedefinitions/clickhouseinstallationtemplates.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/clickhouseinstallationtemplates.clickhouse.altinity.com configured

Warning: resource customresourcedefinitions/clickhouseoperatorconfigurations.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/clickhouseoperatorconfigurations.clickhouse.altinity.com configured
</code></pre>
<p>This is expected and you can safely ignore it.</p>
<h2>Upgrade to v0.23</h2>
<hr />
<p>Now, you proceed with the upgrade to <code>&gt;=v0.23</code> as per the <a href="https://signoz.io/docs/operate/kubernetes/#upgrade-signoz-cluster">usual instructions for Kubernetes</a></p>
<p>‚úÖ Info</p>
<p>If you had single node ClickHouse cluster, you might see some downtime during the upgrade. This is because the ClickHouse cluster is recreated with the new operator. The downtime is usually less than 5 minutes.</p>
<p>However, telemetry data should be queued and ingested once the ClickHouse cluster is up.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/
tag_set: frontend-monitoring, web-vitals
image_urls: 
tracking_id: docs-frontend-monitoring-web-vitals
group_tracking_ids: docs-frontend-monitoring-web-vitals
<h2>Web Vitals Monitoring</h2>
<p>This documentation provides a step-by-step guide to setting up web vitals monitoring using SigNoz and OpenTelemetry.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/#overview
tag_set: frontend-monitoring, web-vitals
image_urls: 
tracking_id: docs-frontend-monitoring-web-vitals-overview
group_tracking_ids: docs-frontend-monitoring-web-vitals
<h2>Web Vitals Monitoring: Overview</h2>
<p>Web vitals are essential metrics to measure the user experience on your website. SigNoz allows you to monitor these vitals seamlessly using OpenTelemetry. The key web vitals are:</p>
<ul>
<li>Largest Contentful Paint (LCP)</li>
<li>First Input Delay (FID)</li>
<li>Cumulative Layout Shift (CLS)</li>
<li>Time to First Byte (TTFB)</li>
<li>Total Blocking Time (TBT)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/#prerequisites
tag_set: frontend-monitoring, web-vitals
image_urls: 
tracking_id: docs-frontend-monitoring-web-vitals-prerequisites
group_tracking_ids: docs-frontend-monitoring-web-vitals
<h2>Web Vitals Monitoring: Prerequisites</h2>
<ul>
<li><a href="https://signoz.io/teams/">SigNoz Cloud</a> account</li>
<li>A web application where you want to monitor web vitals</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/#step-1-setup-otel-collector
tag_set: frontend-monitoring, web-vitals
image_urls: 
tracking_id: docs-frontend-monitoring-web-vitals-step-1-setup-otel-collector
group_tracking_ids: docs-frontend-monitoring-web-vitals
<p>Web Vitals Monitoring: Setup: ## Setup - Step 1: Setup OTel Collector</p>
<p>Install Otel collector binary using <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">these instructions</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/#step-2-install-dependencies
tag_set: frontend-monitoring, web-vitals
image_urls: 
tracking_id: docs-frontend-monitoring-web-vitals-step-2-install-dependencies
group_tracking_ids: docs-frontend-monitoring-web-vitals
<p>Web Vitals Monitoring: Setup: Step 2: Install dependencies</p>
<pre><code>npm install web-vitals
npm install @opentelemetry/api
npm install @opentelemetry/sdk-trace-web
npm install @opentelemetry/auto-instrumentations-web
npm install @opentelemetry/exporter-trace-otlp-http
npm install @opentelemetry/exporter-metrics-otlp-http
npm install @opentelemetry/sdk-metrics
npm install @opentelemetry/resources
npm install @opentelemetry/semantic-conventions
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/#step-3-create-an-instrumentation-file
tag_set: frontend-monitoring, web-vitals
image_urls: 
tracking_id: docs-frontend-monitoring-web-vitals-step-3-create-an-instrumentation-file
group_tracking_ids: docs-frontend-monitoring-web-vitals
<p>Web Vitals Monitoring: Setup: Step 3: Create an instrumentation file</p>
<p>This file (e.g., <code>instrument.ts</code>) is needed to setup the <code>MeterProvider</code> which is used to create custom metrics.</p>
<pre><code>import { MeterProvider, PeriodicExportingMetricReader } from '@opentelemetry/sdk-metrics';
import { OTLPMetricExporter } from '@opentelemetry/exporter-metrics-otlp-http';
import { Resource } from '@opentelemetry/resources';
import { opentelemetry } from '@opentelemetry/api';

// Define your resource, e.g., service name, environment.
const resource = new Resource({
'service.name': 'yourServiceName',
});

// Create a metric reader with OTLP exporter configured to send metrics to a local collector.
const metricReader = new PeriodicExportingMetricReader({
exporter: new OTLPMetricExporter({
    url: 'http://127.0.0.1:4318/v1/metrics',
}),
exportIntervalMillis: 10000, // Export metrics every 10 seconds.
});

// Initialize a MeterProvider with the above configurations.
const myServiceMeterProvider = new MeterProvider({
resource,
readers: [metricReader],
});

// Set the initialized MeterProvider as global to enable metric collection across the app.
opentelemetry.metrics.setGlobalMeterProvider(myServiceMeterProvider);
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/#step-4-capture-web-vitals
tag_set: frontend-monitoring, web-vitals
image_urls: 
tracking_id: docs-frontend-monitoring-web-vitals-step-4-capture-web-vitals
group_tracking_ids: docs-frontend-monitoring-web-vitals
<p>Web Vitals Monitoring: Setup: Step 4: Capture Web Vitals</p>
<p>Use the web-vitals library to capture key performance metrics like CLS, FID, LCP, TTFB, and FCP. These metrics are captured through callback functions that you define.</p>
<pre><code>import { getCLS, getFCP, getFID, getLCP, getTTFB } from 'web-vitals';

function yourCustomCallback(metric) {
  // Send the metric to your analytics server or perform any custom logic
}

getCLS(yourCustomCallback);
getFID(yourCustomCallback);
getLCP(yourCustomCallback);
getTTFB(yourCustomCallback);
getFCP(yourCustomCallback);
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/#step-5-export-web-vitals
tag_set: frontend-monitoring, web-vitals
image_urls: 
tracking_id: docs-frontend-monitoring-web-vitals-step-5-export-web-vitals
group_tracking_ids: docs-frontend-monitoring-web-vitals
<p>Web Vitals Monitoring: Setup: Step 5: Export Web Vitals</p>
<p>Using OpenTelemetry, create Observable Async Gauges to periodically observe and export these metrics. This ensures that the captured web vitals are correctly recorded and sent to SigNoz for monitoring.</p>
<pre><code>import { metrics } from '@opentelemetry/api';

const meter = metrics.getMeter('web-vitals');
const lcp = meter.createObservableGauge('lcp');
const cls = meter.createObservableGauge('cls');
const fid = meter.createObservableGauge('fid');
const ttfb = meter.createObservableGauge('ttfb');
const fcp = meter.createObservableGauge('fcp');

function sendToAnalytics(metric) {
  switch (metric.name) {
    case 'LCP': {
      lcp.addCallback((result) =&gt; {
        result.observe(metric.value);
      });
      break;
    }
    case 'CLS': {
      cls.addCallback((result) =&gt; {
        result.observe(metric.value);
      });
      break;
    }
    case 'FID': {
      fid.addCallback((result) =&gt; {
        result.observe(metric.value);
      });
      break;
    }
    case 'TTFB': {
      ttfb.addCallback((result) =&gt; {
        result.observe(metric.value);
      });
      break;
    }
    case 'FCP': {
      fcp.addCallback((result) =&gt; {
        result.observe(metric.value);
      });
      break;
    }
    default: {
      console.log('unexpected metric name');
    }
  }
}

getCLS(sendToAnalytics);
getFID(sendToAnalytics);
getLCP(sendToAnalytics);
getTTFB(sendToAnalytics);
getFCP(sendToAnalytics);
</code></pre>
<p>This code captures web vitals (LCP, CLS, FID, TTFB, FCP) using the web-vitals library and logs these metrics, setting up observable gauges with OpenTelemetry to record the values for each metric. It defines a callback function, sendToAnalytics, that processes and observes each metric value, ensuring they are monitored appropriately.</p>
<p>To better understand how this is used, you can check out <a href="https://github.com/SigNoz/signoz/pull/4579/files#diff-f8c6adf20de182a86841534a8c156a960598635717c63b17a4be45ec8d340193">this GitHub link</a> which shows how web-vitals are used for monitoring SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/frontend-monitoring/web-vitals/#step-6-setup-dashboard-and-alerts
tag_set: frontend-monitoring, web-vitals
image_urls: https://signoz.io/img/events/launch-week-1/web-vitals.webp, https://signoz.io/img/events/launch-week-1/thresholds-chart.webp
tracking_id: docs-frontend-monitoring-web-vitals-step-6-setup-dashboard-and-alerts
group_tracking_ids: docs-frontend-monitoring-web-vitals
<p>Web Vitals Monitoring: Setup: Step 6: Setup Dashboard and alerts</p>
<p>You can create a <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">custom dashboard</a> and <a href="https://signoz.io/docs/setup-alerts-notification/">set alerts</a> to monitor your core web-vitals and be notified about anything critical.</p>
<p><img src="https://signoz.io/img/events/launch-week-1/web-vitals.webp" alt="Core Web Vitals Monitoring in SigNoz" /></p>
<p><em>Core Web Vitals Monitoring in SigNoz</em></p>
<p>One of the cool feature of SigNoz charts is that you can create thresholds in the chart with different color coding. For example, in the below graph there are three thresholds for <code>good</code>, <code>needs improvement</code>, and <code>critical</code> level.</p>
<p><img src="https://signoz.io/img/events/launch-week-1/thresholds-chart.webp" alt="Thresholds in SigNoz" /></p>
<p><em>Thresholds provide a visual cue to how your performance is</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/traefik-observability/
tag_set: tutorial, traefik-observability
image_urls: 
tracking_id: docs-tutorial-traefik-observability
group_tracking_ids: docs-tutorial-traefik-observability
<h2>Traefik Observability</h2>
<h3>## Overview</h3>
<p>In this tutorial, we will see how to export metrics and traces of Traefik to SigNoz. Visualizing Traefik metrics and traces will help you to understand the performance of services running behind Traefik and troubleshoot issues.</p>
<h3>## Prerequisites</h3>
<ul>
<li>Traefik v3.0 or above</li>
<li>Must have SigNoz running. You can follow the <a href="https://signoz.io/docs/install/">installation guide</a> to install SigNoz.</li>
<li>Must have SigNoz OtelCollector accessible from Traefik</li>
<li>If you don‚Äôt already have a SigNoz Cloud account, you can sign up <a href="https://signoz.io/teams/">here</a>
.</li>
</ul>
<h2>Export Traefik Metrics and Traces to SigNoz</h2>
<hr />
<p>Based on how you are running SigNoz (e.g. SigNoz Cloud, in an independent VM or Kubernetes cluster), you have to provide the address to send data from the above receivers.</p>
<p>SigNoz CloudSelf-Host</p>
<p>In this section, we will see how to export Traefik metrics and traces to SigNoz Cloud.</p>
<p>For metrics, we will have to set the following CLI flags in Traefik:</p>
<ul>
<li><code>--metrics.openTelemetry=true</code></li>
<li><code>--metrics.openTelemetry.grpc=true</code></li>
<li><code>--metrics.openTelemetry.address=ingest.{region}.signoz.cloud:443</code></li>
<li><code>--metrics.openTelemetry.insecure=false</code></li>
<li><code>--metrics.openTelemetry.headers.signoz-access-token=SIGNOZ_INGESTION_KEY</code></li>
</ul>
<p>For traces, we will have to set the following CLI flags in Traefik:</p>
<ul>
<li><code>--tracing.openTelemetry=true</code></li>
<li><code>--tracing.openTelemetry.grpc=true</code></li>
<li><code>--tracing.openTelemetry.address=ingest.{region}.signoz.cloud:443</code></li>
<li><code>--tracing.openTelemetry.insecure=false</code></li>
<li><code>--tracing.openTelemetry.headers.signoz-access-token=SIGNOZ_INGESTION_KEY</code></li>
</ul>
<p>We will take an example <code>docker-compose.yaml</code> with a simple <code>hello-app</code> running behind Traefik.</p>
<pre><code># docker-compose.yaml {13-14,18-19}

version: '3'
services:
  reverse-proxy:
    image: traefik:v3.0.0-beta3
    extra_hosts:
      - signoz:host-gateway
    command:
      - --api.insecure=true
      - --providers.docker
      - --metrics.openTelemetry=true
      - --metrics.openTelemetry.grpc=true
      - --metrics.openTelemetry.insecure=false
      - --metrics.openTelemetry.address=ingest.{region}.signoz.cloud:443
      - --metrics.openTelemetry.headers.signoz-access-token=SIGNOZ_INGESTION_KEY
      - --tracing.openTelemetry=true
      - --tracing.openTelemetry.grpc=true
      - --tracing.openTelemetry.insecure=false
      - --tracing.openTelemetry.address=ingest.{region}.signoz.cloud:443
      - --tracing.openTelemetry.headers.signoz-access-token=SIGNOZ_INGESTION_KEY
    ports:
      - &quot;80:80&quot;
      - &quot;8080:8080&quot;
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
  hello-app:
    image: gcr.io/google-samples/hello-app:2.0
    environment:
      - PORT=8080
    labels:
      traefik.enable: true
      traefik.http.routers.hello-app.rule: Host(`hello-app.docker.localhost`)
      traefik.http.routers.hello-app.entrypoints: http
      traefik.http.routers.hello-app.service: hello-app
</code></pre>
<p>üìù Note</p>
<ul>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the one provided by SigNoz.</li>
<li>Replace <code>{region}</code> with the region of your SigNoz Cloud instance.</li>
</ul>
<p>Refer to the table below for the region-specific endpoints:</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>To start the services, run the following command:</p>
<pre><code>docker compose up -d
</code></pre>
<p>We will visit the <code>hello-app</code> service to generate some traffic.</p>
<pre><code>curl -H Host:hello-app.docker.localhost http://127.0.0.1
</code></pre>
<p>Now, we will visit the SigNoz UI to see the traces and metrics.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Ftutorial%2Ftraefik-traces.webp&amp;w=3840&amp;q=75" alt="Traefik Traces" /></p>
<p>To plot metrics generated from <strong>Traefik</strong>, follow the instructions given in the docs <a href="https://signoz.io/docs/userguide/dashboards/">here</a>.</p>
<p>Check out the <a href="#list-of-metrics">List of metrics from Traefik</a>.</p>
<h2>List of Metrics</h2>
<hr />
<h3>## Traefik Metrics</h3>
<ul>
<li>traefik_config_last_reload_success</li>
<li>traefik_config_reloads_total</li>
<li>traefik_entrypoint_request_duration_seconds_bucket</li>
<li>traefik_entrypoint_request_duration_seconds_count</li>
<li>traefik_entrypoint_request_duration_seconds_sum</li>
<li>traefik_entrypoint_requests_bytes_total</li>
<li>traefik_entrypoint_requests_total</li>
<li>traefik_entrypoint_responses_bytes_total</li>
<li>traefik_open_connections</li>
<li>traefik_service_request_duration_seconds_bucket</li>
<li>traefik_service_request_duration_seconds_count</li>
<li>traefik_service_request_duration_seconds_sum</li>
<li>traefik_service_requests_bytes_total</li>
<li>traefik_service_requests_total</li>
<li>traefik_service_responses_bytes_total</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/ingestion/signoz-cloud/overview/#overview
tag_set: ingestion, signoz-cloud, overview
image_urls: 
tracking_id: docs-ingestion-signoz-cloud-overview-overview
group_tracking_ids: docs-ingestion-signoz-cloud-overview
<h2>Ingestion Overview: Ingestion Overview - Overview</h2>
<p>SigNoz is designed with an OpenTelemetry-first approach, natively supporting the <a href="https://github.com/open-telemetry/opentelemetry-proto/blob/main/docs/specification.md">OpenTelemetry Protocol</a>. In addition to OpenTelemetry compatibility, SigNoz also provides custom in-house endpoints for enhanced functionality.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/ingestion/signoz-cloud/overview/#get-started
tag_set: ingestion, signoz-cloud, overview
image_urls: 
tracking_id: docs-ingestion-signoz-cloud-overview-get-started
group_tracking_ids: docs-ingestion-signoz-cloud-overview
<h2>Ingestion Overview: Get Started</h2>
<ul>
<li>If you haven't already done so, sign up for a <a href="https://signoz.io/teams/">SigNoz Cloud account</a>
.</li>
<li>After logging into your SigNoz Cloud account, create an <a href="/docs/ingestion/signoz-cloud/keys/">Ingestion Key</a>
.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/ingestion/signoz-cloud/overview/#endpoint
tag_set: ingestion, signoz-cloud, overview
image_urls: 
tracking_id: docs-ingestion-signoz-cloud-overview-endpoint
group_tracking_ids: docs-ingestion-signoz-cloud-overview
<h2>Ingestion Overview: Endpoint</h2>
<p>Based on your SigNoz Cloud environment, you must configure your applications to use the relevant endpoint and port from the table below:</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr>
<td>United States (US)</td>
<td>ingest.us.signoz.cloud</td>
<td>443</td>
</tr>
<tr>
<td>Europe (EU)</td>
<td>ingest.eu.signoz.cloud</td>
<td>443</td>
</tr>
<tr>
<td>India (IN)</td>
<td>ingest.in.signoz.cloud</td>
<td>443</td>
</tr>
</tbody>
</table>
<h4>## Notes</h4>
<ul>
<li>SigNoz uses <strong>port 443 for both OTLP/HTTP and OTLP/gRPC protocols</strong>, consolidating traffic through a single port. This approach diverges from the conventional use of separate ports (4317 for OTLP/gRPC and 4318 for OTLP/HTTP) typically seen in OpenTelemetry documentation.</li>
</ul>
<h3>## Language SDK Configuration</h3>
<p>Configuration typically involves setting the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> environment variable. Refer to the SigNoz <a href="/docs/instrumentation/">instrumentation docs</a> for language-specific details.</p>
<p>If you are using the signal agnostic environment variable (<code>OTEL_EXPORTER_OTLP_ENDPOINT</code>), you can simply set <code>OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.&lt;region&gt;.signoz.cloud:443</code> and the exporter should append the appropriate path for the signal type (such as v1/traces or v1/metrics). Similarly, if you are using OTLP/GRPC, setting the variable <code>OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.&lt;region&gt;.signoz.cloud:443</code> (replace with your endpoint from the above table).</p>
<h3>## OpenTelemetry Collector Configuration</h3>
<p>Configure the OpenTelemetry <a href="https://opentelemetry.io/docs/specs/otel/protocol/exporter/">exporter</a> in your collector configuration file to point to the appropriate SigNoz endpoint.</p>
<pre><code>exporters:
  &lt;otlp|otlphttp&gt;:
    endpoint: &lt;endpoint&gt;
</code></pre>
<p>‚ö†Ô∏è Deprecation Notice</p>
<p>The following endpoints are deprecated:</p>
<ul>
<li>ingest.signoz.io</li>
<li>ingest-in.signoz.io</li>
<li>ingest-eu.signoz.io</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/ingestion/signoz-cloud/overview/#authentication-methods
tag_set: ingestion, signoz-cloud, overview
image_urls: 
tracking_id: docs-ingestion-signoz-cloud-overview-authentication-methods
group_tracking_ids: docs-ingestion-signoz-cloud-overview
<h2>Ingestion Overview: Authentication Methods</h2>
<p>All endpoints are protected by header based key authentication. The header <code>signoz-access-token</code> must be set to the ** value of your ingestion key** in order to successfully ingest data. Additionally, SigNoz also supports basic authentication which means that the header <code>authorization</code> may also additionally be used to ingest data.</p>
<h3>## Language SDK Configuration</h3>
<p>The mechanism to configure headers will vary, but OpenTelemetry language SDKs generally support setting the <code>OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;key&gt;</code> environment variable. Refer to the SigNoz <a href="/docs/instrumentation/">instrumentation docs</a> for language-specific details.</p>
<h3>## OpenTelemetry Collector Configuration</h3>
<p>Configure the OpenTelemetry <a href="https://opentelemetry.io/docs/specs/otel/protocol/exporter/">exporter</a> in your collector configuration with the appropriate header values.</p>
<pre><code>exporters:
  &lt;otlp|otlphttp&gt;:
    headers:
      signoz-access-token: &lt;key&gt;
</code></pre>
<p>‚ö†Ô∏è Deprecation Notice</p>
<p>The following ways of authentication are deprecated:</p>
<ul>
<li>Using the <code>signoz-access-token</code> header with the value set <code>Bearer &lt;key&gt;</code> to your Ingestion key.</li>
<li>Using the <code>signoz-access-token</code> header for basic authentication.</li>
<li>Using the <code>authorization</code> header with the value set <code>Bearer &lt;key&gt;</code> to your Ingestion key.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/ingestion/signoz-cloud/overview/#important-considerations
tag_set: ingestion, signoz-cloud, overview
image_urls: 
tracking_id: docs-ingestion-signoz-cloud-overview-important-considerations
group_tracking_ids: docs-ingestion-signoz-cloud-overview
<h2>Ingestion Overview: Important Considerations</h2>
<ul>
<li>
<p><strong>Payload Size Limit</strong>: In order to send data to SigNoz, your payloads must be smaller than the <strong>16MB maximum payload size</strong>. Larger payloads will be rejected with an error status code.</p>
</li>
<li>
<p><strong>Cross-Origin Resource Sharing (CORS)</strong>: CORS is enabled on all SigNoz endpoints if you wish to send telemetry from your frontend applications.</p>
</li>
<li>
<p><strong>Timeouts</strong>: Requests generally take longer when payloads are larger or networks are slower. If your application produces large payloads due to high telemetry volume or long export intervals, you may need to increase the default timeout settings to avoid export errors.</p>
</li>
<li>
<p><strong>Compression</strong>: Enable compression while sending OTLP data. SigNoz supports gzip compression on all OTLP endpoints. Note that this compression does not apply to any custom endpoints supported by SigNoz.</p>
</li>
<li>
<p><strong>Retries</strong>: Implement retry mechanisms to handle transient errors and reduce data loss.</p>
</li>
</ul>
<h3>## Language SDK Configuration</h3>
<ul>
<li>
<p>OpenTelemetry SDKs generally support setting the following environment variables (see <a href="https://opentelemetry.io/docs/specs/otel/configuration/sdk-environment-variables/">OpenTelemetry docs</a> for more info):</p>
<ul>
<li><code>OTEL_BSP_*</code> for spans</li>
<li><code>OTEL_METRIC_EXPORT_*</code> for metrics</li>
<li><code>OTEL_BLRP_*</code> for logs</li>
</ul>
</li>
<li>
<p>Increase the default timeout settings using the <code>OTEL_EXPORTER_OTLP_TIMEOUT</code> environment variable. The value should be set in milliseconds. Example: <code>OTEL_EXPORTER_OTLP_TIMEOUT=30000</code>. This sets the timeout to 30 seconds.</p>
</li>
<li>
<p>Configure compression using the <code>OTEL_EXPORTER_OTLP_COMPRESSION=gzip</code> environment variable.</p>
</li>
<li>
<p>For retries, configuration methods vary by language and SDK. Some SDKs support environment variables (e.g., Java: <code>OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED=true</code>). Programmatic configuration may be necessary in some cases. Refer to your specific SDK's documentation for detailed retry configuration options</p>
</li>
</ul>
<h3>## OpenTelemetry Collector Configuration</h3>
<p>Configure the OpenTelemetry <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/otlpexporter/README.md">otlpexporter</a> or <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/otlphttpexporter/README.md">otlphttpexporter</a> in your collector configuration with the appropriate values. The defaults of these exporters can be found below:</p>
<pre><code>exporters:
  &lt;otlp|otlphttp&gt;:
    ...
    timeout: 5s
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 1000
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/faqs/instrumentation/
tag_set: faqs, instrumentation
image_urls: 
tracking_id: docs-faqs-instrumentation
group_tracking_ids: docs-faqs-instrumentation
<h2>Instrumentation - FAQs</h2>
<p>For instrumentation instructions, follow our docs <a href="https://signoz.io/docs/instrumentation/">here</a>.</p>
<h3>## What are all the ports that will be used by a running instance of SigNoz and its associated dependencies so that I can check with my application ports to avoid conflicts.</h3>
<p>Ensure that the ports¬†<code>8080,</code> <code>3301</code>,¬†<code>4317</code>and¬†<code>4318</code>are open on the machine where you install SigNoz.</p>
<h3>## Do I still use OpenTelemetry SDKs to instrument ourselves and just use SigNoz as an analysis backend? Do I have to use SigNoz for instrumentation too?</h3>
<p>You have to instrument your application using OpenTelemetry SDKs.</p>
<p>Link for Instrumentation using SigNoz - <a href="https://signoz.io/docs/instrumentation">https://signoz.io/docs/instrumentation</a></p>
<p>Once your application is instrumented, you can point your OpenTelemetry exporter to send data to SigNoz installation. By defualt, SigNoz listens on port <code>4317</code> and <code>4318</code> for incoming telemetry data.</p>
<h3>## Which all languages/tech stack is currently supported with SigNoz for instrumentation?</h3>
<p>Python, JavaScript, Java, .NET, Ruby, Rust, Go, Elixir/Erlang, PHP.</p>
<p>Find the documentation for them here - <a href="https://signoz.io/docs/instrumentation/">https://signoz.io/docs/instrumentation/</a></p>
<h3>## Can I use auto instrumentation for my application(s)?</h3>
<p>OpenTelemetry and thus SigNoz, currently supports JavaScript, Java, Ruby, .NET, and Python modules for auto instrumentation.</p>
<p>Refer here: <a href="https://opentelemetry.io"></a>
<a href="https://opentelemetry.io/">https://opentelemetry.io/</a></p>
<p>If your module is auto-instrumentation is supported, you don't need code level changes but just need to add some more libraries in your application.</p>
<h3>## I am confused about <code>&lt;IP of SigNoz&gt;</code> can you provide some examples?</h3>
<p>IP of SigNoz means the host IP OR the IP of the instance where you have installed SigNoz, it could be either your local machine, the cloud providers, or VMs.</p>
<ul>
<li>IP is <code>localhost</code> or <code>127.0.0.1</code> - If it's installed on your local machine</li>
<li>IP is <code>xxx.xx.xx.xx</code>, where x is the public IP address of your AWS, Azure, GCP, or other cloud providers.</li>
<li>IP is<code>http://example.com</code> if SigNoz is hosted on your custom domain.</li>
</ul>
<p>So, to summarize, the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> would look something like this.</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot;

OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://127.0.0.1:4317&quot;

OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://netflix.com:4317&quot;

OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://xxx.xx.xx.xx:4317&quot;
</code></pre>
<p>You can also refer to this <a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> for help.</p>
<h3>## Does SigNoz have some agents for other servers from where I might want to collect data?</h3>
<p>You need to use OpenTelemetry Collector in your application servers to send host metrics data to SigNoz.</p>
<p>Refer here: <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/
tag_set: alerts-management, log-based-alerts
image_urls: 
tracking_id: docs-alerts-management-log-based-alerts
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts</h2>
<p>A Log-based alert allows you to define conditions based on log data, triggering alerts when these conditions are met. Here's a breakdown of the various sections and options available when configuring a Log-based alert:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/#step-1-define-the-log-metric
tag_set: alerts-management, log-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-log-based-1.webp
tracking_id: docs-alerts-management-log-based-alerts-step-1-define-the-log-metric
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts: Step 1: Define the Log Metric</h2>
<p>In this step, you use the <a href="https://signoz.io/docs/userguide/query-builder/#logs-and-traces-query-builder">Logs Query Builder</a> to apply filters and operations on your logs to define conditions which triggers log based alert Some of the fields that are available in Logs Query Builder includes:</p>
<ul>
<li>
<p><strong>Logs</strong>: A field to filter the specific log data to monitor.</p>
</li>
<li>
<p><strong>Aggregate Attribute</strong>: Allows you to select how the log data should be aggregated (e.g., &quot;Count&quot;).</p>
</li>
<li>
<p><strong>Group by</strong>: Provides options to group log data by various attributes, such as &quot;service.name&quot;, &quot;method&quot; or custom attributes.</p>
</li>
<li>
<p><strong><a href="https://signoz.io/docs/userguide/query-builder/#legend-format">Legend Format</a></strong>: Lets you define the format for the legend in the visual representation of the alert.</p>
</li>
<li>
<p><strong>Having</strong>: Apply conditions to filter the results further based on aggregate value.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-log-based-1.webp" alt="Using Query Builder to perform operations on your logs" /></p>
<p><em>Using Query Builder to perform operations on your logs</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/#step-2-define-alert-conditions
tag_set: alerts-management, log-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-log-based-2.webp
tracking_id: docs-alerts-management-log-based-alerts-step-2-define-alert-conditions
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts: Step 2: Define Alert Conditions</h2>
<p>In this step, you define the specific conditions for triggering the alert, as well as the frequency of checking those conditions:</p>
<ul>
<li>
<p><strong>Send a notification when [A] is [above/below] the threshold [in total] during the last [X mins]</strong>: A template to set the threshold and define when the alert condition should be checked.</p>
</li>
<li>
<p><strong>Alert Threshold</strong>: A field to specify the threshold value for the alert condition.</p>
</li>
<li>
<p><strong>More Options</strong> :</p>
<ul>
<li>
<p><strong>Run alert every [X mins]</strong>: This option determines the frequency at which the alert condition is checked and notifications are sent.</p>
</li>
<li>
<p><strong>Send a notification if data is missing for [X] mins</strong>: A field to specify if a notification should be sent when data is missing for a certain period.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-log-based-2.webp" alt="Define the alert conditions" /></p>
<p><em>Define the alert conditions</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/#step-3-alert-configuration
tag_set: alerts-management, log-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-log-based-3.webp
tracking_id: docs-alerts-management-log-based-alerts-step-3-alert-configuration
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts: Step 3: Alert Configuration</h2>
<p>In this step, you set the alert's metadata, including severity, name, and description:</p>
<h3>## Severity</h3>
<p>Set the severity level for the alert (e.g., &quot;Warning&quot; or &quot;Critical&quot;).</p>
<h3>## Alert Name</h3>
<p>A field to name the alert for easy identification.</p>
<h3>## Alert Description</h3>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>You can incorporate result labels in the alert descriptions to make the alerts more informative:</p>
<p><strong>Syntax</strong>: Use <code>{{.Labels.&lt;label-name&gt;}}</code> to insert label values. Label values can be any attribute used in group by. Ensure that all <code>.</code> (dots) in attribute are converted to <code>_</code></p>
<p><strong>Example</strong>: If you have a query that has the label <code>service.name</code> then to use it in the alert description, you will use <code>{{.Labels.service_name}}</code>which creates an alert that is specific to the particular service.</p>
<h3>## Labels</h3>
<p>A field to add labels or tags for categorization. Labels should be added in key value pairs. First enter key (avoid space in key) and set value.</p>
<h3>## Notification channels</h3>
<p>A field to choose the <a href="https://signoz.io/docs/setup-alerts-notification">notification channels</a> from those configured in the Alert Channel settings.</p>
<h3>## Test Notification</h3>
<p>A button to test the alert to ensure that it works as expected.</p>
<p><img src="https://signoz.io/img/docs/alerts/alerts-log-based-3.webp" alt="Configure the alert" /></p>
<p><em>Setting the alert metadata</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/#examples
tag_set: alerts-management, log-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-metric.png, https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-chart.png, https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-conditions.png, https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-configuration.png
tracking_id: docs-alerts-management-log-based-alerts-examples
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts: Examples</h2>
<h3>## 1. Alert when percentage of <code>redis timeout</code> error logs greater than 7% in last 5 mins</h3>
<h4>## Here's a video tutorial for creating this alert:</h4>
<h4>## Step 1: Write Query Builder query to define alert metric</h4>
<p><img src="https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-metric.png" alt="logs builder query for redis timeout logs percentage" /></p>
<p><em>Redis timeout query</em></p>
<p>Here we write 2 queries to calculate error logs percent. First query to count logs which are <code>redis timeout</code> error logs. Second query to count total logs. Then we add a formula to calculate percentage.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-chart.png" alt="error logs percentage chart" /></p>
<p><em>Error log percentage chart</em></p>
<p>‚úÖ Info</p>
<p>Remember to select y-axis unit as Percent(0-100) as we want to apply threshold in percent.</p>
<h4>## Step 2: Set alert conditions</h4>
<p><img src="https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-conditions.png" alt="redis timeout alert condition" /></p>
<p><em>Error logs percentage alert condition</em></p>
<p>The condition is set to trigger a notification if the per-minute error logs percentage exceeds the threshold of 1 second on average in the last five minutes.</p>
<h4>## Step 3: Set alert configuration</h4>
<p><img src="https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-configuration.png" alt="redis timeout alert configuration" /></p>
<p><em>Error logs percentage alert configuration</em></p>
<p>At last configure the alert as <code>Warning</code>, add a name and notification channel.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.19/
tag_set: operate, migration, upgrade-0.19
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.19
group_tracking_ids: docs-operate-migration-upgrade-0.19
<h2>Upgrade to v0.19 from earlier versions</h2>
<p>Before upgrading to v0.19, you need to run the migration script to sanitise the alerts and dashboards data.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.19/#steps-to-run-migration-script
tag_set: operate, migration, upgrade-0.19
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.19-steps-to-run-migration-script
group_tracking_ids: docs-operate-migration-upgrade-0.19
<h2>Upgrade to v0.19 from earlier versions: Steps to run migration script</h2>
<h3>## Command-Line Interface (CLI) Flags</h3>
<p>There are is only one flag in the <code>migrate</code> binary:</p>
<ul>
<li><code>--dataSource</code> : Data Source path. <code>default=signoz.db</code></li>
</ul>
<h3>## For Docker</h3>
<p><code>cd</code> to SigNoz repository and run following commands:</p>
<pre><code>cd deploy/docker/clickhouse-setup

docker run -it -v $PWD/data/signoz/signoz.db:/signoz.db signoz/migrate:0.19
</code></pre>
<p>Output should be similar as below:</p>
<pre><code>Data Source path:  signoz.db
2023/05/20 15:28:22 Total Dashboard found: 2
2023/05/20 15:28:22 625fa391-d9d3-47c1-809a-1a147eea229d
2023/05/20 15:28:22 b05af383-23ec-4061-8f57-0765d45ccd51
2023/05/20 15:28:22 Dashboard 625fa391-d9d3-47c1-809a-1a147eea229d updated
2023/05/20 15:28:22 Dashboard b05af383-23ec-4061-8f57-0765d45ccd51 updated
2023/05/20 15:28:22 Dashboards migrated
2023/05/20 15:28:22 Migrating 1 rules
2023/05/20 15:28:22 Migrating rule 1
2023/05/20 15:28:22 Migrated 1 rules
</code></pre>
<p>At last, trigger a restart of the query-service container:</p>
<pre><code>docker restart query-service
</code></pre>
<h3>## For Docker Swarm</h3>
<p><code>cd</code> to SigNoz repository and run following commands:</p>
<pre><code>cd deploy/swarm/clickhouse-setup

docker run -it -v $PWD/data/signoz/signoz.db:/signoz.db signoz/migrate:0.19
</code></pre>
<p>Output should be similar as below:</p>
<pre><code>Data Source path:  signoz.db
2023/05/20 15:28:22 Total Dashboard found: 2
2023/05/20 15:28:22 625fa391-d9d3-47c1-809a-1a147eea229d
2023/05/20 15:28:22 b05af383-23ec-4061-8f57-0765d45ccd51
2023/05/20 15:28:22 Dashboard 625fa391-d9d3-47c1-809a-1a147eea229d updated
2023/05/20 15:28:22 Dashboard b05af383-23ec-4061-8f57-0765d45ccd51 updated
2023/05/20 15:28:22 Dashboards migrated
2023/05/20 15:28:22 Migrating 1 rules
2023/05/20 15:28:22 Migrating rule 1
2023/05/20 15:28:22 Migrated 1 rules
</code></pre>
<p>At last, trigger a restart of the query-service container:</p>
<pre><code>docker restart query-service
</code></pre>
<p>‚úÖ Info</p>
<p>In case of multi node swarm cluster, run the above commands in the node where query-service is running. To find out which node: <code>docker service ps query-service</code>.</p>
<h3>## For Kubernetes</h3>
<p>To download <code>migrate</code> binary:</p>
<pre><code>wget https://github.com/signoz/signoz-db-migrations/releases/download/v0.19/migrate-v0.19-linux-amd64 -O migrate

sudo chmod +x migrate
</code></pre>
<p>To copy the binary in persistent volume path <code>/var/lib/signoz</code> in <code>query-service</code>:</p>
<pre><code>kubectl cp -n platform ./migrate my-release-signoz-query-service-0:/var/lib/signoz/migrate
</code></pre>
<p>To <code>exec</code> into the <code>query-service</code> container:</p>
<pre><code>kubectl -n platform exec -it pod/my-release-signoz-query-service-0 -- sh
</code></pre>
<p>Now, change directory to the <code>/var/lib/signoz</code> and run the migration script:</p>
<pre><code>cd /var/lib/signoz

./migrate
</code></pre>
<p>You should see output similar to this:</p>
<pre><code>Data Source path:  signoz.db
2023/05/20 15:28:22 Total Dashboard found: 3
2023/05/20 15:28:22 625fa391-d9d3-47c1-809a-1a147eea229d
2023/05/20 15:28:22 b05af383-23ec-4061-8f57-0765d45ccd51
2023/05/20 15:28:22 e730bcd5-5319-4cab-8de7-82edd5f48c72
2023/05/20 15:28:22 Dashboard 625fa391-d9d3-47c1-809a-1a147eea229d updated
2023/05/20 15:28:22 Dashboard b05af383-23ec-4061-8f57-0765d45ccd51 updated
2023/05/20 15:28:22 Dashboard e730bcd5-5319-4cab-8de7-82edd5f48c72 updated
2023/05/20 15:28:22 Dashboards migrated
2023/05/20 15:28:22 Migrating 1 rules
2023/05/20 15:28:22 Migrating rule 1
2023/05/20 15:28:22 Migrated 1 rules
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.19/#upgrade-to-v019
tag_set: operate, migration, upgrade-0.19
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.19-upgrade-to-v019
group_tracking_ids: docs-operate-migration-upgrade-0.19
<h2>Upgrade to v0.19 from earlier versions: Upgrade to v0.19</h2>
<p>Follow the platform specific instructions to upgrade to 0.19 and above.</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/operate/docker-standalone/#upgrade-signoz-cluster">Docker Standalone</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/docker-swarm/#upgrade-signoz-cluster">Docker Swarm</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/kubernetes/#upgrade-signoz-cluster">Kubernetes</a></p>
</li>
</ul>
<p>‚ö†Ô∏è Warning</p>
<p>Prior to upgrading to <code>v0.19</code>, you need to run the migration script.</p>
<p>In case you upgrade and don't run the migration script, you might run into <code>query-service</code> pod crash loop. To solve this, follow the instructions in <a href="#issue---query-service-pod-is-crashing-kubernetes">the section below</a>.</p>
<h3>## Issue - <code>query-service</code> pod is crashing (Kubernetes)</h3>
<p>In case you upgraded to <code>v0.19</code> prior to running the migration script in Kubernetes, you will see <code>query-service</code> pod is crashing due to invalid alerts data.</p>
<p>To solve this, you will have to use <code>migration</code> init container in <code>query-service</code> pod to run the migration script and then restart the pod.</p>
<p>Follow the steps below:</p>
<ol>
<li>
<p>Make sure you have latest chart information from the Helm repositories:</p>
<pre><code>helm repo update
</code></pre>
</li>
<li>
<p>Include the following in <code>override-values.yaml</code> file:</p>
<pre><code>queryService:
  initContainers:
    migration:
      enabled: true
      command:
        - sh
        - -c
        - |
          echo &quot;Running migration&quot;
          wget https://github.com/signoz/signoz-db-migrations/releases/download/v0.19/migrate-v0.19-linux-amd64 -O migrate
          chmod +x migrate
          ./migrate --dataSource /var/lib/signoz/signoz.db
          echo &quot;Migration completed&quot;
</code></pre>
</li>
<li>
<p>Run the following command to upgrade the chart:</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz -f override-values.yaml
</code></pre>
</li>
<li>
<p>Wait for the <code>migration</code> init container to complete and then restart the <code>query-service</code> pod:</p>
<pre><code>kubectl -n platform rollout restart sts -l app.kubernetes.io/component=query-service
</code></pre>
</li>
<li>
<p>(Optional) Once the <code>query-service</code> pod is running, you can delete the <code>migration</code> init container from the <code>override-values.yaml</code> file which you added in step 2, followed by running the following command to upgrade the chart:</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz -f override-values.yaml
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#overview
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-overview
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Redis Metrics and Logs - Overview</h2>
<p>This integration helps you to monitor key Redis metrics and logs, view them with an out-of-the-box dashboard, and parse Redis logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#prerequisites
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-prerequisites
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>A Redis server running version 3.0 or newer</li>
<li>An OpenTelemetry (OTEL) Collector with access to the Redis server
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0 or newer) if not done already</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
<li>Ensure the OTEL collector has access to the Redis server for metrics collection and can read the Redis log file for logs collection</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#collecting-redis-metrics
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-collecting-redis-metrics
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Collecting Redis Metrics</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>redis-metrics-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  redis:
    # The hostname and port of the Redis instance, separated by a colon.
    endpoint: ${env:REDIS_ENDPOINT}
    # The frequency at which to collect metrics from the Redis instance.
    collection_interval: 60s
    # # The password used to access the Redis instance; must match the password specified in the requirepass server configuration option.
    password: ${env:REDIS_PASSWORD}
    # # Defines the network to use for connecting to the server. Valid Values are `tcp` or `Unix`
    # transport: tcp
    # tls:
    #   insecure: false
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/redis.crt
    #   key_file: /etc/ssl/certs/redis.key
    metrics:
      redis.maxmemory:
        enabled: true
      redis.cmd.latency:
        enabled: true

processors:
  # enriches the data with additional host information
  # see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#resource-detection-processor
  resourcedetection/system:
    # add additional detectors if needed
    detectors: [&quot;system&quot;]
    system:
      hostname_sources: [&quot;os&quot;]

exporters:
  # export to SigNoz cloud
  otlp/redis:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/redis:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    metrics/redis:
      receivers: [redis]
      # note: remove this processor if the collector host is not running on the same host as the redis instance
      processors: [resourcedetection/system]
      exporters: [otlp/redis]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># redis endpoint reachable from the otel collector&quot;
export REDIS_ENDPOINT=&quot;localhost:6379&quot;

# password used to access the Redis instance.
# must match the password specified in the requirepass server configuration option.
# can be left empty if the redis server is not configured to require a password.
export REDIS_PASSWORD=&quot;&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config redis-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#collecting-redis-logs
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-collecting-redis-logs
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Collecting Redis Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>redis-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/redis:
    include: [&quot;${env:REDIS_LOG_FILE}&quot;]
    operators:
      # Parse default redis log format
      # pid:role timestamp log_level message
      - type: regex_parser
        if: body matches '^(?P&lt;pid&gt;\\d+):(?P&lt;role&gt;\\w+) (?P&lt;ts&gt;\\d{2} \\w+ \\d{4} \\d{2}:\\d{2}:\\d{2}\\.\\d+) (?P&lt;log_level&gt;[.\\-*#]) (?P&lt;message&gt;.*)$'
        parse_from: body
        regex: '^(?P&lt;pid&gt;\d+):(?P&lt;role&gt;\w+) (?P&lt;ts&gt;\d{2} \w+ \d{4} \d{2}:\d{2}:\d{2}\.\d+) (?P&lt;log_level&gt;[.\-*#]) (?P&lt;message&gt;.*)$'
        timestamp:
          parse_from: attributes.ts
          layout: '02 Jan 2006 15:04:05.000'
          layout_type: gotime
        severity:
          parse_from: attributes.log_level
          overwrite_text: true
          mapping:
            debug: '.'
            info:
              - '-'
              - '*'
            warn: '#'
        on_error: send
      - type: move
        if: attributes.message  nil
        from: attributes.message
        to: body
      - type: remove
        if: attributes.log_level  nil
        field: attributes.log_level
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: add
        field: attributes.source
        value: redis

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/redis-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/redis-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true


service:
  pipelines:
    logs/redis:
      receivers: [filelog/redis]
      processors: [batch]
      exporters: [otlp/redis-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># path of Redis server log file. must be accessible by the otel collector
# typically found in /usr/local/var/log/redis on macOS
# log file location can also be found in the output of `redis-cli CONFIG GET : *`
export REDIS_LOG_FILE=/var/log/redis/redis-server.log

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config redis-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags in the collector run command.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#connect-redis
tag_set: integrations, redis
image_urls: https://signoz.io/img/docs/integrations/redis/redis-integration-search.webp, https://signoz.io/img/docs/integrations/redis/redis-integration-connect.webp, https://signoz.io/img/docs/integrations/redis/redis-integration-listening.webp
tracking_id: docs-integrations-redis-connect-redis
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Connect Redis</h2>
<p>Once you're done with setting up Redis for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the Redis integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-search.webp" alt="Search for Redis in Integrations tab" /></p>
<p><em>Search for Redis in Integrations tab</em></p>
<p>Click on the <code>Connect Redis</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your Redis instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-connect.webp" alt="Connect Redis" /></p>
<p><em>Connect Redis</em></p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-listening.webp" alt="Listening for data from Redis" /></p>
<p><em>Listening for data from Redis</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#redis-dashboard
tag_set: integrations, redis
image_urls: https://signoz.io/img/docs/integrations/redis/redis-integration-dashboard.webp
tracking_id: docs-integrations-redis-redis-dashboard
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Redis dashboard</h2>
<p>Once SigNoz has started listening to your Redis data, head over to the Dashboards tab and search for Redis, this will show you a newly created dashboard which shows differnet Redis metrics.</p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-dashboard.webp" alt="Dashboard for monitoring Redis Metrics" /></p>
<p><em>Dashboard for monitoring Redis Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Redis Dashboard by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/redis/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#data-collected
tag_set: integrations, redis
image_urls: https://signoz.io/img/docs/integrations/redis/redis-integration-data-collected.webp
tracking_id: docs-integrations-redis-data-collected
group_tracking_ids: docs-integrations-redis
<h2>Redis Metrics and Logs: Data Collected: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your Redis Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your Redis instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/img/docs/integrations/redis/redis-integration-data-collected.webp" alt="Log attributes and metrics details for Redis" /></p>
<p><em>Log attributes and metrics details for Redis</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#redis-log-attributes
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-redis-log-attributes
group_tracking_ids: docs-integrations-redis
<p>Redis Metrics and Logs: Data Collected: Redis log attributes</p>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Process ID</td>
<td>attributes.pid</td>
<td>string</td>
</tr>
<tr>
<td>Process Role</td>
<td>attributes.role</td>
<td>string</td>
</tr>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/redis/#redis-metrics
tag_set: integrations, redis
image_urls: 
tracking_id: docs-integrations-redis-redis-metrics
group_tracking_ids: docs-integrations-redis
<p>Redis Metrics and Logs: Data Collected: Redis metrics: Redis metrics: Redis metrics</p>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Unit</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>redis_commands_processed</td>
<td>Sum</td>
<td>number</td>
<td>Total number of commands processed by the server</td>
</tr>
<tr>
<td>redis_cpu_time</td>
<td>Sum</td>
<td>seconds</td>
<td>System CPU consumed by the Redis server in seconds since server start</td>
</tr>
<tr>
<td>redis_keys_expired</td>
<td>Sum</td>
<td>number</td>
<td>Total number of key expiration events</td>
</tr>
<tr>
<td>redis_db_expires</td>
<td>Gauge</td>
<td>number</td>
<td>Number of keyspace keys with an expiration</td>
</tr>
<tr>
<td>redis_commands</td>
<td>Gauge</td>
<td>ops/s</td>
<td>Number of commands processed per second</td>
</tr>
<tr>
<td>redis_replication_offset</td>
<td>Gauge</td>
<td>Bytes</td>
<td>The server's current replication offset</td>
</tr>
<tr>
<td>redis_net_input</td>
<td>Sum</td>
<td>Bytes</td>
<td>The total number of bytes read from the network</td>
</tr>
<tr>
<td>redis_clients_connected</td>
<td>Sum</td>
<td>number</td>
<td>Number of client connections (excluding connections from replicas)</td>
</tr>
<tr>
<td>redis_keys_evicted</td>
<td>Sum</td>
<td>number</td>
<td>Number of evicted keys due to maxmemory limit</td>
</tr>
<tr>
<td>redis_maxmemory</td>
<td>Gauge</td>
<td>Bytes</td>
<td>The value of the maxmemory configuration directive</td>
</tr>
<tr>
<td>redis_clients_max_input_buffer</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Biggest input buffer among current client connections</td>
</tr>
<tr>
<td>redis_cmd_latency</td>
<td>Gauge</td>
<td>seconds</td>
<td>Command execution latency</td>
</tr>
<tr>
<td>redis_memory_lua</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Number of bytes used by the Lua engine</td>
</tr>
<tr>
<td>redis_replication_backlog_first_byte_offset</td>
<td>Gauge</td>
<td>Bytes</td>
<td>The master offset of the replication backlog buffer</td>
</tr>
<tr>
<td>redis_keyspace_hits</td>
<td>Sum</td>
<td>number</td>
<td>Number of successful lookup of keys in the main dictionary</td>
</tr>
<tr>
<td>redis_clients_blocked</td>
<td>Sum</td>
<td>number</td>
<td>Number of clients pending on a blocking call</td>
</tr>
<tr>
<td>redis_connections_rejected</td>
<td>Sum</td>
<td>number</td>
<td>Number of connections rejected because of maxclients limit</td>
</tr>
<tr>
<td>redis_latest_fork</td>
<td>Gauge</td>
<td>microseconds</td>
<td>Duration of the latest fork operation in microseconds</td>
</tr>
<tr>
<td>redis_clients_max_output_buffer</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Longest output list among current client connections</td>
</tr>
<tr>
<td>redis_slaves_connected</td>
<td>Sum</td>
<td>number</td>
<td>Number of connected replicas</td>
</tr>
<tr>
<td>redis_db_keys</td>
<td>Gauge</td>
<td>number</td>
<td>Number of keyspace keys</td>
</tr>
<tr>
<td>redis_keyspace_misses</td>
<td>Sum</td>
<td>number</td>
<td>Number of failed lookup of keys in the main dictionary</td>
</tr>
<tr>
<td>redis_uptime</td>
<td>Sum</td>
<td>seconds</td>
<td>Number of seconds since Redis server start</td>
</tr>
<tr>
<td>redis_memory_used</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Total number of bytes allocated by Redis using its allocator</td>
</tr>
<tr>
<td>redis_net_output</td>
<td>Sum</td>
<td>Bytes</td>
<td>The total number of bytes written to the network</td>
</tr>
<tr>
<td>redis_connections_received</td>
<td>Sum</td>
<td>number</td>
<td>Total number of connections accepted by the server</td>
</tr>
<tr>
<td>redis_rdb_changes_since_last_save</td>
<td>Sum</td>
<td>number</td>
<td>Number of changes since the last dump</td>
</tr>
<tr>
<td>redis_memory_rss</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Number of bytes that Redis allocated as seen by the operating system</td>
</tr>
<tr>
<td>redis_db_avg_ttl</td>
<td>Gauge</td>
<td>milliseconds</td>
<td>Average keyspace keys TTL</td>
</tr>
<tr>
<td>redis_memory_peak</td>
<td>Gauge</td>
<td>Bytes</td>
<td>Peak memory consumed by Redis (in bytes)</td>
</tr>
<tr>
<td>redis_memory_fragmentation_ratio</td>
<td>Gauge</td>
<td>number</td>
<td>Ratio between used_memory_rss and used_memory</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/tutorial/infinite-retention-aws-s3/
tag_set: tutorial, infinite-retention-aws-s3
image_urls: https://signoz.io/img/blog/common/join_slack_cta.webp
tracking_id: docs-tutorial-infinite-retention-aws-s3
group_tracking_ids: docs-tutorial-infinite-retention-aws-s3
<h2>Infinite Retention of OpenTelemetry Data in AWS S3</h2>
<h2>Overview</h2>
<hr />
<p>It is a common practice to backup any data for longer durations due to compliance and audit purposes. You can use <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/awss3exporter">AWS S3 Exporter</a> to retain the OpenTelemetry data as long as we need.</p>
<h2>Limitation of AWS S3 Exporter</h2>
<hr />
<ul>
<li>Data in AWS S3 is inaccessible in SigNoz UI</li>
<li>Need to use third-party tool like <strong>Amazon Athena</strong> to query data</li>
<li>Due to limitation of AWS S3 Exporter, you won't be able differentiate different signals like logs, metrics, and traces - hence the need of including different prefixes</li>
</ul>
<p>‚úÖ Info</p>
<p>If you want to query data stored in AWS S3 using <strong>SigNoz</strong> and do not have the requirement for infinite or very long retention period, then use <a href="/docs/userguide/retention-period/#configuring-cold-storage---amazon-s3">SigNoz's AWS S3 Retention</a> instead.</p>
<h2>Prerequisites</h2>
<hr />
<ul>
<li>Running instance of OpenTelemetry Collector (if not running already, see <a href="/docs/install/">Installation Page</a>
)</li>
<li>Access to AWS S3 Bucket either using AWS credentials as environment variables or IAM roles for ECS tasks or EC2 instances (for more details, <a href="/docs/userguide/retention-period/#configuring-cold-storage---amazon-s3">see here</a>
)</li>
</ul>
<h2>Adding AWS S3 Exporter</h2>
<hr />
<p>In our example, we will use <code>awss3</code> exporter for retaining logs data, where we will be using <code>us-east-1</code> region and <code>otel-data-backup</code> bucket.</p>
<pre><code>exporters:
  awss3/logs:
    s3uploader:
      region: 'us-east-1'
      s3_bucket: 'otel-data-backup'
      s3_prefix: 'logs'
      s3_partition: 'minute'
service:
  pipelines:
    logs:
      exporters: [otlp, awss3/logs]
</code></pre>
<p>Similarly, we can add it for <code>metrics</code> and <code>traces</code> pipelines as well with different prefixes to retain metrics and traces data respectively.</p>
<p>The above configuration needs to be added for your respective OtelCollector(s).</p>
<ul>
<li>In case of <strong>SigNoz Self-hosted</strong>, you can add it in SigNoz OtelCollector configuration to retain all incoming data of <code>logs</code> pipelines.</li>
<li>In case of <strong>SigNoz Cloud</strong>, you will need to update all OtelCollector configuration that is sending data.</li>
<li>For <strong>K8s-Infra</strong> helm chart, you can add it in the <code>otelAgent</code> configuration.</li>
<li>For any <strong>Standalone OtelCollector</strong> deployments that is directly sending data to SigNoz, you can add it in its respective configuration.</li>
</ul>
<p>List of all supported configuration of <code>awss3</code> exporter can be found <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/awss3exporter">here</a>.</p>
<hr />
<p>If you have any feedback or facing issues, feel free to join our slack community to get help!</p>
<p><a href="https://signoz.io/slack"><img src="https://signoz.io/img/blog/common/join_slack_cta.webp" alt="SigNoz Slack community" /></a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/
tag_set: alerts-management, trace-based-alerts
image_urls: 
tracking_id: docs-alerts-management-trace-based-alerts
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts</h2>
<p>A Trace-based alert in SigNoz allows you to define conditions based on trace data, triggering alerts when these conditions are met. Here's a breakdown of the various sections and options available when configuring a Trace-based alert:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/#step-1-define-the-trace-metric
tag_set: alerts-management, trace-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-trace-based-1.webp
tracking_id: docs-alerts-management-trace-based-alerts-step-1-define-the-trace-metric
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts: Step 1: Define the Trace Metric</h2>
<p>In this step, you use the <a href="https://signoz.io/docs/userguide/query-builder/#logs-and-traces-query-builder">Traces Query Builder</a> to perform operations on your Traces to define conditions based on traces data. Some of the fields that are available in Traces Query Builder includes</p>
<ul>
<li>
<p><strong>Traces</strong>: A field to filter the trace data to monitor.</p>
</li>
<li>
<p><strong>Aggregate Attribute</strong>: Allows you to choose how the trace data should be aggregated. You can use functions like &quot;Count&quot;</p>
</li>
<li>
<p><strong>Group by</strong>: Lets you group trace data by different span/trace attributes, like &quot;serviceName&quot;, &quot;Status&quot; or other custom attributes.</p>
</li>
<li>
<p><strong><a href="https://signoz.io/docs/userguide/query-builder/#legend-format">Legend Format</a></strong>: An optional field to define the format for the legend in the visual representation of the alert.</p>
</li>
<li>
<p><strong>Having</strong>: Apply conditions to filter the results further based on aggregate value.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-trace-based-1.webp" alt="Using Query Builder to perform operations on your Traces" /></p>
<p><em>Using Query Builder to perform operations on your Traces</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/#step-2-define-alert-conditions
tag_set: alerts-management, trace-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-trace-based-2.webp
tracking_id: docs-alerts-management-trace-based-alerts-step-2-define-alert-conditions
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts: Step 2: Define Alert Conditions</h2>
<p>In this step, you set specific conditions for triggering the alert and determine the frequency of checking these conditions:</p>
<ul>
<li>
<p><strong>Send a notification when [A] is [above/below] the threshold in total during the last [X] mins</strong>: A template to set the threshold for the alert, allowing you to define when the alert condition should be checked.</p>
</li>
<li>
<p><strong>Alert Threshold</strong>: A field to specify the threshold value for the alert condition.</p>
</li>
<li>
<p><strong>More Options</strong> :</p>
<ul>
<li>
<p><strong>Run alert every [X mins]</strong>: This option determines the frequency at which the alert condition is checked and notifications are sent.</p>
</li>
<li>
<p><strong>Send a notification if data is missing for [X] mins</strong>: A field to specify if a notification should be sent when data is missing for a certain period.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-trace-based-2.webp" alt="Define the alert conditions" /></p>
<p><em>Define the alert conditions</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/#step-3-alert-configuration
tag_set: alerts-management, trace-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-trace-based-3.webp
tracking_id: docs-alerts-management-trace-based-alerts-step-3-alert-configuration
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts: Step 3: Alert Configuration</h2>
<p>In this step, you set the alert's metadata, including severity, name, and description:</p>
<h3>## Severity</h3>
<p>Set the severity level for the alert (e.g., &quot;Warning&quot; or &quot;Critical&quot;).</p>
<h3>## Alert Name</h3>
<p>A field to name the alert for easy identification.</p>
<h3>## Alert Description</h3>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>You can incorporate result labels in the alert descriptions to make the alerts more informative:</p>
<h3>## Test Notification</h3>
<p>A button to test the alert to ensure that it works as expected.</p>
<p><img src="https://signoz.io/img/docs/alerts/alerts-trace-based-3.webp" alt="Configure the alert" /></p>
<p><em>Setting the alert metadata</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/#examples
tag_set: alerts-management, trace-based-alerts
image_urls: 
tracking_id: docs-alerts-management-trace-based-alerts-examples
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts: Examples</h2>
<h3>## 1. Alert when external API latency (P90) is over 1 second for last 5 mins</h3>
<h4>## Here's a video tutorial for creating this alert:</h4>
<h4>## Step 1: Write Query Builder query to define alert metric</h4>
<p><img src="https://signoz.io/docs/alerts-management/p90" alt="traces builder query for external API latency(p90)" /></p>
<p><em>External API latency (P90) query</em></p>
<p>Using <code>externalHttpUrl</code> attribute we can filter specific external API endpoint and then set aggregation attribute to durationNano with P90 aggregation operation to plot a chart which measures 90th percentile latency. You can also choose <code>Avg</code> or anyother operation as aggregate operation depending on your needs.</p>
<p><img src="https://signoz.io/docs/alerts-management/p90" alt="external API latency(p90) chart" /></p>
<p><em>External API latency (P90) chart</em></p>
<p>‚úÖ Info</p>
<p>Remember to select y-axis unit as nanoseconds as our aggregate key is durationNano.</p>
<h4>## Step 2: Set alert conditions</h4>
<p><img src="https://signoz.io/docs/alerts-management/p90" alt="external API latency(p90) condition" /></p>
<p><em>External API latency (P90) alert condition</em></p>
<p>The condition is set to trigger a notification if the per-minute external API latency (P90) exceeds the threshold of 1 second all the time in the last five minutes.</p>
<h4>## Step 3: Set alert configuration</h4>
<p><img src="https://signoz.io/docs/alerts-management/p90" alt="external API latency(p90) alert configuration" /></p>
<p><em>External API latency (P90) alert configuration</em></p>
<p>At last configure the alert as <code>Warning</code>, add a name and notification channel.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.12/
tag_set: operate, migration, upgrade-0.12
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.12
group_tracking_ids: docs-operate-migration-upgrade-0.12
<h2>Upgrade to v0.12 from earlier versions</h2>
<h2>Features of this release</h2>
<hr />
<p>v0.12 introduces distributed clickhouse setup.</p>
<h2>After upgrading to v0.12</h2>
<hr />
<h3>## Querying distributed tables</h3>
<p>The new distributed tables in clickhouse have been named by prefixing <code>distributed_</code> to existing single shard table names. If you have used clickhouse queries in dashboard or alerts, you would need to update the queries with the new table names. Eg, <code>signoz_index_v2</code> now corresponds to the table of a single shard. To query all the shards, query against <code>distributed_signoz_index_v2</code>.</p>
<p>The old table names will continue to work for single node installations as long as you work with a single shard. We recommend changing table names at the earliest to make future upgrade to distributed setup easier.</p>
<p>For detailed instructions to include additional shards, go to <a href="/docs/operate/clickhouse/distributed-clickhouse/">Distributed ClickHouse documentation page</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/increase-clickhouse-pv/
tag_set: operate, clickhouse, increase-clickhouse-pv
image_urls: 
tracking_id: docs-operate-clickhouse-increase-clickhouse-pv
group_tracking_ids: docs-operate-clickhouse-increase-clickhouse-pv
<h2>Increase the ClickHouse Persistent Volume Size</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>You can use the following <code>helm upgrade</code> command to increase the size of the persistent volume used by SigNoz.</p>
<p>Replace the following values to match your environment:</p>
<ul>
<li>Your namespace (this example uses <code>platform</code>)</li>
<li>Your release (this example uses <code>my-release</code>)</li>
<li>Your chart (this example uses <code>signoz/signoz</code>)</li>
<li>The new size of the persistent volume (this example uses <code>25Gi</code>)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/increase-clickhouse-pv/#check-if-volume-expansion-is-allowed
tag_set: operate, clickhouse, increase-clickhouse-pv
image_urls: 
tracking_id: docs-operate-clickhouse-increase-clickhouse-pv-check-if-volume-expansion-is-allowed
group_tracking_ids: docs-operate-clickhouse-increase-clickhouse-pv
<h2>Increase the ClickHouse Persistent Volume Size: Check if Volume Expansion is Allowed</h2>
<p>Before you proceed, run the following command to check if your storage class allows volume expansion:</p>
<pre><code>kubectl get storageclass -o json | jq '.items[]|.metadata.name + &quot;: &quot; + (.allowVolumeExpansion|tostring)'
</code></pre>
<p>Output should be similar as below:</p>
<pre><code>&quot;standard: false&quot;
&quot;standard-resizable: true&quot;
</code></pre>
<p>If the storage class used by SigNoz cluster PVCs returns <code>true</code>, that means you can directly proceed with <a href="#increase-persisent-volume">increase persistent volume section</a>.</p>
<p>If <code>false</code>, proceed with any of the following relevent section:</p>
<ul>
<li>
<p><a href="#eks---amazon-web-service-aws">AWS - EKS</a></p>
</li>
<li>
<p><a href="#gke---google-cloud-platform-gcp">GCP - GKE</a></p>
</li>
<li>
<p><a href="#other-kubernetes-cloud-platform-and-bare-metal-servers">Other Platform</a></p>
</li>
</ul>
<p>‚úÖ Info</p>
<p>If you are unsure about storage class used by SigNoz Cluster PVC, run the command below:</p>
<pre><code>kubectl get -n platform pvc
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/increase-clickhouse-pv/#increase-persistent-volume
tag_set: operate, clickhouse, increase-clickhouse-pv
image_urls: 
tracking_id: docs-operate-clickhouse-increase-clickhouse-pv-increase-persistent-volume
group_tracking_ids: docs-operate-clickhouse-increase-clickhouse-pv
<h2>Increase the ClickHouse Persistent Volume Size: Increase Persistent Volume</h2>
<p>You should be able to increase clickhouse persistent volume size easily with the following, if you have SigNoz cluster with expandable storage class.</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz \
  --set clickhouse.persistence.size=25Gi
</code></pre>
<p>‚úÖ Info</p>
<p>To override values in a Helm chart, you can also use the <code>values</code>/<code>-f</code> flag. See the <a href="https://helm.sh/docs/helm/helm_upgrade/">Helm Upgrade</a> page of the Helm documentation for more details.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/increase-clickhouse-pv/#eks---amazon-web-service-aws
tag_set: operate, clickhouse, increase-clickhouse-pv
image_urls: 
tracking_id: docs-operate-clickhouse-increase-clickhouse-pv-eks--amazon-web-service-aws
group_tracking_ids: docs-operate-clickhouse-increase-clickhouse-pv
<h2>Increase the ClickHouse Persistent Volume Size: EKS - Amazon Web Service (AWS)</h2>
<p>‚úÖ Info</p>
<p>For setting up kubernetes cluster in AWS, refer to <a href="https://aws.amazon.com/eks/">AWS Elastic Kubernetes Service</a>.</p>
<p>You can follow the <a href="#increase-persisent-volume">increase persistent volume section</a> if you had started the SigNoz cluster with expandable storage class as mentioned below:</p>
<pre><code>helm -n platform install my-release signoz/signoz \
   --set clickhouse.installCustomStorageClass=true \
   --set clickhouse.cloud=aws \
   --set clickhouse.persistence.storageClass=gp2-resizable
</code></pre>
<p>If you had started with default configurations, you would have to follow the steps below:</p>
<ol>
<li>
<p>You would have to edit default storage class i.e. <code>gp2</code> and patch the <code>allowVolumeExpansion</code> to <code>true</code>.</p>
<pre><code>kubectl patch storageclass gp2 -p '{&quot;allowVolumeExpansion&quot;: true}'
</code></pre>
<p>Output:</p>
<pre><code>storageclass.storage.k8s.io/gp2 patched
</code></pre>
</li>
<li>
<p>Run helm upgrade command:</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz \
  --set clickhouse.persistence.size=25Gi
</code></pre>
</li>
<li>
<p>(Optional) After the size change reflects in few minutes, you could edit the default storage class <code>gp2</code> back to its previous state.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/increase-clickhouse-pv/#gke---google-cloud-platform-gcp
tag_set: operate, clickhouse, increase-clickhouse-pv
image_urls: 
tracking_id: docs-operate-clickhouse-increase-clickhouse-pv-gke--google-cloud-platform-gcp
group_tracking_ids: docs-operate-clickhouse-increase-clickhouse-pv
<h2>Increase the ClickHouse Persistent Volume Size: GKE - Google Cloud Platform (GCP)</h2>
<p>‚úÖ Info</p>
<p>For setting up kubernetes cluster in GCP, refer to <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a>.</p>
<p>You can follow the <a href="#increase-persisent-volume">increase persistent volume section</a> if you had started the SigNoz cluster with expandable storage class as mentioned below:</p>
<pre><code>helm -n platform install my-release signoz/signoz \
   --set clickhouse.installCustomStorageClass=true \
   --set clickhouse.cloud=gcp \
   --set clickhouse.persistence.storageClass=gce-resizable
</code></pre>
<p>If you had started with default configurations, you would have to follow the steps below:</p>
<ol>
<li>
<p>You would have to edit default storage class i.e. <code>pd-standard</code>, and patch the <code>allowVolumeExpansion</code> to <code>true</code>.</p>
<pre><code>kubectl patch storageclass pd-standard -p '{&quot;allowVolumeExpansion&quot;: true}'
</code></pre>
<p>Output:</p>
<pre><code>storageclass.storage.k8s.io/pd-standard patched
</code></pre>
</li>
<li>
<p>Now, run helm upgrade command:</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz \
  --set clickhouse.persistence.size=25Gi
</code></pre>
</li>
<li>
<p>(Optional) After the size change reflects in few minutes, you could edit the default storage class <code>pd-standard</code> back to its previous state.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/increase-clickhouse-pv/#other-kubernetes-cloud-platform-and-bare-metal-servers
tag_set: operate, clickhouse, increase-clickhouse-pv
image_urls: 
tracking_id: docs-operate-clickhouse-increase-clickhouse-pv-other-kubernetes-cloud-platform-and-bare-metal-servers
group_tracking_ids: docs-operate-clickhouse-increase-clickhouse-pv
<h2>Increase the ClickHouse Persistent Volume Size: Other Kubernetes Cloud Platform and Bare-Metal Servers</h2>
<p>‚úÖ Info</p>
<p>For setting up kubernetes cluster in other cloud platform or bare-metal servers, refer to <a href="https://kubernetes.io/docs/setup/">Kubernetes Getting Started Guide</a>.</p>
<p>You can follow the <a href="#increase-persisent-volume">increase persistent volume section</a> if you had started the SigNoz cluster with expandable storage class - let's say <code>custom-resizable-storage-class</code> as mentioned below:</p>
<pre><code>helm -n platform install my-release signoz/signoz \
   --set clickhouse.persistence.storageClass=custom-resizable-storage-class
</code></pre>
<p>If you had started with default configurations, you would have to follow the steps below:</p>
<ol>
<li>
<p>You would have to edit default storage class i.e. <code>standard</code> in case of <a href="https://kind.sigs.k8s.io">kind cluster</a></p>
<ul>
<li>
<p>it could be different in your scenerio, and patch the <code>allowVolumeExpansion</code> to <code>true</code>.</p>
<p>DEFAULT_STORAGE_CLASS=$(kubectl get storageclass -o=jsonpath='{.items[?(@.metadata.annotations.storageclass.kubernetes.io/is-default-class==&quot;true&quot;)].metadata.name}')</p>
<p>kubectl patch storageclass &quot;$DEFAULT_STORAGE_CLASS&quot; -p '{&quot;allowVolumeExpansion&quot;: true}'</p>
</li>
</ul>
<p>Output should be similar to the following:</p>
<pre><code>storageclass.storage.k8s.io/standard patched
</code></pre>
</li>
<li>
<p>Run helm upgrade command:</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz \
  --set clickhouse.persistence.size=25Gi
</code></pre>
</li>
<li>
<p>(Optional) After the size change reflects in few minutes, you could edit the default storage class <code>standard</code> - or some other storage class as per your previous output, back to its previous state.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/community/
tag_set: community
image_urls: 
tracking_id: docs-community
group_tracking_ids: docs-community
<h2>Community</h2>
<h2>Slack</h2>
<hr />
<p>We have an active slack community with engineers eager to answer your queries on observability, tracing and monitoring. Would love to hear your thoughts and any questions if you have.</p>
<p>We are always there to help you in any issues you may have while running SigNoz. Just drop by and say Hi üëã!</p>
<p>Get your invite for the slack community <a href="https://signoz.io/slack">here</a></p>
<h2>Github</h2>
<hr />
<p>Have a question? Want to suggest an idea? - Head out to our <a href="https://github.com/SigNoz/signoz/discussions">Github Discussions</a>. We would love to hear from you!</p>
<h2>Twitter</h2>
<hr />
<p>Twitter has some amazing audience of developers and engineers. Follow us on Twitter <a href="https://twitter.com/SigNozHQ">@SigNozHQ</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/connect-to-clickhouse/
tag_set: operate, clickhouse, connect-to-clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse-connect-to-clickhouse
group_tracking_ids: docs-operate-clickhouse-connect-to-clickhouse
<h2>Connect to ClickHouse</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>Follow the instructions below for connecting to your ClickHouse database:</p>
<h2>For Docker Users</h2>
<hr />
<ol>
<li>
<p>To exec to <code>clickhouse</code> container:</p>
<pre><code>docker exec -it signoz-clickhouse bash
</code></pre>
</li>
</ol>
<p>Inside the bash shell, run the following to create clickhouse client:</p>
<pre><code>clickhouse client
</code></pre>
<p>Output should be similar to this:</p>
<pre><code>ClickHouse client version 22.4.5.9 (official build).
Connecting to localhost:9000 as user default.
Connected to ClickHouse server version 22.4.5 revision 54455.

5c6e8128ba12 :)
</code></pre>
<h2>For Docker Swarm Users</h2>
<hr />
<p>To exec to <code>clickhouse</code> container:</p>
<pre><code>docker exec -it $(docker ps -q -f name=signoz_clickhouse) bash
</code></pre>
<p>Inside the bash shell, run the following to create clickhouse client:</p>
<pre><code>clickhouse client
</code></pre>
<p>Output should be similar to this:</p>
<pre><code>ClickHouse client version 22.4.5.9 (official build).
Connecting to localhost:9000 as user default.
Connected to ClickHouse server version 22.4.5 revision 54455.

5c6e8128ba12 :)
</code></pre>
<h2>For Kubernetes Users</h2>
<hr />
<p>To exec to <code>clickhouse</code> pod:</p>
<pre><code>kubectl -n platform exec -i --tty pod/chi-signoz-cluster-0-0-0 -- bash
</code></pre>
<p>Inside the bash shell, run the following to create clickhouse client:</p>
<pre><code>clickhouse client
</code></pre>
<p>Output should be similar to this:</p>
<pre><code>ClickHouse client version 22.4.5.9 (official build).
Connecting to localhost:9000 as user default.
Connected to ClickHouse server version 22.4.5 revision 54455.

5c6e8128ba12 :)
</code></pre>
<p>‚ö†Ô∏è Warning</p>
<p>You are connected to your production database, proceed with caution!</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/
tag_set: alerts-management, exceptions-based-alerts
image_urls: 
tracking_id: docs-alerts-management-exceptions-based-alerts
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts</h2>
<p>An Exceptions-based alert in SigNoz allows you to define conditions based on exception data, triggering alerts when these conditions are met. Here's a breakdown of the various sections and options available when configuring an Exceptions-based alert:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/#step-1-define-the-metric-using-clickhouse-query
tag_set: alerts-management, exceptions-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-exceptions-based-1.webp
tracking_id: docs-alerts-management-exceptions-based-alerts-step-1-define-the-metric-using-clickhouse-query
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts: Step 1: Define the Metric Using Clickhouse Query</h2>
<p>In this step, you define the Clickhouse query to retrieve the exception data and set conditions for triggering the alert. The following elements are available:</p>
<ul>
<li>
<p><strong>Clickhouse Query</strong>: A field to write a Clickhouse SQL query that selects and aggregates exception data. The query should define the exception type, time range, and other necessary conditions.</p>
</li>
<li>
<p><strong><a href="https://signoz.io/docs/userguide/query-builder/#legend-format">Legend Format</a></strong>: An optional field to define the format for the legend in the visual representation of the alert.</p>
</li>
<li>
<p><strong>Having</strong>: Apply conditions to filter the results further based on aggregate value.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-exceptions-based-1.webp" alt="Using Clickhouse Query to define metrics" /></p>
<p><em>Using Clickhouse Query to define metrics</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/#step-2-define-alert-conditions
tag_set: alerts-management, exceptions-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-exceptions-based-2.webp
tracking_id: docs-alerts-management-exceptions-based-alerts-step-2-define-alert-conditions
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts: Step 2: Define Alert Conditions</h2>
<p>This step is for setting the specific conditions for triggering the alert and determining the frequency of checking those conditions:</p>
<ul>
<li>
<p><strong>Send a notification when [A] is [above/below] the threshold in total during the last [X] mins</strong>: A template to set the threshold and define when the alert condition should be checked.</p>
</li>
<li>
<p><strong>Alert Threshold</strong>: A field to specify the threshold value for the alert condition.</p>
</li>
<li>
<p><strong>More Options</strong> :</p>
<ul>
<li>
<p><strong>Run alert every [X mins]</strong>: This option determines the frequency at which the alert condition is checked and notifications are sent.</p>
</li>
<li>
<p><strong>Send a notification if data is missing for [X] mins</strong>: A field to specify if a notification should be sent when data is missing for a certain period.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-exceptions-based-2.webp" alt="Define the alert conditions" /></p>
<p><em>Define the alert conditions</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/#step-3-alert-configuration
tag_set: alerts-management, exceptions-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-exceptions-based-3.webp
tracking_id: docs-alerts-management-exceptions-based-alerts-step-3-alert-configuration
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts: Step 3: Alert Configuration</h2>
<p>In this step, you set the alert's metadata, including severity, name, and description:</p>
<h3>## Severity</h3>
<p>Set the severity level for the alert (e.g., &quot;Warning&quot; or &quot;Critical&quot;).</p>
<h3>## Alert Name</h3>
<p>A field to name the alert for easy identification.</p>
<h3>## Alert Description</h3>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>You can incorporate result labels in the alert descriptions to make the alerts more informative:</p>
<p><strong>Syntax</strong>: Use <code>{{.Labels.&lt;label-name&gt;}}</code> to insert label values. Label values can be any attribute used in group by. Ensure that all <code>.</code> (dots) in attribute are converted to <code>_</code></p>
<p><strong>Example</strong>: If you have a query that has the label <code>service.name</code> then to use it in the alert description, you will use <code>{{.Labels.service_name}}</code>which creates an alert that is specific to the particular service.</p>
<h3>## Labels</h3>
<p>A field to add labels or tags for categorization. Labels should be added in key value pairs. First enter key (avoid space in key) and set value.</p>
<h3>## Notification channels</h3>
<p>A field to choose the <a href="https://signoz.io/docs/setup-alerts-notification">notification channels</a> from those configured in the Alert Channel settings.</p>
<h3>## Test Notification</h3>
<p>A button to test the alert to ensure that it works as expected.</p>
<p><img src="https://signoz.io/img/docs/alerts/alerts-exceptions-based-3.webp" alt="Configure the alert" /></p>
<p><em>Setting the alert metadata</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/#examples
tag_set: alerts-management, exceptions-based-alerts
image_urls: 
tracking_id: docs-alerts-management-exceptions-based-alerts-examples
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts: Examples</h2>
<h3>## 1. Alert when exception of type <code>ConnectionError</code> occurs</h3>
<h4>## Here's a video tutorial for creating this alert:</h4>
<ul>
<li>
<p><strong>ClickHouse Query</strong>: Counts occurrences of 'ConnectionError' exceptions within one-minute intervals, grouped by service name. The ClickHouse Query would look like:</p>
<pre><code>SELECT 
    count() as value,
    toStartOfInterval(timestamp, toIntervalMinute(1)) AS interval,
    serviceName
FROM signoz_traces.distributed_signoz_error_index_v2
WHERE exceptionType !='ConnectionError'
AND timestamp BETWEEN {{.start_datetime}} AND {{.end_datetime}}
GROUP BY serviceName, interval;
</code></pre>
</li>
<li>
<p><strong>Alert Threshold</strong>: Set to <strong>0</strong></p>
</li>
<li>
<p><strong>Alert Name</strong>: &quot;Exceptions Alert&quot;</p>
</li>
<li>
<p><strong>Severity</strong>: &quot;Warning&quot;</p>
</li>
<li>
<p><strong>Notification Channels</strong>: signoz-slack-alerts (Slack channel)</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/alerts/alerts-exceptions-based.gif" alt="A gif of Exceptions Based alerts example in SigNoz" /></p>
<p><em>Exceptions Based Alert Example</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.10/
tag_set: operate, migration, upgrade-0.10
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.10
group_tracking_ids: docs-operate-migration-upgrade-0.10
<h2>Upgrade to v0.10 from earlier versions</h2>
<p>v0.10 is a breaking release which requires data migration for errors and exceptions section, if you are upgrading from an older version then you have to run the data migration scripts to be able to see past errors and exceptions data.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.10/#first-upgrade-to-v010
tag_set: operate, migration, upgrade-0.10
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.10-first-upgrade-to-v010
group_tracking_ids: docs-operate-migration-upgrade-0.10
<h2>Upgrade to v0.10 from earlier versions: First upgrade to v0.10</h2>
<p>Follow the platform specific instructions to upgrade to 0.10 and above.</p>
<p>üìù Note</p>
<p>The past exceptions/error data will not be visible on the new application until you run the migration script.</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/operate/docker-standalone/#upgrade">Docker Standalone</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/docker-swarm/#upgrade">Docker Swarm</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/kubernetes/#upgrade">Kubernetes</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.10/#steps-to-run-migration-script
tag_set: operate, migration, upgrade-0.10
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.10-steps-to-run-migration-script
group_tracking_ids: docs-operate-migration-upgrade-0.10
<h2>Upgrade to v0.10 from earlier versions: Steps to run migration script:</h2>
<h3>## For Docker</h3>
<pre><code>docker run --name signoz-migrate --network clickhouse-setup_default \
  -it -d signoz/migrate:0.10 -host=clickhouse -port=9000
</code></pre>
<p>Steps to check logs:</p>
<pre><code>docker logs -f signoz-migrate
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the container before running the migration script again.</p>
<pre><code>docker stop signoz-migrate

docker rm signoz-migrate
</code></pre>
<h3>## For Docker Swarm</h3>
<p>For Swarm, you could follow similar step to that of <a href="#docker">Docker</a>. However, you would need to expose clickhouse container ports to host machine and use host machine IP i.e. <code>172.17.0.17</code> for <code>-host</code> flag instead of <code>clickhouse</code>.</p>
<p>If you do not want to change anything in the current signoz deployment or to expose clickhouse ports even temporarily, you can go through following steps.</p>
<ol>
<li>
<p>To download <code>migration-v0.10</code> binary:</p>
<pre><code>wget https://github.com/SigNoz/signoz-db-migrations/releases/download/v0.10/migration-v0.10-linux-amd64

chmod +x migration-v0.10-linux-amd64
</code></pre>
</li>
<li>
<p>To copy the binary in persistent volume path <code>/var/lib/clickhouse</code> in <code>clickhouse</code> container:</p>
<pre><code>docker cp migration-v0.10-linux-amd64 $(docker ps -q -f name=signoz_clickhouse):/var/lib/clickhouse/migration-0.10
</code></pre>
</li>
<li>
<p>To exec into the <code>clickhouse</code> container:</p>
<pre><code>docker exec -it $(docker ps -q -f name=signoz_clickhouse) bash
</code></pre>
</li>
<li>
<p>Now, change directory to the <code>/var/lib/clickhouse</code> and run the migration script:</p>
<pre><code>cd /var/lib/clickhouse

./migration-0.10
</code></pre>
<p>You should see output similar to this:</p>
<pre><code>127.0.0.1 9000 default 
No TTL found, skipping TTL migration
There are total 1 rows, starting migration... 

Processing 1 rows of serviceName flaskApp 
Writing 1 rows
ServiceName: flaskApp 
Migrated till: 2022-07-15 09:15:04.151093623 +0000 UTC 
TimeNano: 1657876504151093623 
_________**********************************_________ 
Completed migration in:  14.299842ms
Dropping signoz_error_index table
Successfully dropped signoz_error_index
</code></pre>
</li>
<li>
<p>At last, clean up the binary:</p>
<pre><code>rm migration-0.10
</code></pre>
</li>
</ol>
<h3>## For Kubernetes</h3>
<pre><code>kubectl -n platform run -i -t signoz-migrate --image=signoz/migrate:0.10 --restart='Never' \
  -- -host=my-release-clickhouse -port=9000 -userName=admin -password=27ff0399-0d3a-4bd8-919d-17c2181e6fb9
</code></pre>
<p>Steps to check logs:</p>
<pre><code>kubectl -n platform logs -f signoz-migrate
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the pod before running the migration script again.</p>
<pre><code>kubectl -n platform delete pod signoz-migrate
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.10/#in-case-of-upgradation-failure
tag_set: operate, migration, upgrade-0.10
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.10-in-case-of-upgradation-failure
group_tracking_ids: docs-operate-migration-upgrade-0.10
<h2>Upgrade to v0.10 from earlier versions: In case of upgradation failure</h2>
<ol>
<li>Note the <code>ServiceName: xxxxx</code> and <code>TimeNano: xxxxx</code> in the logs of the migration script</li>
<li>Check the recommneded batch size section at the end of this page and use a runtime flag if needed</li>
<li>Re-run migration command using flags <code>service</code> and <code>timeNano</code> and <code>batchSize</code> with values from above as mentioned in the <code>CLI Flags</code> section below</li>
<li>Reach out to us at <a href="https://signoz.io/slack">slack</a></li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.10/#command-line-interface-cli-flags
tag_set: operate, migration, upgrade-0.10
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.10-command-line-interface-cli-flags
group_tracking_ids: docs-operate-migration-upgrade-0.10
<h2>Upgrade to v0.10 from earlier versions: Command-Line Interface (CLI) Flags</h2>
<p>There are some custom flags which can be enabled based on different usecases. All the flags below are <code>optional</code>.</p>
<p>Flags:</p>
<ul>
<li><code>-port</code> : Specify port of clickhouse. <code>default=9000</code></li>
<li><code>-host</code> : Specify host of clickhouse. <code>default=127.0.0.1</code></li>
<li><code>-userName</code> : Specify user name of clickhouse. <code>default=default</code></li>
<li><code>-password</code> : Specify password of clickhouse. <code>default=&quot;&quot;</code></li>
<li><code>-dropOldTable</code> : If it is set to true then the old tables will be dropped after data migration is successful <code>default=true</code></li>
<li><code>-service</code> : If you want to restart the migration starting with the service after it has failed specify the service name with -service. <code>default=&quot;&quot;</code></li>
<li><code>-timeNano</code> : Timestamp in nano after which the migration needs to be restarted. <code>default=&quot;&quot;</code></li>
<li><code>-batchSize</code> : Batch size of the reading/writing to clickhouse as part of migration. <code>default=&quot;70000&quot;</code></li>
</ul>
<p>‚úÖ Info</p>
<p><strong>Recommended batch size:</strong> Larger batch size leads to faster migration. But large batch size requires more memory. On average 1 row takes around 1.5 KBytes uncompressed data. So 70,000 rows uses around ~105 MBytes of data storage. So if you are migrating large data then you should use a larger batch size based on available memory on clickhouse and migration pods. Average row size varies for each system, so you should check the average row size of your system and use a proper batch size. To get the average row size of your table, you can use the following command after <a href="https://signoz.io/docs/operate/clickhouse/connect-to-clickhouse/">connecting to clickhouse</a>
:</p>
<pre><code>SELECT
    database,
    table,
    formatReadableSize(sum(data_uncompressed_bytes) AS usize) AS uncompressed, 
    sum(rows) AS total_rows,
    formatReadableSize(usize/sum(rows)) AS avg_rows_size
FROM system.parts
WHERE (active = 1) AND (database LIKE 'signoz_traces') AND (table LIKE 'signoz_error_index')
GROUP BY
    database,
    table;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/distributed-clickhouse/
tag_set: operate, clickhouse, distributed-clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse-distributed-clickhouse
group_tracking_ids: docs-operate-clickhouse-distributed-clickhouse
<h2>Set Up Distributed ClickHouse for SigNoz</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>‚úÖ Info</p>
<p>In v0.12, SigNoz introduces support for distributed clickhouse. Only multiple shards are supported until v0.42.</p>
<p>In v0.42, support for multiple replicas in distributed clickhouse is added.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/distributed-clickhouse/#prerequisites
tag_set: operate, clickhouse, distributed-clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse-distributed-clickhouse-prerequisites
group_tracking_ids: docs-operate-clickhouse-distributed-clickhouse
<h2>Set Up Distributed ClickHouse for SigNoz: Prerequisites</h2>
<ul>
<li>SigNoz version &gt;= 0.42</li>
<li>SigNoz OtelCollector and Schema Migrator &gt;= 0.88.19</li>
<li>SigNoz Chart &gt;= 0.38.1</li>
<li>Zookeeper (or ClickHouse Keeper) is mandatory for running a distributed ClickHouse cluster</li>
<li>3 nodes Zookeeper cluster recommended for distributed ClickHouse cluster with production environment, while single instance of Zookeeper should suffice for development environment</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/distributed-clickhouse/#distributed-clickhouse-setup-for-signoz
tag_set: operate, clickhouse, distributed-clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse-distributed-clickhouse-distributed-clickhouse-setup-for-signoz
group_tracking_ids: docs-operate-clickhouse-distributed-clickhouse
<h2>Set Up Distributed ClickHouse for SigNoz: Distributed ClickHouse Setup for SigNoz: Distributed ClickHouse Setup for SigNoz</h2>
<p>Basically, distributed ClickHouse cluster consists of the following:</p>
<ul>
<li>More than one clickhouse shard/replica instances</li>
<li>All clickhouse server information included in <code>remote_servers</code> clickhouse config as shards</li>
<li>Zookeeper cluster with 1 or 3 nodes, and including it in <code>zookeeper</code> clickhouse config</li>
</ul>
<p>Follow the instructions in the respective sections below to set up distributed clickhouse with multiple shards/replicas for your SigNoz cluster.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/distributed-clickhouse/#using-docker
tag_set: operate, clickhouse, distributed-clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse-distributed-clickhouse-using-docker
group_tracking_ids: docs-operate-clickhouse-distributed-clickhouse
<p>Set Up Distributed ClickHouse for SigNoz: Distributed ClickHouse Setup for SigNoz: Using Docker</p>
<p>For ClickHouse cluster with 3 shards, you will need to include additional clickhouse services in <code>docker-compose.yaml</code>. You will also need to update <code>clickhouse-cluster.xml</code> to include those additional clickhouse services as multiple shards.</p>
<p>That can be done in the by un-commenting the following in <code>docker-compose.yaml</code>:</p>
<pre><code>x-clickhouse-depend: &amp;clickhouse-depend
  depends_on:
    clickhouse:
      condition: service_healthy
    clickhouse-2:
      condition: service_healthy
    clickhouse-3:
      condition: service_healthy

services:
  ...

  clickhouse-2:
    &lt;&lt;: *clickhouse-defaults
    container_name: signoz-clickhouse-2
    hostname: clickhouse-2
    ports:
      - &quot;9001:9000&quot;
      - &quot;8124:8123&quot;
      - &quot;9182:9181&quot;
    volumes:
      - ./clickhouse-config.xml:/etc/clickhouse-server/config.xml
      - ./clickhouse-users.xml:/etc/clickhouse-server/users.xml
      - ./clickhouse-cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
      # - ./clickhouse-storage.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./data/clickhouse-2/:/var/lib/clickhouse/

  clickhouse-3:
    &lt;&lt;: *clickhouse-defaults
    container_name: signoz-clickhouse-3
    hostname: clickhouse-3
    ports:
      - &quot;9002:9000&quot;
      - &quot;8125:8123&quot;
      - &quot;9183:9181&quot;
    volumes:
      - ./clickhouse-config.xml:/etc/clickhouse-server/config.xml
      - ./clickhouse-users.xml:/etc/clickhouse-server/users.xml
      - ./clickhouse-cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
      # - ./clickhouse-storage.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./data/clickhouse-3/:/var/lib/clickhouse/
</code></pre>
<p>To switch to <strong>3 nodes</strong> Zookeepers cluster from default <strong>1 node</strong> Zookeeper, un-comment the following in <code>docker-compose.yaml</code>:</p>
<pre><code>x-clickhouse-defaults: &amp;clickhouse-defaults
  ...
  depends_on:
    - zookeeper-1
    - zookeeper-2
    - zookeeper-3

services:
  zookeeper-1:
    ...
    environment:
      - ZOO_SERVER_ID=1
      - ZOO_SERVERS=0.0.0.0:2888:3888,zookeeper-2:2888:3888,zookeeper-3:2888:3888
      ...

  zookeeper-2:
    image: bitnami/zookeeper:3.7.0
    container_name: signoz-zookeeper-2
    hostname: zookeeper-2
    user: root
    ports:
      - &quot;2182:2181&quot;
      - &quot;2889:2888&quot;
      - &quot;3889:3888&quot;
    volumes:
      - ./data/zookeeper-2:/bitnami/zookeeper
    environment:
      - ZOO_SERVER_ID=2
      - ZOO_SERVERS=zookeeper-1:2888:3888,0.0.0.0:2888:3888,zookeeper-3:2888:3888
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_AUTOPURGE_INTERVAL=1

  zookeeper-3:
    image: bitnami/zookeeper:3.7.0
    container_name: signoz-zookeeper-3
    hostname: zookeeper-3
    user: root
    ports:
      - &quot;2183:2181&quot;
      - &quot;2890:2888&quot;
      - &quot;3890:3888&quot;
    volumes:
      - ./data/zookeeper-3:/bitnami/zookeeper
    environment:
      - ZOO_SERVER_ID=3
      - ZOO_SERVERS=zookeeper-1:2888:3888,zookeeper-2:2888:3888,0.0.0.0:2888:3888
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_AUTOPURGE_INTERVAL=1
</code></pre>
<p>Next, you will have to un-comment the following from <code>clickhouse-cluster.xml</code>:</p>
<pre><code>        ...
        
            zookeeper-2
            2181
        
        
            zookeeper-3
            2181
        
    

    
        
            ...
            
                
                    clickhouse-2
                    9000
                
            
            
                
                    clickhouse-3
                    9000
</code></pre>
<p>Next, you will have to toggle <code>DOCKER_MULTI_NODE_CLUSTER</code> environment variable to <code>true</code> to ensure migrations are run on new instances (shards) of clickhouse.</p>
<pre><code>services:  
  otel-collector:
    environment:
      - DOCKER_MULTI_NODE_CLUSTER=true
</code></pre>
<p>Finally, we execute the following to apply updated <code>docker-compose.yaml</code>:</p>
<pre><code>cd deploy

./install.sh
</code></pre>
<p>(Optional) After the migration files run once in all clickhouse instances and healthy SigNoz cluster is verified, you need to make sure migration files do not run for every <code>otel-collector</code> container restart.</p>
<p>You can do that by toggling back <code>DOCKER_MULTI_NODE_CLUSTER</code> environment variable back to <code>false</code>.</p>
<pre><code>services:  
  otel-collector:
    environment:
      - DOCKER_MULTI_NODE_CLUSTER=false
</code></pre>
<p>Followed by executing the commands below to apply updated <code>docker-compose.yaml</code>:</p>
<pre><code>cd deploy

./install.sh
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/distributed-clickhouse/#using-docker-swarm
tag_set: operate, clickhouse, distributed-clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse-distributed-clickhouse-using-docker-swarm
group_tracking_ids: docs-operate-clickhouse-distributed-clickhouse
<p>Set Up Distributed ClickHouse for SigNoz: Distributed ClickHouse Setup for SigNoz: Using Docker Swarm</p>
<p>To set up ClickHouse cluster with 3 shards in Docker Swarm, you will need to include additional clickhouse services in <code>docker-compose.yaml</code>. You will also need to update <code>clickhouse-cluster.xml</code> to include those additional clickhouse services as multiple shards.</p>
<p>That can be done in the by un-commenting the following in <code>docker-compose.yaml</code>:</p>
<pre><code>x-clickhouse-depend: &amp;clickhouse-depend
  depends_on:
    clickhouse:
      condition: service_healthy
    clickhouse-2:
      condition: service_healthy
    clickhouse-3:
      condition: service_healthy

services:
  ...

  clickhouse-2:
    &lt;&lt;: *clickhouse-defaults
    hostname: clickhouse-2
    ports:
      - &quot;9001:9000&quot;
      - &quot;8124:8123&quot;
      - &quot;9182:9181&quot;
    volumes:
      - ./clickhouse-config.xml:/etc/clickhouse-server/config.xml
      - ./clickhouse-users.xml:/etc/clickhouse-server/users.xml
      - ./clickhouse-cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
      # - ./clickhouse-storage.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./data/clickhouse-2/:/var/lib/clickhouse/

  clickhouse-3:
    &lt;&lt;: *clickhouse-defaults
    hostname: clickhouse-3
    ports:
      - &quot;9002:9000&quot;
      - &quot;8125:8123&quot;
      - &quot;9183:9181&quot;
    volumes:
      - ./clickhouse-config.xml:/etc/clickhouse-server/config.xml
      - ./clickhouse-users.xml:/etc/clickhouse-server/users.xml
      - ./clickhouse-cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
      # - ./clickhouse-storage.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./data/clickhouse-3/:/var/lib/clickhouse/
</code></pre>
<p>To switch to <strong>3 nodes</strong> Zookeepers cluster from default <strong>1 node</strong> Zookeeper, un-comment the following in <code>docker-compose.yaml</code>:</p>
<pre><code>x-clickhouse-defaults: &amp;clickhouse-defaults
  ...
  depends_on:
    - zookeeper-1
    - zookeeper-2
    - zookeeper-3

services:
  zookeeper-1:
    ...
    environment:
      - ZOO_SERVER_ID=1
      - ZOO_SERVERS=0.0.0.0:2888:3888,zookeeper-2:2888:3888,zookeeper-3:2888:3888
      ...

  zookeeper-2:
    image: bitnami/zookeeper:3.7.0
    hostname: zookeeper-2
    user: root
    ports:
      - &quot;2182:2181&quot;
      - &quot;2889:2888&quot;
      - &quot;3889:3888&quot;
    volumes:
      - ./data/zookeeper-2:/bitnami/zookeeper
    environment:
      - ZOO_SERVER_ID=2
      - ZOO_SERVERS=zookeeper-1:2888:3888,0.0.0.0:2888:3888,zookeeper-3:2888:3888
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_AUTOPURGE_INTERVAL=1

  zookeeper-3:
    image: bitnami/zookeeper:3.7.0
    hostname: zookeeper-3
    user: root
    ports:
      - &quot;2183:2181&quot;
      - &quot;2890:2888&quot;
      - &quot;3890:3888&quot;
    volumes:
      - ./data/zookeeper-3:/bitnami/zookeeper
    environment:
      - ZOO_SERVER_ID=3
      - ZOO_SERVERS=zookeeper-1:2888:3888,zookeeper-2:2888:3888,0.0.0.0:2888:3888
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_AUTOPURGE_INTERVAL=1
</code></pre>
<p>Next, you will have to un-comment the following from <code>clickhouse-cluster.xml</code>:</p>
<pre><code>        ...
        
            zookeeper-2
            2181
        
        
            zookeeper-3
            2181
        
    

    
        
            ...
            
                
                    clickhouse-2
                    9000
                
            
            
                
                    clickhouse-3
                    9000
</code></pre>
<p>Next, you will have to toggle <code>DOCKER_MULTI_NODE_CLUSTER</code> environment variable to <code>true</code> to ensure migrations are run on new instances (shards) of clickhouse.</p>
<pre><code>services:  
  otel-collector:
    environment:
      - DOCKER_MULTI_NODE_CLUSTER=true
</code></pre>
<p>Finally, we execute the following to apply updated <code>docker-compose.yaml</code>:</p>
<pre><code>cd deploy

docker stack deploy -c docker-swarm/clickhouse-setup/docker-compose.yaml signoz
</code></pre>
<p>(Optional) After the migration files run once in all clickhouse instances and healthy SigNoz cluster is verified, you need to make sure migration files do not run for every <code>otel-collector</code> container restart.</p>
<p>You can do that by toggling back <code>DOCKER_MULTI_NODE_CLUSTER</code> environment variable back to <code>false</code>.</p>
<pre><code>services:  
  otel-collector:
    environment:
      - DOCKER_MULTI_NODE_CLUSTER=false
</code></pre>
<p>Followed by executing the commands below to apply updated <code>docker-compose.yaml</code>:</p>
<pre><code>cd deploy

docker stack deploy -c docker-swarm/clickhouse-setup/docker-compose.yaml signoz
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/distributed-clickhouse/#kubernetes-installation
tag_set: operate, clickhouse, distributed-clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse-distributed-clickhouse-kubernetes-installation
group_tracking_ids: docs-operate-clickhouse-distributed-clickhouse
<p>Set Up Distributed ClickHouse for SigNoz: Distributed ClickHouse Setup for SigNoz: Kubernetes Installation</p>
<p>To set up ClickHouse cluster of <strong>2 shards</strong> with <strong>2 replicas each</strong> and <strong>3 nodes</strong> Zookeeper cluster, include the following in <code>override-values.yaml</code>:</p>
<pre><code>clickhouse:
  layout:
    shardsCount: 2
    replicasCount: 2
  zookeeper:
    replicaCount: 3
schemaMigrator:
  enableReplication: true
</code></pre>
<p>‚úÖ Info</p>
<p>In case of single replica in distributed ClickHouse cluster, you can use <code>replicasCount: 1</code> and disable replication by either removing <code>enableReplication</code> or setting <code>enableReplication: false</code> in <code>schemaMigrator</code>.</p>
<p>Followed by <code>helm upgrade</code> command:</p>
<pre><code>helm --namespace platform upgrade my-release signoz/signoz -f override-values.yaml
</code></pre>
<p>To spread ClickHouse instances across multiple nodes in desired order, update <code>clickhouse.podDistribution</code> in <code>values.yaml</code>.</p>
<p>Examples:</p>
<ul>
<li>
<p>All instances in unique nodes:</p>
<pre><code>clickhouse:
  podDistribution:
    - type: ClickHouseAntiAffinity
      topologyKey: kubernetes.io/hostname
</code></pre>
</li>
<li>
<p>Distribute shards of replicas across nodes:</p>
<pre><code>clickhouse:
  podDistribution:
    - type: ReplicaAntiAffinity
      topologyKey: kubernetes.io/hostname
</code></pre>
</li>
<li>
<p>Distribute replicas of shards across nodes:</p>
<pre><code>clickhouse:
  podDistribution:
    - type: ShardAntiAffinity
      topologyKey: kubernetes.io/hostname
</code></pre>
</li>
</ul>
<p>For detailed instructions on the <strong>Pod Distribution</strong>, <a href="https://github.com/Altinity/clickhouse-operator/blob/1414503921da3ae475eb6f9a296d3475a6993768/docs/chi-examples/99-clickhouseinstallation-max.yaml#L428-L481">see here</a>.</p>
<p>‚úÖ Info</p>
<p>Replace <code>my-release</code> and <code>platform</code> from above with appropriate release name and SigNoz namespace respectively.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/clickhouse/external-clickhouse/
tag_set: operate, clickhouse, external-clickhouse
image_urls: 
tracking_id: docs-operate-clickhouse-external-clickhouse
group_tracking_ids: docs-operate-clickhouse-external-clickhouse
<h2>Using External ClickHouse</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>It is recommended to use the ClickHouse instance that is deployed along with Signoz. However, if you want to use an external ClickHouse instance, you can do so by relevant instructions below.</p>
<h3>## Prerequisites</h3>
<ul>
<li>
<p>Must have at least one Zookeeper instance</p>
</li>
<li>
<p>Distributed cluster named <code>cluster</code> set up required: <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/clickhouse-cluster.xml">clickhouse-cluster.xml</a></p>
</li>
<li>
<p>User credentaials passed must have enough privileges to create/manage databases and tables</p>
</li>
<li>
<p>Secure connection is not supported between Signoz and ClickHouse yet</p>
</li>
<li>
<p>(Optional) UDF script and histogram quantile binary required: <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/custom-function.xml">custom-function.xml</a> and <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/user_scripts/histogramQuantile">histogramQuantile binary</a></p>
</li>
</ul>
<p>‚úÖ Info</p>
<p>The name of the cluster must be <code>cluster</code> as it is hardcoded in the SigNoz OtelCollector migration files due to the limitations of <code>golang-migrate</code> library.</p>
<h2>Setting up ClickHouse</h2>
<hr />
<ol>
<li>Run at least an instance of Zookeeper.</li>
<li>Create configuration for distributed cluster named <code>cluster</code> as described in <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/clickhouse-cluster.xml">clickhouse-cluster.xml</a> and include it in <code>/etc/clickhouse-server/config.d/</code>.</li>
<li>Include Zookeeper information in above <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/clickhouse-cluster.xml">clickhouse-cluster.xml</a> file as well.</li>
<li>(Optional) Below steps are only required if you want to use <em>histogram quantile</em> functions using <strong>Query Builder</strong>.
<ul>
<li>Include <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/custom-function.xml">custom function configuration</a> in the <code>/etc/clickhouse-server/</code> directory.</li>
<li>Include <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/user_scripts/histogramQuantile">histogram quantile binary</a> in the <code>/var/lib/clickhouse/user_scripts/</code> directory.</li>
</ul>
</li>
<li>Run at least an instance of ClickHouse with the above configuration.</li>
</ol>
<h2>For Docker and Docker Swarm Users</h2>
<hr />
<p>In Docker and Docker Swarm, we will be modifying the <code>docker-compose.yml</code> file to use an external ClickHouse.</p>
<ol>
<li>Comment out <code>clickhouse</code> and <code>zookeeper</code> services in <code>docker-compose.yml</code>.</li>
<li>Comment out <code>x-clickhouse-defaults</code> and <code>x-clickhouse-depend</code> sections in <code>docker-compose.yml</code>.</li>
<li>Replace <code>clickhouse:9000</code> with relavant ClickHouse endpoint and TCP port in the files below:
<ul>
<li><code>docker-compose.yaml</code></li>
<li><code>otel-collector-config.yaml</code></li>
<li><code>prometheus-config.yml</code></li>
</ul>
</li>
<li>Start SigNoz using relevant instructions.</li>
</ol>
<h2>For Kubernetes Users</h2>
<hr />
<p>In Kubernetes, we will update <code>override-values.yaml</code> file to use an external ClickHouse.</p>
<p>At first, we will disable the default ClickHouse instance by setting <code>clickhouse.enabled</code> to <code>false</code>. Then, we will include the external ClickHouse information in the <code>externalClickhouse</code> section.</p>
<p><em>override-values.yaml</em></p>
<pre><code>clickhouse:
  enabled: false

externalClickhouse:
  host: &lt;clickhouse-endpoint&gt;
  httpPort: 8123
  tcpPort: 9000
  cluster: cluster
  secure: false
  user: &lt;ch-user&gt;
  password: &lt;ch-password&gt;
</code></pre>
<p>To install SigNoz using the above configuration, run the following command:</p>
<pre><code>kubectl create namespace platform
helm --namespace platform install my-release signoz/signoz -f override-values.yaml
</code></pre>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/troubleshooting/signoz-cloud/general-troubleshooting/
tag_set: troubleshooting, signoz-cloud, general-troubleshooting
image_urls: 
tracking_id: docs-troubleshooting-signoz-cloud-general-troubleshooting
group_tracking_ids: docs-troubleshooting-signoz-cloud-general-troubleshooting
<h2>General SigNoz Cloud Troubleshooting</h2>
<p>These are instructions for general troubleshooting for SigNoz Cloud.</p>
<h3>## Q. How to setup SigNoz across different environments?</h3>
<p>Toggle for answer</p>
<p>You can ingest data from across your environments just by adding <code>deployment.environment</code> resource attribute</p>
<p>On SDK level you can add it though environment variable <code>OTEL_RESOURCE_ATTRIBUTES=deployment.environment=prod</code></p>
<p>On collector level you can add it through a <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourceprocessor/README.md">resource</a> processor.</p>
<pre><code>processors:
  resource/env:
    attributes:
    - key: deployment.environment
      value: prod
      action: upsert
...
service:
  pipelines:
    logs:
      processors: [resource/env, batch]
</code></pre>
<h3>## Q. I can't find my Ingestion Key and Region. Where is it ?</h3>
<p>Toggle for answer</p>
<p>To find the details about the Ingestion Key and Region, you can follow this flow in the SigNoz interface</p>
<p><strong>Settings --&gt; Ingestion Settings</strong></p>
<p>The ingestion settings have all the Details about your Ingestion URL, Key and Region which you can directly copy and use in your config files.</p>
<h3>## Q. How Do I cancel my subscription ?</h3>
<p>Toggle for answer</p>
<p>Reach out to us on Intercom - the chatbox at the bottom right corner of your SigNoz Cloud interface.</p>
<h3>## Q. I see a message that my account might be Rate limited ?</h3>
<p>Toggle for answer</p>
<p>For SigNoz Cloud Trial Accounts you might get a message saying <code>&quot;You are sending data at more than 100 RPS, your ingestion may be rate limited. Please reach out to us via Intercom support.&quot;</code> To remove this rate limit, you need to upgrade your plan under the Billing Section of your SigNoz Cloud Interface and add your credit card.</p>
<h3>## Q. I need a Vendor Address for the payment to be approved.</h3>
<p>Toggle for answer</p>
<p>You can use this Address - <strong>SigNoz Inc - 2261 Market Street #4496, San Francisco, CA, 94114</strong></p>
<h3>## Q. What is a DataSource ?</h3>
<p>Toggle for answer</p>
<p>DataSource is the service, db or any instance which generates telemetry data (Traces, Logs and Metrics).</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-dashboards/
tag_set: userguide, manage-dashboards
image_urls: 
tracking_id: docs-userguide-manage-dashboards
group_tracking_ids: docs-userguide-manage-dashboards
<h2>Manage Dashboards in SigNoz</h2>
<p>This section shows how you can create, update, and remove dashboards.</p>
<p>üí° Tip</p>
<p>We have shared some commonly used dashboard JSON files <a href="https://github.com/SigNoz/dashboards">here</a>. You can import them directly to create dashboards in SigNoz.</p>
<h2>Steps to Create a Custom Dashboard</h2>
<hr />
<ol>
<li>
<p>From the sidebar, choose <strong>Dashboards</strong>.</p>
</li>
<li>
<p>Select the <strong>New Dashboard</strong> button.</p>
</li>
<li>
<p>Select the <strong>Edit</strong> button at the far right, and then enter the following information:</p>
<ol>
<li>A descriptive name for your new dashboard.</li>
<li><em>(Optional)</em> Add one ore more tags by selecting the <strong>New Tag</strong> button.</li>
<li><em>(Optional)</em> A brief description of your new dashboard.</li>
</ol>
</li>
<li>
<p>Select the <strong>Save</strong> button at the far right.</p>
</li>
<li>
<p>For each panel you wish to add to your new dashboard, follow the steps in the <a href="/docs/userguide/manage-panels/#add-a-panel-to-a-dashboard">Add a Panel to a Dashboard</a> section.</p>
</li>
<li>
<p><em>(Optional)</em> You can change the order of your panels by dragging and dropping them.</p>
</li>
<li>
<p>When you‚Äôve finished, select the <strong>Save Layout</strong> button.</p>
</li>
</ol>
<h2>Steps to Update a Custom Dashboard</h2>
<hr />
<p>To update the name, description and tags:</p>
<ol>
<li>Select the <strong>Edit</strong> button at the far right.</li>
<li>Make the changes.</li>
<li>Select the <strong>Save Layout</strong> button.</li>
</ol>
<p>To resize a panel:</p>
<ol>
<li>Click the bottom-left corner of the panel you want to resize.</li>
<li>Keep your left mouse button pressed and resize the panel.</li>
<li>When you've finished, select the <strong>Save Layout</strong> button.</li>
</ol>
<p>To change the position of a panel:</p>
<ol>
<li>Drag and drop a panel to the new position.</li>
<li>When you‚Äôve finished, select the <strong>Save Layout</strong> button.</li>
</ol>
<h2>Steps to Remove a Custom Dashboard</h2>
<hr />
<ol>
<li>From the sidebar, choose <strong>Dashboard</strong>.</li>
<li>Find the dashboard you wish to remove. In the <strong>Action</strong> column, select the <strong>Delete</strong> button.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.8.0/
tag_set: operate, migration, upgrade-0.8.0
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.8.0
group_tracking_ids: docs-operate-migration-upgrade-0.8.0
<h2>Upgrade to v0.8.0 from earlier versions</h2>
<p>v0.8.0 is a breaking release which requires data migration, if you are upgrading from an older version then you have to run the data migration scripts to be able to see past data.</p>
<h2>First upgrade to v0.8.0</h2>
<hr />
<p>Follow the platform specific instructions to upgrade to v0.8.0 and above.</p>
<p>üìù Note</p>
<p>The past data will not be visible on the new application until you run the migration script.</p>
<h3>## Upgrade Docker Installation</h3>
<ul>
<li><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/</code></li>
<li><code>git checkout v0.8.0</code></li>
<li><code>./install.sh</code></li>
</ul>
<h3>## Upgrade Kubernetes Installation</h3>
<ul>
<li><code>helm repo update</code></li>
<li><code>helm search repo signoz --versions</code></li>
<li><code>helm -n platform upgrade my-release signoz/signoz</code></li>
</ul>
<h2>Steps to run migration script:</h2>
<hr />
<h3>## For Docker</h3>
<pre><code>docker run --name signoz-migrate --network clickhouse-setup_default \
  -it -d signoz/migrate:0.8 -host=clickhouse -port=9000
</code></pre>
<p>Steps to check logs:</p>
<pre><code>docker logs -f signoz-migrate
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the container before running the migration script again.</p>
<pre><code>docker stop signoz-migrate

docker rm signoz-migrate
</code></pre>
<h3>## For Kubernetes</h3>
<pre><code>kubectl -n platform run -i -t signoz-migrate --image=signoz/migrate:0.8 --restart='Never' \
  -- -host=my-release-clickhouse -port=9000 -userName=admin -password=27ff0399-0d3a-4bd8-919d-17c2181e6fb9
</code></pre>
<p>Steps to check logs:</p>
<pre><code>kubectl -n platform logs -f signoz-migrate
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the pod before running the migration script again.</p>
<pre><code>kubectl -n platform delete pod signoz-migrate
</code></pre>
<p>There are some custom flags which can be enabled based on different usecases.</p>
<p>All the flags below are <code>optional</code></p>
<p>Flags:</p>
<ul>
<li><code>-port</code> : Specify port of clickhouse. <code>default=9000</code></li>
<li><code>-host</code> : Specify host of clickhouse. <code>default=127.0.0.1</code></li>
<li><code>-userName</code> : Specify user name of clickhouse. <code>default=default</code></li>
<li><code>-password</code> : Specify password of clickhouse. <code>default=&quot;&quot;</code></li>
<li><code>-dropOldTable</code> : If it is set to true then the old tables will be dropped after data migration is successful <code>default=true</code></li>
<li><code>-service</code> : If you want to restart the migration starting with the service after it has failed specify the service name with -service. <code>default=&quot;&quot;</code></li>
<li><code>-timeNano</code> : Timestamp in nano after which the migration needs to be restarted. <code>default=&quot;&quot;</code></li>
</ul>
<h3>## Steps to be taken in the browser to clear cache after upgrade</h3>
<h3>## For Chrome</h3>
<h3>## For Firefox</h3>
<h3>## For Safari</h3>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#overview
tag_set: alerts-management, planned-maintenance
image_urls: 
tracking_id: docs-alerts-management-planned-maintenance-overview
group_tracking_ids: docs-alerts-management-planned-maintenance
<h2>Planned Maintenance/Downtime: Planned Maintenance/Downtime - Overview</h2>
<p>Use planned maintenance to schedule maintenance windows for your services. This helps you to avoid alerting during maintenance windows and to resume alerts after the maintenance window is over. The alert evaluation is paused during the maintenance window.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#create-a-maintenance-window
tag_set: alerts-management, planned-maintenance
image_urls: 
tracking_id: docs-alerts-management-planned-maintenance-create-a-maintenance-window
group_tracking_ids: docs-alerts-management-planned-maintenance
<h2>Planned Maintenance/Downtime: Create a Maintenance Window: Create a Maintenance Window</h2>
<p>To create a maintenance window, navigate to the <strong>Configuration</strong> tab in the <strong>Alerts</strong> section. Click on the <strong><code>+ New downtime</code></strong> button at the top right corner and fill in the details.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#types-of-maintenance-windows
tag_set: alerts-management, planned-maintenance
image_urls: 
tracking_id: docs-alerts-management-planned-maintenance-types-of-maintenance-windows
group_tracking_ids: docs-alerts-management-planned-maintenance
<p>Planned Maintenance/Downtime: Create a Maintenance Window: Types of maintenance windows</p>
<p>SigNoz supports two types of maintenance schedules:</p>
<ul>
<li><strong>One-time</strong>: A maintenance window that is scheduled at a specific time for a fixed duration.</li>
<li><strong>Recurring</strong>: A maintenance window that is scheduled at a specific time and repeats at a regular interval. Optionally, you can specify an end date for the maintenance window.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#one-time-maintenance-window
tag_set: alerts-management, planned-maintenance
image_urls: https://signoz.io/img/docs/product-features/alerts/one-time.png
tracking_id: docs-alerts-management-planned-maintenance-one-time-maintenance-window
group_tracking_ids: docs-alerts-management-planned-maintenance
<p>Planned Maintenance/Downtime: Create a Maintenance Window: One-time Maintenance Window</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/one-time.png" alt="one-time maintenance window" /></p>
<p><em>One-time maintenance window</em></p>
<p>Create a one-time maintenance window by clicking on the <strong><code>+ New downtime</code></strong> button at the top right corner. Since this is a one-time maintenance window, leave the <em>Repeats every</em> field with default value <code>Does not repeat</code>.</p>
<p>The following fields are required to create a one-time maintenance window:</p>
<ul>
<li><strong>Name</strong>: A descriptive name for the maintenance window.</li>
<li><strong>Start Time</strong>: The start time of the maintenance window in the selected timezone.</li>
<li><strong>End Time</strong>: The end time of the maintenance window in the selected timezone.</li>
<li><strong>Timezone</strong>: The timezone of the maintenance window. This is the timezone in which the maintenance window is scheduled.</li>
</ul>
<p>Example:</p>
<pre><code>Start time: 2024-08-01 22:00:00
End time: 2024-08-01 23:00:00
Timezone: America/New_York
</code></pre>
<p>This means that the maintenance window is scheduled for 22:00:00 to 23:00:00 in the America/New_York timezone.</p>
<p>The following fields are optional:</p>
<ul>
<li><strong>Silence Alerts</strong>: The alerts that are silenced during the maintenance window. If nothing is selected, all alerts are silenced.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#recurring-maintenance-window
tag_set: alerts-management, planned-maintenance
image_urls: https://signoz.io/img/docs/product-features/alerts/recurring-daily.png
tracking_id: docs-alerts-management-planned-maintenance-recurring-maintenance-window
group_tracking_ids: docs-alerts-management-planned-maintenance
<p>Planned Maintenance/Downtime: Create a Maintenance Window: Recurring Maintenance Window</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/recurring-daily.png" alt="recurring maintenance window" /></p>
<p><em>Recurring maintenance window</em></p>
<p>Create a recurring maintenance window by clicking on the <strong><code>+ New downtime</code></strong> button at the top right corner. Since this is a recurring maintenance window, select the <em>Repeats every</em> field with the desired repetition type.</p>
<p>There are three types of repetition types available:</p>
<ul>
<li><strong>Daily</strong>: The maintenance window repeats every day.</li>
<li><strong>Weekly</strong>: The maintenance window repeats every week on a specific day(s) of the week. The weekly repetition type allows you to select the days of the week on which the maintenance window repeats.</li>
<li><strong>Monthly</strong>: The maintenance window repeats every month on a specific day of the month.</li>
</ul>
<p>The following fields are required to create a recurring maintenance window:</p>
<ul>
<li><strong>Name</strong>: A descriptive name for the maintenance window.</li>
<li><strong>Start Time</strong>: The start time of the maintenance window.</li>
<li><strong>Duration</strong>: The duration of the maintenance window.</li>
<li><strong>Timezone</strong>: The timezone of the maintenance window.</li>
<li><strong>Repeats every</strong>: The type of repetition.</li>
</ul>
<p>The following fields are optional:</p>
<ul>
<li><strong>Ends</strong>: The end date of the maintenance window.</li>
<li><strong>Silence Alerts</strong>: The alerts that are silenced during the maintenance window. If nothing is selected, all alerts are silenced.</li>
</ul>
<p>Examples:</p>
<ol>
<li>
<p>Daily maintenance window that repeats every day for 1 hour starting from 22:00:00 in the America/New_York timezone.</p>
<p>Start time: 2024-08-01 22:00:00
Repeats every: Daily
Duration: 1 hour
Timezone: America/New_York</p>
</li>
<li>
<p>Weekly maintenance window that repeats every week on Monday for 1 hour starting from 22:00:00 in the America/New_York timezone.</p>
<p>Start time: 2024-08-01 22:00:00
Repeats every: Weekly
Weekly Occurence: [Monday]
Duration: 1 hour
Timezone: America/New_York</p>
</li>
<li>
<p>Monthly maintenance window that repeats every month on the 12th day of the month for 1 hour starting from 22:00:00 in the America/New_York timezone.</p>
<p>Start time: 2024-08-12 22:00:00
Repeats every: Monthly
Duration: 1 hour
Timezone: America/New_York</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.9/
tag_set: operate, migration, upgrade-0.9
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.9
group_tracking_ids: docs-operate-migration-upgrade-0.9
<h2>Upgrade to v0.9 from earlier versions</h2>
<p>v0.9 is a breaking release which requires data migration, if you are upgrading from an older version then you have to run the data migration scripts to be able to see past data.</p>
<h2>First upgrade to v0.9</h2>
<hr />
<p>Follow the platform specific instructions to upgrade to 0.9 and above.</p>
<h2>Steps to run migration script:</h2>
<hr />
<h3>## For Docker</h3>
<p>Change the directory to SigNoz repo and run following commands:</p>
<pre><code>cd deploy/docker/clickhouse-setup
</code></pre>
<h4>## For ClickHouse</h4>
<pre><code>docker run --name signoz-migrate-clickhouse --network clickhouse-setup_default -it signoz/migrate:0.9-clickhouse -host=clickhouse -port=9000
</code></pre>
<h4>## For SQLite Database</h4>
<pre><code>docker run --name signoz-migrate-sqlite --network clickhouse-setup_default -it -v $PWD/data/signoz/:/var/lib/signoz/ signoz/migrate:0.9-sqlite -dataSource /var/lib/signoz/signoz.db
</code></pre>
<h3>## For Kubernetes</h3>
<h4>## ClickHouse</h4>
<pre><code>kubectl -n platform run -i -t signoz-migrate-clickhouse --image=signoz/migrate:0.9-clickhouse --restart='Never' \
  -- -host=my-release-clickhouse -port=9000 -userName=admin -password=27ff0399-0d3a-4bd8-919d-17c2181e6fb9
</code></pre>
<p>Steps to check logs:</p>
<pre><code>kubectl -n platform logs -f signoz-migrate-clickhouse
</code></pre>
<p>In case of failure and have to run again, make sure to cleanup the pod before running the migration script again.</p>
<pre><code>kubectl -n platform delete pod signoz-migrate-clickhouse
</code></pre>
<p>There are some custom flags which can be enabled based on different usecases.</p>
<p>All the flags below are <code>optional</code></p>
<p>Flags:</p>
<ul>
<li><code>-port</code> : Specify port of clickhouse. <code>default=9000</code></li>
<li><code>-host</code> : Specify host of clickhouse. <code>default=127.0.0.1</code></li>
<li><code>-userName</code> : Specify user name of clickhouse. <code>default=default</code></li>
<li><code>-password</code> : Specify password of clickhouse. <code>default=&quot;&quot;</code></li>
<li><code>-dropOldTable</code> : If it is set to true then the old tables will be dropped after data migration is successful <code>default=true</code></li>
</ul>
<h4>## SQLite</h4>
<pre><code>wget https://github.com/SigNoz/migration-0.9/releases/download/v0.9/migration-sqlite-v0.9-linux-amd64

chmod +x migration-sqlite-v0.9-linux-amd64
</code></pre>
<p>To copy the binary in persistent volume path <code>/var/lib/signoz</code> in <code>query-service</code>:</p>
<pre><code>kubectl cp -n platform ./migration-sqlite-v0.9-linux-amd64 my-release-signoz-query-service-0:/var/lib/signoz/migration-0.9
</code></pre>
<p>To <code>exec</code> into the <code>query-service</code> container:</p>
<pre><code>kubectl -n platform exec -it pod/my-release-signoz-query-service-0 -- sh
</code></pre>
<p>Now, change directory to the <code>/var/lib/signoz</code> and run the migration script:</p>
<pre><code>cd /var/lib/signoz

./migration-0.9
</code></pre>
<p>You should see output similar to this:</p>
<pre><code>2022/06/30 14:41:27 Total Dashboard found: 1
2022/06/30 14:41:27 a8763330-3828-4aa1-853d-b32a021117be
2022/06/30 14:41:27 Dashboard a8763330-3828-4aa1-853d-b32a021117be updated
2022/06/30 14:41:27 Dashboards migrated
</code></pre>
<p>At last, clean up the binary:</p>
<pre><code>rm migration-0.9
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/query-service/reset-admin-password/
tag_set: operate, query-service, reset-admin-password
image_urls: 
tracking_id: docs-operate-query-service-reset-admin-password
group_tracking_ids: docs-operate-query-service-reset-admin-password
<h2>Reset Admin Password</h2>
<p>In case you have forgotten the root admin password, you can reset it by removing all users, organisation and invites from the SQLite database.</p>
<p>The guide mainly covers the following:</p>
<ul>
<li>Exec into the <code>query-service</code> container</li>
<li>Remove all users, organisation and invites using SQLite shell</li>
<li>Restart the <code>query-service</code> container</li>
</ul>
<h2>Exec into <code>query-service</code> Container</h2>
<hr />
<h3>## Docker Standalone</h3>
<pre><code>docker exec -it signoz-query-service sh
</code></pre>
<h3>## Docker Swarm</h3>
<pre><code>docker exec -it $(docker ps -q -f name=signoz_query-service) sh
</code></pre>
<p>‚úÖ Info</p>
<p>Replace <code>signoz</code> with your Docker stack name.</p>
<h3>## Kubernetes</h3>
<pre><code>kubectl -n platform exec -it pod/my-release-signoz-query-service-0 -- sh
</code></pre>
<p>‚úÖ Info</p>
<p>Replace <code>my-release</code> with your Helm release name. And <code>platform</code> with your SigNoz namespace.</p>
<h2>Steps to Remove All Users, Organisation and Invites</h2>
<hr />
<h3>## Step 1: Install SQLite and Connect to the Database File</h3>
<pre><code>apk update
apk add sqlite

sqlite3 /var/lib/signoz/signoz.db
</code></pre>
<h3>## Step 2: Remove All Users and Organisation</h3>
<p>In the sqlite shell, run the following commands:</p>
<pre><code>select * from users;
select * from organizations;
select * from invites;

delete from users;
delete from organizations;
delete from invites;
</code></pre>
<h3>## Step 3: Verify the Users, Organisation and Invites are Removed</h3>
<pre><code>select * from users;

select * from organizations;

select * from invites;
</code></pre>
<h3>## Step 4: Exit from SQLite Shell and Container Shell</h3>
<p>To exit from the sqlite shell, press <code>CTRL + D</code> and then exit from the <code>query-service</code> container.</p>
<h3>## Step 5: Restart the <code>signoz-query-service</code> Container</h3>
<p>To restart the <code>query-service</code> container, run the following command:</p>
<p><strong>For Docker Users</strong></p>
<pre><code>docker restart signoz-query-service
</code></pre>
<p><strong>For Docker Swarm Users</strong></p>
<pre><code>docker service update --force signoz_query-service
</code></pre>
<p><strong>For Kubernetes Users</strong></p>
<pre><code>kubectl -n platform rollout restart statefulset/my-release-signoz-query-service
</code></pre>
<p>Now, you should be able to create a new admin account from SigNoz UI: <code>http://localhost:3301</code>.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fsqlite-reset-admin-password.webp&amp;w=2048&amp;q=75" alt="Reset Admin Password" /></p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics-cloud/#methods-to-send-metrics
tag_set: userguide, send-metrics-cloud
image_urls: 
tracking_id: docs-userguide-send-metrics-cloud-methods-to-send-metrics
group_tracking_ids: docs-userguide-send-metrics-cloud
<h2>Send Metrics to SigNoz Cloud: Send Metrics to SigNoz Cloud - Methods to Send Metrics</h2>
<p>There are two primary ways to send metrics to SigNoz:</p>
<ol>
<li>From your application</li>
<li>From OpenTelemetry Collector</li>
</ol>
<p>This document will cover sending metrics from the OpenTelemetry Collector.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics-cloud/#prerequisites
tag_set: userguide, send-metrics-cloud
image_urls: 
tracking_id: docs-userguide-send-metrics-cloud-prerequisites
group_tracking_ids: docs-userguide-send-metrics-cloud
<h2>Send Metrics to SigNoz Cloud: Prerequisites</h2>
<p>Before you begin, ensure you have the following:</p>
<ul>
<li>A running instance of SigNoz Cloud.</li>
<li>OpenTelemetry Collector installed.</li>
<li>Appropriate access and permissions.</li>
</ul>
<p>In this document, we will cover how to send metrics from OpenTelemetry Collector. The Collector is a swiss-army knife that can collect metrics from various sources and send them to SigNoz.</p>
<ul>
<li>
<p><a href="#how-does-opentelemetry-collector-collect-data">How does OpenTelemetry Collector collect data</a></p>
</li>
<li>
<p><a href="#enable-a-specific-metric-receiver">Enable a Specific Metric Receiver</a></p>
</li>
<li>
<p><a href="#enable-a-prometheus-receiver">Enable a Prometheus Receiver</a></p>
</li>
<li>
<p><a href="#find-metrics-available-in-signoz">Find Metrics available in SigNoz</a></p>
<ul>
<li><a href="#metrics-from-hostmetrics-receiver">Metrics from Hostmetrics receiver</a></li>
</ul>
</li>
<li>
<p><a href="#related-videos">Related Videos</a></p>
</li>
<li>
<p><a href="#get-help">Get Help</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics-cloud/#how-does-opentelemetry-collector-collect-data
tag_set: userguide, send-metrics-cloud
image_urls: 
tracking_id: docs-userguide-send-metrics-cloud-how-does-opentelemetry-collector-collect-data
group_tracking_ids: docs-userguide-send-metrics-cloud
<h2>Send Metrics to SigNoz Cloud: How does OpenTelemetry Collector collect data?</h2>
<p>Data collection in OpenTelemetry Collector is facilitated through receivers. Receivers are configured via YAML under the top-level <code>receivers</code> section. To ensure a valid configuration, at least one receiver must be enabled.</p>
<p>Below is an example of an¬†<strong><code>otlp</code></strong>¬†receiver:</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
      http:
</code></pre>
<p>The OTLP receiver accepts data through gRPC or HTTP in the <a href="https://github.com/open-telemetry/opentelemetry-proto/blob/main/docs/specification.md">OTLP</a> format.</p>
<p>Here‚Äôs a sample configuration for an otlp receiver:</p>
<pre><code>receivers:
  otlp:
    protocols:
      http:
        endpoint: &quot;localhost:4318&quot;
        cors:
          allowed_origins:
            - http://test.com
            # Origins can have wildcards with *, use * by itself to match any origin.
            - https://*.example.com
          allowed_headers:
            - Example-Header
          max_age: 7200
</code></pre>
<p>To see more configuration options for otlp receiver, you can checkout <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/receiver/otlpreceiver/README.md">this link</a>.</p>
<p>Once a receiver is configured, it needs to be¬†<strong>enabled</strong>¬†to start the data flow. This involves setting up¬†<strong>pipelines</strong>¬†within a¬†<strong><code>service</code></strong>. A¬†<strong>pipeline</strong>¬†acts as a streamlined pathway for data, outlining how it should be processed and where it should go. A pipeline comprises of the following:</p>
<ul>
<li><strong>Receivers:</strong>¬†These are entry points for data into the OpenTelemetry Collector, responsible for collecting data from various sources and feeding it into the pipeline.</li>
<li><strong>Processors:</strong> Metrics data is processed using the <strong><code>batch</code></strong> processor. This processor batches metrics before exporting them, optimizing the data flow.</li>
<li><strong>Exporters:</strong> Metrics processed through this pipeline are exported to the OTLP endpoint mentioned in the configuration file.</li>
</ul>
<p>Below is an example pipeline configuration:</p>
<pre><code>service:
  pipelines:
    metrics:
      receivers: [otlp, httpcheck]
      processors: [batch]
      exporters: [otlp]
</code></pre>
<p>Here‚Äôs a breakdown of the above metrics pipeline:</p>
<ul>
<li><strong>Receivers:</strong>¬†This pipeline is configured to receive metrics data from two sources: OTLP and HTTP Check. The¬†<strong><code>otlp</code></strong>¬†receiver collects metrics using both gRPC and HTTP protocols, while the¬†<strong><code>httpcheck</code></strong>¬†receiver gathers metrics from the HTTP endpoint.</li>
<li><strong>Processors:</strong>¬†Metrics data is processed using the¬†<strong><code>batch</code></strong>¬†processor. This processor likely batches metrics before exporting them, optimizing the data flow.</li>
<li><strong>Exporters:</strong>¬†Metrics processed through this pipeline are exported to the OTLP destination. The¬†<strong><code>otlp</code></strong>¬†exporter sends data to an endpoint specified in the configuration.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics-cloud/#enable-a-specific-metric-receiver
tag_set: userguide, send-metrics-cloud
image_urls: 
tracking_id: docs-userguide-send-metrics-cloud-enable-a-specific-metric-receiver
group_tracking_ids: docs-userguide-send-metrics-cloud
<h2>Send Metrics to SigNoz Cloud: Enable a Specific Metric Receiver</h2>
<p>SigNoz supports all the receivers that are listed in the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver">opentelemetry-collector-contrib</a> GitHub repository. To configure a new metric receiver, you must edit the <code>receivers</code> and <code>service::pipelines</code> sections of the <code>otel-collector-config.yaml</code> file. The following example shows the default configuration in which the <code>hostmetrics</code> receiver is enabled:</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
        endpoint: localhost:4317
      http:
        endpoint: localhost:4318
  hostmetrics:
    collection_interval: 30s
    scrapers:
      cpu: {}
      disk: {}
      load: {}
      filesystem: {}
      memory: {}
      network: {}
      paging: {}
      process:
        mute_process_name_error: true
        mute_process_exe_error: true
        mute_process_io_error: true
      processes: {}
processors:
  batch:
    send_batch_size: 1000
    timeout: 10s
  # Ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md
  resourcedetection:
    detectors: [env, system, ec2] # include ec2 for AWS, gcp for GCP and azure for Azure.
    # Using OTEL_RESOURCE_ATTRIBUTES envvar, env detector adds custom labels.
    timeout: 2s
    override: false
    system:
      hostname_sources: [os] # alternatively, use [dns,os] for setting FQDN as host.name and os as fallback
exporters:
  otlp:
    endpoint: &quot;ingest.{region}.signoz.cloud:443&quot; # replace {region} with your region
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
  logging:
    verbosity: detailed
service:
  telemetry:
    metrics:
      address: localhost:8888
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics/hostmetrics:
      receivers: [hostmetrics]
      processors: [resourcedetection, batch]
      exporters: [otlp]
</code></pre>
<p>To enable a new OpenTelemetry receiver, follow the steps below:</p>
<ol>
<li>
<p>Open the <code>otel-collector-config.yaml</code> file in a plain-text editor.</p>
</li>
<li>
<p>Configure your receivers. The following example shows how you can enable a <code>rabbitmq</code> receiver:</p>
<p>receivers:
otlp:
protocols:
grpc:
endpoint: localhost:4317
http:
endpoint: localhost:4318
hostmetrics:
collection_interval: 30s
scrapers:
cpu: {}
disk: {}
load: {}
filesystem: {}
memory: {}
network: {}
paging: {}
process:
mute_process_name_error: true
mute_process_exe_error: true
mute_process_io_error: true
processes: {}
rabbitmq:
endpoint: http://localhost:15672
username: &lt;RABBITMQ_USERNAME&gt;
password: &lt;RABBITMQ_PASSWORD&gt;
collection_interval: 30s
processors:
batch:
send_batch_size: 1000
timeout: 10s</p>
<h1>Ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md</h1>
<p>resourcedetection:
detectors: [env, system, ec2] # include ec2 for AWS, gcp for GCP and azure for Azure.
# Using OTEL_RESOURCE_ATTRIBUTES envvar, env detector adds custom labels.
timeout: 2s
override: false
system:
hostname_sources: [os] # alternatively, use [dns,os] for setting FQDN as host.name and os as fallback
exporters:
otlp:
endpoint: &quot;ingest.{region}.signoz.cloud:443&quot; # replace {region} with your region
tls:
insecure: false
headers:
&quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
logging:
verbosity: detailed
service:
telemetry:
metrics:
address: localhost:8888
pipelines:
metrics:
receivers: [otlp, rabbitmq]
processors: [batch]
exporters: [otlp]
metrics/hostmetrics:
receivers: [hostmetrics]
processors: [resourcedetection, batch]
exporters: [otlp]</p>
</li>
</ol>
<p>For details about configuring OpenTelemetry receivers, see the <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/receiver/README.md">README</a> page of the <code>opentelemetry-collector</code> GitHub repository.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics-cloud/#enable-a-prometheus-receiver
tag_set: userguide, send-metrics-cloud
image_urls: 
tracking_id: docs-userguide-send-metrics-cloud-enable-a-prometheus-receiver
group_tracking_ids: docs-userguide-send-metrics-cloud
<h2>Send Metrics to SigNoz Cloud: Enable a Prometheus Receiver</h2>
<p>SigNoz supports all the exporters that are listed on the <a href="https://prometheus.io/docs/instrumenting/exporters/">Exporters and Integrations</a> page of the Prometheus documentation. If you have a running Prometheus instance, and you expose metrics in Prometheus, then you can scrape them in SigNoz by configuring Prometheus receivers in the <code>receivers::prometheus::config::scrape_configs</code> section of the <code>otel-collector-config.yaml</code> file.</p>
<p>To enable a Prometheus receiver, follow the steps below:</p>
<ol>
<li>
<p>Open the <code>otel-collector-config.yaml</code> file in a plain-text editor.</p>
</li>
<li>
<p>Enable a new Prometheus receiver. Depending on your use case, there are two ways in which you can enable a new Prometheus exporter:</p>
<ul>
<li>
<p><strong>By creating a new job</strong>: The following example shows how you can enable a Prometheus receiver by creating a new job named <code>my-new-job</code>:</p>
<pre><code>  ...
  # Data sources: metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: &quot;otel-collector&quot;
          scrape_interval: 30s
          static_configs:
            - targets: [&quot;otel-collector:8889&quot;]
        - job_name: &quot;my-new-job&quot;
          scrape_interval: 30s
          static_configs:
            - targets: [&quot;localhost:8080&quot;]
  ...
# This file was truncated for brevity.
</code></pre>
</li>
<li>
<p><strong>By adding a new target to an existing job</strong>: The following example shows the default <code>otel-collector</code> job to which a new target (<code>localhost:8080</code>) was added:</p>
<pre><code>  ...
  # Data sources: metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: &quot;otel-collector&quot;
          scrape_interval: 30s
          static_configs:
            - targets: [&quot;otel-collector:8889&quot;, &quot;localhost:8080&quot;]       
  ...
# This file was truncated for brevity.
</code></pre>
</li>
</ul>
<p>Note that all the jobs are scraped in parallel, and all targets inside a job are scraped serially. For more details about configuring jobs and targets, see the following sections of the Prometheus documentation:</p>
<ul>
<li>
<p><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config"><code>&lt;scrape_config&gt;</code></a></p>
</li>
<li>
<p><a href="https://prometheus.io/docs/concepts/jobs_instances/">Jobs and Instances</a></p>
</li>
</ul>
<p>If you'd like to learn more about how to monitor Prometheus Metrics with OpenTelemetry Collector, refer to this <a href="https://signoz.io/blog/opentelemetry-collector-prometheus-receiver/#how-does-opentelemetry-collector-collect-data">blog</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics-cloud/#find-metrics-available-in-signoz
tag_set: userguide, send-metrics-cloud
image_urls: 
tracking_id: docs-userguide-send-metrics-cloud-find-metrics-available-in-signoz
group_tracking_ids: docs-userguide-send-metrics-cloud
<h2>Send Metrics to SigNoz Cloud: Find Metrics available in SigNoz</h2>
<p>You can use this metrics to plot in the <a href="/docs/userguide/manage-dashboards/">Dashboard</a> section.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics-cloud/#related-videos
tag_set: userguide, send-metrics-cloud
image_urls: 
tracking_id: docs-userguide-send-metrics-cloud-related-videos
group_tracking_ids: docs-userguide-send-metrics-cloud
<h2>Send Metrics to SigNoz Cloud: Related Videos</h2>
<ul>
<li><a href="https://youtu.be/QGJYNYzfM9o">How to view Prometheus Metrics in SigNoz</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.8.1/
tag_set: operate, migration, upgrade-0.8.1
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.8.1
group_tracking_ids: docs-operate-migration-upgrade-0.8.1
<h2>Upgrade to v0.8.1 from earlier versions</h2>
<p>SigNoz v0.8.1 requires users to run migration script for dashboard.</p>
<p>If you had created dashboard in v0.8.0 and before, you would need to run this script to sanitise dashboard data.</p>
<h2>First Upgrade to v0.8.1</h2>
<hr />
<p>Follow the platform specific instructions to upgrade to 0.8.1 and above.</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/operate/docker-standalone/#upgrade">Docker Standalone</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/docker-swarm/#upgrade">Docker Swarm</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/operate/kubernetes/#upgrade">Kubernetes</a></p>
</li>
</ul>
<p>‚ö†Ô∏è Warning</p>
<p>After upgrading to 0.8.1, it is recommended to run the migration script before modifying any existing dashboards. It could potentially cause irreversible changes to dashboard data.</p>
<h2>Steps to run migration script</h2>
<hr />
<h3>## For Docker</h3>
<p><code>cd</code> to SigNoz repository and run following commands:</p>
<pre><code>cd deploy/docker/clickhouse-setup

docker run -it -v $PWD/data/signoz/:/var/lib/signoz/ signoz/migrate-dashboard:0.8.1
</code></pre>
<p>Output should be similar as below:</p>
<pre><code>2022/06/08 18:27:49 Total Dashboard found: 2
2022/06/08 18:27:49 e2e2ff6d-29ca-444c-8625-d64218a990bc
2022/06/08 18:27:49 683ac919-b858-4387-b14f-bebd55f074fa
2022/06/08 18:27:49 Dashboard e2e2ff6d-29ca-444c-8625-d64218a990bc updated
2022/06/08 18:27:49 Dashboard 683ac919-b858-4387-b14f-bebd55f074fa updated
2022/06/08 18:27:49 Dashboards migrated
</code></pre>
<h3>## For Docker Swarm</h3>
<p><code>cd</code> to SigNoz repository and run following commands:</p>
<pre><code>cd deploy/swarm/clickhouse-setup

docker run -it -v $PWD/data/signoz/:/var/lib/signoz/ signoz/migrate-dashboard:0.8.1
</code></pre>
<p>Output should be similar as below:</p>
<pre><code>2022/06/08 18:27:49 Total Dashboard found: 2
2022/06/08 18:27:49 e2e2ff6d-29ca-444c-8625-d64218a990bc
2022/06/08 18:27:49 683ac919-b858-4387-b14f-bebd55f074fa
2022/06/08 18:27:49 Dashboard e2e2ff6d-29ca-444c-8625-d64218a990bc updated
2022/06/08 18:27:49 Dashboard 683ac919-b858-4387-b14f-bebd55f074fa updated
2022/06/08 18:27:49 Dashboards migrated
</code></pre>
<p>‚úÖ Info</p>
<p>In case of multi node swarm cluster, run the above commands in the node where query-service is running. To find out which node: <code>docker service ps query-service</code>.</p>
<h3>## For Kubernetes</h3>
<p>To download <code>migrate-dashboard</code> binary:</p>
<pre><code>wget https://github.com/signoz/migrate-dashboard/releases/download/v0.8.1/migrate-dashboard-v0.8.1-linux-amd64.tar.gz

tar xzvf migrate-dashboard-v0.8.1-linux-amd64.tar.gz
</code></pre>
<p>To copy the binary in persistent volume path <code>/var/lib/signoz</code> in <code>query-service</code>:</p>
<pre><code>kubectl cp -n platform ./migrate-dashboard my-release-signoz-query-service-0:/var/lib/signoz/migrate-dashboard
</code></pre>
<p>To <code>exec</code> into the <code>query-service</code> container:</p>
<pre><code>kubectl -n platform exec -it pod/my-release-signoz-query-service-0 -- sh
</code></pre>
<p>Now, change directory to the <code>/var/lib/signoz</code> and run the migration script:</p>
<pre><code>cd /var/lib/signoz

./migrate-dashboard
</code></pre>
<p>You should see output similar to this:</p>
<pre><code>2022/06/08 18:27:49 Total Dashboard found: 3
2022/06/08 18:27:49 e2e2ff6d-29ca-444c-8625-d64218a990bc
2022/06/08 18:27:49 683ac919-b858-4387-b14f-bebd55f074fa
2022/06/08 18:27:49 f10d6c5e-fb2d-4e4c-9c37-f0b2fdf7f3db
2022/06/08 18:27:49 Dashboard e2e2ff6d-29ca-444c-8625-d64218a990bc updated
2022/06/08 18:27:49 Dashboard 683ac919-b858-4387-b14f-bebd55f074fa updated
2022/06/08 18:27:49 Dashboard f10d6c5e-fb2d-4e4c-9c37-f0b2fdf7f3db updated
2022/06/08 18:27:49 Dashboards migrated
</code></pre>
<p>At last, clean up the binary:</p>
<pre><code>rm migrate-dashboard
</code></pre>
<h2>Command-Line Interface (CLI) Flags</h2>
<hr />
<p>There are is only one flag in the <code>migrate-dashboard</code> binary:</p>
<ul>
<li><code>--dataSource</code> : Data Source path. <code>default=/var/lib/signoz/signoz.db</code></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#traces-schema
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-traces-schema
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<h2>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema</h2>
<p>Traces has 6 tables used for different usecases</p>
<p>All Traces table follows <a href="https://github.com/open-telemetry/semantic-conventions/blob/main/docs/general/trace.md">OpenTelemetry Trace Semantic conventions</a></p>
<p>‚úÖ Info</p>
<p>The distributed tables in clickhouse have been named by prefixing¬†distributed_¬†to existing single shard table names. If you want to use clickhouse queries in dashboard or alerts, you should use the distributed table names. Eg,¬†signoz_index_v2¬†now corresponds to the table of a single shard. To query all the shards, query against¬†distributed_signoz_index_v2.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#distributed_signoz_index_v2
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-distributed_signoz_index_v2
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: distributed_signoz_index_v2</p>
<p>This is primary table of Traces which is queried to fetch spans and apply aggregation on spans. It has over 30 different columns which helps in faster filtering on most common attributes following <a href="https://github.com/open-telemetry/semantic-conventions/blob/main/docs/general/trace.md">OpenTelemetry Trace Semantic conventions</a>.</p>
<pre><code>(
    `timestamp` DateTime64(9) CODEC(DoubleDelta, LZ4),
    `traceID` FixedString(32) CODEC(ZSTD(1)),
    `spanID` String CODEC(ZSTD(1)),
    `parentSpanID` String CODEC(ZSTD(1)),
    `serviceName` LowCardinality(String) CODEC(ZSTD(1)),
    `name` LowCardinality(String) CODEC(ZSTD(1)),
    `kind` Int8 CODEC(T64, ZSTD(1)),
    `durationNano` UInt64 CODEC(T64, ZSTD(1)),
    `statusCode` Int16 CODEC(T64, ZSTD(1)),
    `component` LowCardinality(String) CODEC(ZSTD(1)),
    `dbSystem` LowCardinality(String) CODEC(ZSTD(1)),
    `dbName` LowCardinality(String) CODEC(ZSTD(1)),
    `dbOperation` LowCardinality(String) CODEC(ZSTD(1)),
    `peerService` LowCardinality(String) CODEC(ZSTD(1)),
    `events` Array(String) CODEC(ZSTD(2)),
    `httpMethod` LowCardinality(String) CODEC(ZSTD(1)),
    `httpUrl` LowCardinality(String) CODEC(ZSTD(1)),
    `httpRoute` LowCardinality(String) CODEC(ZSTD(1)),
    `httpHost` LowCardinality(String) CODEC(ZSTD(1)),
    `msgSystem` LowCardinality(String) CODEC(ZSTD(1)),
    `msgOperation` LowCardinality(String) CODEC(ZSTD(1)),
    `hasError` Bool CODEC(T64, ZSTD(1)),
    `rpcSystem` LowCardinality(String) CODEC(ZSTD(1)),
    `rpcService` LowCardinality(String) CODEC(ZSTD(1)),
    `rpcMethod` LowCardinality(String) CODEC(ZSTD(1)),
    `responseStatusCode` LowCardinality(String) CODEC(ZSTD(1)),
    `stringTagMap` Map(String, String) CODEC(ZSTD(1)),
    `numberTagMap` Map(String, Float64) CODEC(ZSTD(1)),
    `boolTagMap` Map(String, Bool) CODEC(ZSTD(1)),
    `resourceTagsMap` Map(LowCardinality(String), String) CODEC(ZSTD(1)),
    INDEX idx_service serviceName TYPE bloom_filter GRANULARITY 4,
    INDEX idx_name name TYPE bloom_filter GRANULARITY 4,
    INDEX idx_kind kind TYPE minmax GRANULARITY 4,
    INDEX idx_duration durationNano TYPE minmax GRANULARITY 1,
    INDEX idx_hasError hasError TYPE set(2) GRANULARITY 1,
    INDEX idx_httpRoute httpRoute TYPE bloom_filter GRANULARITY 4,
    INDEX idx_httpUrl httpUrl TYPE bloom_filter GRANULARITY 4,
    INDEX idx_httpHost httpHost TYPE bloom_filter GRANULARITY 4,
    INDEX idx_httpMethod httpMethod TYPE bloom_filter GRANULARITY 4,
    INDEX idx_timestamp timestamp TYPE minmax GRANULARITY 1,
    INDEX idx_rpcMethod rpcMethod TYPE bloom_filter GRANULARITY 4,
    INDEX idx_responseStatusCode responseStatusCode TYPE set(0) GRANULARITY 1,
    INDEX idx_resourceTagsMapKeys mapKeys(resourceTagsMap) TYPE bloom_filter(0.01) GRANULARITY 64,
    INDEX idx_resourceTagsMapValues mapValues(resourceTagsMap) TYPE bloom_filter(0.01) GRANULARITY 64,
    PROJECTION timestampSort
    (
        SELECT *
        ORDER BY timestamp
    )
)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#columns-in-the-distributed_signoz_index_v2-table
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-columns-in-the-distributed_signoz_index_v2-table
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: Columns in the distributed_signoz_index_v2 table</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>timestamp</strong></td>
<td>Time when the span generated at the source</td>
</tr>
<tr>
<td><strong>traceID</strong></td>
<td>Trace ID. <a href="https://www.w3.org/TR/trace-context/#trace-id">W3C Trace Context</a></td>
</tr>
<tr>
<td><strong>spanID</strong></td>
<td>Span ID</td>
</tr>
<tr>
<td><strong>parentSpanID</strong></td>
<td>Parent Span ID</td>
</tr>
<tr>
<td><strong>serviceName</strong></td>
<td>Name of the service</td>
</tr>
<tr>
<td><strong>name</strong></td>
<td>Name of the span</td>
</tr>
<tr>
<td><strong>kind</strong></td>
<td>Kind of the span. <a href="https://opentelemetry.io/docs/concepts/signals/traces/#span-kind">OpenTelemetry Span Kind</a></td>
</tr>
<tr>
<td><strong>durationNano</strong></td>
<td>Duration of the span in nanoseconds</td>
</tr>
<tr>
<td><strong>statusCode</strong></td>
<td>Status code of the span. <a href="https://opentelemetry.io/docs/concepts/signals/traces/#span-status">OpenTelemetry Status Code</a></td>
</tr>
<tr>
<td><strong>component</strong></td>
<td>Component of the span. Eg: <code>http</code>, <code>grpc</code> etc. Derived from <code>component</code> attribute of a span</td>
</tr>
<tr>
<td><strong>dbSystem</strong></td>
<td>Database system of the span. Eg: <code>mysql</code>, <code>postgres</code> etc. Derived from <code>db.system</code> attribute of a span</td>
</tr>
<tr>
<td><strong>dbName</strong></td>
<td>Database name of the span. Derived from <code>db.name</code> attribute of a span</td>
</tr>
<tr>
<td><strong>dbOperation</strong></td>
<td>Database operation of the span. Derived from <code>db.operation</code> attribute of a span</td>
</tr>
<tr>
<td><strong>peerService</strong></td>
<td>Peer service of the span. Derived from <code>peer.service</code> attribute of a span</td>
</tr>
<tr>
<td><strong>events</strong></td>
<td>Events of the span. It is an array of stringified json of span events</td>
</tr>
<tr>
<td><strong>httpMethod</strong></td>
<td>HTTP method of the span. Derived from <code>http.method</code> attribute of a span</td>
</tr>
<tr>
<td><strong>httpUrl</strong></td>
<td>HTTP url of the span. Derived from <code>http.url</code> attribute of a span</td>
</tr>
<tr>
<td><strong>httpRoute</strong></td>
<td>HTTP route of the span. Derived from <code>http.route</code> attribute of a span</td>
</tr>
<tr>
<td><strong>httpHost</strong></td>
<td>HTTP host of the span. Derived from <code>http.host</code> attribute of a span</td>
</tr>
<tr>
<td><strong>msgSystem</strong></td>
<td>Messaging system of the span. Derived from <code>messaging.system</code> attribute of a span</td>
</tr>
<tr>
<td><strong>msgOperation</strong></td>
<td>Messaging operation of the span. Derived from <code>messaging.operation</code> attribute of a span</td>
</tr>
<tr>
<td><strong>hasError</strong></td>
<td>Whether the span has error or not</td>
</tr>
<tr>
<td><strong>rpcSystem</strong></td>
<td>RPC system of the span. Derived from <code>rpc.system</code> attribute of a span</td>
</tr>
<tr>
<td><strong>rpcService</strong></td>
<td>RPC service of the span. Derived from <code>rpc.service</code> attribute of a span</td>
</tr>
<tr>
<td><strong>rpcMethod</strong></td>
<td>RPC method of the span. Derived from <code>rpc.method</code> attribute of a span</td>
</tr>
<tr>
<td><strong>responseStatusCode</strong></td>
<td>Response status code of the span. Derived from <code>http.status_code</code>, <code>rpc.grpc.status_code</code> and <code>rpc.jsonrpc.error_code</code> attribute of a span</td>
</tr>
<tr>
<td><strong>stringTagMap</strong></td>
<td>Map of all string tags/attributes of the span</td>
</tr>
<tr>
<td><strong>numberTagMap</strong></td>
<td>Map of all number tags/attributes of the span</td>
</tr>
<tr>
<td><strong>boolTagMap</strong></td>
<td>Map of all bool tags/attributes of the span</td>
</tr>
<tr>
<td><strong>resourceTagsMap</strong></td>
<td>Map of all resource tags/attributes of the span</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#distributed_signoz_spans
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-distributed_signoz_spans
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: distributed_signoz_spans</p>
<p>This table stores span json model which is used to create trace tree and displayed on trace detail page.</p>
<pre><code>(
    `timestamp` DateTime64(9) CODEC(DoubleDelta, LZ4),
    `traceID` FixedString(32) CODEC(ZSTD(1)),
    `model` String CODEC(ZSTD(9))
)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#columns-in-the-distributed_signoz_spans-table
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-columns-in-the-distributed_signoz_spans-table
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: Columns in the distributed_signoz_spans table</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>timestamp</strong></td>
<td>Time when the span generated at the source</td>
</tr>
<tr>
<td><strong>traceID</strong></td>
<td>Trace ID. <a href="https://www.w3.org/TR/trace-context/#trace-id">W3C Trace Context</a></td>
</tr>
<tr>
<td><strong>model</strong></td>
<td>Stringified json of the span</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#distributed_signoz_error_index_v2
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-distributed_signoz_error_index_v2
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: distributed_signoz_error_index_v2</p>
<p>This table stores error events derived from spans</p>
<pre><code>(
    `timestamp` DateTime64(9) CODEC(DoubleDelta, LZ4),
    `errorID` FixedString(32) CODEC(ZSTD(1)),
    `groupID` FixedString(32) CODEC(ZSTD(1)),
    `traceID` FixedString(32) CODEC(ZSTD(1)),
    `spanID` String CODEC(ZSTD(1)),
    `serviceName` LowCardinality(String) CODEC(ZSTD(1)),
    `exceptionType` LowCardinality(String) CODEC(ZSTD(1)),
    `exceptionMessage` String CODEC(ZSTD(1)),
    `exceptionStacktrace` String CODEC(ZSTD(1)),
    `exceptionEscaped` Bool CODEC(T64, ZSTD(1)),
    `resourceTagsMap` Map(LowCardinality(String), String) CODEC(ZSTD(1)),
    INDEX idx_error_id errorID TYPE bloom_filter GRANULARITY 4,
    INDEX idx_resourceTagsMapKeys mapKeys(resourceTagsMap) TYPE bloom_filter(0.01) GRANULARITY 64,
    INDEX idx_resourceTagsMapValues mapValues(resourceTagsMap) TYPE bloom_filter(0.01) GRANULARITY 64
)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#columns-in-the-distributed_signoz_error_index_v2-table
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-columns-in-the-distributed_signoz_error_index_v2-table
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: Columns in the distributed_signoz_error_index_v2 table</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>timestamp</strong></td>
<td>Time when the span generated at the source</td>
</tr>
<tr>
<td><strong>errorID</strong></td>
<td>Error ID. <a href="https://www.w3.org/TR/trace-context/#trace-id">W3C Trace Context</a></td>
</tr>
<tr>
<td><strong>groupID</strong></td>
<td>Group ID of the error</td>
</tr>
<tr>
<td><strong>traceID</strong></td>
<td>Trace ID. <a href="https://www.w3.org/TR/trace-context/#trace-id">W3C Trace Context</a></td>
</tr>
<tr>
<td><strong>spanID</strong></td>
<td>Span ID</td>
</tr>
<tr>
<td><strong>serviceName</strong></td>
<td>Name of the service. Derived from <code>service.name</code> attribute of a span</td>
</tr>
<tr>
<td><strong>exceptionType</strong></td>
<td>Exception type of the error. Derived from <code>exception.type</code> attribute of a span</td>
</tr>
<tr>
<td><strong>exceptionMessage</strong></td>
<td>Exception message of the error. Derived from <code>exception.message</code> attribute of a span</td>
</tr>
<tr>
<td><strong>exceptionStacktrace</strong></td>
<td>Exception stacktrace of the error. Derived from <code>exception.stacktrace</code> attribute of a span</td>
</tr>
<tr>
<td><strong>exceptionEscaped</strong></td>
<td>Whether the exception is escaped or not. Derived from <code>exception.escaped</code> attribute of a span</td>
</tr>
<tr>
<td><strong>resourceTagsMap</strong></td>
<td>Map of all resource tags/attributes of the span</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#distributed_top_level_operations
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-distributed_top_level_operations
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: distributed_top_level_operations</p>
<p>This table stores top operations and service name.</p>
<pre><code>(
    `name` LowCardinality(String) CODEC(ZSTD(1)),
    `serviceName` LowCardinality(String) CODEC(ZSTD(1))
)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#columns-in-the-distributed_top_level_operations-table
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-columns-in-the-distributed_top_level_operations-table
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: Columns in the distributed_top_level_operations table</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>name</strong></td>
<td>Name of the span</td>
</tr>
<tr>
<td><strong>serviceName</strong></td>
<td>Name of the service</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#distributed_span_attributes_keys
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-distributed_span_attributes_keys
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: distributed_span_attributes_keys</p>
<p>This table stores all the attributes keys of the span.</p>
<pre><code>(
    `tagKey` LowCardinality(String) CODEC(ZSTD(1)),
    `tagType` Enum8('tag' = 1, 'resource' = 2) CODEC(ZSTD(1)),
    `dataType` Enum8('string' = 1, 'bool' = 2, 'float64' = 3) CODEC(ZSTD(1)),
    `isColumn` Bool CODEC(ZSTD(1))
)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#columns-in-the-distributed_span_attributes_keys-table
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-columns-in-the-distributed_span_attributes_keys-table
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: Columns in the distributed_span_attributes_keys table</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>tagKey</strong></td>
<td>Name of the attribute</td>
</tr>
<tr>
<td><strong>tagType</strong></td>
<td>Type of the attribute. It can be <code>tag</code> or <code>resource</code></td>
</tr>
<tr>
<td><strong>dataType</strong></td>
<td>Data type of the attribute. It can be <code>string</code>, <code>bool</code> or <code>float64</code></td>
</tr>
<tr>
<td><strong>isColumn</strong></td>
<td>Whether the attribute is a column or not</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#distributed_span_attributes
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-distributed_span_attributes
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: distributed_span_attributes</p>
<p>This table stores all the attributes of the span.</p>
<pre><code>(
    `timestamp` DateTime CODEC(DoubleDelta, ZSTD(1)),
    `tagKey` LowCardinality(String) CODEC(ZSTD(1)),
    `tagType` Enum8('tag' = 1, 'resource' = 2) CODEC(ZSTD(1)),
    `dataType` Enum8('string' = 1, 'bool' = 2, 'float64' = 3) CODEC(ZSTD(1)),
    `stringTagValue` String CODEC(ZSTD(1)),
    `float64TagValue` Nullable(Float64) CODEC(ZSTD(1)),
    `isColumn` Bool CODEC(ZSTD(1))
)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#columns-in-the-distributed_span_attributes-table
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-columns-in-the-distributed_span_attributes-table
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<p>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: Columns in the distributed_span_attributes table</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>timestamp</strong></td>
<td>Time when the span generated at the source</td>
</tr>
<tr>
<td><strong>tagKey</strong></td>
<td>Name of the attribute</td>
</tr>
<tr>
<td><strong>tagType</strong></td>
<td>Type of the attribute. It can be <code>tag</code> or <code>resource</code></td>
</tr>
<tr>
<td><strong>dataType</strong></td>
<td>Data type of the attribute. It can be <code>string</code>, <code>bool</code> or <code>float64</code></td>
</tr>
<tr>
<td><strong>stringTagValue</strong></td>
<td>String value of the attribute</td>
</tr>
<tr>
<td><strong>float64TagValue</strong></td>
<td>Float64 value of the attribute</td>
</tr>
<tr>
<td><strong>isColumn</strong></td>
<td>Whether the attribute is a column or not</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/writing-clickhouse-traces-query/#writing-clickhouse-queries-for-dashboard-panels
tag_set: userguide, writing-clickhouse-traces-query
image_urls: 
tracking_id: docs-userguide-writing-clickhouse-traces-query-writing-clickhouse-queries-for-dashboard-panels
group_tracking_ids: docs-userguide-writing-clickhouse-traces-query
<h2>Writing traces based ClickHouse queries for building dashboard panels: Writing traces based ClickHouse queries for building dashboard panels - Traces Schema: Writing Clickhouse Queries for Dashboard Panels</h2>
<h3>## Timeseries</h3>
<p>This panel is used when you want to view your aggregated data in a timeseries.</p>
<h4>## Examples</h4>
<h5>## Plotting a chart on 100ms interval</h5>
<p>Plot a chart of 1 minute showing count of spans in 100ms interval of service frontend with duration &gt; 50ms</p>
<pre><code>SELECT fromUnixTimestamp64Milli(intDiv( toUnixTimestamp64Milli ( timestamp ), 100) * 100) AS interval, 
toFloat64(count()) AS count 
FROM (
 SELECT timestamp 
 FROM signoz_traces.distributed_signoz_index_v2 
 WHERE serviceName='frontend' 
 AND durationNano&gt;=50*exp10(6) 
 AND timestamp BETWEEN {{.start_datetime}} AND {{.end_datetime}})
 GROUP BY interval ORDER BY interval ASC;
</code></pre>
<h3>## Value</h3>
<p>For the value type panel, the overall query will be similar to timeseries, just that you will have to get the absolute value at the end. You can reduce your end result to either average, latest, sum, min, or max.</p>
<h4>## Examples</h4>
<h5>## Average duration of spans where <code>method = 'POST'</code> , <code>service_name = 'sample-service'</code></h5>
<pre><code>SELECT 
    avg(value) as value, 
    any(ts) as ts FROM (
        SELECT 
            toStartOfInterval((timestamp), INTERVAL 1 MINUTE) AS ts, 
            toFloat64(count()) AS value 
        FROM 
            signoz_traces.distributed_signoz_index_v2
        WHERE 
            timestamp BETWEEN {{.start_datetime}} AND {{.end_datetime}} AND
            httpMethod = 'POST' AND 
            serviceName = 'sample-service'
        GROUP BY ts 
        ORDER BY ts ASC
    )
</code></pre>
<h3>## Table</h3>
<p>This is used when you want to view the timeseries data in a tabular format.</p>
<p>The query is similar to timeseries query but instead of using time interval we use just use <code>now() as ts</code> in select.</p>
<h4>## Examples</h4>
<h5>## GroupBy a tag/attribute in distributed tracing data</h5>
<pre><code>SELECT now() as ts, 
stringTagMap['example_string_attribute'] AS attribute_name, 
toFloat64(avg(durationNano)) AS value 
FROM signoz_traces.distributed_signoz_index_v2  
WHERE timestamp BETWEEN {{.start_datetime}} AND {{.end_datetime}}
GROUP BY (attribute_name, ts) order by (attribute_name, ts) ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics/
tag_set: userguide, send-metrics
image_urls: 
tracking_id: docs-userguide-send-metrics
group_tracking_ids: docs-userguide-send-metrics
<h2>Send Metrics to SigNoz (Self Hosted)</h2>
<p>By default, when you install SigNoz, only the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md">Hostmetric receiver</a> is enabled. Before you can query other metrics, you must first enable additional receivers in SigNoz.</p>
<p>There are two ways in which you can send metrics to SigNoz using OpenTelemetry:</p>
<ul>
<li>
<p><a href="#enable-a-specific-metric-receiver">Enable a Specific Metric Receiver</a></p>
</li>
<li>
<p><a href="#enable-a-prometheus-receiver">Enable a Prometheus Receiver</a></p>
</li>
<li>
<p><a href="#find-metrics-available-in-signoz">Find Metrics available in SigNoz</a></p>
<ul>
<li><a href="#metrics-from-hostmetrics-receiver">Metrics from Hostmetrics receiver</a></li>
</ul>
</li>
<li>
<p><a href="#related-videos">Related Videos</a></p>
</li>
<li>
<p><a href="#get-help">Get Help</a></p>
</li>
</ul>
<p>Depending on your choice, use one of the sections below.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics/#enable-a-specific-metric-receiver
tag_set: userguide, send-metrics
image_urls: 
tracking_id: docs-userguide-send-metrics-enable-a-specific-metric-receiver
group_tracking_ids: docs-userguide-send-metrics
<h2>Send Metrics to SigNoz (Self Hosted): Enable a Specific Metric Receiver</h2>
<p>SigNoz supports all the receivers that are listed in the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver">opentelemetry-collector-contrib</a> GitHub repository. To configure a new metric receiver, you must edit the <code>receivers</code> section of the <code>deploy/docker/clickhouse-setup/otel-collector-config.yaml</code> file. The following example shows the default configuration in which the <code>hostmetrics</code> receiver is enabled:</p>
<pre><code>receivers:
  otlp/spanmetrics:
    protocols:
      grpc:
        endpoint: &quot;localhost:12345&quot;
  otlp:
    protocols:
      grpc:
      http:
  jaeger:
    protocols:
      grpc:
      thrift_http:
  hostmetrics:
    collection_interval: 30s
    scrapers:
      cpu:
      load:
      memory:
      disk:
      filesystem:
      network:
processors:
  batch:
    send_batch_size: 1000
    timeout: 10s
# This file was truncated for brevity
</code></pre>
<p>To enable a new OpenTelemetry receiver, follow the steps below:</p>
<ol>
<li>
<p>Move into the directory in which you installed SigNoz, and open the <code>deploy/docker/clickhouse-setup/otel-collector-config.yaml</code> file in a plain-text editor.</p>
</li>
<li>
<p>Configure your receivers. The following example shows how you can enable a receiver named <code>examplereceiver</code>:</p>
<p>receivers:
otlp/spanmetrics:
protocols:
grpc:
endpoint: &quot;localhost:12345&quot;
otlp:
protocols:
grpc:
http:
jaeger:
protocols:
grpc:
thrift_http:
hostmetrics:
collection_interval: 30s
scrapers:
cpu:
load:
memory:
disk:
filesystem:
network:
examplereceiver:
endpoint: 1.2.3.4:8080
processors:
batch:
send_batch_size: 1000
timeout: 10s</p>
<h1>This file was truncated for brevity.</h1>
</li>
</ol>
<p>For details about configuring OpenTelemetry receivers, see the <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/receiver/README.md">README</a> page of the <code>opentelemetry-collector</code> GitHub repository. 3.</p>
<p>Save the changes you‚Äôve made and restart SigNoz. Depending on how you installed SigNoz, follow the steps in one of the sections below:</p>
<ul>
<li>
<p><a href="/docs/install/docker">Docker Standalone</a></p>
</li>
<li>
<p><a href="/docs/install/docker-swarm">Docker Swarm</a></p>
</li>
<li>
<p><a href="/docs/install/kubernetes">Kubernetes</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics/#enable-a-prometheus-receiver
tag_set: userguide, send-metrics
image_urls: 
tracking_id: docs-userguide-send-metrics-enable-a-prometheus-receiver
group_tracking_ids: docs-userguide-send-metrics
<h2>Send Metrics to SigNoz (Self Hosted): Enable a Prometheus Receiver</h2>
<p>SigNoz supports all the exporters that are listed on the <a href="https://prometheus.io/docs/instrumenting/exporters/">Exporters and Integrations</a> page of the Prometheus documentation. If you have a running Prometheus instance, and you expose metrics in Prometheus, then you can scrape them in SigNoz by configuring Prometheus receivers in the <code>receivers.prometheus.config.scrape_configs</code> section of the <code>deploy/docker/clickhouse-setup/otel-collector-config.yaml</code> file.</p>
<p>To enable a Prometheus receiver, follow the steps below:</p>
<ol>
<li>
<p>Open the <code>deploy/docker/clickhouse-setup/otel-collector-config.yaml</code> file in a plain-text editor.</p>
</li>
<li>
<p>Enable a new Prometheus receiver. Depending on your use case, there are two ways in which you can enable a new Prometheus exporter:</p>
<ul>
<li>
<p><strong>By creating a new job</strong>: The following example shows how you can enable a Prometheus receiver by creating a new job named <code>my-new-job</code>:</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
      http:

  # Data sources: metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: &quot;otel-collector&quot;
          scrape_interval: 30s
          static_configs:
            - targets: [&quot;otel-collector:8889&quot;]
        - job_name: &quot;my-new-job&quot;
          scrape_interval: 30s
          static_configs:
            - targets: [&quot;localhost:8080&quot;]          
processors:
  batch:
    send_batch_size: 1000
    timeout: 10s
# This file was truncated for brevity.
</code></pre>
</li>
<li>
<p><strong>By adding a new target to an existing job</strong>: The following example shows the default <code>otel-collector</code> job to which a new target (<code>localhost:8080</code>) was added:</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
      http:

  # Data sources: metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: &quot;otel-collector&quot;
          scrape_interval: 30s
          static_configs:
            - targets: [&quot;otel-collector:8889&quot;, &quot;localhost:8080&quot;]       
processors:
  batch:
    send_batch_size: 1000
    timeout: 10s
# This file was truncated for brevity.
</code></pre>
</li>
</ul>
<p>‚úÖ Info</p>
<p>To correctly scrape from prometheus instance running on the host machine, replace <code>localhost:8080</code> with <code>172.17.0.1:8080</code> for Linux and <code>host.docker.internal:8080</code> for MacOS.</p>
<p>Note that all the jobs are scraped in parallel, and all targets inside a job are scraped serially. For more details about configuring jobs and targets, see the following sections of the Prometheus documentation:</p>
<ul>
<li>
<p><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config"><code>&lt;scrape_config</code>&gt;</a></p>
</li>
<li>
<p><a href="https://prometheus.io/docs/concepts/jobs_instances/">Jobs and Instances</a></p>
</li>
</ul>
</li>
<li>
<p>Save the changes you‚Äôve made and restart SigNoz. Depending on how you installed SigNoz, follow the steps in one of the sections below:</p>
<ul>
<li>
<p><a href="/docs/install/docker">Docker Standalone</a></p>
</li>
<li>
<p><a href="/docs/install/docker-swarm">Docker Swarm</a></p>
</li>
<li>
<p><a href="/docs/install/kubernetes">Kubernetes</a></p>
</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics/#find-metrics-available-in-signoz
tag_set: userguide, send-metrics
image_urls: 
tracking_id: docs-userguide-send-metrics-find-metrics-available-in-signoz
group_tracking_ids: docs-userguide-send-metrics
<h2>Send Metrics to SigNoz (Self Hosted): Find Metrics available in SigNoz</h2>
<p>You can connect to SigNoz's ClickHouse instance and find the metrics SigNoz is storing. As of now this is bit of manual process, we are bringing capabilities to auto-suggest available metrics soon.</p>
<p>You can follow the below steps:</p>
<ol>
<li>
<p><a href="https://clickhouse.com/docs/en/getting-started/install/">Install ClickHouse client</a></p>
</li>
<li>
<p>Connect to the ClickHouse container</p>
<p>docker exec -it signoz-clickhouse bash</p>
</li>
<li>
<p>Run the clickhouse-client command to connect to the database service</p>
<p>clickhouse client --host &lt;SigNoz IP&gt;  --port 9000</p>
</li>
<li>
<p>Run the query to list metrics</p>
<p>select DISTINCT(JSONExtractString(time_series_v2.labels,'<strong>name</strong>')) as metrics from signoz_metrics.distributed_time_series_v2</p>
</li>
<li>
<p>If needed, dump in a csv file and parse it locally</p>
<p>select DISTINCT(labels) from signoz_metrics.distributed_time_series_v2 INTO OUTFILE 'output.csv'</p>
</li>
</ol>
<p>You can use this metrics to plot in the <a href="/docs/userguide/manage-dashboards/">Dashboard</a> section.</p>
<h3>## Metrics from Hostmetrics receiver</h3>
<p>Metrics which are available if hostmetrics is enabled. This is enabled in SigNoz default installation.</p>
<table>
<thead>
<tr>
<th>Metrics</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>system_filesystem_usage_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_network_dropped_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_cpu_time_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_disk_merged_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_disk_io_time_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_disk_operations_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_network_errors_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_network_io_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_disk_weighted_io_time_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_network_packets_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_disk_operation_time_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_cpu_load_average_5m</code></td>
<td></td>
</tr>
<tr>
<td><code>system_memory_usage_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_disk_pending_operations_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_disk_io_total</code></td>
<td></td>
</tr>
<tr>
<td><code>system_cpu_load_average_15m</code></td>
<td></td>
</tr>
<tr>
<td><code>system_cpu_load_average_1m</code></td>
<td></td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-metrics/#related-videos
tag_set: userguide, send-metrics
image_urls: 
tracking_id: docs-userguide-send-metrics-related-videos
group_tracking_ids: docs-userguide-send-metrics
<h2>Send Metrics to SigNoz (Self Hosted): Related Videos</h2>
<ul>
<li><a href="https://youtu.be/QGJYNYzfM9o">How to view Prometheus Metrics in SigNoz</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/alerts-management/
tag_set: userguide, alerts-management
image_urls: 
tracking_id: docs-userguide-alerts-management
group_tracking_ids: docs-userguide-alerts-management
<h2>Alerts</h2>
<p>Alerts in SigNoz can help you to define which data to monitor, set thresholds to detect potential problems, and specify who should be notified and how. This can help you to identify critical issues and reduce noise. This document will help you in understanding how to set up and use alerts effectively.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/alerts-management/#managing-alerts
tag_set: userguide, alerts-management
image_urls: 
tracking_id: docs-userguide-alerts-management-managing-alerts
group_tracking_ids: docs-userguide-alerts-management
<h2>Alerts: Managing Alerts</h2>
<p><img src="https://signoz.io/img/docs/product-features/alerts/alerts-alert-rules-tab.gif" alt="A gif explaining the Alerts Rules Tab in SigNoz" /></p>
<p><em>Features of Alert Rules Tab</em></p>
<p>The Alert Rules Tab in SigNoz provides an overview of the alert defined by the user. This section allows you to view, edit, or manage alert rules, along with their associated metadata. Here's a breakdown of the features available:</p>
<h3>## Alert Rule Columns</h3>
<ul>
<li><strong>Status</strong>: Indicates whether the alert rule is enabled (OK) or disabled.</li>
<li><strong>Alert Name</strong>: The name given to the alert rule for easy identification.</li>
<li><strong>Severity</strong>: The level of severity assigned to the alert. For example, <code>warning</code>, <code>critical</code> etc.</li>
<li><strong>Labels</strong>: Displays any labels associated with the alert rule. Labels can help in categorizing alerts.</li>
</ul>
<h3>## Additional Alert Rule Options</h3>
<ul>
<li><strong>Filter by Created At, Created By, Updated At, and Updated By</strong>: The filter option in the top-right corner allows you to customize which fields are displayed. You can choose to show fields like when was the alert created who created the alert, when it was last updated, and who updated it.</li>
<li><strong>Sorting Columns</strong>: By hovering over a column name and clicking it, you can sort the list of alert rules in ascending or descending order based on that column's data.</li>
<li><strong>New Alert</strong>: At the top-right corner, the &quot;<strong>+ New Alert</strong>&quot; button lets you create a new alert rule.</li>
</ul>
<h3>## Navigation and Search</h3>
<ul>
<li><strong>Search Bar</strong>: At the top of the tab, you can search for specific alert rules by <strong>name</strong>, <strong>severity</strong>, or <strong>label</strong>.</li>
<li><strong>Pagination Controls</strong>: At the bottom-right corner, you can navigate through multiple pages of alert rules.</li>
<li><strong>Actions Menu</strong>: Found on the right side of each row, this menu allows you to perform additional actions on the alert, such as <strong>Enable</strong>, <strong>Edit</strong>, <strong>Clone</strong> and <strong>Delete</strong>.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/alerts-management/#triggered-alerts-tab
tag_set: userguide, alerts-management
image_urls: 
tracking_id: docs-userguide-alerts-management-triggered-alerts-tab
group_tracking_ids: docs-userguide-alerts-management
<h2>Alerts: Triggered Alerts Tab</h2>
<p><img src="https://signoz.io/img/docs/product-features/alerts/alerts-triggered-tab.gif" alt="A gif explaining the Triggered Alerts Tab in SigNoz" /></p>
<p><em>Features of Triggered Alerts Tab</em></p>
<p>The Triggered Alerts Tab shows the currently firing alerts. It provides a real-time view of alerts, allowing you to quickly assess which alerts are active and require attention. Here's a detailed description of the tab's features:</p>
<h3>## Triggered Alert Columns</h3>
<ul>
<li><strong>Status</strong>: Shows whether the alert is currently firing. It can have values like &quot;Firing.&quot;</li>
<li><strong>Alert Name</strong>: The name of the triggered alert.</li>
<li><strong>Severity</strong>: Indicates the severity of the triggered alert (e.g., &quot;warning&quot;).</li>
<li><strong>Tags</strong>: Displays additional information or tags related to the alert.</li>
<li><strong>Firing Since</strong>: The timestamp indicating when the alert started firing.</li>
</ul>
<h3>## Additional Triggered Alert Options</h3>
<ul>
<li><strong>Filter by Tags</strong>: You can apply filters to narrow down the list of triggered alerts based on specific tags.</li>
<li><strong>Group by</strong>: The &quot;Group by&quot; feature allows you to group alerts based on various criteria, such as alert name, severity etc.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/alerts-management/#creating-a-new-alert-in-signoz
tag_set: userguide, alerts-management
image_urls: 
tracking_id: docs-userguide-alerts-management-creating-a-new-alert-in-signoz
group_tracking_ids: docs-userguide-alerts-management
<h2>Alerts: Creating a New Alert in SigNoz</h2>
<p>After setting up a new notification channel, you can create an alert by clicking the &quot;New Alert&quot; button in the Alerts Tab. You will see four types of alerts to choose from:</p>
<ul>
<li>
<p><strong><a href="../../alerts-management/metrics-based-alerts">Metric-based Alert</a></strong>: Sends a notification when a condition occurs in metric data (e.g., CPU usage, memory utilization, request rates). You can set thresholds or rate-based conditions.</p>
</li>
<li>
<p><strong><a href="../../alerts-management/log-based-alerts">Log-based Alert</a></strong>: Sends a notification when a condition occurs in log data (e.g., specific patterns, keywords, error messages). You can set conditions based on log entries or error codes.</p>
</li>
<li>
<p><strong><a href="../../alerts-management/trace-based-alerts">Trace-based Alert</a></strong>: Sends a notification when a condition occurs in trace data (e.g., latency, errors, specific trace events). You can define conditions to trigger the alert based on distributed system traces.</p>
</li>
<li>
<p><strong><a href="../../alerts-management/exceptions-based-alerts">Exceptions-based Alert</a></strong>: Sends a notification when a condition occurs in exceptions data (e.g., application exceptions or errors). You can set conditions to trigger the alert when specific exceptions are detected.</p>
</li>
<li>
<p><strong><a href="../../alerts-management/planned-maintenance">Planned Maintenance</a></strong>: Sends a notification when a condition occurs in exceptions data (e.g., application exceptions or errors). You can set conditions to trigger the alert when specific exceptions are detected.</p>
</li>
</ul>
<p>These four types of alerts offer flexibility in monitoring different system aspects.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/drop-metrics/
tag_set: userguide, drop-metrics
image_urls: 
tracking_id: docs-userguide-drop-metrics
group_tracking_ids: docs-userguide-drop-metrics
<h2>Guide to drop metrics</h2>
<p>The filter processor in OpenTelemetry allows you to drop metrics based on their name, label values, or other attributes. This is useful if you want to exclude certain metrics from being sent to SigNoz.</p>
<p>The filter processor is configured in the <code>processors::filter</code> section of the <code>otel-collector-config.yaml</code> file.</p>
<p>üìù Note</p>
<p>The processor needs to be added to the metrics pipeline to take effect.</p>
<pre><code>metrics:
  receivers: [otlp]
  processors: [filter/drop_metrics_by_name, batch]
  exporters: [otlp]
</code></pre>
<ol>
<li>
<p>Drop metrics by name</p>
<p>processors:
filter/drop_metrics_by_name:
metrics:
exclude:
match_type: strict
metric_names:
- http.client.request.body.size
- http.client.response.body.size</p>
</li>
<li>
<p>Drop metrics by name regex</p>
<p>processors:
filter/drop_metrics_by_name_regex:
metrics:
exclude:
match_type: regexp
metric_names:
- http.client.*</p>
</li>
<li>
<p>Drop metrics by resource attributes (like service.name, host.name, k8s.pod.name, etc.)</p>
<p>processors:
filter/drop_metrics_by_label_values:
metrics:
metric:
- resource.attributes[&quot;k8s.pod.name&quot;] == &quot;test-pod&quot;</p>
</li>
<li>
<p>Drop metrics by resource attributes regex</p>
<p>processors:
filter/drop_metrics_by_label_values_regex:
metrics:
metric:
- IsMatch(resource.attributes[&quot;k8s.pod.name&quot;], &quot;test-pod-.*&quot;)</p>
</li>
<li>
<p>Drop metrics by metric attributes (like http.method, message.operation, etc.)</p>
<p>processors:
filter/drop_metrics_by_label_values:
metrics:
datapoint:
- attributes[&quot;http.method&quot;] == &quot;GET&quot;</p>
</li>
<li>
<p>Drop metrics by metric attributes regex</p>
<p>processors:
filter/drop_metrics_by_label_values_regex:
metrics:
datapoint:
- IsMatch(attributes[&quot;http.method&quot;], &quot;GET|POST&quot;)</p>
</li>
</ol>
<p>Refer to the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor">OpenTelemetry documentation</a> for more details on how to configure the filter processor.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/migration/upgrade-0.51/
tag_set: operate, migration, upgrade-0.51
image_urls: 
tracking_id: docs-operate-migration-upgrade-0.51
group_tracking_ids: docs-operate-migration-upgrade-0.51
<h2>Upgrade to v0.51 from SigNoz versions older than v0.49.0</h2>
<p>If you are upgrading from SigNoz version &lt; v0.49.0 directly to version &gt;= 0.51.0 please follow this guide.</p>
<ul>
<li>
<p>Exec into the ClickHouse container.</p>
<p>On Docker:</p>
<pre><code>docker exec -it signoz-clickhouse bash
</code></pre>
<p>On Kubernetes:</p>
<pre><code>kubectl -n platform exec -it pod/chi-signoz-cluster-0-0-0 -- bash
</code></pre>
</li>
<li>
<p>Start the ClickHouse client by running <code>clickhouse client</code></p>
</li>
<li>
<p>Drop the body index</p>
<pre><code>ALTER TABLE signoz_logs.logs ON CLUSTER {{.SIGNOZ_CLUSTER}} DROP INDEX IF EXISTS body_idx
</code></pre>
<p>Don't worry if the query timeout's, it will keep running in the background, you can proceed to the next step and wait.</p>
</li>
<li>
<p>Wait for the mutation to complete, you can check by running the below command</p>
<pre><code>select * from system.mutations where is_done=0;
</code></pre>
<p>Wait until the <code>DROP INDEX</code> mutation is complete. If the above query return's empty result you can proceed to the next step.</p>
</li>
<li>
<p>Add the new body index</p>
<pre><code>ALTER TABLE signoz_logs.logs ON CLUSTER {{.SIGNOZ_CLUSTER}} ADD INDEX IF NOT EXISTS body_idx lower(body) TYPE ngrambf_v1(4, 60000, 5, 0) GRANULARITY 1
</code></pre>
</li>
</ul>
<p>If you face any issue please reach out to us on <a href="https://signoz.io/slack">slack</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/query-service/user-invitation-smtp/
tag_set: operate, query-service, user-invitation-smtp
image_urls: https://signoz.io/img/blog/common/join_slack_cta.webp
tracking_id: docs-operate-query-service-user-invitation-smtp
group_tracking_ids: docs-operate-query-service-user-invitation-smtp
<h2>Enable SMTP for User Invitations</h2>
<h2>Configuring Query Service</h2>
<hr />
<p>The following environment variables need to be set for <code>query-service</code> to send email for user invitations in SigNoz.</p>
<ul>
<li><strong>SMTP_ENABLED</strong>: Enable SMTP for sending email invitations.</li>
<li><strong>SMTP_FROM</strong>: The email address from which the invitations will be sent.</li>
<li><strong>SMTP_HOST</strong>: The SMTP host obtained from your email provider.</li>
<li><strong>SMTP_PORT</strong>: The SMTP port obtained from your email provider.</li>
<li><strong>SMTP_USERNAME</strong>: The SMTP user obtained from your email provider.</li>
<li><strong>SMTP_PASSWORD</strong>: The SMTP password obtained from your email provider.</li>
</ul>
<p>‚úÖ Info</p>
<p>This section is only required for <strong>Self-Hosted</strong> users. Cloud users don't need to follow this step.</p>
<h3>## Docker</h3>
<p>Based on your Docker installation, you can include the following section in your Docker Compose YAML file to configure query-service.</p>
<ul>
<li>
<p>Docker Standalone: <code>deploy/docker/clickhouse-setup/docker-compose.yaml</code></p>
</li>
<li>
<p>Docker Swarm: <code>deploy/docker-swarm/clickhouse-setup/docker-compose.yaml</code></p>
<p>services:
query-service:
environment:
- SMTP_ENABLED=true
- SMTP_FROM=&lt;email address&gt;
- SMTP_HOST=&lt;smtp host&gt;
- SMTP_PORT=&lt;smtp port&gt;
- SMTP_USERNAME=&lt;smtp user&gt;
- SMTP_PASSWORD=&lt;smtp password&gt;</p>
</li>
</ul>
<h3>## Kubernetes</h3>
<p>You can include the following section in your Helm override values YAML file.</p>
<pre><code>queryService:
  additionalEnvs:
    SMTP_ENABLED: &quot;true&quot;
    SMTP_FROM: &lt;email address&gt;
    SMTP_HOST: &lt;smtp host&gt;
    SMTP_PORT: &lt;smtp port&gt;
    SMTP_AUTH_USERNAME: &lt;smtp user&gt;
    SMTP_AUTH_PASSWORD: &lt;smtp password&gt;
</code></pre>
<ul>
<li><a href="https://signoz.io/docs/operate/configuration/#query-service">Query Service Configuration</a></li>
</ul>
<hr />
<p>If you have any feedback or facing issues, feel free to join our slack community to get help!</p>
<p><a href="https://signoz.io/slack"><img src="https://signoz.io/img/blog/common/join_slack_cta.webp" alt="SigNoz Slack community" /></a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/navigate-user-interface/
tag_set: userguide, navigate-user-interface
image_urls: 
tracking_id: docs-userguide-navigate-user-interface
group_tracking_ids: docs-userguide-navigate-user-interface
<h2>Navigate the User Interface</h2>
<p>The layout of the application is comprised of sections, panes, dashboards, graphs, entities, and navigation elements. The following sections describe each of these elements.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/navigate-user-interface/#sections
tag_set: userguide, navigate-user-interface
image_urls: 
tracking_id: docs-userguide-navigate-user-interface-sections
group_tracking_ids: docs-userguide-navigate-user-interface
<h2>Navigate the User Interface: Sections</h2>
<p>A section provides access to all the functionalities you need to perform a specific activity. For example, the <strong>Metrics</strong> section provides an overview of all your applications. The following example screenshot shows the four applications that come with the <a href="https://github.com/jaegertracing/jaeger/tree/main/examples/hotrod">Hot R.O.D.</a> demo application:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fmetrics-section-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Metrics section" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/navigate-user-interface/#panes
tag_set: userguide, navigate-user-interface
image_urls: 
tracking_id: docs-userguide-navigate-user-interface-panes
group_tracking_ids: docs-userguide-navigate-user-interface
<h2>Navigate the User Interface: Panes</h2>
<p>A pane is a subdivision of a section dedicated to a set of functionalities. For example, the <strong>Application Details</strong> section contains three separate panes:</p>
<ul>
<li>Application Metrics</li>
<li>External Calls</li>
<li>Database Calls</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fthree-panes-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Screenshot showing three panes" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/navigate-user-interface/#dashboards
tag_set: userguide, navigate-user-interface
image_urls: 
tracking_id: docs-userguide-navigate-user-interface-dashboards
group_tracking_ids: docs-userguide-navigate-user-interface
<h2>Navigate the User Interface: Dashboards</h2>
<p>A dashboard is a set of one or more graphs. In the <strong>Dashboard</strong> section, you can create, update, and delete dashboards:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdahsboard-section-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Dashboard section" /></p>
<p>The following example screenshot shows a dashboard named <strong>CPU Load</strong> containing two graphs - <strong>Load Average</strong> and <strong>Total CPU Time</strong>:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2FCPU-Load-v0.6.2.webp&amp;w=3840&amp;q=75" alt="CPU Load Dashboard" /></p>
<h3>## Predefined and Custom Graphs</h3>
<p>In SigNoz, a graph can either display a metric over a time interval or only the most recent value.</p>
<p>SigNoz supports two types of graphs:</p>
<ul>
<li><strong>Predefined graphs</strong> that you can find in the <strong>Metrics</strong> section. You can't modify any of these graphs.</li>
<li><strong>Custom graphs</strong> are graphs you create, update, and remove in the <strong>Dashboard</strong> section.</li>
</ul>
<p><strong>Common Operations</strong></p>
<p>This section describes the common operations that you can perform on a graph.</p>
<ul>
<li>
<p>You can hover over a graph to see the actual values at a specific point in time:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fgraph-hover-over-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Hover over a graph" /></p>
</li>
<li>
<p>If your graph shows multiple data series, depending on what you want to highlight, you can enable or disable a specific data series by clicking it at the bottom of the chart. The following example disables the <code>p95</code> latency:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdisable-p95-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Disable p95" /></p>
<p>‚úÖ Info</p>
<p>When you enable or disable a specific data series, the system could automatically change the scale of the vertical (value) axis.</p>
</li>
<li>
<p>If your graph shows application-related metrics, you can click a point on the graph and then select <strong>View Traces</strong> to open the <strong>Traces</strong> page for that specific point in time:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fview-traces-v0.6.2.webp&amp;w=3840&amp;q=75" alt="View traces" /></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/navigate-user-interface/#entities
tag_set: userguide, navigate-user-interface
image_urls: 
tracking_id: docs-userguide-navigate-user-interface-entities
group_tracking_ids: docs-userguide-navigate-user-interface
<h2>Navigate the User Interface: Entities</h2>
<p>An entity represents a specific type of data, such as an alert. Each entity consists of a series of properties. For example, an alert is defined by a name, expression, period, and labels.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/navigate-user-interface/#navigation-elements
tag_set: userguide, navigate-user-interface
image_urls: 
tracking_id: docs-userguide-navigate-user-interface-navigation-elements
group_tracking_ids: docs-userguide-navigate-user-interface
<h2>Navigate the User Interface: Navigation Elements</h2>
<p>The following illustration shows the <strong>Metrics</strong> section and the primary navigation elements:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fprimary-navigation-elements-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Primary navigation elements" /></p>
<p><strong>Legend</strong>:</p>
<ol>
<li>
<p><strong>Sidebar</strong>: Lists all the sections. Select a section to open it. The current section is highlighted in blue.</p>
</li>
<li>
<p><strong>Expand/Collapse</strong> <strong>button</strong>: Select this button to collapse the sidebar, allowing more room for the main part of the page. If the sidebar is already collapsed, select this button to expand it again.</p>
</li>
<li>
<p><strong>Main body</strong>: This example shows the <strong>Metrics</strong> section.</p>
</li>
<li>
<p><strong>Global Time Period</strong>: With SigNoz, you can specify the time range for which you want to view data by setting the global time period. SigNoz allows you to either indicate a specific time range such as the last five minutes or specify a start and end date by selecting <strong>Custom</strong> from the drop-down menu:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fspecify-global-time-period-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Specify global time period" /></p>
</li>
</ol>
<p>‚úÖ Info</p>
<p>On some pages, you can override the global time period. For example, when creating a custom graph, you can use the drop-down list under <strong>Panel Time Preferences</strong> to override the global time period:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Foverride-global-time-period-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Override the Global Time Period" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/ec2-monitoring/
tag_set: ec2-monitoring
image_urls: 
tracking_id: docs-ec2-monitoring
group_tracking_ids: docs-ec2-monitoring
<h2>EC2 Monitoring</h2>
<ul>
<li><a href="/docs/aws-monitoring/ec2-logs/">üìÑÔ∏è Application Server Logs: This guide provides detailed instructions on how to send...</a></li>
<li><a href="/docs/aws-monitoring/ec2-infra-metrics/">üìÑÔ∏è Infrastructure Metrics: This documentation guides you through integrating AWS EC2 infrastructure...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/
tag_set: userguide, collecting-ecs-logs-and-metrics
image_urls: 
tracking_id: docs-userguide-collecting-ecs-logs-and-metrics
group_tracking_ids: docs-userguide-collecting-ecs-logs-and-metrics
<h2>ECS Infra Metrics and Logs Collection using Daemon Service</h2>
<p>This documentation will guide you about how to collect metrics and logs from your ECS infrastructure using a daemon service. The daemon service will run a container in each nodes of your ECS cluster. The container will collect metrics and logs from the instance and send them to SigNoz.</p>
<p>Select the type of SigNoz instance you are running: <strong>SigNoz Cloud</strong> or <strong>Self-Hosted</strong>.</p>
<p>SigNoz CloudSelf-Host</p>
<h3>## Prerequisites</h3>
<ul>
<li>An ECS cluster running with at least one task definition</li>
<li>ECS cluster of launch type <strong>EC2</strong> or <strong>External</strong></li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud account</a></li>
</ul>
<p>üìù Note</p>
<p>If you want to collect metrics and other data for Fargate launch type, checkout <a href="https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/">this documentation</a>.</p>
<p>Select the type of ECS cluster you are running: <strong>EC2</strong> or <strong>External</strong>.</p>
<p>EC2External</p>
<p>Below are the steps to collect your metrics and logs from ECS infrastructure:</p>
<ul>
<li>
<p><a href="#step-1-daemon-service-template">Create a Daemon Service Template</a></p>
</li>
<li>
<p><a href="#step-2-create-signoz-otelcollector-config">Create OpenTelemetry Collector Config file</a></p>
</li>
<li>
<p><a href="#step-3-create-daemon-service">Create Daemon Service</a></p>
</li>
<li>
<p><a href="#step-4-verify-daemon-service">Verify Daemon Service</a></p>
</li>
<li>
<p><a href="#step-5-verify-data-in-signoz">Verify Data in SigNoz</a></p>
</li>
<li>
<p><a href="#optional-step-6-clean-up">Clean Up (Optional)</a></p>
</li>
</ul>
<p>Send Data from your Application deployed on ECS:</p>
<ul>
<li><a href="#send-data-from-applications">Send Data from your application</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/#step-1-daemon-service-template
tag_set: userguide, collecting-ecs-logs-and-metrics
image_urls: 
tracking_id: docs-userguide-collecting-ecs-logs-and-metrics-step-1-daemon-service-template
group_tracking_ids: docs-userguide-collecting-ecs-logs-and-metrics
<p>ECS Infra Metrics and Logs Collection using Daemon Service: Setting up Daemon Service: ## Setting up Daemon Service - Step 1: Daemon Service Template</p>
<p>This step creates a new service within your Amazon ECS (Elastic Container Service) cluster. The purpose of this service is to deploy a container that functions as a daemon. This service will run a container that will send data such as ECS infrastructure metrics and logs from docker containers and send it to SigNoz Cloud. It also acts as a gateway for any incoming OTLP telemetry data from your applications or services. This means it can receive telemetry data (such as traces and metrics) sent using OTLP from your applications, and then forward this data to SigNoz Cloud for analysis and visualization.</p>
<p>We will use CloudFormation template which includes parameters and configurations that define how the daemon service should be set up. For example, it specifies the container image to use for the daemon, the necessary environment variables, and network settings. The template is designed to be customizable, allowing you to adjust certain parameters to fit your specific ECS setup and requirements to create the daemon service with our custom parameters mentioned in Step 3.</p>
<p>Download the <a href="https://github.com/SigNoz/benchmark/blob/main/ecs/ec2/daemon-template.yaml">daemon-template.yaml</a> using the command below:</p>
<pre><code>wget https://github.com/SigNoz/benchmark/raw/main/ecs/ec2/daemon-template.yaml
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/#step-2-create-signoz-otelcollector-config
tag_set: userguide, collecting-ecs-logs-and-metrics
image_urls: https://signoz.io/img/docs/ecs-docs/ecs-otelcol-config-ssm.webp, https://signoz.io/img/docs/common/ingestion-key-details.webp
tracking_id: docs-userguide-collecting-ecs-logs-and-metrics-step-2-create-signoz-otelcollector-config
group_tracking_ids: docs-userguide-collecting-ecs-logs-and-metrics
<p>ECS Infra Metrics and Logs Collection using Daemon Service: Setting up Daemon Service: Step 2: Create SigNoz OtelCollector Config</p>
<p><img src="https://signoz.io/img/docs/ecs-docs/ecs-otelcol-config-ssm.webp" alt="otelcol-daeomn parameter in AWS Parameter Store" /></p>
<p><em>OTel Collector config in AWS Parameter Store</em></p>
<ul>
<li>
<p>Navigate to AWS Parameter Store and create a new parameter named <code>/ecs/signoz/otelcol-daemon.yaml</code>.</p>
</li>
<li>
<p>Download the <a href="https://github.com/SigNoz/benchmark/blob/main/ecs/otelcol-daemon.yaml">otelcol-daemon YAML</a> configuration file:</p>
<pre><code>wget https://github.com/SigNoz/benchmark/raw/main/ecs/otelcol-daemon.yaml
</code></pre>
</li>
<li>
<p>Update <code>{region}</code> and <code>SIGNOZ_INGESTION_KEY</code> values in your YAML configuration file and copy the updated content of the <code>otelcol-sidecar.yaml</code> file and paste it in the value field of the <code>/ecs/signoz/otelcol-daemon.yaml</code> parameter that you created.</p>
</li>
</ul>
<p>You will be able to get <code>{region}</code> and <code>SIGNOZ_INGESTION_KEY</code> values in your <a href="https://signoz.io/teams/">SigNoz Cloud account</a> under <strong>Settings --&gt; Ingestion Settings</strong>.</p>
<p><img src="https://signoz.io/img/docs/common/ingestion-key-details.webp" alt="Ingestion key details" /></p>
<p><em>Ingestion details in SigNoz dashboard</em></p>
<p>‚úÖ Info</p>
<p>After successful set up, feel free to remove <code>logging</code> exporter if it gets too noisy. To do so, simply remove the <code>logging</code> exporter from the <code>exporters</code> list in the following pipelines: <code>traces</code>, <code>metrics</code>, and <code>logs</code> from the <code>otelcol-daemon.yaml</code> file.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/#step-3-create-daemon-service
tag_set: userguide, collecting-ecs-logs-and-metrics
image_urls: 
tracking_id: docs-userguide-collecting-ecs-logs-and-metrics-step-3-create-daemon-service
group_tracking_ids: docs-userguide-collecting-ecs-logs-and-metrics
<p>ECS Infra Metrics and Logs Collection using Daemon Service: Setting up Daemon Service: Step 3: Create Daemon Service</p>
<p>Now configure the daemon service you created in Step 1 with the OpenTelemetry Collector (OTelCollector) configuration you prepared and stored in AWS Parameter Store in Step 2. The goal is to ensure the daemon service is properly set up to collect, process, and export your ECS infrastructure metrics and logs to SigNoz Cloud.</p>
<p>As a first step to configure the daemon service, you need to set the environment variable by running the command (using your AWS CLI) below:</p>
<pre><code>export CLUSTER_NAME=&lt;YOUR-ECS-CLUSTER-NAME&gt;
export REGION=&lt;YOUR-ECS-REGION&gt;
export COMMAND=--config=env:SIGNOZ_CONFIG_CONTENT
export SIGNOZ_CONFIG_PATH=/ecs/signoz/otelcol-daemon.yaml
</code></pre>
<p>Where, <code>&lt;YOUR-ECS-CLUSTER-NAME&gt;</code> - Name of your ECS cluster. For example, <code>my-test-cluster</code> <code>&lt;YOUR-ECS-REGION&gt;</code> - Region in which your ECS cluster is running. For example, <code>us-east-1</code></p>
<p>üìù Note</p>
<p>Make sure you have <code>CLUSTER_NAME</code> and <code>REGION</code> environment variables set to the proper values before running any <code>aws</code> commands.</p>
<p>Now with the environment variables set, you proceed to run an AWS CloudFormation command to create the stack that represents your daemon service. This command uses the aws <code>cloudformation create-stack</code> for creating a new stack.</p>
<p>Now run the following command (using your AWS CLI) to create the daemon service:</p>
<pre><code>aws cloudformation create-stack --stack-name AOCECS-daemon-${CLUSTER_NAME}-${REGION} \
    --template-body file://daemon-template.yaml \
    --parameters ParameterKey=ClusterName,ParameterValue=${CLUSTER_NAME} \
    ParameterKey=CreateIAMRoles,ParameterValue=True \
		ParameterKey=command,ParameterValue=${COMMAND} \
		ParameterKey=SigNozConfigPath,ParameterValue=${SIGNOZ_CONFIG_PATH} \
    --capabilities CAPABILITY_NAMED_IAM \
    --region ${REGION}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/#step-4-verify-daemon-service
tag_set: userguide, collecting-ecs-logs-and-metrics
image_urls: 
tracking_id: docs-userguide-collecting-ecs-logs-and-metrics-step-4-verify-daemon-service
group_tracking_ids: docs-userguide-collecting-ecs-logs-and-metrics
<p>ECS Infra Metrics and Logs Collection using Daemon Service: Setting up Daemon Service: Step 4: Verify Daemon Service</p>
<p>To verify that the daemon service is running, you can run the following command:</p>
<pre><code>aws ecs list-tasks --cluster ${CLUSTER_NAME} --region ${REGION}
</code></pre>
<p>You should see the task ARN of the daemon service in the output.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/#step-5-verify-data-in-signoz
tag_set: userguide, collecting-ecs-logs-and-metrics
image_urls: https://signoz.io/img/docs/ecs-docs/ecs-daemon-hostmetrics-output.webp
tracking_id: docs-userguide-collecting-ecs-logs-and-metrics-step-5-verify-data-in-signoz
group_tracking_ids: docs-userguide-collecting-ecs-logs-and-metrics
<p>ECS Infra Metrics and Logs Collection using Daemon Service: Setting up Daemon Service: Step 5: Verify Data in SigNoz</p>
<p>To verify that the data is being sent to SigNoz Cloud, you can go to the dashboard section of SigNoz and import one of the following dashboards below:</p>
<ul>
<li>
<p><a href="https://github.com/SigNoz/dashboards/raw/chore/ecs-dashboards/ecs-infra-metrics/instance-metrics.json">instance-metrics.json</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/dashboards/raw/main/hostmetrics/hostmetrics-with-variable.json">hostmetrics-with-variable.json</a></p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/ecs-docs/ecs-daemon-hostmetrics-output.webp" alt="Hostmetrics Dashboard showing data for ECS cluster" /></p>
<p><em>Hostmetrics Dashboard showing data for ECS cluster</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/#optional-step-6-clean-up
tag_set: userguide, collecting-ecs-logs-and-metrics
image_urls: 
tracking_id: docs-userguide-collecting-ecs-logs-and-metrics-optional-step-6-clean-up
group_tracking_ids: docs-userguide-collecting-ecs-logs-and-metrics
<p>ECS Infra Metrics and Logs Collection using Daemon Service: Setting up Daemon Service: (Optional) Step 6: Clean Up</p>
<p>In a cloud environment where resources are billed based on usage, cleaning up resources is crucial. This step involves removing the daemon service and any associated resources that were created during the setup process to collect and forward metrics and logs from your ECS infrastructure to SigNoz. To clean up the daemon service, you can run the following command:</p>
<pre><code>aws cloudformation delete-stack --stack-name AOCECS-daemon-${CLUSTER_NAME}-${REGION} --region ${REGION}
</code></pre>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/#send-data-from-applications
tag_set: userguide, collecting-ecs-logs-and-metrics
image_urls: 
tracking_id: docs-userguide-collecting-ecs-logs-and-metrics-send-data-from-applications
group_tracking_ids: docs-userguide-collecting-ecs-logs-and-metrics
<h2>ECS Infra Metrics and Logs Collection using Daemon Service: Setting up Daemon Service: Send Data from Applications</h2>
<p>In this section, we will see how to send data from applications deployed in ECS to SigNoz <code>{props.name}</code> using Daemon Service we created in the previous section.</p>
<h3>## Step 1: Add OpenTelemetry Instrumentation to your Application</h3>
<p>To add OpenTelemetry instrumentation to your application, you can follow the docs <a href="https://signoz.io/docs/instrumentation/">here</a>.</p>
<p>üìù Note</p>
<p>This step include adding the <a href="https://opentelemetry.io/docs/instrumentation/">OpenTelemetry SDK</a> as well as the initialization code to your application codebase and rebuilding the application container image.</p>
<h3>## Step 2: Add Entrypoint to your Application Container</h3>
<p>We need to add an entrypoint to the application container to set the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> environment variable to the endpoint of the daemon service. Such that the application container can send data to the daemon container running in the same host.</p>
<p>EC2External</p>
<p><strong>Method 1: Updating entrypoint in task definition</strong></p>
<p>We need to obtain the endpoint or IP address of the instance on which the task is running. We can do this by querying the metadata service of the instance. For EC2, the metadata service is available at <code>169.254.169.254</code>.</p>
<p>The <code>entryPoint</code> will look like:</p>
<pre><code>{
      ...,
      &quot;entryPoint&quot;: [\
        &quot;sh&quot;,\
        &quot;-c&quot;,\
        &quot;export OTEL_EXPORTER_OTLP_ENDPOINT=\&quot;http://$(curl http://169.254.169.254/latest/meta-data/local-ipv4):4317\&quot;; &lt;Application Startup Commands&gt;&quot;\
      ],
			&quot;command&quot;: [],
			...
}
</code></pre>
<ul>
<li>Replace <code>&lt;Application Startup Commands&gt;</code> with the commands to start your application.</li>
</ul>
<h3>## Step 3: Add Service Name of your Application</h3>
<p>To add the service name of your application, you need to set the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable of the application container to <code>service.name=&lt;your-service-name&gt;</code>.</p>
<p>In your task definition, you can add the following lines.</p>
<pre><code>...
    ContainerDefinitions:
        - Name: &lt;your-container-name&gt;
          ...
          Environment:
            - Name: OTEL_RESOURCE_ATTRIBUTES
              Value: service.name=&lt;your-service-name&gt;
          ...
...
</code></pre>
<p>In case of JSON task definition, you can add the following lines.</p>
<pre><code>...
    &quot;containerDefinitions&quot;: [\
        {\
            &quot;name&quot;: &quot;&lt;your-container-name&gt;&quot;,\
            ...\
            &quot;environment&quot;: [\
                {\
                    &quot;name&quot;: &quot;OTEL_RESOURCE_ATTRIBUTES&quot;,\
                    &quot;value&quot;: &quot;service.name=&lt;your-service-name&gt;&quot;\
                }\
            ],\
            ...\
        }\
    ],
...
</code></pre>
<h3>## Step 4: Rebuild and Deploy Application Container</h3>
<p>After following previous step, you need to rebuild the application container and deploy it to ECS cluster.</p>
<h3>## Step 5: Verify Data in SigNoz</h3>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#overview
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: 
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-overview
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<h2>Collecting Data from ECS using Sidecar: Collecting Data from ECS using Sidecar - Overview</h2>
<p>This documentation will show you how to collect data from your ECS infrastructure using sidecar. The sidecar method involves deploying an additional container (the &quot;sidecar&quot;) that runs in each application container of your ECS cluster. A sidecar container, is an auxiliary container that offers supporting features like logging and monitoring to the main application container. The sidecar container will collect metrics and forward any received OTLP data to SigNoz.</p>
<p>Select the type of SigNoz instance you are running: <strong>SigNoz Cloud</strong> or <strong>Self-Hosted</strong>.</p>
<p>SigNoz CloudSelf-Host</p>
<p>Below are the steps to collect your metrics and logs from ECS infrastructure:</p>
<ul>
<li>
<p><a href="#prerequisites">Prerequisites</a></p>
</li>
<li>
<p><a href="#step-1-create-signoz-otelcollector-config">Create OpenTelemetry Collector Config file</a></p>
</li>
<li>
<p><a href="#step-2-create-sidecar-collector-container">Create Sidecar Collector Container</a></p>
</li>
<li>
<p><a href="#step-3-deploy-the-task-definition">Deploy Task Definition</a></p>
</li>
<li>
<p><a href="#step-4-verify-data-in-signoz">Verify Data in SigNoz</a></p>
</li>
<li>
<p><a href="#send-traces-data-from-applications">Send Traces Data from your application</a></p>
</li>
<li>
<p><a href="#send-logs-data-from-applications">Send Logs Data from your application</a></p>
</li>
</ul>
<h3>## Prerequisites</h3>
<ul>
<li>An ECS cluster running with at least one task definition</li>
<li>ECS cluster can be either of the launch types: <strong>Fargate</strong>, <strong>EC2</strong> or <strong>External</strong></li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud account</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#step-1-create-signoz-otelcollector-config
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: https://signoz.io/img/docs/ecs-docs/ecs-otelcol-sidecar-config-ssm.webp, https://signoz.io/img/docs/common/ingestion-key-details.webp
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-step-1-create-signoz-otelcollector-config
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<p>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: ## Setting up Sidecar Container - Step 1: Create SigNoz OtelCollector Config</p>
<p><img src="https://signoz.io/img/docs/ecs-docs/ecs-otelcol-sidecar-config-ssm.webp" alt="otelcol-sidecar parameter in AWS Parameter Store" /></p>
<p><em>OTel Collector config in AWS Parameter Store</em></p>
<ul>
<li>
<p>Navigate to AWS Parameter Store and create a new parameter named <code>/ecs/signoz/otelcol-sidecar.yaml</code>.</p>
</li>
<li>
<p>Download the <a href="https://github.com/SigNoz/benchmark/blob/main/ecs/otelcol-sidecar.yaml">otelcol-sidecar YAML</a> configuration file:</p>
<pre><code>wget https://github.com/SigNoz/benchmark/raw/main/ecs/otelcol-sidecar.yaml
</code></pre>
</li>
<li>
<p>Update <code>{region}</code> and <code>SIGNOZ_INGESTION_KEY</code> values in your YAML configuration file and copy the updated content of the <code>otelcol-sidecar.yaml</code> file and paste it in the value field of the <code>/ecs/signoz/otelcol-sidecar.yaml</code> parameter that you created.</p>
</li>
</ul>
<p>You will be able to get <code>{region}</code> and <code>SIGNOZ_INGESTION_KEY</code> values in your <a href="https://signoz.io/teams/">SigNoz Cloud account</a> under <strong>Settings --&gt; Ingestion Settings</strong>.</p>
<p><img src="https://signoz.io/img/docs/common/ingestion-key-details.webp" alt="Ingestion key details" /></p>
<p><em>Ingestion details in SigNoz dashboard</em></p>
<p>‚úÖ Info</p>
<p>After successful set up, feel free to remove <code>logging</code> exporter if it gets too noisy. To do so, simply remove the <code>logging</code> exporter from the <code>exporters</code> list in the following pipelines: <code>traces</code>, <code>metrics</code>, <code>metrics/aws</code>, and <code>logs</code> from the <code>otelcol-sidecar.yaml</code> file.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#step-2-create-sidecar-collector-container
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: 
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-step-2-create-sidecar-collector-container
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<p>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: Step 2: Create Sidecar Collector Container</p>
<p>This step involves integrating the SigNoz collector into your ECS task definitions as a sidecar container. The sidecar collector container will run alongside your application container(s) within the same ECS task and will collect ECS container metrics and send them to SigNoz Cloud. It also acts as a gateway to send any telemetry data from your application container to SigNoz Cloud.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#update-task-definition-of-your-application
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: 
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-update-task-definition-of-your-application
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<p>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: Update task definition of your application</p>
<p>In your ECS <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html">task definition</a>
, include a new container definition specifically for the sidecar container. This container will operate alongside your main application container(s) within the same task definition. The JSON configuration for that will look like this:</p>
<pre><code>{
    ...
    &quot;containerDefinitions&quot;: [\
        ...,\
        {\
            &quot;name&quot;: &quot;signoz-collector&quot;,\
            &quot;image&quot;: &quot;signoz/signoz-otel-collector:0.88.13&quot;,\
            &quot;user&quot;: &quot;root&quot;,\
            &quot;command&quot;: [\
                &quot;--config=env:SIGNOZ_CONFIG_CONTENT&quot;\
            ],\
            &quot;secrets&quot;: [\
                {\
                &quot;name&quot;: &quot;SIGNOZ_CONFIG_CONTENT&quot;,\
                &quot;valueFrom&quot;: &quot;/ecs/signoz/otelcol-sidecar.yaml&quot;\
                }\
            ],\
            &quot;memory&quot;: 1024,\
            &quot;cpu&quot;: 512,\
            &quot;essential&quot;: true,\
            &quot;portMappings&quot;: [\
                {\
                    &quot;protocol&quot;: &quot;tcp&quot;,\
                    &quot;containerPort&quot;: 4317\
                },\
                {\
                    &quot;protocol&quot;: &quot;tcp&quot;,\
                    &quot;containerPort&quot;: 4318\
                },\
                {\
                    &quot;containerPort&quot;: 8006,\
                    &quot;protocol&quot;: &quot;tcp&quot;\
                }\
            ],\
            &quot;healthCheck&quot;: {\
                &quot;command&quot;: [\
                    &quot;CMD-SHELL&quot;,\
                    &quot;wget -qO- http://localhost:13133/ || exit 1&quot;\
                ],\
                &quot;interval&quot;: 5,\
                &quot;timeout&quot;: 6,\
                &quot;retries&quot;: 5,\
                &quot;startPeriod&quot;: 1\
            },\
            &quot;logConfiguration&quot;: {\
                &quot;logDriver&quot;: &quot;awslogs&quot;,\
                &quot;options&quot;: {\
                &quot;awslogs-group&quot;: &quot;/ecs/signoz-otel-EC2-sidcar&quot;,\
                &quot;awslogs-region&quot;: &quot;&lt;aws-region&gt;&quot;,\
                &quot;awslogs-stream-prefix&quot;: &quot;ecs&quot;,\
                &quot;awslogs-create-group&quot;: &quot;True&quot;\
                }\
            }\
        }\
    ]
...
}
</code></pre>
<p>The sidecar container will run the SigNoz collector and includes essential components like the <strong>name</strong> (&quot;signoz-collector&quot;), <strong>image</strong> (e.g., &quot;signoz/signoz-otel-collector:0.88.7&quot;), and <strong>command</strong> for execution, alongside <strong>secrets</strong> for secure configuration. It also specifies <strong>memory</strong> and <strong>CPU</strong> resources, <strong>port mappings</strong> for network communication, a <strong>health check</strong> to ensure operational integrity, and <strong>log configuration</strong> for output management. These elements collectively enable the container to efficiently collect and forward telemetry data to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#update-ecs-task-execution-role
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: 
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-update-ecs-task-execution-role
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<p>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: Update ECS Task Execution Role</p>
<p>The ECS Task Execution Role is an AWS IAM role that grants permissions to the ECS agent to make AWS API calls on behalf of the user. When integrating SigNoz as a sidecar container, the task execution role needs additional permissions to access the AWS Systems Manager Parameter Store, where the SigNoz configuration is stored and to access CloudWatch Logs for log management of the sidecar containers.</p>
<p>To modify the ECS Task Execution Role, follow these steps:</p>
<ol>
<li>
<p><strong>Identify the Role:</strong> Identify the IAM role used by your <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html">ECS tasks for execution</a>
. This role is specified when creating an ECS service or can be found in the ECS task definition. It's often named something like <strong>ecsTaskExecutionRole</strong>.</p>
</li>
<li>
<p><strong>Edit the Role:</strong> Navigate to the IAM console in the AWS Management Console, find the role by name, and open its details page.</p>
</li>
<li>
<p><strong>Attach Policies or Add inline Policy:</strong></p>
<p>There are two ways to grant access to the Parameter store:</p>
<ul>
<li>
<p><strong>Attach AWS Managed Policies:</strong> If the role doesn't already have the following policies, attach them:</p>
<ul>
<li><code>AmazonSSMReadOnlyAccess</code></li>
<li><code>CloudWatchLogsFullAccess</code></li>
</ul>
</li>
<li>
<p><strong>Add Inline Policy:</strong> Alternatively, for more granular control, you can create an inline policy that specifically grants access to only the necessary resources in the Parameter Store. The JSON for the inline policy will be:</p>
<p>{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [<br />
{<br />
&quot;Action&quot;: [<br />
&quot;ssm:GetParameter&quot;<br />
],<br />
&quot;Resource&quot;: [<br />
&quot;arn:aws:ssm:&lt;aws-region&gt;:&lt;aws-account-id&gt;:parameter/ecs/signoz/otelcol-sidecar.yaml&quot;<br />
],<br />
&quot;Effect&quot;: &quot;Allow&quot;<br />
},<br />
{<br />
&quot;Effect&quot;: &quot;Allow&quot;,<br />
&quot;Action&quot;: [<br />
&quot;logs:CreateLogStream&quot;,<br />
&quot;logs:CreateLogGroup&quot;,<br />
&quot;logs:PutLogEvents&quot;,<br />
&quot;logs:DescribeLogStreams&quot;,<br />
&quot;logs:DescribeLogGroups&quot;<br />
],<br />
&quot;Resource&quot;: &quot;*&quot;<br />
}<br />
]
}</p>
</li>
</ul>
</li>
</ol>
<p>Alternatively, you can add the policy <code>AmazonSSMReadOnlyAccess</code> and <code>CloudWatchLogsFullAccess</code> to the ECS Task Execution Role.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#update-ecs-task-role
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: 
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-update-ecs-task-role
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<p>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: Update ECS Task Role</p>
<p>The ECS Task Role is distinct from the ECS Task Execution Role. While the execution role grants the ECS agent permission to manage the lifecycle of containers (like pulling images and storing logs), the task role is assumed by the task itself, including your application and any sidecar containers. This role provides the permissions necessary for the application to make AWS API calls directly.</p>
<p>To update the ECS Task Role, follow these steps:</p>
<ol>
<li>
<p><strong>Identify the Role:</strong> Determine the IAM role your ECS tasks are currently using to interact with AWS services. This role is specified in the ECS task definition under the &quot;taskRoleArn&quot; field.</p>
</li>
<li>
<p><strong>Edit the Role</strong>: Go to the IAM section of the AWS Management Console, locate the role by its name, and open its configuration.</p>
</li>
<li>
<p><strong>Attach Policies or Add Inline Policy:</strong></p>
<p>There are two ways to grant access to the Parameter store:</p>
<ul>
<li>
<p><strong>Attach AWS Managed Policies:</strong> If the role doesn't already have the following policies, attach them:</p>
<ul>
<li><code>AmazonSSMReadOnlyAccess</code></li>
<li><code>CloudWatchLogsFullAccess</code></li>
</ul>
</li>
<li>
<p><strong>Add Inline Policy for Granular Access:</strong> For tighter security, you might opt to create an inline policy that specifies exactly which resources the tasks can access and what actions they can perform on those resources. This is particularly important for accessing specific resources like the Parameter Store parameters used by the SigNoz sidecar. The JSON for the inline policy will be:</p>
<p>{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [<br />
{<br />
&quot;Action&quot;: [<br />
&quot;ssm:GetParameter&quot;<br />
],<br />
&quot;Resource&quot;: [<br />
&quot;arn:aws:ssm:&lt;aws-region&gt;:&lt;aws-account-id&gt;:parameter/ecs/signoz/otelcol-sidecar.yaml&quot;<br />
],<br />
&quot;Effect&quot;: &quot;Allow&quot;<br />
},<br />
{<br />
&quot;Effect&quot;: &quot;Allow&quot;,<br />
&quot;Action&quot;: [<br />
&quot;logs:CreateLogStream&quot;,<br />
&quot;logs:CreateLogGroup&quot;,<br />
&quot;logs:PutLogEvents&quot;,<br />
&quot;logs:DescribeLogStreams&quot;,<br />
&quot;logs:DescribeLogGroups&quot;<br />
],<br />
&quot;Resource&quot;: &quot;*&quot;<br />
}<br />
]
}</p>
</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#step-3-deploy-the-task-definition
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: 
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-step-3-deploy-the-task-definition
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<p>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: Step 3: Deploy the task definition</p>
<p>If your application runs as an ECS service, you update the service to use the new revision of your task definition. This tells ECS to start new tasks based on this updated definition and gracefully replace the old tasks with the new ones, ensuring minimal disruption to your application.</p>
<p>If you're not using a service and instead run tasks directly, you manually start a new task using the updated task definition. This approach is less common for long-running applications but might be used for testing or one-off executions.</p>
<p>üìù Note</p>
<p>Once the task is running, you should be able to see SigNoz sidecar container logs in CloudWatch Logs because we have set the <code>logDriver</code> parameter to be <code>awslogs</code> in our task definition.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#step-4-verify-data-in-signoz
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: https://signoz.io/img/docs/ecs-docs/ecs-daemon-hostmetrics-output.webp
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-step-4-verify-data-in-signoz
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<p>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: Step 4: Verify data in SigNoz</p>
<p>To verify that your sidecar container is running, go to the Dashboard section of SigNoz and import the dashboard <code>ECS - Container Metrics</code> Dashboard from <a href="https://github.com/SigNoz/dashboards/raw/main/ecs-infra-metrics/container-metrics.json">here</a>.</p>
<p><img src="https://signoz.io/img/docs/ecs-docs/ecs-daemon-hostmetrics-output.webp" alt="ECS Container Metrics Dashboard showing data for ECS cluster" /></p>
<p><em>ECS Container Metrics Dashboard showing data for ECS cluster</em></p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#send-traces-data-from-applications
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: 
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-send-traces-data-from-applications
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<h2>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: Send Traces Data from Applications</h2>
<p>In this section, we will see how to send traces data from applications deployed in ECS to SigNoz using sidecar container that we deployed in the previous section.</p>
<h3>## Add OpenTelemetry Instrumentation to your Application</h3>
<p>Instrumentation involves modifying your application code to generate telemetry data. This could mean adding specific OpenTelemetry API calls or relying on auto-instrumentation provided by the SDK for common libraries and frameworks.</p>
<p>To add OpenTelemetry instrumentation to your application, follow the docs <a href="https://signoz.io/docs/instrumentation/">here</a>.</p>
<p>üìù Note</p>
<p>This step can also include adding the OpenTelemetry SDK as well as the initialization code to your application codebase and rebuilding the application container.</p>
<h3>## Configure OTLP Endpoint</h3>
<p>In your application task definition, you need to set the OTLP endpoint to the endpoint of the sidecar container. This can be done by setting the environment variable <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> to the endpoint of the sidecar container.</p>
<p>Select the network mode of your ECS task definition:</p>
<p>BridgeAWS VPC</p>
<pre><code>{
    ...
    &quot;containerDefinitions&quot;: [\
        {\
            &quot;name&quot;: &quot;&lt;your-container-name&gt;&quot;,\
            &quot;environment&quot;: [\
                {\
                    &quot;name&quot;: &quot;OTEL_EXPORTER_OTLP_ENDPOINT&quot;,\
                    &quot;value&quot;: &quot;http://signoz-collector:4317&quot;\
                },\
                {\
                    &quot;name&quot;: &quot;OTEL_RESOURCE_ATTRIBUTES&quot;,\
                    &quot;value&quot;: &quot;service.name=&lt;your-service-name&gt;&quot;\
                }\
            ],\
            &quot;links&quot;: [\
                &quot;signoz-collector&quot;\
            ],\
            ...\
        }\
    ]
}
</code></pre>
<h3>## Rebuild and Deploy Application Container</h3>
<p>After instrumenting your application and configuring the OTLP endpoint, you'll need to rebuild your application container with these changes and deploy it to ECS cluster using the same task definition that we used in the previous section.</p>
<h3>## Verify data in SigNoz</h3>
<p>To verify that the data is being sent to SigNoz, you will need to generate some traffic to your application. Now go to the <strong>Services</strong> section of SigNoz and you should be able to see your application in the services list.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/#send-logs-data-from-applications
tag_set: userguide, collecting-ecs-sidecar-infra
image_urls: 
tracking_id: docs-userguide-collecting-ecs-sidecar-infra-send-logs-data-from-applications
group_tracking_ids: docs-userguide-collecting-ecs-sidecar-infra
<h2>Collecting Data from ECS using Sidecar: Setting up Sidecar Container: Send Logs Data from Applications</h2>
<p>In this section, we will see how to send logs data from applications deployed in ECS to SigNoz using sidecar container that we deployed in the previous section.</p>
<h3>## Configure Log Router</h3>
<p>In your application code, you need to configure the Fluent Bit log router to your application to the sidecar otel-collector container.</p>
<pre><code>{
    ...
    {
        &quot;name&quot;: &quot;signoz-log-router&quot;,
        &quot;image&quot;: &quot;906394416424.dkr.ecr.us-west-2.amazonaws.com/aws-for-fluent-bit:stable&quot;,
        &quot;cpu&quot;: 250,
        &quot;memory&quot;: 512,
        &quot;essential&quot;: true,
        &quot;dependsOn&quot;: [\
            {\
                &quot;containerName&quot;: &quot;signoz-collector&quot;,\
                &quot;condition&quot;: &quot;HEALTHY&quot;\
            }\
        ],
        &quot;logConfiguration&quot;: {
            &quot;logDriver&quot;: &quot;awslogs&quot;,
            &quot;options&quot;: {
                &quot;awslogs-create-group&quot;: &quot;True&quot;,
                &quot;awslogs-group&quot;: &quot;/ecs/ecs-signoz-log-router&quot;,
                &quot;awslogs-region&quot;: &quot;us-east-1&quot;,
                &quot;awslogs-stream-prefix&quot;: &quot;ecs&quot;
            }
        },
        &quot;firelensConfiguration&quot;: {
            &quot;type&quot;: &quot;fluentbit&quot;,
            &quot;options&quot;: {
                &quot;enable-ecs-log-metadata&quot;: &quot;true&quot;
            }
        }
    }
}
</code></pre>
<p>‚úÖ Info</p>
<p>When collecting logs from multiple applications, it is recommended to use <code>&lt;application-name&gt;-log-router</code> pattern instead of <code>signoz-log-router</code> for container name and <code>awslogs-group</code>. It helps to separate log router of different application.</p>
<h3>## Send Logs to Sidecar Container</h3>
<p>In your application task definition, you need to use <code>awsfirelens</code> log driver to send logs to the sidecar otel-collector container via Fluent Bit log router.</p>
<p>BridgeAWS VPC</p>
<pre><code>{
    ...
    &quot;containerDefinitions&quot;: [\
        {\
            &quot;name&quot;: &quot;&lt;your-container-name&gt;&quot;,\
            &quot;dependsOn&quot;: [\
                {\
                    &quot;containerName&quot;: &quot;signoz-log-router&quot;,\
                    &quot;condition&quot;: &quot;START&quot;\
                }\
            ],\
            &quot;logConfiguration&quot;: {\
                &quot;logDriver&quot;: &quot;awsfirelens&quot;,\
                &quot;options&quot;: {\
                    &quot;Name&quot;: &quot;forward&quot;,\
                    &quot;Match&quot;: &quot;*&quot;,\
                    &quot;Host&quot;: &quot;signoz-collector&quot;,\
                    &quot;Port&quot;: &quot;8006&quot;,\
                    &quot;tls&quot;: &quot;off&quot;,\
                    &quot;tls.verify&quot;: &quot;off&quot;\
                }\
            },\
            &quot;links&quot;: [\
                &quot;signoz-collector&quot;\
            ],\
            ...\
        }\
    ]
}
</code></pre>
<h3>## Rebuild and Deploy Application Container</h3>
<p>Now you can rebuild your application container and deploy it to ECS cluster using the same task definition that we updated in the previous section.</p>
<h3>## Verify data in SigNoz</h3>
<p>To verify that the logs are being sent to SigNoz, you will need to generate some logs from your application. After which you can go to the Logs page in SigNoz UI and you should see logs from your application.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/
tag_set: install, kubernetes
image_urls: 
tracking_id: docs-install-kubernetes
group_tracking_ids: docs-install-kubernetes
<h2>Kubernetes</h2>
<p>Learn how to install SigNoz on Kubernetes with Helm.</p>
<ul>
<li><a href="/docs/install/kubernetes/aws">üìÑÔ∏è Deploying to AWS: First, we need to set up a Kubernetes cluster (see the official AWS documentation for more info).</a></li>
<li><a href="/docs/install/kubernetes/gcp">üìÑÔ∏è Deploying to GCP: First, we need to set up a Kubernetes cluster (see the official GCP documentation for more info).</a></li>
<li><a href="/docs/install/kubernetes/others">üìÑÔ∏è Deploying with Helm directly: Follow the steps on this page to install SigNoz on other Kubernetes Cloud Platform and bare-metal servers with Helm</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/gcp-monitoring/gcp-fns/
tag_set: gcp-monitoring, gcp-fns
image_urls: 
tracking_id: docs-gcp-monitoring-gcp-fns
group_tracking_ids: docs-gcp-monitoring-gcp-fns
<h2>Cloud Functions</h2>
<ul>
<li><a href="/docs/gcp-monitoring/gcp-fns/logging/">üìÑÔ∏è Logging: This guide provides a detailed walkthrough on how to set up a Google Cloud Function to send the logs to SigNoz.</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/query-builder/
tag_set: product-features, query-builder
image_urls: https://signoz.io/img/docs/product-features/query-builder/product-features-query-builder.webp
tracking_id: docs-product-features-query-builder
group_tracking_ids: docs-product-features-query-builder
<h2>Query Builder</h2>
<h3>## Introduction</h3>
<p>The SigNoz Query Builder is a powerful tool that simplifies the process of filtering, aggregating, and visualizing data across the three signals, i.e. , Logs, Traces and Metrics.</p>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/product-features-query-builder.webp" alt="Query Builder in SigNoz Logs Explorer" /></p>
<p><em>Query Builder in SigNoz Logs Explorer</em></p>
<h3>## Key Features</h3>
<ul>
<li><strong>Filtering</strong>: Apply various filters to refine data based on specific attributes.</li>
<li><strong>Aggregation and Grouping</strong>: Perform calculations like count, sum, average, and more while grouping data by selected attributes.</li>
<li><strong>Result Manipulation</strong>: Order, limit, and format result outputs according to your requirements.</li>
<li><strong>Multiple Queries and Functions</strong>: Run several queries simultaneously and apply mathematical functions on queries.</li>
<li><strong>Spatial and Temporal Aggregations</strong>: Analyze metrics data over time or across various dimensions for deeper insights.</li>
</ul>
<p>For detailed instructions on using each feature of the Query Builder, please refer to this <a href="https://signoz.io/docs/userguide/query-builder/">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/authentication/
tag_set: userguide, authentication
image_urls: 
tracking_id: docs-userguide-authentication
group_tracking_ids: docs-userguide-authentication
<h2>Authentication and Login</h2>
<p>Authentication is available in SigNoz from <code>v0.8.0</code> If you are on an earlier version, please upgrade to see this.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/authentication/#supported-roles
tag_set: userguide, authentication
image_urls: 
tracking_id: docs-userguide-authentication-supported-roles
group_tracking_ids: docs-userguide-authentication
<h2>Authentication and Login: Supported Roles</h2>
<p>SigNoz currently supports 3 roles:</p>
<ul>
<li>Admin</li>
<li>Editor</li>
<li>Viewer</li>
</ul>
<p>The person who installs SigNoz and registers for the first time is by default assigned the role of an admin.</p>
<ul>
<li>If you are logging in SigNoz for the first time, just create an account. This will make you an admin.</li>
<li>If you would like to invite more members, they would need an invite link from admin. You can create invite link from <code>Settings-&gt; Org Settings</code> tab</li>
</ul>
<p>Admins can add new users by creating invites for them and sending them invite link to create an account.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fuser-invite.webp&amp;w=3840&amp;q=75" alt="user-invite" /></p>
<p>üìù Note</p>
<p>As of <code>version 0.8.0</code>, news users can only be invited by Admins to a SigNoz instance. New users will get an invite link from admin which they can use to signup to SigNoz.</p>
<p>When new user is invited by an admin, he can be assigned a role of Admin, Editor or Viewer</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/authentication/#how-to-edit-member-details
tag_set: userguide, authentication
image_urls: 
tracking_id: docs-userguide-authentication-how-to-edit-member-details
group_tracking_ids: docs-userguide-authentication
<h2>Authentication and Login: How to Edit Member Details?</h2>
<p>You can edit permission levels of members by going to <code>Settings-&gt; Org Settings</code> tab and then Members table. You can also generate password reset link if they have forgotten their password and it needs to be reset.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fedit-member.webp&amp;w=3840&amp;q=75" alt="edit-member" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/authentication/#permission-matrix-for-admin-editor-and-viewer
tag_set: userguide, authentication
image_urls: 
tracking_id: docs-userguide-authentication-permission-matrix-for-admin-editor-and-viewer
group_tracking_ids: docs-userguide-authentication
<h2>Authentication and Login: Permission Matrix for Admin, Editor and Viewer:</h2>
<table>
<thead>
<tr>
<th>Features</th>
<th>Admin</th>
<th>Editor</th>
<th>Viewer</th>
</tr>
</thead>
<tbody>
<tr>
<td>View Application List</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Search/Filter Application List</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>View Metrics Detail Page</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>View Dashboards</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Create New Dashboards</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚ùå</td>
</tr>
<tr>
<td>Import Dashboards</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚ùå</td>
</tr>
<tr>
<td>Export Dashboards</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>View Trace Filter Page &amp; do Filtering</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>View Trace Detail Page</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>View Alerts</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Search / Filter Alerts</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Add New Alerts</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚ùå</td>
</tr>
<tr>
<td>View Settings Page</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
</tr>
<tr>
<td>View Service Map</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Invite New Members</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
</tr>
<tr>
<td>Revoke Invite of Members</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
</tr>
<tr>
<td>Create New Alert Channels</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
</tr>
<tr>
<td>Set Retention Period</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
</tr>
<tr>
<td>Set own Password and Name</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Change Org Name</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
</tr>
<tr>
<td>Edit / Delete Member Details</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚ùå</td>
</tr>
<tr>
<td>View Version Information Page (from left panel)</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Change Theme (dark/light)</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/guides/drop-spans/#overview
tag_set: traces-management, guides, drop-spans
image_urls: 
tracking_id: docs-traces-management-guides-drop-spans-overview
group_tracking_ids: docs-traces-management-guides-drop-spans
<h2>Control traces volume: Control traces volume - Overview</h2>
<p>There are two stages where you can control the volume of spans:</p>
<ol>
<li><a href="#at-the-application">At the application</a>
<ul>
<li>by customizing the SDK and/or Instrumentation</li>
</ul>
</li>
<li><a href="#at-the-collector">At the collector</a>
<ul>
<li>with the help of processors</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/guides/drop-spans/#at-the-application
tag_set: traces-management, guides, drop-spans
image_urls: 
tracking_id: docs-traces-management-guides-drop-spans-at-the-application
group_tracking_ids: docs-traces-management-guides-drop-spans
<h2>Control traces volume: At the application</h2>
<p>Dropping spans at the application is often referred to as HEAD sampling because the sampling decision is made at the beginning of the trace.</p>
<h3>## TraceIdRatioBased sampler</h3>
<p>The <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/9758cdddee20c699b620aac1d7f777ccd490f252/specification/trace/sdk.md#traceidratiobased"><code>TraceIdRatioBased</code></a> sampler makes a random sampling result based on the sampling probability given. If the sampling probability is 0.001, then 1 out of 1000 traces will be sampled. This sampler can be configured by setting the <code>OTEL_TRACES_SAMPLER</code> environment variable.</p>
<pre><code>OTEL_TRACES_SAMPLER=&quot;parentbased_traceidratio&quot;
OTEL_TRACES_SAMPLER_ARG=0.001
</code></pre>
<h3>## Custom Sampler</h3>
<p>The <code>Sampler</code> interface allows you to implement your own custom sampling logic. This is useful if you want to drop spans based on certain conditions instead of random sampling alone. Your custom sampler should implement the <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/9758cdddee20c699b620aac1d7f777ccd490f252/specification/trace/sdk.md#shouldsample"><code>ShouldSample</code></a> method. The <strong>ShouldSample</strong> method is called for each span and returns a <code>SamplingDecision</code>. The <strong>SamplingDecision</strong> is a struct that contains a <code>SamplingResult</code> which can be <code>DROP</code>, <code>RECORD_ONLY</code> or <code>RECORD_AND_SAMPLE</code>. If,</p>
<ul>
<li><code>SamplingResult</code> is <code>DROP</code>, then the span is dropped.</li>
<li><code>SamplingResult</code> is <code>RECORD_ONLY</code>, then the span is sampled but not recorded.</li>
<li><code>SamplingResult</code> is <code>RECORD_AND_SAMPLE</code>, then the span is sampled and recorded.</li>
</ul>
<p>Please refer to the official SDK documentation for more details on how to implement a custom sampler.</p>
<h3>## Exclude certain routes</h3>
<p>Some instrumentations allow you to exclude certain routes from creating spans using the environment variable or callbacks. For example, OpenTelemetry Python allows you to exclude certain routes from creating spans using the <code>OTEL_PYTHON_EXCLUDED_URLS</code> environment variable. Please refer to the instrumentation documentation you are using for more details on how to exclude certain routes.</p>
<pre><code>OTEL_PYTHON_EXCLUDED_URLS=&quot;https://example.com/exclude&quot;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/guides/drop-spans/#at-the-collector
tag_set: traces-management, guides, drop-spans
image_urls: 
tracking_id: docs-traces-management-guides-drop-spans-at-the-collector
group_tracking_ids: docs-traces-management-guides-drop-spans
<h2>Control traces volume: At the Collector</h2>
<p>There are mainly three ways to control the volume of spans at the collector level.</p>
<ol>
<li>Drop certain attributes from spans</li>
<li>Use Tail Sampling Processor / Probabilistic Sampling Processor to drop entire trace</li>
<li>Use Filter Processor to drop spans</li>
</ol>
<p>Dropping spans at the collector is often referred to as TAIL sampling because the sampling decision is made at the end of the trace.</p>
<h3>## Drop certain attributes from spans</h3>
<p>There are two processors to drop certain attributes from spans.</p>
<ol>
<li>Attributes Processor (The attributes processor is used to drop span attributes)</li>
<li>Resource Processor (The resource processor is used to drop resource attributes)</li>
</ol>
<p>üìù Note</p>
<p>The processor needs to be added to the traces pipeline to take effect.</p>
<pre><code>traces:
  receivers: [otlp]
  processors: [attributes/drop_process_attributes, batch]
  exporters: [otlp]
</code></pre>
<ol>
<li>
<p>Drop http.method span attribute</p>
<p>attributes/drop_span_attributes:
actions:
- key: http.method
action: delete</p>
</li>
<li>
<p>Drop span resource attributes</p>
<p>resource/drop_span_resource_attributes:
attributes:
- key: process.runtime.description
action: delete
- key: telemetry.distro.name
action: delete
- key: process.executable.path
action: delete
- key: process.runtime.name
action: delete
- key: process.command_args
action: delete</p>
</li>
</ol>
<h3>## Drop entire trace</h3>
<p>There are two processors to drop entire traces.</p>
<ol>
<li>Probabilistic Sampling Processor</li>
<li>Tail Sampling Processor</li>
</ol>
<p>Please refer to the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/probabilisticsamplerprocessor">OpenTelemetry documentation</a> for more details on how to configure the probabilistic sampling processor and <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor">OpenTelemetry documentation</a> for more details on how to configure the tail sampling processor.</p>
<h3>## Filter Processor</h3>
<p>üìù Note</p>
<p>Dropping individual spans at the collector can lead to broken traces.</p>
<p>The filter processor in OpenTelemetry allows you to drop spans based on name, status, span kind, attributes, events, etc. This is useful if you want to exclude certain spans from being sent to SigNoz.</p>
<p>The filter processor is configured in the <code>processors::filter</code> section of the <code>otel-collector-config.yaml</code> file.</p>
<p>üìù Note</p>
<p>The processor needs to be added to the traces pipeline to take effect.</p>
<pre><code>traces:
  receivers: [otlp]
  processors: [filter/drop_spans_by_name, batch]
  exporters: [otlp]
</code></pre>
<h4>## Drop Spans</h4>
<ol>
<li>
<p>Drop spans by name</p>
<p>processors:
filter/drop_spans_by_name:
traces:
span:
- 'name == &quot;test-span&quot;'</p>
</li>
<li>
<p>Drop spans by status</p>
<p>processors:
filter/drop_spans_by_status:
traces:
span:
- 'status.code == STATUS_CODE_OK'</p>
</li>
<li>
<p>Drop spans by resource attributes (like service.name, host.name, k8s.pod.name, etc.)</p>
<p>processors:
filter/drop_spans_by_attribute_values:
traces:
span:
- 'resource.attributes[&quot;k8s.pod.name&quot;] == &quot;test-pod&quot;'</p>
</li>
<li>
<p>Drop spans by resource attributes regex</p>
<p>processors:
filter/drop_spans_by_attribute_values_regex:
traces:
span:
- 'IsMatch(resource.attributes[&quot;k8s.pod.name&quot;], &quot;test-pod-.*&quot;)'</p>
</li>
<li>
<p>Drop spans by span attributes (like http.method, user_agent.name, etc.)</p>
<p>processors:
filter/drop_spans_by_attribute_values:
traces:
span:
- 'attributes[&quot;http.method&quot;] == &quot;GET&quot;'</p>
</li>
<li>
<p>Drop spans by span attributes regex</p>
<p>processors:
filter/drop_spans_by_attribute_values_regex:
traces:
span:
- 'IsMatch(attributes[&quot;http.method&quot;], &quot;GET|POST&quot;)'</p>
</li>
</ol>
<p>Refer to the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor">OpenTelemetry documentation</a> for more details on how to configure the filter processor.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/histogram/#histogram
tag_set: dashboards, panel-types, histogram
image_urls: 
tracking_id: docs-dashboards-panel-types-histogram-histogram
group_tracking_ids: docs-dashboards-panel-types-histogram
<h1>Histogram Panel Type: Histogram Panel Type - Histogram</h1>
<p>A Histogram is a plot chart that shows the frequency distribution of a set of measurements. This helps in identifying the distribution of data, skewness, etc.</p>
<p>This panel type allows users to create a chart where the data is grouped into buckets (such as ‚Äú40 to 49‚Äù, ‚Äú50 to 59‚Äù, etc.), and then plotted as bars. Similar to a Bar Graph, but in a Histogram, each bar represents a range of data. The panel type allows the user to control the number of bins, and will automatically calculate the size of each bin accordingly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/histogram/#support-signals
tag_set: dashboards, panel-types, histogram
image_urls: 
tracking_id: docs-dashboards-panel-types-histogram-support-signals
group_tracking_ids: docs-dashboards-panel-types-histogram
<p>Histogram Panel Type: Data Formats: ## Data Formats - Support signals</p>
<ul>
<li>Logs</li>
<li>Traces</li>
<li>Metrics</li>
</ul>
<p>This panel type supports any time series data. The time series data can be from logs, traces, or metrics.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/histogram/#one-series
tag_set: dashboards, panel-types, histogram
image_urls: https://signoz.io/img/docs/dashboards/panel-types/histogram-viz-single-ts.webp
tracking_id: docs-dashboards-panel-types-histogram-one-series
group_tracking_ids: docs-dashboards-panel-types-histogram
<p>Histogram Panel Type: Data Formats: ### ## Examples - One Series</p>
<p>The following table shows the requests per second (req/s) for a service over a period of time.</p>
<p>Show table</p>
<table>
<thead>
<tr>
<th>Timestamp</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024-06-06 00:27:00</td>
<td>46.68333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:28:00</td>
<td>49.88333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:29:00</td>
<td>45.61666666666667</td>
</tr>
<tr>
<td>2024-06-06 00:30:00</td>
<td>44.9</td>
</tr>
<tr>
<td>2024-06-06 00:31:00</td>
<td>50.5</td>
</tr>
<tr>
<td>2024-06-06 00:32:00</td>
<td>45.15</td>
</tr>
<tr>
<td>2024-06-06 00:33:00</td>
<td>47.983333333333334</td>
</tr>
<tr>
<td>2024-06-06 00:34:00</td>
<td>46.016666666666666</td>
</tr>
<tr>
<td>2024-06-06 00:35:00</td>
<td>46.85</td>
</tr>
<tr>
<td>2024-06-06 00:36:00</td>
<td>46</td>
</tr>
<tr>
<td>2024-06-06 00:37:00</td>
<td>48.81666666666667</td>
</tr>
<tr>
<td>2024-06-06 00:38:00</td>
<td>42.93333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:39:00</td>
<td>44.6</td>
</tr>
<tr>
<td>2024-06-06 00:40:00</td>
<td>47.43333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:41:00</td>
<td>44.6</td>
</tr>
<tr>
<td>2024-06-06 00:42:00</td>
<td>42.63333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:43:00</td>
<td>46.53333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:44:00</td>
<td>46.9</td>
</tr>
<tr>
<td>2024-06-06 00:45:00</td>
<td>43.4</td>
</tr>
<tr>
<td>2024-06-06 00:46:00</td>
<td>47.28333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:47:00</td>
<td>46.833333333333336</td>
</tr>
<tr>
<td>2024-06-06 00:48:00</td>
<td>47.65</td>
</tr>
<tr>
<td>2024-06-06 00:49:00</td>
<td>44.6</td>
</tr>
<tr>
<td>2024-06-06 00:50:00</td>
<td>43.766666666666666</td>
</tr>
<tr>
<td>2024-06-06 00:51:00</td>
<td>46.583333333333336</td>
</tr>
<tr>
<td>2024-06-06 00:52:00</td>
<td>47.13333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:53:00</td>
<td>46.85</td>
</tr>
<tr>
<td>2024-06-06 00:54:00</td>
<td>47.96666666666667</td>
</tr>
<tr>
<td>2024-06-06 00:55:00</td>
<td>44.61666666666667</td>
</tr>
</tbody>
</table>
<p>When configured to use 15 buckets, the chart is rendered as follows:</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/histogram-viz-single-ts.webp" alt="Histogram with one series and 15 buckets" /></p>
<p><em>Histogram with one series and 15 buckets.</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/histogram/#multiple-series
tag_set: dashboards, panel-types, histogram
image_urls: https://signoz.io/img/docs/dashboards/panel-types/histogram-viz-multiple-ts.webp, https://signoz.io/img/docs/dashboards/panel-types/histogram-viz-multilple-series-merged-into-one.webp
tracking_id: docs-dashboards-panel-types-histogram-multiple-series
group_tracking_ids: docs-dashboards-panel-types-histogram
<p>Histogram Panel Type: Data Formats: Multiple Series</p>
<p>The following tables show the requests per second (RPS) by status code over time.</p>
<ol>
<li>Series with STATUS_CODE_UNSET</li>
</ol>
<p>Show table for Series with STATUS_CODE_UNSET</p>
<table>
<thead>
<tr>
<th>Timestamp</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024-06-06 00:38:00</td>
<td>40.8</td>
</tr>
<tr>
<td>2024-06-06 00:39:00</td>
<td>42.4</td>
</tr>
<tr>
<td>2024-06-06 00:40:00</td>
<td>45.05</td>
</tr>
<tr>
<td>2024-06-06 00:41:00</td>
<td>42.4</td>
</tr>
<tr>
<td>2024-06-06 00:42:00</td>
<td>40.55</td>
</tr>
<tr>
<td>2024-06-06 00:43:00</td>
<td>44.2</td>
</tr>
<tr>
<td>2024-06-06 00:44:00</td>
<td>44.6</td>
</tr>
<tr>
<td>2024-06-06 00:45:00</td>
<td>41.233333333333334</td>
</tr>
<tr>
<td>2024-06-06 00:46:00</td>
<td>44.95</td>
</tr>
<tr>
<td>2024-06-06 00:47:00</td>
<td>44.5</td>
</tr>
<tr>
<td>2024-06-06 00:48:00</td>
<td>45.31666666666667</td>
</tr>
<tr>
<td>2024-06-06 00:49:00</td>
<td>42.4</td>
</tr>
<tr>
<td>2024-06-06 00:50:00</td>
<td>41.6</td>
</tr>
<tr>
<td>2024-06-06 00:51:00</td>
<td>44.25</td>
</tr>
<tr>
<td>2024-06-06 00:52:00</td>
<td>44.8</td>
</tr>
<tr>
<td>2024-06-06 00:53:00</td>
<td>44.55</td>
</tr>
<tr>
<td>2024-06-06 00:54:00</td>
<td>45.6</td>
</tr>
<tr>
<td>2024-06-06 00:55:00</td>
<td>44.61666666666667</td>
</tr>
<tr>
<td>2024-06-06 00:56:00</td>
<td>44.8</td>
</tr>
<tr>
<td>2024-06-06 00:57:00</td>
<td>43.3</td>
</tr>
<tr>
<td>2024-06-06 00:58:00</td>
<td>42.36666666666667</td>
</tr>
<tr>
<td>2024-06-06 00:59:00</td>
<td>41.53333333333333</td>
</tr>
<tr>
<td>2024-06-06 01:00:00</td>
<td>48</td>
</tr>
<tr>
<td>2024-06-06 01:01:00</td>
<td>42.233333333333334</td>
</tr>
<tr>
<td>2024-06-06 01:02:00</td>
<td>40.45</td>
</tr>
<tr>
<td>2024-06-06 01:03:00</td>
<td>48.7</td>
</tr>
<tr>
<td>2024-06-06 01:04:00</td>
<td>43.88333333333333</td>
</tr>
<tr>
<td>2024-06-06 01:05:00</td>
<td>46.333333333333336</td>
</tr>
<tr>
<td>2024-06-06 01:06:00</td>
<td>49.6</td>
</tr>
</tbody>
</table>
<ol start="2">
<li>Series with STATUS_CODE_ERROR</li>
</ol>
<p>Show table for Series with STATUS_CODE_ERROR</p>
<table>
<thead>
<tr>
<th>Timestamp</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024-06-06 00:38:00</td>
<td>2.1333333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:39:00</td>
<td>2.2</td>
</tr>
<tr>
<td>2024-06-06 00:40:00</td>
<td>2.3833333333333333</td>
</tr>
<tr>
<td>2024-06-06 00:41:00</td>
<td>2.2</td>
</tr>
<tr>
<td>2024-06-06 00:42:00</td>
<td>2.0833333333333335</td>
</tr>
<tr>
<td>2024-06-06 00:43:00</td>
<td>2.3333333333333335</td>
</tr>
<tr>
<td>2024-06-06 00:44:00</td>
<td>2.3</td>
</tr>
<tr>
<td>2024-06-06 00:45:00</td>
<td>2.1666666666666665</td>
</tr>
<tr>
<td>2024-06-06 00:46:00</td>
<td>2.3333333333333335</td>
</tr>
<tr>
<td>2024-06-06 00:47:00</td>
<td>2.3333333333333335</td>
</tr>
<tr>
<td>2024-06-06 00:48:00</td>
<td>2.3333333333333335</td>
</tr>
<tr>
<td>2024-06-06 00:49:00</td>
<td>2.2</td>
</tr>
<tr>
<td>2024-06-06 00:50:00</td>
<td>2.1666666666666665</td>
</tr>
<tr>
<td>2024-06-06 00:51:00</td>
<td>2.3333333333333335</td>
</tr>
<tr>
<td>2024-06-06 00:52:00</td>
<td>2.3333333333333335</td>
</tr>
<tr>
<td>2024-06-06 00:53:00</td>
<td>2.3</td>
</tr>
<tr>
<td>2024-06-06 00:54:00</td>
<td>2.3666666666666667</td>
</tr>
<tr>
<td>2024-06-06 00:55:00</td>
<td>2.216666666666667</td>
</tr>
<tr>
<td>2024-06-06 00:56:00</td>
<td>2.3333333333333335</td>
</tr>
<tr>
<td>2024-06-06 00:57:00</td>
<td>2.2666666666666666</td>
</tr>
<tr>
<td>2024-06-06 00:58:00</td>
<td>2.2</td>
</tr>
<tr>
<td>2024-06-06 00:59:00</td>
<td>2.15</td>
</tr>
<tr>
<td>2024-06-06 01:00:00</td>
<td>2.5</td>
</tr>
<tr>
<td>2024-06-06 01:01:00</td>
<td>2.216666666666667</td>
</tr>
<tr>
<td>2024-06-06 01:02:00</td>
<td>2.1166666666666667</td>
</tr>
<tr>
<td>2024-06-06 01:03:00</td>
<td>2.533333333333333</td>
</tr>
<tr>
<td>2024-06-06 01:04:00</td>
<td>2.2666666666666666</td>
</tr>
<tr>
<td>2024-06-06 01:05:00</td>
<td>2.4166666666666665</td>
</tr>
<tr>
<td>2024-06-06 01:06:00</td>
<td>2.5833333333333335</td>
</tr>
</tbody>
</table>
<p>When configured to use 15 buckets, the chart is rendered as follows:</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/histogram-viz-multiple-ts.webp" alt="Histogram with two series and 15 buckets" /></p>
<p><em>Histogram with two series and 15 buckets.</em></p>
<ol start="3">
<li>Merging the multiple series into one Histogram</li>
</ol>
<p>Multiple series can be merged into a Histogram using the option <code>MERGE ALL SERIES INTO ONE</code> from the right side of the panel configuration.</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/histogram-viz-multilple-series-merged-into-one.webp" alt="Histogram with two series merged into one and 15 buckets" /></p>
<p><em>Histogram with two series merged into one and 15 buckets.</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/histogram/#configuration
tag_set: dashboards, panel-types, histogram
image_urls: 
tracking_id: docs-dashboards-panel-types-histogram-configuration
group_tracking_ids: docs-dashboards-panel-types-histogram
<h2>Histogram Panel Type: Data Formats: Configuration</h2>
<h3>## Number of Buckets</h3>
<p>The number of buckets to create in the Histogram. The default value is 30.</p>
<h3>## Bucket Width</h3>
<p>The size of the buckets. Automatic bucket sizing is applied by default.</p>
<h3>## Merging Series</h3>
<p>This allows users to combine the values from multiple series into one Histogram.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/
tag_set: userguide, metrics
image_urls: 
tracking_id: docs-userguide-metrics
group_tracking_ids: docs-userguide-metrics
<h2>View Services</h2>
<p>This page walks you through the <strong>Services</strong> section and gets you started with monitoring your application. You‚Äôll learn the following:</p>
<ul>
<li>What are application metrics</li>
<li>How to use the <strong>Services</strong> section to see an overview of your applications</li>
<li>How to view details about a specific application</li>
</ul>
<p>This section uses the¬†<a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a> sample application that comes preinstalled with SigNoz and generates sample data that you can query. You can apply the concepts and techniques you‚Äôll learn to monitor your own applications.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#prerequisites
tag_set: userguide, metrics
image_urls: 
tracking_id: docs-userguide-metrics-prerequisites
group_tracking_ids: docs-userguide-metrics
<h2>View Services: Prerequisites</h2>
<p>This section assumes that your application is already instrumented. For details about how you can instrument your application, see the <a href="/docs/instrumentation/">Instrument Your Application</a> section.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#what-are-application-metrics
tag_set: userguide, metrics
image_urls: 
tracking_id: docs-userguide-metrics-what-are-application-metrics
group_tracking_ids: docs-userguide-metrics
<h2>View Services: What Are Application Metrics?</h2>
<p>Application metrics represent a characteristic of your application as a value at a specific point in time. For example, an application metric is the number of requests per second your application serves. SigNoz collects information as a sequence of data points every minute and then represents the data through time in a graphical form. The X-axis is time, and the Y-axis is the value.</p>
<p>The <strong>Services</strong> section relies on the rate, errors, and duration (‚ÄùRED‚Äù) method to help you predict the experience of your users and includes the following keys metrics:</p>
<ul>
<li><strong>P99 Latency:</strong> the amount of time your application spends processing each of the fastest 99% of requests. For example, if the value of the <code>P99</code> latency is 760 ms, 99% percent of requests have responses that are equal to or faster than 760 ms.</li>
<li><strong>Error Rate</strong>: the percentage of failing requests i.e ratio of error requests to the total requests.</li>
<li><strong>Requests per Second</strong>: the number of requests your application processes per second.</li>
<li><strong>Key Operations</strong>: It lists the key APIs and operations which the particular application is serving.</li>
<li><strong>Apdex</strong>: It is a score between 0 and 1 that helps you measure the user satisfaction.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#open-the-services-section
tag_set: userguide, metrics
image_urls: https://signoz.io/img/docs/services-overview.webp, https://signoz.io/img/docs/trace-request-shop.webp
tracking_id: docs-userguide-metrics-open-the-services-section
group_tracking_ids: docs-userguide-metrics
<h2>View Services: Open the Services Section</h2>
<p>From the sidebar, select <strong>Services</strong>:</p>
<p><img src="https://signoz.io/img/docs/services-overview.webp" alt="SigNoz UI showing the Services section" /></p>
<p><em>Services Section</em></p>
<p>This page provides an overview of your applications‚Äô health and performance. It shows the list of your applications formatted as a table and, for each application, SigNoz displays the RED metrics mentioned above.</p>
<p>This page shows all the instrumented applications sending the data to SigNoz. This includes web servers, message brokers/queuing systems, web/mobile clients, cron jobs, and more.</p>
<p><strong>What services are shown? And how are the RED metrics calculated?</strong></p>
<p>We rely on the semantic conventions provided by OpenTelemetry. Every unique <code>service.name</code> configured and received is part of the service list.</p>
<p><img src="https://signoz.io/img/docs/trace-request-shop.webp" alt="A trace Request in SigNoz UI" /></p>
<p><em>A typical trace request</em></p>
<p>The following logic is used for the RED metrics generation of each service. In a distributed trace, a request goes through several entities performing various kinds of work. There is an entry point span for each service that took part in the trace journey. This can be thought of as a sub-root span for the service. This sub-root span can have many child spans which could be doing work in parallel or sequential or a combination of both. From an outside perspective this sub-root span work is an operation done by the service and how much time it took to complete this operation is the duration metric. For a web server, this is an API endpoint returning some data and request time is the duration metric. For a messaging consumer service, this is a consume trigger, and till it is done with the message received. For a mobile client application, this could be a button click to submit a form and the time taken to fulfill the request.</p>
<ul>
<li>Operations/s - Number of sub-root spans seen for a service</li>
<li>PXX - Quantile of the duration of the sub-root spans</li>
<li>Error rate - Number of sub-root spans with status error / Total number of sub-root spans</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#sort-the-list-of-applications
tag_set: userguide, metrics
image_urls: https://signoz.io/img/docs/sort-list-applications.webp
tracking_id: docs-userguide-metrics-sort-the-list-of-applications
group_tracking_ids: docs-userguide-metrics
<h2>View Services: Sort the List of Applications</h2>
<p>You can switch the sorting order of the values in a column by clicking its heading: first click for ascending order, second click for descending order, and a third click to cancel the sorting.</p>
<p><img src="https://signoz.io/img/docs/sort-list-applications.webp" alt="SigNoz UI showing how to sort the applications in the Services Section" /></p>
<p><em>Sort Applications</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#filter-the-list-of-applications
tag_set: userguide, metrics
image_urls: https://signoz.io/img/docs/resource-attribute-filtering.webp
tracking_id: docs-userguide-metrics-filter-the-list-of-applications
group_tracking_ids: docs-userguide-metrics
<h2>View Services: Filter the List of Applications</h2>
<p>You can add attributes to applications and filter based on these attributes.</p>
<p><img src="https://signoz.io/img/docs/resource-attribute-filtering.webp" alt="SigNoz UI showing how to filter your application using attributes" /></p>
<p><em>Attributes based filtering</em></p>
<p>You can add attributes with <code>OTEL_RESOURCE_ATTRIBUTES</code> flag when starting the application. The below example shows how to set values for <code>service.namespace</code> and <code>deployment.environment</code></p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=&quot;service.name=flaskApp,service.namespace=sampleapps,deployment.environment=play&quot; OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://3.11.144.34:4317&quot;
OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument python3 app.py
</code></pre>
<p>By default, you can filter based on <code>service.namespace</code> and <code>deployment.environment</code> dimensions.</p>
<p>To add another dimension, update the dimension fields of <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/otel-collector-config.yaml#L107">config.yaml file</a> and then deploy the yaml file again.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#global-time
tag_set: userguide, metrics
image_urls: https://signoz.io/img/docs/global-time.webp
tracking_id: docs-userguide-metrics-global-time
group_tracking_ids: docs-userguide-metrics
<h2>View Services: Global Time</h2>
<p>On the top right corner of your application's dashboard, you have the ability to select the time frame for the data displayed in <strong>all the metric graphs</strong>. You can choose from a range of options, spanning from the last 5 minutes to the past week, and even opt for a custom date range.</p>
<p><img src="https://signoz.io/img/docs/global-time.webp" alt="SigNoz UI showing how to apply a time interval for all your metrics chart" /></p>
<p><em>Global Time</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#view-magnified-graphs
tag_set: userguide, metrics
image_urls: https://signoz.io/img/docs/view-enlarged-graphs.webp, https://signoz.io/img/docs/filter-series.webp
tracking_id: docs-userguide-metrics-view-magnified-graphs
group_tracking_ids: docs-userguide-metrics
<h2>View Services: View Magnified graphs</h2>
<p>You can magnify the graphs by hovering over the Metric name, clicking the dropdown arrow, and selecting the &quot;View&quot; option.</p>
<p><img src="https://signoz.io/img/docs/view-enlarged-graphs.webp" alt="SigNoz UI showing how view a magnified view of a graph" /></p>
<p><em>View Magnified Graphs</em></p>
<h3>## Filter Series</h3>
<p>Inside the Metric's graph, you can use the &quot;Filter Series&quot; search bar to locate specific labels, allowing you to include or exclude them from the graph. For instance, in the image below, our search for 'p9' within the latency graph resulted in the display of 'p90' and 'p99' latency data. Click on the <strong>Save</strong> Button to save any changes.</p>
<p><img src="https://signoz.io/img/docs/filter-series.webp" alt="SigNoz UI showing the filter series feature inside a graph" /></p>
<p><em>Filter Series</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#view-details-about-an-application
tag_set: userguide, metrics
image_urls: 
tracking_id: docs-userguide-metrics-view-details-about-an-application
group_tracking_ids: docs-userguide-metrics
<h2>View Services: View Details About an Application: View Details About an Application</h2>
<p>The RED metrics help you spot performance bottlenecks or failures across all your applications. For example, if the error rate of an application increases, you can assume that these errors will impact the experience of your customers. Once you‚Äôve identified a potential issue, select a row to open the application details page:</p>
<p>The application details pane contains three panes:</p>
<ul>
<li>Application Metrics - Overview</li>
<li>Database Calls</li>
<li>External Calls</li>
</ul>
<p><img src="https://signoz.io/docs/userguide/metrics/Overview" alt="SigNoz UI showing the three panes on Applications details, i.e., Application Monitoring(Overview), Database call metrics, External Metrics" /></p>
<p><em>Panes on Application Details</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#application-metrics-in-signoz
tag_set: userguide, metrics
image_urls: https://signoz.io/img/docs/application-latency.webp, https://signoz.io/img/docs/operations-per-second.webp, https://signoz.io/img/docs/error-percentage.webp, https://signoz.io/img/docs/key-operations.webp, https://signoz.io/img/docs/apdex-score.webp, https://signoz.io/img/docs/apdex-score-threshold.webp
tracking_id: docs-userguide-metrics-application-metrics-in-signoz
group_tracking_ids: docs-userguide-metrics
<p>View Services: View Details About an Application: Application Metrics in SigNoz</p>
<p>The application metrics pane is comprised of five graphs:</p>
<p><strong>Application Latency in Milliseconds</strong><br />
This graph shows the <code>P99</code>, <code>P95</code>, and <code>P50</code> latencies for the selected period of time.</p>
<p><img src="https://signoz.io/img/docs/application-latency.webp" alt="SigNoz UI showing application 50th/90th/99th Percentile latencies" /></p>
<p><em>Application Latency</em></p>
<p><strong>Operations per Second</strong><br />
This graph shows the number of operations (Example requests) per second your application currently serves.</p>
<p><img src="https://signoz.io/img/docs/operations-per-second.webp" alt="SigNoz UI showing application Operations per second or requests per second" /></p>
<p><em>Application Operations per second</em></p>
<p><strong>Error Percentage</strong><br />
This graph shows the percentage of errors of the total sum of requests.</p>
<p><img src="https://signoz.io/img/docs/error-percentage.webp" alt="SigNoz UI showing application Error Percentage" /></p>
<p><em>Application Error Percentage</em></p>
<p><strong>Key Operations</strong><br />
This list helps you find the slow operations of your application. You can select a column heading to sort the list by the values in that column. Select the column heading again to reverse the sort order or to cancel sorting.</p>
<p><img src="https://signoz.io/img/docs/key-operations.webp" alt="SigNoz UI showing application Key Operations such as API requests" /></p>
<p><em>Application Key Operations</em></p>
<p><strong>Apdex</strong><br />
Application Performance Index <a href="https://www.apdex.org/">(Apdex)</a> is an open standard that defines a method to report, benchmark, and rate application response time. An Apdex score helps you understand and identify the impact on application performance over time. The Apdex score indicates the end users' level of satisfaction from 0 (least satisfied) to 1 (most satisfied). Threshold is an aribtary value which is set to <code>0.5</code> by default and can be changed according to the requirements.</p>
<p><img src="https://signoz.io/img/docs/apdex-score.webp" alt="SigNoz UI showing application Apdex Score which helps to identify the application performance over time" /></p>
<p><em>Application Apdex Score</em></p>
<p>You can change the Threshold value by going to Settings on top-right corner of your SigNoz dashboard.</p>
<p><img src="https://signoz.io/img/docs/apdex-score-threshold.webp" alt="SigNoz UI showing how to change the threshold of Apdex Score for an application in Settings" /></p>
<p><em>Change Apdex Score Threshold</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#database-calls-in-signoz
tag_set: userguide, metrics
image_urls: 
tracking_id: docs-userguide-metrics-database-calls-in-signoz
group_tracking_ids: docs-userguide-metrics
<p>View Services: View Details About an Application: Database Calls in SigNoz</p>
<p>This pane shows details about the database calls that your application makes. The spans should have the following span attributes to be counted in this panel</p>
<ul>
<li>
<p><code>span.kind!=2</code> which means these are spans of kind anything except <code>SERVER</code>. You can read more details on SpanKinds <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/api.md#spankind">here</a></p>
</li>
<li>
<p><code>db.system</code> should be present as span attribute</p>
</li>
</ul>
<p>If your services are making DB calls and your Database Call panels show as empty, please make sure that:</p>
<ol>
<li>Your spans have the above attributes.</li>
<li>You have used appropriate libraries for instrumenting packages which you use to make DB calls from your application</li>
</ol>
<p>The graphs in this pane provide the following information:</p>
<ul>
<li>The number of database calls per second</li>
<li>The average duration of your database calls. expressed in milliseconds</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/metrics/#external-calls-in-signoz
tag_set: userguide, metrics
image_urls: 
tracking_id: docs-userguide-metrics-external-calls-in-signoz
group_tracking_ids: docs-userguide-metrics
<p>View Services: View Details About an Application: External Calls in SigNoz</p>
<p>The external calls pane allows you to track the external services your applications depend on.</p>
<p>The spans should have the following span attributes to be counted in this panel</p>
<ul>
<li>
<p><code>span.kind=3</code> which means these are spans of kind <a href="https://github.com/open-telemetry/opentelemetry-proto/blob/main/opentelemetry/proto/trace/v1/trace.proto#L139"><code>CLIENT</code></a>
. You can read more details on SpanKinds <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/api.md#spankind">here</a></p>
</li>
<li>
<p>One of the following sets of attributes</p>
<ul>
<li>rpc.system, rpc.service, rpc.method</li>
<li>rpc.system, net.peer.name, net.peer.port</li>
<li>rpc.system, net.peer.ip, net.peer.port</li>
<li>http.host</li>
<li>net.peer.name, net.peer.port</li>
<li>net.peer.ip, net.peer.port</li>
<li>http.url</li>
<li>peer.service</li>
</ul>
</li>
</ul>
<p>The remote host address is constructed from one of the attribute sets in the order listed above. This includes any database calls that have transport other than unix domain socket or pipe, or a call to another http host, or an aws lambda function and generally any out of process call over the network.</p>
<p>If your services are making external calls but External Call panels show as empty, please make sure that your spans have the above attributes.</p>
<p>The graphs in this pane provide the following information:</p>
<ul>
<li>The percentage of external calls that resulted in errors.</li>
<li>The average duration of all your external calls.</li>
<li>The number of external calls per second by address.</li>
<li>The average duration of your external calls by address.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/
tag_set: instrumentation, tomcat
image_urls: https://signoz.io/img/docs/opentelemetry_java_instrument.webp
tracking_id: docs-instrumentation-tomcat
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Tomcat applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Tomcat application.</p>
<p>OpenTelemetry Java is the language-specific implementation of OpenTelemetry in Java that can be used to instrument Tomcat applications.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Tomcat application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/opentelemetry_java_instrument.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Tomcat applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>There are two types of application instrumentation:</p>
<ul>
<li>
<p><strong>Auto Instrumentation</strong><br />
A completely automatic and out of box experience, with minimal code changes. For your Tomcat application, we recommend getting started with auto instrumentation.</p>
</li>
<li>
<p><strong>Manual Instrumentation</strong><br />
It involves writing instrumentation using OpenTelemetry SDK and API manually. You would need to get a handle to an instance of the <code>OpenTelemetry</code> interface, acquire a tracer, and create spans manually.</p>
</li>
</ul>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in a Tomcat application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#requirements
tag_set: instrumentation, tomcat
image_urls: 
tracking_id: docs-instrumentation-tomcat-requirements
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: Requirements</h2>
<p>Java 8 or higher</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#send-traces-to-signoz-cloud
tag_set: instrumentation, tomcat
image_urls: 
tracking_id: docs-instrumentation-tomcat-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud</h2>
<p>OpenTelemetry provides a handy Java JAR agent that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>
<h4>## Send traces directly to SigNoz Cloud</h4>
<p>OpenTelemetry Java agent can send traces directly to SigNoz Cloud.</p>
<p><strong>Step 1.</strong> Download otel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Enable the instrumentation agent and run your application</p>
<p>If you run your <code>.war</code> package by putting in <code>webapps</code> folder, just add <code>setenv.sh</code> in your Tomcat <code>bin</code> folder.</p>
<p>This should set these environment variables and start sending telemetry data to SigNoz Cloud.</p>
<pre><code>export CATALINA_OPTS=&quot;$CATALINA_OPTS -javaagent:/path/to/opentelemetry-javaagent.jar&quot;
export OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.{region}.signoz.cloud:443
export OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt;
</code></pre>
<ul>
<li><code>&lt;app_name&gt;</code> is the name for your application</li>
<li><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />
<h4>## Send traces via OTel Collector binary</h4>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Java application.</p>
<p><strong>Step 1.</strong> Download OTel java binary agent</p>
<pre><code>wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
<p><strong>Step 2.</strong> Enable the instrumentation agent and run your application</p>
<p>If you run your <code>.war</code> package by putting in <code>webapps</code> folder, just add <code>setenv.sh</code> in your Tomcat <code>bin</code> folder.</p>
<p>This should set these environment variables and start sending telemetry data to SigNoz Cloud.</p>
<pre><code>export CATALINA_OPTS=&quot;$CATALINA_OPTS -javaagent:/path/to/opentelemetry-javaagent.jar&quot;
</code></pre>
<ul>
<li>path/to - Update it to the path of your downloaded Java JAR agent.</li>
</ul>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, tomcat
image_urls: 
tracking_id: docs-instrumentation-tomcat-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: Send Traces to Self-Hosted SigNoz</h2>
<p>You can use OpenTelemetry Java to send your traces directly to SigNoz. OpenTelemetry provides a <strong>handy Java JAR agent</strong> that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from a number of popular libraries and frameworks.</p>
<h3>## Steps to auto-instrument Tomcat applications for traces</h3>
<p><a href="https://signoz.io/opentelemetry/java-auto-instrumentation/">OpenTelemetry Java auto-instrumentation</a> supports collecting telemetry data from a huge number of libraries and frameworks. You can check out the full list <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/supported-libraries.md">here</a>.</p>
<ol>
<li>
<p><strong>Download the latest OpenTelemetry Java JAR agent</strong><br />
Download the latest <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar">Java JAR agent</a>
. You can also use the terminal to get the file using the following command:</p>
<pre><code> wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
</code></pre>
</li>
<li>
<p><strong>Enable the instrumentation agent and run your application</strong><br />
If you run your <code>.war</code> package by putting in <code>webapps</code> folder, just add <code>setenv.sh</code> in your Tomcat <code>bin</code> folder.</p>
<p>This should set these environment variables and start sending telemetry data to SigNoz backend specified in the IP</p>
<pre><code>export CATALINA_OPTS=&quot;$CATALINA_OPTS -javaagent:/path/to/opentelemetry-javaagent.jar&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=http://&lt;IP of SigNoz Backend&gt;:4317
export OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt;
</code></pre>
<p>where <code>app_name</code> is the name you want to set for your application and <code>IP of SigNoz Backend</code> is the IP where SigNoz backend is accessible.</p>
</li>
</ol>
<p>With the above environment variables, we are configuring the exporter to send data to SigNoz backend. By default, OpenTelemetry Java agent uses¬†<a href="https://github.com/open-telemetry/opentelemetry-java/tree/main/exporters/otlp">OTLP exporter</a> configured to send data.</p>
<p>üìù Note</p>
<p>Two things to note about the environment variables:</p>
<p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code> - This is the endpoint of the machine where SigNoz is installed.</p>
<p><code>path/to</code> - Update it to the path of your downloaded Java JAR agent.</p>
<p>If you have installed SigNoz on a machine with IP <code>http://40.76.59.122</code> and your Java JAR agent is saved at <code>/Users/john/Downloads/</code>, then the final command looks like:</p>
<pre><code>export CATALINA_OPTS=&quot;$CATALINA_OPTS -javaagent:/Users/john/Downloads/opentelemetry-javaagent.jar&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=http://40.76.59.122:4317
export OTEL_RESOURCE_ATTRIBUTES=service.name=Tomcat-SigNoz
</code></pre>
<p>Here‚Äôs a handy <a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> to figure out which address to use to send data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° Remember to allow incoming requests to port 4317 of the machine where SigNoz backend is hosted.</p>
<p>Here's a video on how to instrument Tomcat applications with SigNoz and a <a href="https://signoz.io/opentelemetry/tomcat/">blog</a> with step by step instructions.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, tomcat
image_urls: https://signoz.io/img/docs/java_app_services_list.webp
tracking_id: docs-instrumentation-tomcat-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/java_app_services_list.webp" alt="Java Application in the list of services being monitored in SigNoz" /></p>
<p><em>Java Application in the list of services being monitored in SigNoz</em></p>
<h3>## Configuring the agent</h3>
<p>The agent is highly configurable. You can check out all the configuration options available <a href="https://opentelemetry.io/docs/instrumentation/java/automatic/agent-config/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#disabled-instrumentations
tag_set: instrumentation, tomcat
image_urls: 
tracking_id: docs-instrumentation-tomcat-disabled-instrumentations
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: Disabled instrumentations</h2>
<p>Some instrumentations can produce too many spans and make traces very noisy. For this reason, the following instrumentations are disabled by default:</p>
<ul>
<li><code>jdbc-datasource</code>¬†which creates spans whenever the¬†<code>java.sql.DataSource#getConnection</code>¬†method is called.</li>
<li><code>dropwizard-metrics</code>,¬†which might create very low-quality metrics data because of the lack of label/attribute support in the Dropwizard metrics API.</li>
</ul>
<p>To enable them, add the¬†<code>otel.instrumentation.&lt;name&gt;.enabled</code>¬†system property:¬†<code>-Dotel.instrumentation.jdbc-datasource.enabled=true</code></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#manual-instrumentation
tag_set: instrumentation, tomcat
image_urls: 
tracking_id: docs-instrumentation-tomcat-manual-instrumentation
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: Manual Instrumentation</h2>
<p>For manual instrumentation of Java application, refer to the docs <a href="https://opentelemetry.io/docs/instrumentation/java/manual/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#troubleshooting-your-installation
tag_set: instrumentation, tomcat
image_urls: 
tracking_id: docs-instrumentation-tomcat-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: Troubleshooting your installation</h2>
<p>If spans are not being reported to SigNoz, try running in debug mode by setting <code>OTEL_LOG_LEVEL=debug</code>:</p>
<p>The debug log level will print out the configuration information. It will also emit every span to the console, which should look something like:</p>
<pre><code>Span {
  attributes: {},
  links: [],
  events: [],
  status: { code: 0 },
  endTime: [ 1597810686, 885498645 ],
  _ended: true,
  _duration: [ 0, 43333 ],
  name: 'bar',
  spanContext: {
    traceId: 'eca3cc297720bd705e734f4941bca45a',
    spanId: '891016e5f8c134ad',
    traceFlags: 1,
    traceState: undefined
  },
  parentSpanId: 'cff3a2c6bfd4bbef',
  kind: 0,
  startTime: [ 1597810686, 885455312 ],
  resource: Resource { labels: [Object] },
  instrumentationLibrary: { name: 'example', version: '*' },
  _logger: ConsoleLogger {
    debug: [Function],
    info: [Function],
    warn: [Function],
    error: [Function]
  },
  _traceParams: {
    numberOfAttributesPerSpan: 32,
    numberOfLinksPerSpan: 32,
    numberOfEventsPerSpan: 128
  },
  _spanProcessor: MultiSpanProcessor { _spanProcessors: [Array] }
},
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#sample-java-application
tag_set: instrumentation, tomcat
image_urls: 
tracking_id: docs-instrumentation-tomcat-sample-java-application
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: Sample Java Application</h2>
<p>We have included a sample Java application with README.md at <a href="https://github.com/SigNoz/distributed-tracing-java-sample">Sample Java App Github Repo.</a></p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/tomcat/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, tomcat
image_urls: 
tracking_id: docs-instrumentation-tomcat-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-tomcat
<h2>Tomcat OpenTelemetry Instrumentation: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Nestjs applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Nestjs application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#requirements
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-requirements
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation: Requirements</h2>
<p><strong>Supported Versions</strong></p>
<ul>
<li><code>&gt;=4.0.0</code></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#send-traces-to-signoz-cloud
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
<ul>
<li><a href="#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
<ul>
<li><a href="#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;&lt;SIGNOZ_ENDPOINT&gt;&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_ACCESS_TOKEN&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-send-traces-directly-to-signoz-cloud--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api@^1.6.0                                                                       
npm install --save @opentelemetry/sdk-node@^0.45.0
npm install --save @opentelemetry/auto-instrumentations-node@^0.39.4
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.45.0
</code></pre>
<p><strong>Step 2.</strong> Create <code>tracer.ts</code> file<br />
You need to configure the endpoint for SigNoz cloud in this file.</p>
<pre><code>'use strict';

import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { Resource } from '@opentelemetry/resources';
import * as opentelemetry from '@opentelemetry/sdk-node';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';

// Configure the SDK to export telemetry data to the console
// Enable all auto-instrumentations from the meta package
const exporterOptions = {
  //highlight-start
  url: 'https://ingest.{region}.signoz.cloud:443/v1/traces',
  //highlight-end
};

const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: '&lt;service_name&gt;',
  }),
});

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start();

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk
    .shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0));
});

export default sdk;
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of your service.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443/v1/traces</td>
</tr>
</tbody>
</table>
<p><strong>Step 3.</strong> On <code>main.ts</code> file or file where your app starts import tracer using below command.</p>
<p>‚úÖ Info</p>
<p>The below import should be the first line in the main file of your application (Ex -&gt; <code>main.ts</code>)</p>
<pre><code>const tracer = require('./tracer')
</code></pre>
<p><strong>Step 4.</strong> Start the tracer<br />
In the <code>async function boostrap</code> section of the application code, initialize the tracer as follows:</p>
<pre><code>const tracer = require('./tracer')

import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
  // All of your application code and any imports that should leverage
  // OpenTelemetry automatic instrumentation must go here.

async function bootstrap() {
    // highlight-start
    await tracer.start();
    //highlight-end
    const app = await NestFactory.create(AppModule);
    await app.listen(3001);
  }
  bootstrap();
</code></pre>
<p><strong>Step 5.</strong> Run the application</p>
<pre><code>OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;&quot; nest start
</code></pre>
<p>The data captured with OpenTelemetry from your application should start showing on the SigNoz dashboard.</p>
<p><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
<p><strong>Step 6.</strong> You can validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-send-traces-via-otel-collector-binary--no-code-automatic-instrumentation
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - No Code Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended-1
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended-1
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4318/v1/traces&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-send-traces-via-otel-collector-binary--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - Code Level Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api@^1.6.0
npm install --save @opentelemetry/sdk-node@^0.45.0
npm install --save @opentelemetry/auto-instrumentations-node@^0.39.4
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.45.0
</code></pre>
<p><strong>Step 2.</strong> Create <code>tracer.ts</code> file</p>
<pre><code>'use strict';

import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { Resource } from '@opentelemetry/resources';
import * as opentelemetry from '@opentelemetry/sdk-node';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';

// Configure the SDK to export telemetry data to the console
// Enable all auto-instrumentations from the meta package
const exporterOptions = {
  //highlight-start
  url: 'http://localhost:4318/v1/traces',
  //highlight-end
};

const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new opentelemetry.NodeSDK({
  traceExporter,
  instrumentations: [getNodeAutoInstrumentations()],
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: '&lt;service_name&gt;',
  }),
});

// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start();

// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
  sdk
    .shutdown()
    .then(() =&gt; console.log('Tracing terminated'))
    .catch((error) =&gt; console.log('Error terminating tracing', error))
    .finally(() =&gt; process.exit(0));
});

export default sdk;
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of your service.</li>
</ul>
<p><strong>Step 3.</strong> On <code>main.ts</code> file or file where your app starts import tracer using below command.</p>
<p>‚úÖ Info</p>
<p>The below import should be the first line in the main file of your application (Ex -&gt; <code>main.ts</code>)</p>
<pre><code>const tracer = require('./tracer')
</code></pre>
<p><strong>Step 4.</strong> Start the tracer<br />
In the <code>async function boostrap</code> section of the application code, initialize the tracer as follows:</p>
<pre><code>const tracer = require('./tracer')

import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
  // All of your application code and any imports that should leverage
  // OpenTelemetry automatic instrumentation must go here.

async function bootstrap() {
    // highlight-start
    await tracer.start();
    //highlight-end
    const app = await NestFactory.create(AppModule);
    await app.listen(3001);
  }
  bootstrap();
</code></pre>
<p><strong>Step 5.</strong> Run the application</p>
<pre><code>  nest start
</code></pre>
<p><strong>Step 6.</strong> You can validate if your application is sending traces to SigNoz cloud <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, nestjs
image_urls: https://signoz.io/img/docs/nestjs_instrumentation.webp
tracking_id: docs-instrumentation-nestjs-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Send Traces to Self-Hosted SigNoz</h2>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Nestjs application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/nestjs_instrumentation.webp" alt="All in one auto instrumentation library - identifies and instruments packages used by your NestJS application" /></p>
<p><em>All in one auto instrumentation library - identifies and instruments packages used by your NestJS application</em></p>
<p>You have two choices for instrumenting your NestJS application with OpenTelemetry.</p>
<ul>
<li>
<p><strong><a href="#using-the-all-in-one-auto-instrumentation-library">Use the all-in-one auto-instrumentation library(Recommended)</a></strong><br />
The auto-instrumentation library of OpenTelemetry is a meta package that provides a simple way to initialize multiple Nodejs instrumnetations.</p>
<p>‚úÖ Info</p>
<p>If you are on K8s, you should checkout <a href="/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">opentelemetry operators</a> which enable auto instrumenting Javascript applications very easily.</p>
</li>
<li>
<p><strong><a href="#using-a-specific-auto-instrumentation-library">Use a specific auto-instrumentation library</a></strong><br />
You can use individual auto-instrumentation libraries too for a specific component of your application. For example, you can use <code>@opentelemetry/instrumentation-nestjs-core</code> for instrumenting the Nestjs web framework.</p>
</li>
</ul>
<p>Let's see how to instrument your Nestjs application with OpenTelemetry.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#using-the-all-in-one-auto-instrumentation-library
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-using-the-all-in-one-auto-instrumentation-library
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Using the all-in-one auto-instrumentation library</p>
<p>The recommended way to instrument your Nestjs application is to use the all-in-one auto-instrumentation library - <code>@opentelemetry/auto-instrumentations-node</code>. It provides a simple way to initialize multiple Nodejs instrumentations.</p>
<p>Internally, it calls the specific auto-instrumentation library for components used in the application. You can see the complete list <a href="https://github.com/open-telemetry/opentelemetry-js-contrib/tree/main/metapackages/auto-instrumentations-node#supported-instrumentations">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#steps-to-auto-instrument-nestjs-application
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-steps-to-auto-instrument-nestjs-application
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Nestjs application</p>
<ol>
<li>
<p>Install the dependencies<br />
We start by installing the relevant dependencies.</p>
<pre><code>npm install --save @opentelemetry/sdk-node
npm install --save @opentelemetry/auto-instrumentations-node
npm install --save @opentelemetry/exporter-trace-otlp-http
</code></pre>
<p>üìù Note</p>
<p>If you run into any error, you might want to use these pinned versions of OpenTelemetry libraries used in this <a href="https://github.com/SigNoz/sample-NestJs-app/blob/master/package.json">GitHub repo</a>
.</p>
</li>
<li>
<p>Create a <code>tracer.ts</code> file</p>
<p>'use strict';</p>
<p>import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { Resource } from '@opentelemetry/resources';
import * as opentelemetry from '@opentelemetry/sdk-node';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';</p>
<p>// Configure the SDK to export telemetry data to the console
// Enable all auto-instrumentations from the meta package
const exporterOptions = {
//highlight-start
url: 'http://localhost:4318/v1/traces',
//highlight-end
};</p>
<p>const traceExporter = new OTLPTraceExporter(exporterOptions);
const sdk = new opentelemetry.NodeSDK({
traceExporter,
instrumentations: [getNodeAutoInstrumentations()],
resource: new Resource({
[SemanticResourceAttributes.SERVICE_NAME]: '&lt;service_name&gt;',
}),
});</p>
<p>// initialize the SDK and register with the OpenTelemetry API
// this enables the API to record telemetry
sdk.start();</p>
<p>// gracefully shut down the SDK on process exit
process.on('SIGTERM', () =&gt; {
sdk
.shutdown()
.then(() =&gt; console.log('Tracing terminated'))
.catch((error) =&gt; console.log('Error terminating tracing', error))
.finally(() =&gt; process.exit(0));
});</p>
<p>export default sdk;</p>
</li>
</ol>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of your service.</li>
</ul>
<ol start="3">
<li>
<p>On <code>main.ts</code> file or file where your app starts import tracer using below command.</p>
<p>‚úÖ Info</p>
<p>The below import should be the first line in the main file of your application (Ex -&gt; <code>main.ts</code>)</p>
<pre><code>const tracer = require('./tracer')
</code></pre>
</li>
<li>
<p>Start the tracer<br />
In the <code>async function boostrap</code> section of the application code, initialize the tracer as follows:</p>
<pre><code>const tracer = require('./tracer')

import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
  // All of your application code and any imports that should leverage
  // OpenTelemetry automatic instrumentation must go here.

async function bootstrap() {
    // highlight-start
    await tracer.start();
    //highlight-end
    const app = await NestFactory.create(AppModule);
    await app.listen(3001);
  }
  bootstrap();
</code></pre>
</li>
<li>
<p>Run the application</p>
<pre><code>nest start
</code></pre>
</li>
</ol>
<p>The data captured with OpenTelemetry from your application should start showing on the SigNoz dashboard.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, nestjs
image_urls: https://signoz.io/img/docs/nestjs_application_instrumented.webp
tracking_id: docs-instrumentation-nestjs-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces</p>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the <code>Services</code> tab. Hit the <code>Refresh</code> button on the top right corner, and your application should appear in the list of <code>Applications</code>.</li>
<li>Go to the <code>Traces</code> tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs <a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/nestjs_application_instrumented.webp" alt="Nestjs Application in the list of services being monitored in SigNoz" /></p>
<p><em>Nestjs Application in the list of services being monitored in SigNoz</em></p>
<p>If you don't see your application reported in the list of services, try our <a href="https://signoz.io/docs/install/troubleshooting/">troubleshooting</a> guide.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#using-a-specific-auto-instrumentation-library
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-using-a-specific-auto-instrumentation-library
group_tracking_ids: docs-instrumentation-nestjs
<p>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Using a specific auto-instrumentation library</p>
<p>If you want to instrument only your Nestjs framework, then you need to use the following package:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-nestjs-core
</code></pre>
<p>üìù Note</p>
<p>In the above case, you will have to install packages for all the components that you want to instrument with OpenTelemetry individually. You can find detailed instructions <a href="https://signoz.io/docs/instrumentation/javascript/#using-a-specific-auto-instrumentation-library">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#instrumentation-modules-for-databases
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-instrumentation-modules-for-databases
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Instrumentation Modules for Databases</h2>
<p>The <code>@opentelemetry/auto-instrumentations-node</code> can inititialize instrumentation for popular databases. Hence it‚Äôs recommended to <a href="#using-the-all-in-one-auto-instrumentation-library">get started</a> with it.</p>
<p>But if you are using <a href="#using-a-specific-auto-instrumentation-library">specific auto-instrumentation packages</a>
, here‚Äôs a list of packages for popular databases.</p>
<h3>## MongoDB instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>‚Ä¢ <code>&gt;=3.3 &lt;5</code></p>
<p>Module that provides automatic instrumentation for MongoDB:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-mongodb
</code></pre>
<h3>## Redis Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>This package supports¬†<code>redis@^2.6.0</code> and¬†<code>redis@^3.0.0</code> For version¬†<code>redis@^4.0.0</code>, please use¬†<code>@opentelemetry/instrumentation-redis-4</code></p>
<pre><code>npm install --save @opentelemetry/instrumentation-redis
</code></pre>
<h3>## MySQL Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<p>‚Ä¢ <code>2.x</code></p>
<p>Module that provides automatic instrumentation for MySQL:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-mysql
</code></pre>
<h3>## Memcached Instrumentation</h3>
<p>üìù Note</p>
<p>If you‚Äôre using <code>@opentelemetry/auto-instrumentations-node</code>, you don‚Äôt need to install specific modules for your database.</p>
<p><strong>Supported Versions</strong></p>
<ul>
<li><code>&gt;=2.2</code></li>
</ul>
<p>Module that provides automatic instrumentation for Memcached:</p>
<pre><code>npm install --save @opentelemetry/instrumentation-memcached
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#troubleshooting-your-installation
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Troubleshooting your installation</h2>
<p>Set an environment variable to run the OpenTelemetry launcher in debug mode, where it logs details about the configuration and emitted spans:</p>
<pre><code>export OTEL_LOG_LEVEL=debug
</code></pre>
<p>The output may be very verbose with some benign errors. Early in the console output, look for logs about the configuration. Next, look for lines like the ones below, which are emitted when spans are emitted to SigNoz.</p>
<pre><code>{
  &quot;traceId&quot;: &quot;985b66d592a1299f7d12ebca56ca1fe3&quot;,
  &quot;parentId&quot;: &quot;8d62a70aa335a227&quot;,
  &quot;name&quot;: &quot;bar&quot;,
  &quot;id&quot;: &quot;17ada85c3d55376a&quot;,
  &quot;kind&quot;: 0,
  &quot;timestamp&quot;: 1685674607399000,
  &quot;duration&quot;: 299,
  &quot;attributes&quot;: {},
  &quot;status&quot;: { &quot;code&quot;: 0 },
  &quot;events&quot;: []
}
{
  &quot;traceId&quot;: &quot;985b66d592a1299f7d12ebca56ca1fe3&quot;,
  &quot;name&quot;: &quot;foo&quot;,
  &quot;id&quot;: &quot;8d62a70aa335a227&quot;,
  &quot;kind&quot;: 0,
  &quot;timestamp&quot;: 1585130342183948,
  &quot;duration&quot;: 315,
  &quot;attributes&quot;: {
    &quot;name&quot;: &quot;value&quot;
  },
  &quot;status&quot;: { &quot;code&quot;: 0 },
  &quot;events&quot;: [\
    {\
      &quot;name&quot;: &quot;event in foo&quot;,\
      &quot;time&quot;: [1585130342, 184213041]\
    }\
  ]
}
</code></pre>
<p><em>Running short applications (Lambda/Serverless/etc)</em> If your application exits quickly after startup, you may need to explicitly shutdown the tracer to ensure that all spans are flushed:</p>
<pre><code>opentelemetry.trace.getTracer('your_tracer_name').getActiveSpanProcessor().shutdown()
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#sample-nestjs-application
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-sample-nestjs-application
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Sample NestJs Application</h2>
<ul>
<li>We have included a sample NestJs application with README.md at <a href="https://github.com/SigNoz/sample-NestJs-app">Sample NestJs App Github Repo.</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#further-reading
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-further-reading
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Further Reading</h2>
<ul>
<li>
<p><a href="https://signoz.io/blog/nodejs-performance-monitoring/">Nodejs Performance Monitoring</a></p>
</li>
<li>
<p><a href="https://signoz.io/blog/distributed-tracing-nodejs/">Implementing Distributed Tracing in a Nodejs application</a></p>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/nestjs/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, nestjs
image_urls: 
tracking_id: docs-instrumentation-nestjs-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-nestjs
<h2>Nestjs OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/hypercorn-unicorn-support/
tag_set: instrumentation, hypercorn-unicorn-support
image_urls: 
tracking_id: docs-instrumentation-hypercorn-unicorn-support
group_tracking_ids: docs-instrumentation-hypercorn-unicorn-support
<h2>Hypercorn/Unicorn Support</h2>
<p>The OpenTelemetry SDK is not fork-safe, meaning you cannot use the same instance of the SDK in multiple processes. Specifically, the <code>BatchSpanProcessor</code>, <code>PeriodicExportingMetricReader</code>, and <code>BatchLogProcessor</code> must not be copied from a parent process to a child process. Instead, the parent process and child process must create their own instances. If instances of these processors/readers are copied from a parent to a child process, they will not function correctly.</p>
<p>These components spawn background threads to send data to the backend and maintain state protected by a lock, which is not fork-safe (see Python issue <a href="https://bugs.python.org/issue6721">6721</a>
). As a result, they face the same limitations as any code using locks or mutexes.</p>
<p>A common workaround is to create new instances of these processors/readers in each process using the <code>register_at_fork</code> hook from the <code>os</code> module. However, Unicorn/Hypercorn use the <code>spawn</code> method to start worker processes, which does not invoke the <code>register_at_fork</code> hook. Consequently, this workaround is not applicable to Unicorn/Hypercorn.</p>
<p>The recommended solution is to use <a href="https://docs.gunicorn.org/"><code>gunicorn</code></a> with the worker class <code>uvicorn.workers.UvicornWorker</code> since it supports the <code>register_at_fork</code> hook.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular
group_tracking_ids: docs-instrumentation-angular
<h2>Angular OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Angular applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Nestjs application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#send-traces-to-signoz-cloud
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-angular
<h2>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetesWindows</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
<ul>
<li><a href="#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
<ul>
<li><a href="#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation">No Code Automatic Instrumentation</a>
(recommended)</li>
<li><a href="#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation">Code Level Automatic Instrumentation</a></li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended
group_tracking_ids: docs-instrumentation-angular
<p>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;&lt;SIGNOZ_ENDPOINT&gt;&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=&lt;SIGNOZ_ACCESS_TOKEN&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
<tr>
<td>SIGNOZ_ENDPOINT *</td>
<td>This is ingestion URL which you must have got in mail after registering on SigNoz cloud</td>
</tr>
<tr>
<td>SIGNOZ_ACCESS_TOKEN *</td>
<td>This is Ingestion Key which you must have got in mail after registering on SigNoz cloud</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#send-traces-directly-to-signoz-cloud---code-level-automatic-instrumentation
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-send-traces-directly-to-signoz-cloud--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-angular
<p>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - Code Level Automatic Instrumentation</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/sdk-trace-web@^1.21.0                                                                   
npm install --save @opentelemetry/instrumentation@^0.48.0
npm install --save @opentelemetry/auto-instrumentations-web@^0.36.0
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.48.0
npm install --save @opentelemetry/resources@^1.21.0
npm install --save @opentelemetry/propagator-b3@^1.21.0
npm install --save @opentelemetry/semantic-conventions@^1.21.0
</code></pre>
<p><strong>Step 2.</strong> Create <code>instrument.ts</code> file<br />
You need to configure the endpoint for SigNoz cloud in this file.</p>
<pre><code>import { registerInstrumentations } from '@opentelemetry/instrumentation';
import {
  WebTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
  BatchSpanProcessor,
} from '@opentelemetry/sdk-trace-web';
import { getWebAutoInstrumentations } from '@opentelemetry/auto-instrumentations-web';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { Resource } from '@opentelemetry/resources';
import { B3Propagator } from '@opentelemetry/propagator-b3';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';

const resource = Resource.default().merge(
  new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: '&lt;service_name&gt;',
    [SemanticResourceAttributes.SERVICE_VERSION]: '0.1.0',
  })
);

const provider = new WebTracerProvider({ resource });

provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()));

provider.addSpanProcessor(
  new BatchSpanProcessor(
    new OTLPTraceExporter({
      url: 'https://ingest.{REGION}.signoz.cloud:443/v1/traces',
      headers: {
        'signoz-access-token': '{SIGNOZ_INGESTION_KEY}',
      },
    })
  )
);

provider.register({
  propagator: new B3Propagator(),
});

registerInstrumentations({
  instrumentations: [\
    getWebAutoInstrumentations({\
      '@opentelemetry/instrumentation-document-load': {},\
      '@opentelemetry/instrumentation-user-interaction': {},\
      '@opentelemetry/instrumentation-fetch': {\
        propagateTraceHeaderCorsUrls: /.+/,\
      },\
      '@opentelemetry/instrumentation-xml-http-request': {\
        propagateTraceHeaderCorsUrls: /.+/,\
      },\
    }),\
  ],
});
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of your service.</li>
<li><code>SIGNOZ_INGESTION_KEY</code> : You can find your ingestion key from SigNoz cloud account details sent on your email.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443/v1/traces</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443/v1/traces</td>
</tr>
</tbody>
</table>
<p><strong>Step 3.</strong> Add the below import to your <code>main.ts</code> file.</p>
<pre><code>import './app/instrument';
</code></pre>
<p><strong>Step 4.</strong> Run the application</p>
<pre><code>ng serve
</code></pre>
<p>The data captured with OpenTelemetry from your application should start showing on the SigNoz Traces Explorer.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#send-traces-via-otel-collector-binary---no-code-automatic-instrumentation
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-send-traces-via-otel-collector-binary--no-code-automatic-instrumentation
group_tracking_ids: docs-instrumentation-angular
<p>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - No Code Automatic Instrumentation</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>üìù Note</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Javascript application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#send-traces-directly-to-signoz-cloud---no-code-automatic-instrumentation-recommended-1
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-send-traces-directly-to-signoz-cloud--no-code-automatic-instrumentation-recommended-1
group_tracking_ids: docs-instrumentation-angular
<p>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces directly to SigNoz Cloud - No Code Automatic Instrumentation (recommended)</p>
<p><strong>Step 1.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/api
npm install --save @opentelemetry/auto-instrumentations-node
</code></pre>
<p><strong>Step 2.</strong> Run the application</p>
<pre><code>export OTEL_TRACES_EXPORTER=&quot;otlp&quot;
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4318/v1/traces&quot;
export OTEL_NODE_RESOURCE_DETECTORS=&quot;env,host,os&quot;
export OTEL_SERVICE_NAME=&quot;&lt;APP_NAME&gt;&quot;
export NODE_OPTIONS=&quot;--require @opentelemetry/auto-instrumentations-node/register&quot;
&lt;your_run_command&gt;
</code></pre>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_NAME *</td>
<td>Name you want to give to your rust application</td>
</tr>
</tbody>
</table>
<p>replace <code>&lt;your_run_command&gt;</code> with the run command of your application</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#send-traces-via-otel-collector-binary---code-level-automatic-instrumentation
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-send-traces-via-otel-collector-binary--code-level-automatic-instrumentation
group_tracking_ids: docs-instrumentation-angular
<p>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send traces via OTel Collector binary - Code Level Automatic Instrumentation</p>
<p><strong>Step 1.</strong> Install OpenTelemetry Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM.</p>
<p>While creating the <code>config.yaml</code> during the installation fo the OTel Collector Binary, you need to enable CORS under the receivers section of the config file. This is needed so that you don't get CORS error which can hinder sending your Traces to SigNoz Cloud. See the code snippet below to understand how you can enable CORS in your config file:</p>
<pre><code>      http:
+        cors:
+          allowed_origins:
+            - &lt;Frontend-application-URL&gt;  # URL of your Frontend application. Example -&gt; http://localhost:4200, https://netflix.com etc.
</code></pre>
<p><code>&lt;Frontend-application-URL&gt;</code> - URL where your frontend application is running. For Example, <a href="http://localhost:4200">http://localhost:4200</a> or <a href="https://netflix.com">https://netflix.com</a> etc.</p>
<p><strong>NOTE:</strong> Make sure to restart your collector after making the config changes</p>
<p><strong>Step 2.</strong> Install OpenTelemetry packages</p>
<pre><code>npm install --save @opentelemetry/sdk-trace-web@^1.21.0                                                                   
npm install --save @opentelemetry/instrumentation@^0.48.0
npm install --save @opentelemetry/auto-instrumentations-web@^0.36.0
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.48.0
npm install --save @opentelemetry/resources@^1.21.0
npm install --save @opentelemetry/propagator-b3@^1.21.0
npm install --save @opentelemetry/semantic-conventions@^1.21.0
</code></pre>
<p><strong>Step 3.</strong> Create <code>instrument.ts</code> file</p>
<pre><code>import { registerInstrumentations } from '@opentelemetry/instrumentation';
import {
  WebTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
  BatchSpanProcessor,
} from '@opentelemetry/sdk-trace-web';
import { getWebAutoInstrumentations } from '@opentelemetry/auto-instrumentations-web';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { Resource } from '@opentelemetry/resources';
import { B3Propagator } from '@opentelemetry/propagator-b3';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';

const resource = Resource.default().merge(
  new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: '&lt;service_name&gt;',
    [SemanticResourceAttributes.SERVICE_VERSION]: '0.1.0',
  })
);

const provider = new WebTracerProvider({ resource });

provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()));

provider.addSpanProcessor(
  new BatchSpanProcessor(
    new OTLPTraceExporter({
      url: 'http://localhost:4318/v1/traces',
    })
  )
);

provider.register({
  propagator: new B3Propagator(),
});

registerInstrumentations({
  instrumentations: [\
    getWebAutoInstrumentations({\
      '@opentelemetry/instrumentation-document-load': {},\
      '@opentelemetry/instrumentation-user-interaction': {},\
      '@opentelemetry/instrumentation-fetch': {\
        propagateTraceHeaderCorsUrls: /.+/,\
      },\
      '@opentelemetry/instrumentation-xml-http-request': {\
        propagateTraceHeaderCorsUrls: /.+/,\
      },\
    }),\
  ],
});
</code></pre>
<ul>
<li><code>&lt;service_name&gt;</code> : Name of your service.</li>
</ul>
<p><strong>Step 4.</strong> Add the below import to your <code>main.ts</code> file.</p>
<pre><code>import './app/instrument';
</code></pre>
<p><strong>Step 5.</strong> Run the application</p>
<pre><code>ng serve
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#send-traces-to-signoz-self-host
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-send-traces-to-signoz-self-host
group_tracking_ids: docs-instrumentation-angular
<h2>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Send Traces to SigNoz Self-Host</h2>
<h3>## Instrumenting your Angular App with OpenTelemetry üõ†</h3>
<h4>## Pre-requisites</h4>
<p>Enable CORS in the OTel Receiver. Add the below required changes in the <code>otel-collector-config.yaml</code> file which can be generally found <a href="https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/otel-collector-config.yaml">here</a>.</p>
<pre><code>      http:
+        cors:
+          allowed_origins:
+            - https://netflix.com  # URL of your Frontend application
</code></pre>
<blockquote>
<p>Make sure to restart the container after making the config changes</p>
</blockquote>
<p><strong>Step 1.</strong> Install OpenTelemetry packages.</p>
<pre><code>npm install --save @opentelemetry/sdk-trace-web@^1.21.0                                                                   
npm install --save @opentelemetry/instrumentation@^0.48.0
npm install --save @opentelemetry/auto-instrumentations-web@^0.36.0
npm install --save @opentelemetry/exporter-trace-otlp-http@^0.48.0
npm install --save @opentelemetry/resources@^1.21.0
npm install --save @opentelemetry/propagator-b3@^1.21.0
npm install --save @opentelemetry/semantic-conventions@^1.21.0
</code></pre>
<p><strong>Step 2.</strong> Create <code>instrument.ts</code> file<br />
You need to configure the endpoint for SigNoz Self-Host in this file.</p>
<pre><code>import { registerInstrumentations } from '@opentelemetry/instrumentation';
import {
  WebTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
  BatchSpanProcessor,
} from '@opentelemetry/sdk-trace-web';
import { getWebAutoInstrumentations } from '@opentelemetry/auto-instrumentations-web';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { Resource } from '@opentelemetry/resources';
import { B3Propagator } from '@opentelemetry/propagator-b3';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';

const resource = Resource.default().merge(
  new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: '&lt;service_name&gt;',
    [SemanticResourceAttributes.SERVICE_VERSION]: '0.1.0',
  })
);

const provider = new WebTracerProvider({ resource });

provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()));

provider.addSpanProcessor(
  new BatchSpanProcessor(
    new OTLPTraceExporter({
      url: 'http://127.0.0.1:4318/v1/traces', //url of OpenTelemetry Collector',
    })
  )
);

provider.register({
  propagator: new B3Propagator(),
});

registerInstrumentations({
  instrumentations: [\
    getWebAutoInstrumentations({\
      '@opentelemetry/instrumentation-document-load': {},\
      '@opentelemetry/instrumentation-user-interaction': {},\
      '@opentelemetry/instrumentation-fetch': {\
        propagateTraceHeaderCorsUrls: /.+/,\
      },\
      '@opentelemetry/instrumentation-xml-http-request': {\
        propagateTraceHeaderCorsUrls: /.+/,\
      },\
    }),\
  ],
});
</code></pre>
<p>Again, <code>http://localhost:4318/v1/traces</code> is the default url for sending your tracing data. We are assuming you have installed SigNoz on your localhost. Based on your environment, you can update it accordingly. It should be in the following format:</p>
<p><code>http://&lt;IP of SigNoz backend&gt;:4318/v1/traces</code></p>
<p><strong>NOTE:</strong> Remember to allow incoming requests to port 4318 of machine where SigNoz backend is hosted.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#sample-angular-app
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-sample-angular-app
group_tracking_ids: docs-instrumentation-angular
<h2>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Sample Angular App</h2>
<ul>
<li><a href="https://github.com/SigNoz/angular-otel-native">Sample Angular App</a>
<ul>
<li>contains <code>instrument.ts</code> file with placeholder for URL and Ingestion Details (For SigNoz Cloud)</li>
</ul>
</li>
</ul>
<p><em>The below Frequently Asked Questions (FAQs) are specifically meant for users who are using SigNoz as Self-Hosted.</em></p>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, angular
image_urls: 
tracking_id: docs-instrumentation-angular-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-angular
<h2>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/angular/#conclusion
tag_set: instrumentation, angular
image_urls: https://signoz.io/img/blog/common/join_slack_cta.webp
tracking_id: docs-instrumentation-angular-conclusion
group_tracking_ids: docs-instrumentation-angular
<h2>Angular OpenTelemetry Instrumentation: Send traces to SigNoz Cloud: Conclusion</h2>
<p>OpenTelemetry is the future for setting up observability for cloud-native apps. It is backed by a huge community and covers a wide variety of technology and frameworks. Using OpenTelemetry, engineering teams can instrument polyglot and distributed applications and be assured about compatibility with a lot of technologies.</p>
<p>In this guide, you have learned how to instrument your Angular application with OpenTelemetry and send telemetry data to SigNoz. Here‚Äôs a quick recap of the steps covered:</p>
<ol>
<li>Installation of OpenTelemetry Packages: You installed the necessary OpenTelemetry packages to get started.</li>
<li>Configuration: You created the instrument.ts file to configure OpenTelemetry with the appropriate endpoints and settings for sending data to SigNoz.</li>
<li>Implementation: You integrated the instrumentation setup by importing the instrument.ts file into your application's main.ts.</li>
<li>Running the Application: Finally, you ran your Angular application to start collecting and sending telemetry data to SigNoz.</li>
</ol>
<p>By following these steps, you can monitor and trace your Angular application's performance, gaining valuable insights into its behavior and user interactions.</p>
<p>If you encounter any issues or have any questions, please feel free to reach out for support. Join our Slack community and get assistance from our team and other users in the #support channel.</p>
<p><a href="https://signoz.io/slack"><img src="https://signoz.io/img/blog/common/join_slack_cta.webp" alt="SigNoz Slack community" /></a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your FastAPI applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your FastAPI application.</p>
<p>Once the telemetry data is collected, you can configure an exporter to send the data to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#requirements
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-requirements
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Requirements</h2>
<ul>
<li>Python 3.8 or newer</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#send-traces-to-signoz-cloud
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetes</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-fastapi
<p>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud</p>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
<p><strong>Step 4.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
opentelemetry-instrument &lt;your_run_command&gt;
</code></pre>
<ul>
<li><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</li>
<li><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>python manage.py runserver --noreload</code></li>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the api token provided by SigNoz. You can find it in the email sent by SigNoz with your cloud account details.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, you can disable the auto reload with <code>--noreload</code>.</p>
<p>Step 5. Validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#send-traces-via-otel-collector-binary
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-fastapi
<p>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Python application.</p>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
<p><strong>Step 4.</strong> To run your application and send data to collector in same VM:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>python manage.py runserver --noreload</code></p>
<p><em><code>http://localhost:4317</code></em> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively.</p>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4317 with <code>&lt;IP Address of the VM&gt;:4317</code>.</p>
<p><strong>Step 5.</strong> You can validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, fastapi
image_urls: https://signoz.io/img/docs/otel_python_instrumentation.webp
tracking_id: docs-instrumentation-fastapi-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Send Traces to Self-Hosted SigNoz</h2>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your FastAPI application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/otel_python_instrumentation.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Python applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in FastAPI.</p>
<p>You can use OpenTelemetry to send your traces directly to SigNoz. OpenTelemetry provides a handy distro in Python that can help you get started with automatic instrumentation. We recommend using it to get started quickly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#steps-to-auto-instrument-fastapi-app-for-traces
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-steps-to-auto-instrument-fastapi-app-for-traces
group_tracking_ids: docs-instrumentation-fastapi
<p>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument FastAPI app for traces</p>
<ol>
<li>
<p><strong>Create a virtual environment</strong></p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
</li>
<li>
<p><strong>Install the OpenTelemetry dependencies</strong></p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
</li>
<li>
<p><strong>Add automatic instrumentation</strong><br />
The below command inspects the dependencies of your application and installs the instrumentation packages relevant for your FastAPI application.</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
</li>
<li>
<p><strong>Run your application</strong><br />
In the final run command, you can configure environment variables and flags. Flags for exporters:</p>
<p>For running your application, there are a few things that you need to keep in mind. Below are the notes:</p>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, if you use <code>--reload</code> or <code>reload=True</code>, it enables the reloader mode which breaks OpenTelemetry isntrumentation.</p>
<p>For running applications with application servers which are based on <a href="#running-applications-with-gunicorn-uwsgi">pre fork model</a>
, like Gunicorn, uWSGI you have to add a post_fork hook or a @postfork decorator in your configuration.</p>
<p>To start sending data to SigNoz, use the following run command:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>python manage.py runserver --noreload</code></p>
<p><code>IP of SigNoz backend</code> is the IP of the machine where you installed SigNoz. If you have installed SigNoz on <code>localhost</code>, the endpoint will be <code>http://localhost:4317</code> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively. Remember to allow incoming requests to port¬†<strong>4317</strong>/<strong>4318</strong>¬†of machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, fastapi
image_urls: https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp
tracking_id: docs-instrumentation-fastapi-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp" alt="Python Application in the list of services being monitored in SigNoz" /></p>
<p><em>Python Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#database-instrumentation
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-database-instrumentation
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Database Instrumentation</h2>
<p>Make sure that the DB client library you are using has the corresponding instrumentation library, and the version of the DB client library is supported by OpenTelemetry.</p>
<h3>## MongoDB Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MongoDB database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MongoDB, the instrumentation library is <code>opentelemetry-instrumentation-pymongo</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Redis Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your Redis database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Redis, the instrumentation library is <code>opentelemetry-instrumentation-redis</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## MySQL Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MySQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MySQL, we have two isntrumentation libraries:</p>
<ul>
<li>opentelemetry-instrumentation-mysql</li>
<li>opentelemetry-instrumentation-pymysql</li>
</ul>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Postgres Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your PostgreSQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Postgres, the instrumentation library is <code>opentelemetry-instrumentation-psycopg2</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<p>üìù Note</p>
<p><code>psycopg2-binary</code> is not supported by opentelemetry auto instrumentation libraries as it is not recommended for production use. Please use <code>psycopg2</code> to see DB calls also in your trace data in SigNoz</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#running-applications-with-gunicorn-uwsgi
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-running-applications-with-gunicorn-uwsgi
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Running applications with Gunicorn, uWSGI</h2>
<p>For application servers which are based on pre fork model like Gunicorn, uWSGI you have to add a <code>post_fork</code> hook or a <code>@postfork</code> decorator in your configuration.</p>
<p>Check this <a href="https://opentelemetry-python.readthedocs.io/en/latest/examples/fork-process-model/README.html">documentation</a> from OpenTelemetry on how to set it up.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-python/tree/main/docs/examples/fork-process-model">Here's</a> a working example where a gunicorn server is configured with <code>post_fork</code> hook.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#troubleshooting-your-installation
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Troubleshooting your installation</h2>
<h4>## Spans are not being reported</h4>
<p>If spans are not being reported to SigNoz, try enabling debug exporter which writes the JSON formatted trace data to the console by setting env var OTEL_TRACES_EXPORTER=console.</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=python_app OTEL_TRACES_EXPORTER=console opentelemetry-instrument &lt;your run command&gt;


{
    &quot;name&quot;: &quot;alice&quot;,
    &quot;context&quot;: {
        &quot;trace_id&quot;: &quot;0xedb7caf0c8b082a9578460a201759193&quot;,
        &quot;span_id&quot;: &quot;0x57cf7eee198e1fed&quot;,
        &quot;trace_state&quot;: &quot;[]&quot;
    },
    &quot;kind&quot;: &quot;SpanKind.INTERNAL&quot;,
    &quot;parent_id&quot;: null,
    &quot;start_time&quot;: &quot;2022-03-27T14:55:18.804758Z&quot;,
    &quot;end_time&quot;: &quot;2022-03-27T14:55:18.804805Z&quot;,
    &quot;status&quot;: {
        &quot;status_code&quot;: &quot;UNSET&quot;
    },
    &quot;attributes&quot;: {},
    &quot;events&quot;: [],
    &quot;links&quot;: [],
    &quot;resource&quot;: {
        &quot;telemetry.sdk.language&quot;: &quot;python&quot;,
        &quot;telemetry.sdk.name&quot;: &quot;opentelemetry&quot;,
        &quot;telemetry.sdk.version&quot;: &quot;1.10.0&quot;,
        &quot;service.name&quot;: &quot;my-service&quot;
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#sample-fastapi-application
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-sample-fastapi-application
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Sample FastAPI Application</h2>
<ul>
<li>
<p><a href="https://github.com/SigNoz/opentelemetry-python/tree/main/docs/examples/fork-process-model">Working example</a> where we have configured a gunicorn server with <code>post_fork</code> hook.</p>
</li>
<li>
<p>We have included a sample FastAPI application with README.md at <a href="https://github.com/SigNoz/sample-fastAPI-app">Sample FastAPI App Github Repo.</a></p>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/fastapi/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, fastapi
image_urls: 
tracking_id: docs-instrumentation-fastapi-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-fastapi
<h2>FastAPI OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/troubleshooting/signoz-cloud/traces-troubleshooting/
tag_set: troubleshooting, signoz-cloud, traces-troubleshooting
image_urls: 
tracking_id: docs-troubleshooting-signoz-cloud-traces-troubleshooting
group_tracking_ids: docs-troubleshooting-signoz-cloud-traces-troubleshooting
<h2>Traces SigNoz Cloud Troubleshooting</h2>
<p>These are instructions for traces troubleshooting for SigNoz Cloud.</p>
<p>We will be adding troubleshooting instructions here soon ... üôÇ</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/logs-explorer/#introduction
tag_set: product-features, logs-explorer
image_urls: 
tracking_id: docs-product-features-logs-explorer-introduction
group_tracking_ids: docs-product-features-logs-explorer
<h2>Logs Explorer in SigNoz: Logs Explorer in SigNoz - Introduction</h2>
<p>The Logs Explorer page in SigNoz enables developers to filter, examine, and analyze logs. There are two ways to filter the logs and each way has 3 different views available in Logs Explorer which include:</p>
<ul>
<li>
<p><a href="#list-view">List View</a></p>
</li>
<li>
<p><a href="#time-series-view">Time Series View</a></p>
</li>
<li>
<p><a href="#table-view">Table View</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/logs-explorer/#operations-on-data
tag_set: product-features, logs-explorer
image_urls: https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-search.webp, https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-query-builder.webp
tracking_id: docs-product-features-logs-explorer-operations-on-data
group_tracking_ids: docs-product-features-logs-explorer
<h2>Logs Explorer in SigNoz: Operations on Data</h2>
<p>The search feature in Logs Explorer allows you to filter your logs data using different attributes (like <code>serviceName</code>, <code>status</code> etc.) using operators (like <code>IN</code>, <code>=</code> etc.). You can filter using multiple attributes simultaneously.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-search.webp" alt="Search feature in Logs Explorer" /></p>
<p><em>Search feature in Logs Explorer</em></p>
<h3>## Query Builder</h3>
<p>Just like search, the Query Builder also allows you to filter your logs data based on different attributes using operators. The Query Builder provides you with additional powerful features like aggregation, grouping etc. on your data. To learn more about the Query Builder, refer to this <a href="https://signoz.io/docs/userguide/query-builder/">documentation</a>.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-query-builder.webp" alt="Query Builder feature in Logs Explorer" /></p>
<p><em>Query Builder feature in Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/logs-explorer/#views
tag_set: product-features, logs-explorer
image_urls: https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-list-view-download.webp, https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-timeseries-view.webp, https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-table-view.webp
tracking_id: docs-product-features-logs-explorer-views
group_tracking_ids: docs-product-features-logs-explorer
<h2>Logs Explorer in SigNoz: Views</h2>
<p>There are three views available in both Search and Query Builder.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-views.gif" alt="Different Views in Logs Explorer" /></p>
<p><em>Different Views in Logs Explorer</em></p>
<h3>## List View</h3>
<p>The list view in logs explorer has the following features:</p>
<p><strong>Download logs</strong></p>
<p>You can download your logs in two format:</p>
<ul>
<li>Excel (.xlsx)</li>
<li>CSV</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-list-view-download.webp" alt="Download Logs feature in List View of Logs Explorer" /></p>
<p><em>Download Logs feature in List View of Logs Explorer</em></p>
<hr />
<h4>## Settings</h4>
<p>The settings option has three sections:</p>
<p><strong>Format</strong></p>
<p>There are three formats available which represents the logs in different formats :</p>
<ol>
<li><strong>Raw</strong>: Shows the logs entries in the raw format as they appear in the logging system.</li>
<li><strong>Default</strong>: It provides a balanced view by formatting a raw log based on different attributes to make it more readable.</li>
<li><strong>Column</strong>: It organizes the logs data into different attribute Columns to provide a tabular view of logs.</li>
</ol>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-list-view-settings-format.gif" alt="Different logs format" /></p>
<p><em>Different logs format</em></p>
<p><strong>Max lines per row</strong></p>
<p>The maximum number of lines of text that each log entry is allowed to display.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-list-view-settings-max-number-rows.gif" alt="Maximum lines per log rows" /></p>
<p><em>Maximum lines per log rows</em></p>
<p><strong>Columns</strong></p>
<p>You can add new columns for the different attributes available in your logs. This only works for <strong>Default</strong> and <strong>Column</strong> format. By default the log is formatted into <code>Timestamp</code> and <code>Body</code> in the Default and Column views.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-list-view-settings-columns.gif" alt="Add or remove attributes columns" /></p>
<p><em>Add or remove attributes columns</em></p>
<hr />
<p><strong>Show in Context</strong></p>
<p>When you hover over a log in any format, at the right most side, you'll find an option to <code>Show in Context</code>.</p>
<p>This will by default show you 10 logs before and 10 logs after the log that you have selected which can be used in faster debugging.</p>
<p>You can load more than 10 logs logs and also filter the logs by a particular attribute.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-list-view-show-in-context.gif" alt="Show in context feature of List View in Logs Explorer" /></p>
<p><em>Show in context feature of List View in Logs Explorer</em></p>
<hr />
<p><strong>Copy Link</strong></p>
<p>You can copy the link to a particular log line which can be shared and accessed by anyone who has access to same SigNoz instance.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-list-view-copy-link.gif" alt="Copy Link feature of List View in Logs Explorer" /></p>
<p><em>Copy Link feature of List View in Logs Explorer</em></p>
<hr />
<h3>## Time Series View</h3>
<p>The Time Series View provides a graphical representation of logs data over time based on the filters or operations applied.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-timeseries-view.webp" alt="Timeseries View in the Logs Explorer" /></p>
<p><em>Timeseries View in Logs Explorer</em></p>
<h3>## Table View</h3>
<p>The Table View provides a tabular representation of different operations like the count logs, average of duration etc., based on the filters applied.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-table-view.webp" alt="Table View in the Logs Explorer" /></p>
<p><em>Table View in Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/logs-explorer/#log-details
tag_set: product-features, logs-explorer
image_urls: https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details.webp
tracking_id: docs-product-features-logs-explorer-log-details
group_tracking_ids: docs-product-features-logs-explorer
<h2>Logs Explorer in SigNoz: Log Details</h2>
<p>When you click on a log line in any format it will open up a Log Detail panel which gives a comprehensive view of your log. This is only available in the List View.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details.webp" alt="Log Details Panel in Logs Explorer" /></p>
<p><em>Log Details Panel in Logs Explorer</em></p>
<p>The panel provides the following sections:</p>
<h3>## Overview</h3>
<p>Overview has two sections:</p>
<p><strong>Body</strong></p>
<p>This shows the complete body of the log in <code>JSON</code> format by default. Once you toggle on <code>Wrap text</code>, the logs get converted to the Raw format.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details-overview-1.gif" alt="Log Body in Overview section of Log Details" /></p>
<p><em>Log Body in Overview section of Log Details</em></p>
<hr />
<p><strong>Attributes</strong></p>
<p>This the attributes of the logs in a tabular format showing the attribute name and its value.</p>
<p>The following features are available in this section:</p>
<p><strong>Pin an attribute</strong></p>
<p>You can pin an attribute using the pin icon shown on the left side of the attribute name. The pinned attribute will be shown at the top when you open a new log line.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details-overview-2.gif" alt="Pin an attribute" /></p>
<p><em>Pin an attribute</em></p>
<p><strong>Search Attribute</strong></p>
<p>You can search for an attribute available in your log using the search bar.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details-overview-3.gif" alt="Search an attribute" /></p>
<p><em>Pin an attribute</em></p>
<p><strong>Filter for Value</strong></p>
<p>Using this, you can instantly filter the logs based on a particular attribute and its value using the IN operator.</p>
<p>For example, if the attribute is <code>container_id</code> and it's value is <code>debian</code> then using Filter for Value will create a filter <code>container_id IN debian</code>. This will list the logs where the <code>container_id</code> attribute is <code>debian</code>.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details-overview-4.gif" alt="Filter for Value" /></p>
<p><em>Filter for Value</em></p>
<p><strong>Filter out Value</strong></p>
<p>Using this, you can instantly filter the logs based on a particular attribute and its value using the NOT_IN operator.</p>
<p>For example, if the attribute is <code>container_id</code> and it's value is <code>debian</code> then using Filter out Value will create a filter <code>container_id NOT_IN debian</code>. This will list the logs where the <code>container_id</code> attribute is not <code>debian</code>.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details-overview-5.gif" alt="Filter out Value" /></p>
<p><em>Filter out Value</em></p>
<hr />
<h3>## JSON</h3>
<p>This section displays the log in JSON format. You can:</p>
<p><strong>Wrap Text</strong></p>
<p>Switching the toggle on for wrap text will convert your log to raw format from JSON.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details-json-1.gif" alt="Wrap text" /></p>
<p><em>Wrap text</em></p>
<p><strong>Copy Log</strong></p>
<p>Copy your log in either JSON or Raw form.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details-json-2.gif" alt="Copy Log" /></p>
<p><em>Copy Log</em></p>
<h3>## Context</h3>
<p>This section will by default show you 10 logs before and 10 logs after the log that you have selected which can be used in faster debugging.</p>
<p>You can:</p>
<ul>
<li>Load more than 10 logs.</li>
<li>Filter the logs by a particular attribute.</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-log-details-context.gif" alt="Context in Log Details" /></p>
<p><em>Context in Log Details</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/logs-explorer/#live-view
tag_set: product-features, logs-explorer
image_urls: 
tracking_id: docs-product-features-logs-explorer-live-view
group_tracking_ids: docs-product-features-logs-explorer
<h2>Logs Explorer in SigNoz: Live View</h2>
<p>The Live View streams new log entries continuously as they occur. This can help in debugging the real-time incidents.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-live-view.gif" alt="Switch to Live view in Logs Explorer" /></p>
<p><em>Switch to Live view in Logs Explorer</em></p>
<p>These are the following features available in live view:</p>
<p><strong>Search Filter</strong></p>
<p>You can filter your logs based on different attributes. Just below the search filter section, you get a histogram which shows the count of logs at different times for the particular filter(s).</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-live-view-1.gif" alt="Search filter in Live view" /></p>
<p><em>Search filter in Live view</em></p>
<p><strong>Format</strong></p>
<p>Just like <a href="#list-view">List View</a>
, there are three formats in which you can view your logs:</p>
<ul>
<li>Raw</li>
<li>Table</li>
<li>List (Default)</li>
</ul>
<p>By hovering over the <code>Format</code> button in the Raw and Table View, you can change the <code>Max lines per Row</code> for your logs.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-live-view-2.gif" alt="Different formats in Live view" /></p>
<p><em>Different formats in Live view</em></p>
<p><strong>Pause and Resume</strong></p>
<p>The Live Logs can be paused to stop the real-time feed. This allows users to scroll through the logs without new entries being pushed.</p>
<p><img src="https://signoz.io/img/docs/product-features/logs-explorer/logs-explorer-live-view-3.gif" alt="Pause and Resume in Live view in Logs Explorer" /></p>
<p><em>Pause and Resume Live view in Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>Follow the steps on this page to install SigNoz on other Kubernetes Cloud Platform and bare-metal servers with Helm.</p>
<p>Follow the steps on this page to install SigNoz on Kubernetes with Helm.</p>
<p>The <a href="https://signoz.io/docs/install/kubernetes/">SigNoz Helm chart</a> will install the following components into your Kubernetes cluster:</p>
<ul>
<li>Query Service (backend service)</li>
<li>Web UI (frontend)</li>
<li>OpenTelemetry Collectors</li>
<li>Alertmanager</li>
<li>ClickHouse chart (datastore)</li>
<li>K8s-Infra chart (k8s infra metrics/logs collectors)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#prerequisites
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-prerequisites
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Prerequisites</h2>
<ul>
<li>
<p>You must have a Kubernetes cluster</p>
</li>
<li>
<p>Kubernetes version &gt;= <code>1.22</code></p>
</li>
<li>
<p><code>x86-64</code>/<code>amd64</code> workloads as currently <code>arm64</code> architecture is not supported</p>
</li>
<li>
<p>Helm version &gt;= <code>3.8</code></p>
</li>
<li>
<p>You must have <code>kubectl</code> access to your cluster</p>
</li>
<li>
<p>The following table describes the hardware requirements that are needed to install SigNoz on Kubernetes:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Minimal Requirements</th>
<th>Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory</td>
<td>8 GB</td>
<td>16 GB</td>
</tr>
<tr>
<td>CPU</td>
<td>4 cores</td>
<td>8 cores</td>
</tr>
<tr>
<td>Storage</td>
<td>30 GB</td>
<td>80 GB</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Suggestion: In case you want to use your own custom storage class for PVCs, you can set <code>global.storageClass</code> configuration to desired storage class.</p>
</li>
<li>
<p>Suggestion: In case you do not have any other storage class which supports volume expansion, you can patch default storage class definition by setting <code>allowVolumeExpansion</code> to <code>True</code> (this enables PVC resize).</p>
<pre><code>DEFAULT_STORAGE_CLASS=$(kubectl get storageclass -o=jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class==&quot;true&quot;)].metadata.name}')

kubectl patch storageclass &quot;$DEFAULT_STORAGE_CLASS&quot; -p '{&quot;allowVolumeExpansion&quot;: true}'
</code></pre>
<p>‚úÖ Info</p>
<p>To list storage class in your Kubernetes cluster: <code>kubectl get storageclass</code>.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#chart-configuration
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-chart-configuration
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Chart configuration</h2>
<p>You can find an overview of the parameters that can be configured during installation under <a href="https://github.com/SigNoz/charts/tree/main/charts/signoz#configuration">chart configuration</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#install-signoz-on-kubernetes-with-helm
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-install-signoz-on-kubernetes-with-helm
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Install SigNoz on Kubernetes with Helm</h2>
<ol>
<li>
<p>Add the SigNoz Helm repository to your client with name <code>signoz</code> by running the following command:</p>
<pre><code>helm repo add signoz https://charts.signoz.io
</code></pre>
</li>
<li>
<p>Verify that the repository is accessible to the Helm CLI by entering the following command:</p>
<pre><code>helm repo list
</code></pre>
</li>
<li>
<p>Use the <code>kubectl create ns</code> command to create a new namespace. SigNoz recommends you use <code>platform</code> for your new namespace:</p>
<pre><code>kubectl create ns platform
</code></pre>
</li>
<li>
<p>Run the following command to install the chart with the release name <code>my-release</code> and namespace <code>platform</code>:</p>
<p>helm --namespace platform install my-release signoz/signoz</p>
</li>
</ol>
<p>Output:</p>
<pre><code>NAME: my-release
LAST DEPLOYED: Mon May 23 20:34:55 2022
NAMESPACE: platform
STATUS: deployed
REVISION: 1
NOTES:
1. You have just deployed SigNoz cluster:

- frontend version: '0.8.0'
- query-service version: '0.8.0'
- alertmanager version: '0.23.0-0.1'
- otel-collector version: '0.43.0-0.1'
- otel-collector-metrics version: '0.43.0-0.1'
</code></pre>
<p>*<em>Note that the above command installs the latest stable version of SigNoz.</em></p>
<p>(Optional) To install a different version, you can use the <code>--set</code> flag to specify the version you wish to install. The following example command installs SigNoz version <code>0.8.0</code>:</p>
<pre><code>helm --namespace platform install my-release signoz/signoz \
  --set frontend.image.tag=&quot;0.8.0&quot; \
  --set queryService.image.tag=&quot;0.8.0&quot;
</code></pre>
<p>‚úÖ Info</p>
<ul>
<li>If you use the <code>--set</code> flag, ensure that you specify the same versions for the <code>frontend</code> and <code>queryService</code> images. Specifying different versions could lead the SigNoz cluster to behave abnormally.</li>
<li>Do not use the <code>latest</code> or <code>develop</code> tags in a production environment. Specifying these tags could install different versions of SigNoz on your cluster and could lead to data loss.</li>
</ul>
<ol start="5">
<li>
<p>You can access SigNoz by setting up port forwarding and browsing to the specified port. The following <code>kubectl port-forward</code> example command forwards all connections made to <code>localhost:3301</code> to <code>&lt;signoz-frontend-service&gt;:3301</code>:</p>
<p>export SERVICE_NAME=$(kubectl get svc --namespace platform -l &quot;app.kubernetes.io/component=frontend&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)</p>
<p>kubectl --namespace platform port-forward svc/$SERVICE_NAME 3301:3301</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#verify-the-installation
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-verify-the-installation
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Verify the Installation</h2>
<p>Using the <code>kubectl -n platform get pods</code> command, monitor the SigNoz deployment process. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n platform get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                                                        READY   STATUS    RESTARTS   AGE
chi-signoz-cluster-0-0-0                                    1/1     Running   0          8m21s
clickhouse-operator-8cff468-n5s99                           2/2     Running   0          8m55s
my-release-signoz-alertmanager-0                            1/1     Running   0          8m54s
my-release-signoz-frontend-78774f44d7-wl87p                 1/1     Running   0          8m55s
my-release-signoz-otel-collector-66c8c7dc9d-d8v5c           1/1     Running   0          8m55s
my-release-signoz-otel-collector-metrics-68bcfd5556-9tkgh   1/1     Running   0          8m55s
my-release-signoz-query-service-0                           1/1     Running   0          8m54s
my-release-zookeeper-0                                      1/1     Running   0          8m54s
</code></pre>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#optional-install-a-sample-application-and-generate-tracing-data
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-optional-install-a-sample-application-and-generate-tracing-data
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: (Optional) Install a Sample Application and Generate Tracing Data</h2>
<p>Follow the steps in this section to install a sample application named <a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a>
, and generate tracing data.</p>
<ol>
<li>
<p>Use the HotROD install script below to create a <code>sample-application</code> namespace and deploy HotROD application on it:</p>
<pre><code>curl -sL https://github.com/SigNoz/signoz/raw/develop/sample-apps/hotrod/hotrod-install.sh \
  | HELM_RELEASE=my-release SIGNOZ_NAMESPACE=platform bash
</code></pre>
</li>
<li>
<p>Using the <code>kubectl -n sample-application get pods</code> command, monitor the sample application pods. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n sample-application get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                            READY   STATUS    RESTARTS   AGE
hotrod-55bd58cc8d-mzxq8         1/1     Running   0          2m
locust-master-b65744bbf-l7v7n   1/1     Running   0          2m
locust-slave-688c86bcb7-ngx7w   1/1     Running   0          2m
</code></pre>
</li>
<li>
<p>Use the following command to generate load:</p>
<pre><code>kubectl --namespace sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl -X POST -F \
  'user_count=6' -F 'spawn_rate=2' http://locust-master:8089/swarm
</code></pre>
</li>
<li>
<p>Browse to <code>http://localhost:3301</code> and see the metrics and traces for your sample application.</p>
</li>
<li>
<p>Use the following command to stop load generation:</p>
<pre><code>kubectl -n sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl \
  http://locust-master:8089/stop
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/kubernetes">Kubernetes Operate</a> section for detailed instructions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#next-steps
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-next-steps
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Next Steps</h2>
<ul>
<li>
<p><a href="/docs/instrumentation/overview">Instrument Your Application</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">Use OpenTelemetry Operator for automatic instrumentation (if your applications are in k8s)</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-dashboards/
tag_set: userguide, manage-dashboards
image_urls: 
tracking_id: docs-userguide-manage-dashboards
group_tracking_ids: docs-userguide-manage-dashboards
<h2>Manage Dashboards in SigNoz</h2>
<p>This section shows how you can create, update, and remove dashboards.</p>
<p>üí° Tip</p>
<p>We have shared some commonly used dashboard JSON files <a href="https://github.com/SigNoz/dashboards">here</a>. You can import them directly to create dashboards in SigNoz.</p>
<h2>Steps to Create a Custom Dashboard</h2>
<hr />
<ol>
<li>
<p>From the sidebar, choose <strong>Dashboards</strong>.</p>
</li>
<li>
<p>Select the <strong>New Dashboard</strong> button.</p>
</li>
<li>
<p>Select the <strong>Edit</strong> button at the far right, and then enter the following information:</p>
<ol>
<li>A descriptive name for your new dashboard.</li>
<li><em>(Optional)</em> Add one ore more tags by selecting the <strong>New Tag</strong> button.</li>
<li><em>(Optional)</em> A brief description of your new dashboard.</li>
</ol>
</li>
<li>
<p>Select the <strong>Save</strong> button at the far right.</p>
</li>
<li>
<p>For each panel you wish to add to your new dashboard, follow the steps in the <a href="/docs/userguide/manage-panels/#add-a-panel-to-a-dashboard">Add a Panel to a Dashboard</a> section.</p>
</li>
<li>
<p><em>(Optional)</em> You can change the order of your panels by dragging and dropping them.</p>
</li>
<li>
<p>When you‚Äôve finished, select the <strong>Save Layout</strong> button.</p>
</li>
</ol>
<h2>Steps to Update a Custom Dashboard</h2>
<hr />
<p>To update the name, description and tags:</p>
<ol>
<li>Select the <strong>Edit</strong> button at the far right.</li>
<li>Make the changes.</li>
<li>Select the <strong>Save Layout</strong> button.</li>
</ol>
<p>To resize a panel:</p>
<ol>
<li>Click the bottom-left corner of the panel you want to resize.</li>
<li>Keep your left mouse button pressed and resize the panel.</li>
<li>When you've finished, select the <strong>Save Layout</strong> button.</li>
</ol>
<p>To change the position of a panel:</p>
<ol>
<li>Drag and drop a panel to the new position.</li>
<li>When you‚Äôve finished, select the <strong>Save Layout</strong> button.</li>
</ol>
<h2>Steps to Remove a Custom Dashboard</h2>
<hr />
<ol>
<li>From the sidebar, choose <strong>Dashboard</strong>.</li>
<li>Find the dashboard you wish to remove. In the <strong>Action</strong> column, select the <strong>Delete</strong> button.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-panels/
tag_set: userguide, manage-panels
image_urls: 
tracking_id: docs-userguide-manage-panels
group_tracking_ids: docs-userguide-manage-panels
<h2>Manage Panels</h2>
<p>This section shows how you can create, update, and remove a panel.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-panels/#prerequisites
tag_set: userguide, manage-panels
image_urls: 
tracking_id: docs-userguide-manage-panels-prerequisites
group_tracking_ids: docs-userguide-manage-panels
<h2>Manage Panels: Prerequisites</h2>
<ul>
<li>This section assumes that your application is already instrumented. For details about how you can instrument your application, see the <a href="/docs/instrumentation/">Instrument Your Application</a> section.</li>
<li>This section assumes that you are familiar with the basics of monitoring applications.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-panels/#steps-to-add-a-panel-to-a-dashboard
tag_set: userguide, manage-panels
image_urls: https://signoz.io/img/docs/orange-circle.webp
tracking_id: docs-userguide-manage-panels-steps-to-add-a-panel-to-a-dashboard
group_tracking_ids: docs-userguide-manage-panels
<h2>Manage Panels: Steps to Add a Panel to a Dashboard</h2>
<p>SigNoz supports two types of panels: time series, which displays a metric over a time interval, and value, which displays only the most recent value. To add a panel to a dashboard, follow the steps below:</p>
<ol>
<li>From the sidebar, choose <strong>Dashboards</strong>.</li>
<li>Find the dashboard to which you want to add a new panel.</li>
<li>Select <strong>Add Panel</strong>.</li>
<li>Select either <strong>Time Series</strong> or <strong>Value</strong>.</li>
<li>Populate the following fields:
<ol>
<li><strong>Panel Title</strong>: Enter a descriptive name for your panel.</li>
<li><strong>Description</strong>: Enter a brief and meaningful description of your new panel.</li>
<li><em>(Optional)</em> <strong>Panel Time Preference</strong>: You can use the the drop-down list to specify the time range for which you want to view data. The time range you specify here overrides the global value.</li>
<li><em>(Optional)</em> <strong>Y Axis Unit</strong>: Specify the unit of measurement for the y-axis.</li>
</ol>
</li>
<li>To specify the data displayed on your panel, you can use:
<ul>
<li>The query builder. The query builder provides an easy-to-use graphical interface that allows you create custom queries. For instructions, see the <a href="/docs/userguide/create-a-custom-query/">Create a Custom Query</a> page.</li>
<li>The declarative query language based on SQL that ClickHouse supports. For details, see the <a href="https://clickhouse.com/docs/en/sql-reference/">SQL Reference</a> page of the ClickHouse documentation.</li>
<li>PromQL. For details see the <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Prometheus Querying Language</a> page of the Prometheus documentation.</li>
</ul>
</li>
<li><em>(Optional)</em> If you‚Äôre using the query builder, you can also transform your data by adding mathematical functions. For example, you can divide the value that a query returns by a number. The following mathematical functions are supported: <em>exp</em>,¬†<em>log</em>,¬†<em>ln</em>,¬†<em>exp2</em>,¬†<em>log2</em>,¬†<em>exp10</em>,¬†<em>log10</em>,¬†<em>sqrt</em>,¬†<em>cbrt</em>,¬†<em>erf</em>,¬†<em>erfc</em>,¬†<em>lgamma</em>,¬†<em>tgamma</em>,¬†<em>sin</em>,¬†<em>cos</em>,¬†<em>tan</em>,¬†<em>asin</em>,¬†<em>acos</em>,¬†<em>atan</em>,¬†<em>degrees</em>,¬†<em>radians.</em></li>
<li><em>(Optional)</em> The result of the panel query returns a timestamp, float value, and optional set of attributes. The attributes from the response can be used in legend formatting.</li>
<li><em>(Optional)</em> You can plot up to ten queries on the same panel. To plot a new query, select the <strong>+ Query</strong> button.</li>
<li>When you‚Äôve finished, select the <strong>Save</strong> button.</li>
</ol>
<p>Note the following about panels:</p>
<ul>
<li>The total number of queries and functions you can plot on a single panel must be less or equal to ten.</li>
<li>Every time you add or modify a function or formula, you must select the <strong>Stage &amp; Run Query</strong> button. If you do not select this button, the panel will not be updated and your changes will be lost whenever you move to a different tab. When you have unsaved changes, the system will display an orange circle next to the name of the tab that you've modified:</li>
</ul>
<p><img src="https://signoz.io/img/docs/orange-circle.webp" alt="Orange Circle Showing Unsaved Changes" /></p>
<p><em>Orange Circle Showing Unsaved Changes.</em></p>
<ul>
<li>You can hide or unhide a function or formula by selecting the eye icon at its left and then selecting the <strong>Stage &amp; Run Query</strong> button.</li>
<li>When you install SigNoz, only the data provided by the Hostmetric receiver is available. To enable more metric receivers, see the <a href="/docs/userguide/send-metrics/">Send Metrics to SigNoz</a> section.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-panels/#steps-to-update-a-panel
tag_set: userguide, manage-panels
image_urls: 
tracking_id: docs-userguide-manage-panels-steps-to-update-a-panel
group_tracking_ids: docs-userguide-manage-panels
<h2>Manage Panels: Steps to Add a Panel to a Dashboard: Steps to Update a Panel</h2>
<ol>
<li>From the sidebar, choose <strong>Dashboard</strong>.</li>
<li>Find the dashboard in which you created the panel you wish to update, and then select the pencil icon located at the top right corner of your panel.</li>
<li>Make the changes.</li>
<li>When you‚Äôve finished, select the <strong>Save</strong> button.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#overview
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-overview
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: PostgreSQL Metrics and Logs - Overview</h2>
<p>This integration helps you to monitor key Postgres metrics and logs, view them with an out-of-the-box dashboard, and parse Postgres logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#prerequisites
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-prerequisites
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>
<p>A Postgres server running version 9.6 or newer</p>
<ul>
<li>You can check the server version with the SQL statement: <code>SELECT version();</code></li>
</ul>
</li>
<li>
<p>A Postgres user with required permissions for metrics collection</p>
<ul>
<li>
<p>To create a monitoring user for Postgres versions 10+, run:</p>
<pre><code>CREATE USER monitoring WITH PASSWORD '&lt;PASSWORD&gt;';
GRANT pg_monitor TO monitoring;
GRANT SELECT ON pg_stat_database TO monitoring;
</code></pre>
</li>
<li>
<p>To create a monitoring user for Postgres versions &gt;= 9.6 and &lt; 10, run:</p>
<pre><code>CREATE USER monitoring WITH PASSWORD '&lt;PASSWORD&gt;';
GRANT SELECT ON pg_stat_database TO monitoring;
</code></pre>
</li>
</ul>
</li>
<li>
<p>An OpenTelemetry (OTEL) Collector with access to the Postgres server</p>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0 or newer) if not done already</li>
<li>Ensure you can provide config files to the collector and set environment variables</li>
<li>The collector must be able to access the Postgres server as a client using the monitoring user</li>
<li>For log collection, the collector must be able to read the Postgres server log file</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#collecting-postgres-metrics
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-collecting-postgres-metrics
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Collecting Postgres Metrics</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>postgres-metrics-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  postgresql:
    # The endpoint of the postgresql server. Whether using TCP or Unix sockets, this value should be host:port. If transport is set to unix, the endpoint will internally be translated from host:port to /host.s.PGSQL.port
    endpoint: ${env:POSTGRESQL_ENDPOINT}
    # The frequency at which to collect metrics from the Postgres instance.
    collection_interval: 60s
    # The username used to access the postgres instance
    username: ${env:POSTGRESQL_USERNAME}
    # The password used to access the postgres instance
    password: ${env:POSTGRESQL_PASSWORD}
    # The list of databases for which the receiver will attempt to collect statistics. If an empty list is provided, the receiver will attempt to collect statistics for all non-template databases
    databases: []
    # # Defines the network to use for connecting to the server. Valid Values are `tcp` or `unix`
    # transport: tcp
    tls:
      # set to false if SSL is enabled on the server
      insecure: true
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/postgres.crt
    #   key_file: /etc/ssl/certs/postgres.key
    metrics:
      postgresql.database.locks:
        enabled: true
      postgresql.deadlocks:
        enabled: true
      postgresql.sequential_scans:
        enabled: true

processors:
  # enriches the data with additional host information
  # see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#resource-detection-processor
  resourcedetection/system:
    # add additional detectors if needed
    detectors: [&quot;system&quot;]
    system:
      hostname_sources: [&quot;os&quot;]

exporters:
  # export to SigNoz cloud
  otlp/postgres:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/postgres:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    metrics/postgresql:
      receivers: [postgresql]
      # note: remove this processor if the collector host is not running on the same host as the postgres instance
      processors: [resourcedetection/system]
      exporters: [otlp/postgres]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># password for Postgres monitoring user&quot;
export POSTGRESQL_USERNAME=&quot;monitoring&quot;

# password for Postgres monitoring user&quot;
export POSTGRESQL_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;

# Postgres endpoint reachable from the otel collector&quot;
export POSTGRESQL_ENDPOINT=&quot;host:port&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config postgres-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#collecting-postgres-logs
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-collecting-postgres-logs
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Collecting Postgres Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>postgres-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/postgresql:
    include: [&quot;${env:POSTGRESQL_LOG_FILE}&quot;]
    operators:
      # Parse default postgresql text log format.
      # `log_line_prefix` postgres setting defaults to '%m [%p] ' which logs the timestamp and the process ID
      # See https://www.postgresql.org/docs/current/runtime-config-logging.html#GUC-LOG-LINE-PREFIX for more details
      - type: regex_parser
        if: body matches '^(?P&lt;ts&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.?[0-9]*? [A-Z]*) \\[(?P&lt;pid&gt;[0-9]+)\\] (?P&lt;log_level&gt;[A-Z]*). (?P&lt;message&gt;.*)$'
        parse_from: body
        regex: '^(?P&lt;ts&gt;\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.?[0-9]*? [A-Z]*) \[(?P&lt;pid&gt;[0-9]+)\] (?P&lt;log_level&gt;[A-Z]*). (?P&lt;message&gt;.*)$'
        timestamp:
          parse_from: attributes.ts
          layout: '%Y-%m-%d %H:%M:%S %Z'
        severity:
          parse_from: attributes.log_level
          mapping:
            debug:
              - DEBUG1
              - DEBUG2
              - DEBUG3
              - DEBUG4
              - DEBUG5
            info:
              - INFO
              - LOG
              - NOTICE
              - DETAIL
            warn: WARNING
            error: ERROR
            fatal:
              - FATAL
              - PANIC
        on_error: send
      - type: move
        if: attributes.message  nil
        from: attributes.message
        to: body
      - type: remove
        if: attributes.log_level  nil
        field: attributes.log_level
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: add
        field: attributes.source
        value: postgres

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/postgres-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/postgres-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    logs/postgresql:
      receivers: [filelog/postgresql]
      processors: [batch]
      exporters: [otlp/postgresql-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># path of Postgres server log file. must be accessible by the otel collector
# typically found in /usr/local/var/log/postgresql on macOS
# running `SELECT pg_current_logfile();` can also give you the location of postgresql log file
export POSTGRESQL_LOG_FILE=/var/log/postgresql/postgresql.log

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config postgres-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags in the collector run command.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#connect-postgressql
tag_set: integrations, postgresql
image_urls: https://signoz.io/img/docs/integrations/postgres/postgres-integration-search.webp, https://signoz.io/img/docs/integrations/postgres/postgres-integration-connect.webp, https://signoz.io/img/docs/integrations/postgres/postgres-integration-listening.webp
tracking_id: docs-integrations-postgresql-connect-postgressql
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Connect PostgresSQL</h2>
<p>Once you're done with setting up PostgresSQL for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the PostgresSQL integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-search.webp" alt="Search for PostgresSQL in Integrations tab" /></p>
<p><em>Search for PostgresSQL in Integrations tab</em></p>
<p>Click on the <code>Connect PostgresSQL</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your PostgresSQL instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-connect.webp" alt="Connect PostgresSQL" /></p>
<p><em>Connect PostgresSQL</em></p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-listening.webp" alt="Listening for data from PostgresSQL" /></p>
<p><em>Listening for data from PostgresSQL</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#postgressql-dashboard
tag_set: integrations, postgresql
image_urls: https://signoz.io/img/docs/integrations/postgres/postgres-integration-dashboard.webp
tracking_id: docs-integrations-postgresql-postgressql-dashboard
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: PostgresSQL dashboard</h2>
<p>Once SigNoz has started listening to your PostgresSQL data, head over to the Dashboards tab and search for Postgres, this will show you a newly created dashboard which shows differnet PostgresSQL metrics.</p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-dashboard.webp" alt="Dashboard for monitoring PostgresSQL Metrics" /></p>
<p><em>Dashboard for monitoring PostgresSQL Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Redis Dashboard by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/postgres/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#data-collected
tag_set: integrations, postgresql
image_urls: https://signoz.io/img/docs/integrations/postgres/postgres-integration-data-collected.webp
tracking_id: docs-integrations-postgresql-data-collected
group_tracking_ids: docs-integrations-postgresql
<h2>PostgreSQL Metrics and Logs: Data Collected: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your PostgresSQL Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your PostgresSQL instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/img/docs/integrations/postgres/postgres-integration-data-collected.webp" alt="Log attributes and metrics details for PostgresSQL" /></p>
<p><em>Log attributes and metrics details for PostgresSQL</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#postgrsql-log-attributes
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-postgrsql-log-attributes
group_tracking_ids: docs-integrations-postgresql
<p>PostgreSQL Metrics and Logs: Data Collected: PostgrSQL log attributes</p>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Process ID</td>
<td>attributes.pid</td>
<td>string</td>
</tr>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/postgresql/#postgresql-metrics
tag_set: integrations, postgresql
image_urls: 
tracking_id: docs-integrations-postgresql-postgresql-metrics
group_tracking_ids: docs-integrations-postgresql
<p>PostgreSQL Metrics and Logs: Data Collected: PostgreSQL metrics</p>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Unit</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>postgresql_backends</td>
<td>sum</td>
<td>number</td>
<td>The number of backends.</td>
</tr>
<tr>
<td>postgresql_bgwriter_buffers_allocated</td>
<td>sum</td>
<td>number</td>
<td>Number of buffers allocated.</td>
</tr>
<tr>
<td>postgresql_bgwriter_buffers_writes</td>
<td>sum</td>
<td>number</td>
<td>Number of buffers written.</td>
</tr>
<tr>
<td>postgresql_bgwriter_checkpoint_count</td>
<td>sum</td>
<td>number</td>
<td>The number of checkpoints performed.</td>
</tr>
<tr>
<td>postgresql_bgwriter_duration</td>
<td>sum</td>
<td>milliseconds</td>
<td>Total time spent writing and syncing files to disk by checkpoints.</td>
</tr>
<tr>
<td>postgresql_bgwriter_maxwritten</td>
<td>sum</td>
<td>number</td>
<td>Number of times the background writer stopped a cleaning scan because it had written too many buffers.</td>
</tr>
<tr>
<td>postgresql_blocks_read</td>
<td>sum</td>
<td>number</td>
<td>The number of blocks read.</td>
</tr>
<tr>
<td>postgresql_commits</td>
<td>sum</td>
<td>number</td>
<td>The number of commits.</td>
</tr>
<tr>
<td>postgresql_connection_max</td>
<td>gauge</td>
<td>number</td>
<td>Configured maximum number of client connections allowed</td>
</tr>
<tr>
<td>postgresql_database_count</td>
<td>sum</td>
<td>number</td>
<td>Number of user databases.</td>
</tr>
<tr>
<td>postgresql_database_locks</td>
<td>gauge</td>
<td>number</td>
<td>The number of database locks.</td>
</tr>
<tr>
<td>postgresql_db_size</td>
<td>sum</td>
<td>Bytes</td>
<td>The database disk usage.</td>
</tr>
<tr>
<td>postgresql_deadlocks</td>
<td>sum</td>
<td>number</td>
<td>The number of deadlocks.</td>
</tr>
<tr>
<td>postgresql_index_scans</td>
<td>sum</td>
<td>number</td>
<td>The number of index scans on a table.</td>
</tr>
<tr>
<td>postgresql_index_size</td>
<td>gauge</td>
<td>Bytes</td>
<td>The size of the index on disk.</td>
</tr>
<tr>
<td>postgresql_operations</td>
<td>sum</td>
<td>number</td>
<td>The number of db row operations.</td>
</tr>
<tr>
<td>postgresql_replication_data_delay</td>
<td>gauge</td>
<td>Bytes</td>
<td>The amount of data delayed in replication.</td>
</tr>
<tr>
<td>postgresql_rollbacks</td>
<td>sum</td>
<td>number</td>
<td>The number of rollbacks.</td>
</tr>
<tr>
<td>postgresql_rows</td>
<td>sum</td>
<td>number</td>
<td>The number of rows in the database.</td>
</tr>
<tr>
<td>postgresql_sequential_scans</td>
<td>sum</td>
<td>number</td>
<td>The number of sequential scans.</td>
</tr>
<tr>
<td>postgresql_table_count</td>
<td>sum</td>
<td>number</td>
<td>Number of user tables in a database.</td>
</tr>
<tr>
<td>postgresql_table_size</td>
<td>sum</td>
<td>Bytes</td>
<td>Disk space used by a table.</td>
</tr>
<tr>
<td>postgresql_table_vacuum_count</td>
<td>sum</td>
<td>number</td>
<td>Number of times a table has manually been vacuumed.</td>
</tr>
<tr>
<td>postgresql_temp_files</td>
<td>sum</td>
<td>number</td>
<td>The number of temp files.</td>
</tr>
<tr>
<td>postgresql_wal_age</td>
<td>gauge</td>
<td>seconds</td>
<td>Age of the oldest WAL file.</td>
</tr>
<tr>
<td>postgresql_wal_delay</td>
<td>gauge</td>
<td>seconds</td>
<td>Time between flushing recent WAL locally and receiving notification that the standby server has completed an operation with it.</td>
</tr>
<tr>
<td>postgresql_wal_lag</td>
<td>gauge</td>
<td>seconds</td>
<td>Time between flushing recent WAL locally and receiving notification that the standby server has completed an operation with it.</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/
tag_set: dashboards, panel-types
image_urls: 
tracking_id: docs-dashboards-panel-types
group_tracking_ids: docs-dashboards-panel-types
<h2>Panel types in Dashboards</h2>
<p>The panel types currently available when creating a Dashboard in SigNoz.</p>
<ul>
<li><a href="/docs/dashboards/panel-types/bar/">üìÑÔ∏è Bar Chart: Learn how to use Bar Chart panel type</a></li>
<li><a href="/docs/dashboards/panel-types/histogram/">üìÑÔ∏è Histogram: Learn how to use Histogram panel type</a></li>
<li><a href="/docs/dashboards/panel-types/list/">üìÑÔ∏è List Chart: Learn how to use List Chart panel type</a></li>
<li><a href="/docs/dashboards/panel-types/pie/">üìÑÔ∏è Pie Chart: Learn how to use Pie Chart panel type</a></li>
<li><a href="/docs/dashboards/panel-types/table/">üìÑÔ∏è Table: Learn how to use Table panel type</a></li>
<li><a href="/docs/dashboards/panel-types/timeseries/">üìÑÔ∏è Timeseries: Learn how to use Timeseries panel type</a></li>
<li><a href="/docs/dashboards/panel-types/value/">üìÑÔ∏è Value: Learn how to use Value panel type</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#overview
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-overview
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: MongoDB Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key MongoDB metrics and logs, view them with an out-of-the-box dashboard, and parse MongoDB logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#prerequisites
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-prerequisites
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>A MongoDB server running version 4.4 or newer</li>
<li>An OpenTelemetry (OTEL) Collector with access to the MongoDB server
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0 or newer) if not done already</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
<li>Ensure the OTEL collector has access to the MongoDB server for metrics collection and can read the MongoDB log file for logs collection</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#collecting-mongodb-metrics
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-collecting-mongodb-metrics
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Collecting MongoDB Metrics</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>mongodb-metrics-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  mongodb:
    # - For standalone MongoDB deployments this is the hostname and port of the mongod instance
    # - For replica sets specify the hostnames and ports of the mongod instances that are in the replica set configuration. If the replica_set field is specified, nodes will be autodiscovered.
    # - For a sharded MongoDB deployment, please specify a list of the mongos hosts.
    hosts:
      - endpoint: ${env:MONGODB_ENDPOINT}
    # If authentication is required, the user can with clusterMonitor permissions can be provided here
    username: ${env:MONGODB_USERNAME}
    # If authentication is required, the password can be provided here.
    password: ${env:MONGODB_PASSWORD}
    collection_interval: 60s
    # If TLS is enabled, the following fields can be used to configure the connection
    tls:
      insecure: true
      insecure_skip_verify: true
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/mongodb.crt
    #   key_file: /etc/ssl/certs/mongodb.key
    metrics:
      mongodb.lock.acquire.count:
        enabled: true
      mongodb.lock.acquire.time:
        enabled: true
      mongodb.lock.acquire.wait_count:
        enabled: true
      mongodb.lock.deadlock.count:
        enabled: true
      mongodb.operation.latency.time:
        enabled: true

processors:
  # enriches the data with additional host information
  # see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#resource-detection-processor
  resourcedetection/system:
    # add additional detectors if needed
    detectors: [&quot;system&quot;]
    system:
      hostname_sources: [&quot;os&quot;]

exporters:
  # export to SigNoz cloud
  otlp/mongodb:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/mongodb:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    metrics/mongodb:
      receivers: [mongodb]
      # note: remove this processor if the collector host is not running on the same host as the mongo instance
      processors: [resourcedetection/system]
      exporters: [otlp/mongodb]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># MongoDB endpoint reachable from the otel collector
export MONGODB_ENDPOINT=&quot;host:port&quot;

# MongoDB monitoring user credentials
export MONGODB_USERNAME=&quot;monitoring&quot;
export MONGODB_PASSWORD=&quot;your_secure_password&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.us.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config mongodb-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#collecting-mongodb-logs
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-collecting-mongodb-logs
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Collecting MongoDB Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>mongodb-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/mongodb:
    include: [&quot;${env:MONGODB_LOG_FILE}&quot;]
    operators:
      # Parse structured mongodb logs
      # For more details, see https://www.mongodb.com/docs/manual/reference/log-messages/#structured-logging
      - type: json_parser
        if: body matches '^\\s*{\\s*&quot;.*}\\s*$'
        parse_from: body
        parse_to: attributes
        timestamp:
          parse_from: attributes.t.$$date
          layout: '2006-01-02T15:04:05.000-07:00'
          layout_type: gotime
        severity:
          parse_from: attributes.s
          overwrite_text: true
          mapping:
            debug:
              - D1
              - D2
              - D3
              - D4
              - D5
            info: I
            warn: W
            error: E
            fatal: F
      - type: flatten
        if: attributes.attr  nil
        field: attributes.attr
      - type: move
        if: attributes.msg  nil
        from: attributes.msg
        to: body
      - type: move
        if: attributes.c  nil
        from: attributes.c
        to: attributes.component
      - type: move
        if: attributes.id  nil
        from: attributes.id
        to: attributes.mongo_log_id
      - type: remove
        if: attributes.t  nil
        field: attributes.t
      - type: remove
        if: attributes.s  nil
        field: attributes.s
      - type: add
        field: attributes.source
        value: mongodb

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/mongodb-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/mongodb-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true


service:
  pipelines:
    logs/mongodb:
      receivers: [filelog/mongodb]
      processors: [batch]
      exporters: [otlp/mongodb-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># path of MongoDB server log file. must be accessible by the otel collector
export MONGODB_LOG_FILE=&quot;/var/log/mongodb/mongodb.log&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.us.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config mongodb-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple --config flags in the collector run command.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#connect-mongodb
tag_set: integrations, mongodb
image_urls: https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-search.webp, https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-connect.webp, https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-listening.webp
tracking_id: docs-integrations-mongodb-connect-mongodb
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Connect MongoDB</h2>
<p>Once you're done with setting up MongoDB for collecting metrics and logs, head over to the integrations tab in SigNoz and search for the MongoDB integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-search.webp" alt="Search for MongoDB in Integrations tab" /></p>
<p><em>Search for MongoDB in Integrations tab</em></p>
<p>Click on the <code>Connect Mongo</code> Button, and select I have already configured, this will start listening for data from your MongoDB instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-connect.webp" alt="Connect MongoDB" /></p>
<p><em>Connect MongoDB</em></p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-listening.webp" alt="Listening for data from MongoDB" /></p>
<p><em>Listening for data from MongoDB</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#mongodb-dashboard
tag_set: integrations, mongodb
image_urls: https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-dashboard.webp
tracking_id: docs-integrations-mongodb-mongodb-dashboard
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: MongoDB Dashboard</h2>
<p>Once SigNoz has started listening to your MongoDB data, head over to the Dashboards tab and search for Mongo. This will show you a newly created dashboard which displays various MongoDB metrics.</p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-dashboard.webp" alt="Dashboard for monitoring MongoDB Metrics" /></p>
<p><em>Dashboard for monitoring MongoDB Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Redis Dashboard by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/mongo/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#data-collected
tag_set: integrations, mongodb
image_urls: https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-dashboard.webp
tracking_id: docs-integrations-mongodb-data-collected
group_tracking_ids: docs-integrations-mongodb
<h2>MongoDB Metrics and Logs: Data Collected: Data Collected</h2>
<p>When you switch to the Data Collected tab of your MongoDB Integration, it shows you details about the different logs attributes and the metrics types that you can monitor for your MongoDB instance. The tables below give you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/img/docs/integrations/mongodb/mongodb-integration-dashboard.webp" alt="Dashboard for monitoring MongoDB Metrics" /></p>
<p><em>Dashboard for monitoring MongoDB Metrics</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#mongodb-log-attributes
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-mongodb-log-attributes
group_tracking_ids: docs-integrations-mongodb
<p>MongoDB Metrics and Logs: Data Collected: MongoDB log attributes</p>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
<tr>
<td>MongoDB Component</td>
<td>attributes.component</td>
<td>string</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/mongodb/#mongodb-metrics
tag_set: integrations, mongodb
image_urls: 
tracking_id: docs-integrations-mongodb-mongodb-metrics
group_tracking_ids: docs-integrations-mongodb
<p>MongoDB Metrics and Logs: Data Collected: MongoDB metrics: MongoDB metrics: MongoDB metrics</p>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Unit</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>mongodb_cache_operations</td>
<td>Sum</td>
<td>number</td>
<td>The number of cache operations of the instance.</td>
</tr>
<tr>
<td>mongodb_collection_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of collections.</td>
</tr>
<tr>
<td>mongodb_data_size</td>
<td>Sum</td>
<td>Bytes</td>
<td>The size of the collection. Data compression does not affect this value.</td>
</tr>
<tr>
<td>mongodb_connection_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of connections.</td>
</tr>
<tr>
<td>mongodb_extent_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of extents.</td>
</tr>
<tr>
<td>mongodb_global_lock_time</td>
<td>Sum</td>
<td>milliseconds</td>
<td>The time the global lock has been held.</td>
</tr>
<tr>
<td>mongodb_index_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of indexes.</td>
</tr>
<tr>
<td>mongodb_index_size</td>
<td>Sum</td>
<td>Bytes</td>
<td>Sum of the space allocated to all indexes in the database, including free index space.</td>
</tr>
<tr>
<td>mongodb_memory_usage</td>
<td>Sum</td>
<td>Bytes</td>
<td>The amount of memory used.</td>
</tr>
<tr>
<td>mongodb_object_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of objects.</td>
</tr>
<tr>
<td>mongodb_operation_latency_time</td>
<td>Gauge</td>
<td>microseconds</td>
<td>The latency of operations.</td>
</tr>
<tr>
<td>mongodb_operation_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of operations executed.</td>
</tr>
<tr>
<td>mongodb_operation_repl_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of replicated operations executed.</td>
</tr>
<tr>
<td>mongodb_storage_size</td>
<td>Sum</td>
<td>Bytes</td>
<td>The total amount of storage allocated to this collection.</td>
</tr>
<tr>
<td>mongodb_database_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of existing databases.</td>
</tr>
<tr>
<td>mongodb_index_access_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of times an index has been accessed.</td>
</tr>
<tr>
<td>mongodb_document_operation_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of document operations executed.</td>
</tr>
<tr>
<td>mongodb_network_io_receive</td>
<td>Sum</td>
<td>Bytes</td>
<td>The number of bytes received.</td>
</tr>
<tr>
<td>mongodb_network_io_transmit</td>
<td>Sum</td>
<td>Bytes</td>
<td>The number of bytes transmitted.</td>
</tr>
<tr>
<td>mongodb_network_request_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of requests received by the server.</td>
</tr>
<tr>
<td>mongodb_operation_time</td>
<td>Sum</td>
<td>milliseconds</td>
<td>The total time spent performing operations.</td>
</tr>
<tr>
<td>mongodb_session_count</td>
<td>Sum</td>
<td>number</td>
<td>The total number of active sessions.</td>
</tr>
<tr>
<td>mongodb_cursor_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of open cursors maintained for clients.</td>
</tr>
<tr>
<td>mongodb_cursor_timeout_count</td>
<td>Sum</td>
<td>number</td>
<td>The number of cursors that have timed out.</td>
</tr>
<tr>
<td>mongodb_lock_acquire_count</td>
<td>Sum</td>
<td>number</td>
<td>Number of times the lock was acquired in the specified mode.</td>
</tr>
<tr>
<td>mongodb_lock_acquire_wait_count</td>
<td>Sum</td>
<td>number</td>
<td>Number of times the lock acquisitions encountered waits because the locks were held in a conflicting mode.</td>
</tr>
<tr>
<td>mongodb_lock_acquire_time</td>
<td>Sum</td>
<td>microseconds</td>
<td>Cumulative wait time for the lock acquisitions.</td>
</tr>
<tr>
<td>mongodb_lock_deadlock_count</td>
<td>Sum</td>
<td>number</td>
<td>Number of times the lock acquisitions encountered deadlocks.</td>
</tr>
<tr>
<td>mongodb_health</td>
<td>Gauge</td>
<td>number</td>
<td>The health status of the server.</td>
</tr>
<tr>
<td>mongodb_uptime</td>
<td>Sum</td>
<td>milliseconds</td>
<td>The amount of time that the server has been running.</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#overview
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-overview
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: AWS RDS (MySQL) Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key AWS RDS (MySQL) metrics and logs, view them with an out-of-the-box dashboards, and parse MySQL logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#prerequisites
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-prerequisites
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>
<p>AWS Credentials and Permissions:</p>
<ul>
<li>Set up proper AWS credentials (e.g., <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables)</li>
<li>Required IAM permissions:
<ul>
<li><code>cloudwatch:ListMetrics</code></li>
<li><code>cloudwatch:GetMetricStatistics</code></li>
<li><code>cloudwatch:GetMetricData</code></li>
<li><code>tag:GetResources</code> (if aws_tag_select feature is used)</li>
<li><code>logs:DescribeLogGroups</code></li>
<li><code>logs:FilterLogEvents</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Java Runtime Environment (JRE) version 11 or newer for the CloudWatch Exporter (Not required if using the Docker container)</p>
</li>
<li>
<p>An OpenTelemetry (OTEL) Collector (v0.88.0+):</p>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0+) if not already done</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
</ul>
</li>
<li>
<p>OTEL collector access to the MySQL server (optional, for MySQL engine metrics)</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#step-1-set-up-the-prometheus-cloudwatch-exporter
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-step-1-set-up-the-prometheus-cloudwatch-exporter
group_tracking_ids: docs-integrations-aws-rds-mysql
<p>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: ## Collecting RDS MySQL Metrics - Step 1: Set up the Prometheus CloudWatch Exporter</p>
<ol>
<li>
<p>Download the exporter:</p>
<pre><code>curl -sLSO https://github.com/prometheus/cloudwatch_exporter/releases/download/v0.15.5/cloudwatch_exporter-0.15.5-jar-with-dependencies.jar
</code></pre>
</li>
<li>
<p>Configure the Prometheus exporter Save the following config for collecting AWS RDS metrics in a file named <code>aws-rds-mysql-metrics.yaml</code> and update the region key with relevant value.</p>
<hr />
<p>region: us-east-1
metrics:</p>
<ul>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: BinLogDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: BurstBalance
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: CheckpointLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ConnectionAttempts
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: CPUUtilization
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DatabaseConnections
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DiskQueueDepth
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DiskQueueDepthLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: EBSByteBalance%
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: EBSIOBalance%
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeableMemory
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeStorageSpace
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeStorageSpaceLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: MaximumUsedTransactionIDs
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: NetworkReceiveThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: NetworkTransmitThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: OldestReplicationSlotLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPS
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPSLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPSLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatency
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatencyLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatencyLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadThroughputLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicaLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicationChannelLag
aws_dimensions: [DBInstanceIdentifier]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicationSlotDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: TransactionLogsDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: TransactionLogsGeneration
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteIOPS
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteLatency
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: SwapUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoad
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoadCPU
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoadNonCPU
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
</ul>
</li>
<li>
<p>Run the following command:</p>
<p>java -jar cloudwatch_exporter-0.15.5-jar-with-dependencies.jar 9106 aws-rds-mysql-metrics.yaml</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#step-2-create-the-otel-collector-config-file
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-step-2-create-the-otel-collector-config-file
group_tracking_ids: docs-integrations-aws-rds-mysql
<p>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Step 2: Create the OTEL Collector Config File</p>
<p>Create <code>mysql-metrics-collection-config.yaml</code>:</p>
<pre><code>receivers:
  mysql:
    # The hostname and port of the MySQL instance, separated by a colon.
    endpoint: ${env:MYSQL_ENDPOINT}
    # The username used to access the MySQL instance.
    username: ${env:MYSQL_USERNAME}
    # The password used to access the MySQL instance.
    password: ${env:MYSQL_PASSWORD}
    # The frequency at which to collect metrics from the Redis instance.
    collection_interval: 60s
    # Additional configuration for query to build mysql.statement_events.count and mysql.statement_events.wait.time metrics
    statement_events:
      digest_text_limit: 120
      time_limit: 24h
      limit: 250
    # tls:
    #   insecure: false
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/redis.crt
    #   key_file: /etc/ssl/certs/redis.key
    metrics:
      mysql.client.network.io:
        enabled: true
      mysql.commands:
        enabled: true
      mysql.connection.count:
        enabled: true
      mysql.connection.errors:
        enabled: true
      mysql.joins:
        enabled: true
      mysql.query.count:
        enabled: true
      mysql.query.slow.count:
        enabled: true
      mysql.replica.sql_delay:
        enabled: true
      mysql.replica.time_behind_source:
        enabled: true

  # Collecting cloudwatch metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: 'aws-cloudwatch-metrics'
          scrape_timeout: 120s
          scrape_interval: 300s
          static_configs:
            - targets: ['0.0.0.0:9106']

exporters:
  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true
  # export to SigNoz cloud
  otlp/signoz:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

service:
  pipelines:
    metrics/mysql:
      receivers: [mysql, prometheus]
      processors: []
      exporters: [otlp/signoz]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#step-3-set-environment-variables
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-step-3-set-environment-variables
group_tracking_ids: docs-integrations-aws-rds-mysql
<p>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Step 3: Set Environment Variables</p>
<pre><code># The accessible endpoint where MySQL server is running
export MYSQL_ENDPOINT=&quot;&lt;mysql-server-endpoint&gt;&quot;

export MYSQL_USERNAME=&quot;&lt;username&gt;&quot;

# The password to use for accessing mysql instance
export MYSQL_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#step-3-use-the-collector-config-file
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-step-3-use-the-collector-config-file
group_tracking_ids: docs-integrations-aws-rds-mysql
<p>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Step 3: Use the Collector Config File</p>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config mysql-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#collecting-rds-logs
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-collecting-rds-logs
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Collecting RDS Logs</h2>
<p>The log collection of RDS instance requires specifying the list of log group names. From the AWS CloudWatch console, please find the log group(s) relevant to the integration.</p>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create <code>mysql-logs-collection-config.yaml</code>:</p>
<pre><code>receivers:
  awscloudwatch/rds_mysql_logs:
    region: us-east-1
    logs:
      poll_interval: 1m
      groups:
        named:
          # replace the following name with your log group for RDS logs
          /aws/rds/:

processors:
  attributes/add_source_mysql:
    actions:
      - key: source
        value: &quot;rds_mysql&quot;
        action: insert
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/mysql_logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true

service:
  pipelines:
    logs/mysql:
      receivers: [awscloudwatch/rds_mysql_logs]
      processors: [attributes/add_source_mysql, batch]
      exporters: [otlp/mysql_logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<pre><code># region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Run the Collector</h3>
<p>Add to your collector run command:</p>
<pre><code>--config mysql-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#connect-aws-rds-mysql
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-connect-aws-rds-mysql
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Connect AWS RDS (MySQL)</h2>
<p>Once you're done with setting up AWS RDS (MySQL) for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the AWS RDS (MySQL) integration.</p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-mysql/MySQL" alt="Search for AWS RDS (MySQL) in Integrations tab" /></p>
<p><em>Search for AWS RDS (MySQL) in Integrations tab</em></p>
<p>Click on the <code>Connect AWS RDS (MySQL)</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your AWS RDS (MySQL) instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-mysql/MySQL" alt="Connect AWS RDS (MySQL)" /></p>
<p><em>Connect AWS RDS (MySQL)</em></p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-mysql/MySQL" alt="Listening for data from RedAWS RDS (MySQL)is" /></p>
<p><em>Listening for data from AWS RDS (MySQL)</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#aws-rds-mysql-dashboard
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-aws-rds-mysql-dashboard
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: AWS RDS (MySQL) dashboard</h2>
<p>Once SigNoz has started listening to your AWS RDS (MySQL) data, head over to the Dashboards tab and search for mysql, this will show you two newly created dashboard which shows differnet AWS RDS (MySQL) metrics.</p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-mysql/MySQL" alt="Dashboard for monitoring AWS RDS (MySQL) Metrics" /></p>
<p><em>Dashboards for monitoring AWS RDS (MySQL) Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Dashboards by importing the JSON files available <a href="https://github.com/SigNoz/signoz/tree/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_rds_mysql/assets/dashboards">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-mysql/#data-collected
tag_set: integrations, aws-rds-mysql
image_urls: 
tracking_id: docs-integrations-aws-rds-mysql-data-collected
group_tracking_ids: docs-integrations-aws-rds-mysql
<h2>AWS RDS (MySQL) Metrics and Logs: Collecting RDS MySQL Metrics: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your AWS RDS (MySQL) Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your AWS RDS (MySQL) instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-mysql/MySQL" alt="Log attributes and metrics details for AWS RDS (MySQL)" /></p>
<p><em>Log attributes and metrics details for AWS RDS (MySQL)</em></p>
<h3>## AWS RDS (MySQL) log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Body</td>
<td>body</td>
<td>string</td>
</tr>
</tbody>
</table>
<h3>## AWS RDS (MySQL) metrics</h3>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<p>To find a complete list of metrics you can checkout this <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_rds_mysql/integration.json#L58">link</a> or the <strong>Data Collected</strong> tab.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-variables/#prerequisites
tag_set: userguide, manage-variables
image_urls: 
tracking_id: docs-userguide-manage-variables-prerequisites
group_tracking_ids: docs-userguide-manage-variables
<h2>Manage Variables in SigNoz: Manage Variables in SigNoz - Prerequisites</h2>
<ul>
<li>This section assumes that your application is already instrumented. For details about how you can instrument your application, see the <a href="/docs/instrumentation/">Instrument Your Application</a> section.</li>
<li>This section assumes that you are familiar with the basics of monitoring applications.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-variables/#variables
tag_set: userguide, manage-variables
image_urls: 
tracking_id: docs-userguide-manage-variables-variables
group_tracking_ids: docs-userguide-manage-variables
<h2>Manage Variables in SigNoz: Variables</h2>
<p>Variables are now supported for the dashboard panel chart for all three query types. The widget plots can use these variables and update the charts dynamically without updating the query for each value. When the value of the variable changes, the refreshed dashboard will reflect the new value. Variables allow users to make interactive dashboards. Variables are most commonly used to avoid hard coding the values for host names, Kubernetes pods, namespaces, etc. Variables are independent for each dashboard. Variables are shown using the dropdowns at the top of the dashboard.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-variables/#how-to-add-a-variable-to-a-dashboard
tag_set: userguide, manage-variables
image_urls: 
tracking_id: docs-userguide-manage-variables-how-to-add-a-variable-to-a-dashboard
group_tracking_ids: docs-userguide-manage-variables
<h2>Manage Variables in SigNoz: How to add a Variable to a Dashboard?</h2>
<p>To add a variable to a dashboard, follow the steps below:</p>
<ol>
<li>From the left sidebar, choose <strong>Dashboards</strong>.</li>
<li>Find the dashboard to which you want to add a new variable.</li>
<li>Select <strong>Configure</strong>.</li>
<li>Select <strong>Add Variable</strong> from the <strong>Variables</strong> section tab.</li>
<li>Populate the following fields:
<ol>
<li><strong>Name</strong>: Name of the variable. This is used in Metrics builder/ClickHouse query/ PromQL later</li>
<li><strong>Description</strong>: Enter a brief and meaningful description of your variable.</li>
<li><strong>Type</strong>: Select the variable type from the list of available choices, and the UI changes accordingly.</li>
<li><em>(Optional)</em> <strong>Sort</strong>: Sort the values from the above variable type if applicable.</li>
<li><em>(Optional)</em> <strong>Enable multiple values to be checked</strong>: Make dropdown multi-select</li>
<li><em>(Optional)</em> <strong>Include an option for ALL values</strong>: Show the <code>ALL</code> option which includes all options from the multi-select dropdown</li>
</ol>
</li>
<li>When you‚Äôve finished, select the <strong>Save</strong> button.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-variables/#supported-variables-types
tag_set: userguide, manage-variables
image_urls: 
tracking_id: docs-userguide-manage-variables-supported-variables-types
group_tracking_ids: docs-userguide-manage-variables
<h2>Manage Variables in SigNoz: Supported Variables types:</h2>
<p>SigNoz supports three ways of creating variables: query, custom, and textbox.</p>
<ol>
<li>
<p>Query - A ClickHouse SQL query that fetches data from ClickHouse (only supported data source at the time) server SigNoz configured to use</p>
<p>Example: You want to query the list of host names from the metrics database and use them as a variable. Any ClickHouse SQL query that returns a column with desired values is valid. For instance, a query to fetch all host names that are reporting the CPU metric collection would be</p>
<pre><code>SELECT DISTINCT JSONExtractString(labels, 'host_name') AS host_name
FROM signoz_metrics.distributed_time_series_v4_1day
WHERE metric_name = 'system_cpu_time'
</code></pre>
<p>Follwing image shows above case</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fvar_with_clickhouse_sql.png&amp;w=3840&amp;q=75" alt="Variables-with-ClickHouse" /></p>
</li>
<li>
<p>Custom - A comma-separated list values as a Variable</p>
<p>Example: It is common to have some attributes with a known and small number of unique values, such as the region or availability zone of the application deployment. The custom variable type allows you to achieve this with comma-delimited values. An example for reference</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fcustom-regions-vars.webp&amp;w=3840&amp;q=75" alt="Variables-with-Custom" /></p>
</li>
<li>
<p>Textbox - A free text input field with an optional default value</p>
<p>Example: Free text as a variable</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Ftext-box-limit-variable.webp&amp;w=3840&amp;q=75" alt="Variables-with-Text" /></p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-variables/#variable-syntax-and-usage-in-queries
tag_set: userguide, manage-variables
image_urls: 
tracking_id: docs-userguide-manage-variables-variable-syntax-and-usage-in-queries
group_tracking_ids: docs-userguide-manage-variables
<h2>Manage Variables in SigNoz: Variable syntax and usage in queries</h2>
<p>All metric queries can refer to variables using any of the following syntax.</p>
<ol>
<li><code>{{.variable_name}}</code></li>
<li><code>$variable_name</code></li>
<li><code>[[variable_name]]</code></li>
<li><code>{{variable_name}}</code></li>
</ol>
<p>For example, A PromQL query for service request rate using dynamic service selection would be.</p>
<pre><code>sum(rate(signoz_calls_total{service_name=&quot;$service_name&quot;}[5m]))
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/manage-variables/#chaining-variables
tag_set: userguide, manage-variables
image_urls: 
tracking_id: docs-userguide-manage-variables-chaining-variables
group_tracking_ids: docs-userguide-manage-variables
<h2>Manage Variables in SigNoz: Chaining Variables</h2>
<p>It is often useful to chain variables to make the dashboard dynamic.</p>
<h3>## Example</h3>
<p>The following example shows how to chain the service name variable to make the endpoint URL dynamic.</p>
<ol>
<li>
<p>Write the query for service with name <code>service_name</code></p>
<pre><code>SELECT DISTINCT JSONExtractString(labels, 'service_name') AS service_name
FROM signoz_metrics.distributed_time_series_v4_1day
WHERE metric_name = 'signoz_calls_total'
</code></pre>
</li>
<li>
<p>Write the query for endpoint with name <code>operation</code></p>
<pre><code>SELECT DISTINCT JSONExtractString(labels, 'operation') AS operation
FROM signoz_metrics.distributed_time_series_v4_1day
WHERE metric_name = 'signoz_calls_total' AND JSONExtractString(labels, 'service_name') IN {{.service_name}}
</code></pre>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#overview
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-overview
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: AWS RDS (PostgreSQL) Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key AWS RDS PostgreSQL metrics and logs, view them with an out-of-the-box dashboards, and parse PostgreSQL logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#prerequisites
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-prerequisites
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have the following:</p>
<ol>
<li><strong>AWS Credentials and Permissions</strong>:</li>
</ol>
<ul>
<li>Set up proper AWS credentials (e.g., <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables).</li>
<li>Required IAM permissions:
<ul>
<li><code>cloudwatch:ListMetrics</code></li>
<li><code>cloudwatch:GetMetricStatistics</code></li>
<li><code>cloudwatch:GetMetricData</code></li>
<li><code>logs:DescribeLogGroups</code></li>
<li><code>logs:FilterLogEvents</code></li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>Java Runtime Environment (JRE) 11+</strong>:</li>
</ol>
<ul>
<li>Required for the CloudWatch Exporter.</li>
<li>Alternative: Use the <a href="https://github.com/prometheus/cloudwatch_exporter#docker-images">Docker image</a>
.</li>
</ul>
<ol start="3">
<li><strong>OpenTelemetry Collector</strong>:</li>
</ol>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install an OTEL Collector</a>
(v0.88.0+) if not already done.</li>
<li>Ensure you can provide config files to the collector and set environment variables and command line flags used for running it.</li>
</ul>
<ol start="4">
<li><strong>PostgreSQL Server Access</strong>:</li>
</ol>
<ul>
<li>The OTEL collector must have client access to the Postgres server (optional if only collecting CloudWatch metrics).</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#step-1-set-up-the-prometheus-cloudwatch-exporter
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-step-1-set-up-the-prometheus-cloudwatch-exporter
group_tracking_ids: docs-integrations-aws-rds-postgres
<p>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: ## Collecting RDS PostgreSQL Metrics - Step 1: Set up the Prometheus CloudWatch Exporter</p>
<ol>
<li>
<p>Download the exporter:</p>
<pre><code>curl -sLSO https://github.com/prometheus/cloudwatch_exporter/releases/download/v0.15.5/cloudwatch_exporter-0.15.5-jar-with-dependencies.jar
</code></pre>
</li>
<li>
<p>Configure the Prometheus exporter Save the following config for collecting AWS RDS metrics in a file named <code>aws-rds-postgres-metrics.yaml</code> and update the region key with relevant value.</p>
<hr />
<p>region: us-east-1
metrics:</p>
<ul>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: BinLogDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: BurstBalance
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: CheckpointLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ConnectionAttempts
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: CPUUtilization
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DatabaseConnections
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DiskQueueDepth
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DiskQueueDepthLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: EBSByteBalance%
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: EBSIOBalance%
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeableMemory
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeStorageSpace
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: FreeStorageSpaceLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: MaximumUsedTransactionIDs
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: NetworkReceiveThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: NetworkTransmitThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: OldestReplicationSlotLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPS
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPSLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadIOPSLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatency
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatencyLocalStorage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadLatencyLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReadThroughputLogVolume
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicaLag
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicationChannelLag
aws_dimensions: [DBInstanceIdentifier]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: ReplicationSlotDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: TransactionLogsDiskUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: TransactionLogsGeneration
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteIOPS
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteLatency
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: WriteThroughput
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: SwapUsage
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoad
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoadCPU
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
<li>
<p>aws_namespace: AWS/RDS
aws_metric_name: DBLoadNonCPU
aws_dimensions: [DBInstanceIdentifier]
aws_statistics: [Average, Maximum]</p>
</li>
</ul>
</li>
<li>
<p>Run the following command:</p>
<p>java -jar cloudwatch_exporter-0.15.5-jar-with-dependencies.jar 9106 aws-rds-postgres-metrics.yaml</p>
</li>
<li>
<p>Verify the CloudWatch metrics</p>
</li>
</ol>
<p>Visit <a href="http://localhost:9106/metrics">http://localhost:9106/metrics</a> and confirm the <code>aws_rds_*</code> metrics are avialable.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#step-2-create-the-otel-collector-config-file
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-step-2-create-the-otel-collector-config-file
group_tracking_ids: docs-integrations-aws-rds-postgres
<p>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Step 2: Create the OTEL Collector Config File</p>
<p>Create <code>postgres-metrics-collection-config.yaml</code>:</p>
<pre><code>receivers:
  postgresql:
    # The endpoint of the postgresql server. Whether using TCP or Unix sockets, this value should be host:port. If transport is set to unix, the endpoint will internally be translated from host:port to /host.s.PGSQL.port
    endpoint: ${env:POSTGRESQL_ENDPOINT}
    # The frequency at which to collect metrics from the Postgres instance.
    collection_interval: 60s
    # The username used to access the postgres instance
    username: ${env:POSTGRESQL_USERNAME}
    # The password used to access the postgres instance
    password: ${env:POSTGRESQL_PASSWORD}
    # The list of databases for which the receiver will attempt to collect statistics. If an empty list is provided, the receiver will attempt to collect statistics for all non-template databases
    databases: [&quot;pgtestdb&quot;]
    # # Defines the network to use for connecting to the server. Valid Values are `tcp` or `unix`
    # transport: tcp
    tls:
      insecure_skip_verify: true
    #   ca_file: /etc/ssl/certs/ca-certificates.crt
    #   cert_file: /etc/ssl/certs/postgres.crt
    #   key_file: /etc/ssl/certs/postgres.key
    metrics:
      postgresql.database.locks:
        enabled: true
      postgresql.deadlocks:
        enabled: true
      postgresql.sequential_scans:
        enabled: true

  prometheus:
    config:
      scrape_configs:
        - job_name: 'aws-cloudwatch-metrics'
          scrape_timeout: 120s
          scrape_interval: 300s
          static_configs:
            - targets: ['0.0.0.0:9106']

exporters:
  # export to local collector
  otlp/local:
    endpoint: &quot;localhost:4317&quot;
    tls:
      insecure: true
  # export to SigNoz cloud
  otlp/signoz:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

service:
  pipelines:
    metrics/postgresql:
      receivers: [postgresql, prometheus]
      processors: []
      exporters: [otlp/signoz]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#step-3-set-environment-variables
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-step-3-set-environment-variables
group_tracking_ids: docs-integrations-aws-rds-postgres
<p>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Step 3: Set Environment Variables</p>
<pre><code># The accessible endpoint where PostgreSQL server is running
export POSTGRESQL_ENDPOINT=&quot;&lt;postgres-server-endpoint&gt;&quot;

export POSTGRESQL_USERNAME=&quot;&lt;username&gt;&quot;

# The password to use for accessing postgres instance
export POSTGRESQL_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#step-3-use-the-collector-config-file
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-step-3-use-the-collector-config-file
group_tracking_ids: docs-integrations-aws-rds-postgres
<p>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Step 3: Use the Collector Config File</p>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config postgres-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#collecting-rds-logs
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-collecting-rds-logs
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Collecting RDS Logs</h2>
<p>The log collection of RDS instance requires specifying the list of log group names. From the AWS CloudWatch console, please find the log group(s) relevant to the integration.</p>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create <code>postgres-logs-collection-config.yaml</code>:</p>
<pre><code>receivers:
  awscloudwatch/rds_postgres_logs:
    region: us-east-1
    logs:
      poll_interval: 1m
      groups:
        named:
          # replace with your RDS log group name
          /aws/rds/:

processors:
  attributes/add_source_postgres:
    actions:
      - key: source
        value: &quot;rds_postgres&quot;
        action: insert
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  otlp/postgres_logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

service:
  pipelines:
    logs/postgres:
      receivers: [awscloudwatch/rds_postgres_logs]
      processors: [attributes/add_source_postgres, batch]
      exporters: [otlp/postgres_logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<pre><code># region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Run the Collector</h3>
<p>Add to your collector run command:</p>
<pre><code>--config postgres-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#connect-aws-rds-postgresql
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-connect-aws-rds-postgresql
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Connect AWS RDS (PostgreSQL)</h2>
<p>Once you're done with setting up AWS RDS (PostgreSQL) for collecting metrics and logs, head over to the intergrations tab in SigNoz and search for the AWS RDS (PostgreSQL) integration.</p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-postgres/PostgreSQL" alt="Search for AWS RDS (PostgreSQL) in Integrations tab" /></p>
<p><em>Search for AWS RDS (PostgreSQL) in Integrations tab</em></p>
<p>Click on the <code>Connect AWS RDS (PostgreSQL)</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your AWS RDS (PostgreSQL) instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-postgres/PostgreSQL" alt="Connect AWS RDS (PostgreSQL)" /></p>
<p><em>Connect AWS RDS (PostgreSQL)</em></p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-postgres/PostgreSQL" alt="Listening for data from RedAWS RDS (PostgreSQL)is" /></p>
<p><em>Listening for data from AWS RDS (PostgreSQL)</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#aws-rds-postgresql-dashboard
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-aws-rds-postgresql-dashboard
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: AWS RDS (PostgreSQL) dashboard</h2>
<p>Once SigNoz has started listening to your AWS RDS (PostgreSQL) data, head over to the Dashboards tab and search for postgres, this will show you two newly created dashboard which shows differnet AWS RDS (PostgreSQL) metrics.</p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-postgres/PostgreSQL" alt="Dashboard for monitoring AWS RDS (PostgreSQL) Metrics" /></p>
<p><em>Dashboards for monitoring AWS RDS (PostgreSQL) Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Dashboards by importing the JSON files available <a href="https://github.com/SigNoz/signoz/tree/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_rds_postgres/assets/dashboards">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/aws-rds-postgres/#data-collected
tag_set: integrations, aws-rds-postgres
image_urls: 
tracking_id: docs-integrations-aws-rds-postgres-data-collected
group_tracking_ids: docs-integrations-aws-rds-postgres
<h2>AWS RDS (PostgreSQL) Metrics and Logs: Collecting RDS PostgreSQL Metrics: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your AWS RDS (PostgreSQL) Integrations, it shows you details about the different logs attributes and the metrics types that you can monitor for your AWS RDS (PostgreSQL) instance. The tables below gives you a list of the different logs attributes and metrics available.</p>
<p><img src="https://signoz.io/docs/integrations/aws-rds-postgres/PostgreSQL" alt="Log attributes and metrics details for AWS RDS (PostgreSQL)" /></p>
<p><em>Log attributes and metrics details for AWS RDS (PostgreSQL)</em></p>
<h3>## AWS RDS (PostgreSQL) log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Body</td>
<td>body</td>
<td>string</td>
</tr>
</tbody>
</table>
<h3>## AWS RDS (PostgreSQL) metrics</h3>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<p>To find a complete list of metrics you can checkout this <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/aws_rds_postgres/integration.json#L58">link</a> or the <strong>Data Collected</strong> tab.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query
group_tracking_ids: docs-userguide-create-a-custom-query
<h2>Create a Custom Query</h2>
<p>This page describes the elements that define a custom query and provides several examples of using the Query Builder to create custom queries.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#elements-that-define-a-custom-query
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-elements-that-define-a-custom-query
group_tracking_ids: docs-userguide-create-a-custom-query
<h2>Create a Custom Query: Elements that Define a Custom Query: Elements that Define a Custom Query</h2>
<p>To create a custom query, you must specify the following elements:</p>
<ul>
<li>Aggregation function</li>
<li>Metric</li>
<li>Where Clause</li>
<li>Group By Clause</li>
<li>Legend</li>
</ul>
<p>The sections below describe these elements.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#aggregation-function
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-aggregation-function
group_tracking_ids: docs-userguide-create-a-custom-query
<p>Create a Custom Query: Elements that Define a Custom Query: Aggregation Function</p>
<p>Aggregation is a method of reducing the amount of data that SigNoz displays, and you can use it to identify patterns or outliers in your data. SigNoz allows you to perform both temporal and spatial aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#temporal-aggregation
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-temporal-aggregation
group_tracking_ids: docs-userguide-create-a-custom-query
<p>Create a Custom Query: Elements that Define a Custom Query: Temporal Aggregation</p>
<p>SigNoz re-aggregates into longer intervals the metrics you collect at a high frequency, allowing low-resolution time series to be pre-calculated or used in place of the original metric data. SigNoz auto-adjusts this interval based on the time range selected to limit the number of points it plots on the chart. The following example diagram shows the aggregation of data into ten second intervals:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Ftemporal_aggregation.webp&amp;w=3840&amp;q=75" alt="Temporal Aggregation" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#spatial-aggregation
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-spatial-aggregation
group_tracking_ids: docs-userguide-create-a-custom-query
<p>Create a Custom Query: Elements that Define a Custom Query: Spatial Aggregation</p>
<p>If you collect a metric and you don‚Äôt need all the attributes, you can remove unwanted attributes. This way, your metrics will only have the attributes that you need. The following diagram shows the aggregation of the entire set of time series into a single value at each interval of time:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fspatial_aggregation.webp&amp;w=3840&amp;q=75" alt="Spatial Aggregation" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#supported-aggregation-functions
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-supported-aggregation-functions
group_tracking_ids: docs-userguide-create-a-custom-query
<p>Create a Custom Query: Elements that Define a Custom Query: Supported Aggregation Functions</p>
<p>SigNoz supports the following aggregation functions:</p>
<ul>
<li>NOOP (No aggregation)</li>
<li>COUNT (Number of values in each set of data)</li>
<li>COUNT_DISTINCT (Number of distinct values in each set of data)</li>
<li>SUM (Sum of all the values)</li>
<li>AVG (Average of all the values)</li>
<li>MAX (Maximum of all the values)</li>
<li>MIN (Minimum of all the values)</li>
<li>P05 (5th percentile of values)</li>
<li>P10 (10th percentile of values)</li>
<li>P20 (20th percentile of values)</li>
<li>P25 (25th percentile of values)</li>
<li>P50 (50th percentile of values)</li>
<li>P75 (75th percentile of values)</li>
<li>P90 (90th percentile of values)</li>
<li>P95 (95th percentile of values)</li>
<li>P99 (99th percentile of values)</li>
<li>RATE (Rate of change of value for each time series)</li>
<li>SUM_RATE (Sum of the rate of change of each time series)</li>
<li>RATE_SUM (Rate of change on sum aggregated values)</li>
<li>RATE_AVG (Rate of change on avg aggregated values)</li>
<li>RATE_MAX (Rate of change on max aggregated values)</li>
<li>RATE_MIN (Rate of change on min aggregated values)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#how-do-metrics-work-in-signoz
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-how-do-metrics-work-in-signoz
group_tracking_ids: docs-userguide-create-a-custom-query
<p>Create a Custom Query: Elements that Define a Custom Query: How do Metrics work in SigNoz?</p>
<p>Application metrics represent a characteristic of your application as a value at a specific point in time. For example, an application metric is the number of requests per second your application serves. SigNoz collects information as a sequence of data points every minute and then represents the data through time in a graphical form. The X-axis is time, and the Y-axis is the value.</p>
<p>You can use a <code>where</code> clause to specify the filtering condition for your query, enabling you to select only data that meets the specified criteria. SigNoz will only plot data for which the where clause evaluates to <code>true</code>, and it‚Äôll exclude everything else. For example, you can use a where clause to plot only the HTTP requests from a service named <code>foo</code> that returned a <code>500</code> status code.</p>
<p>The following operators are supported:</p>
<ul>
<li><code>IN</code></li>
<li><code>NIN</code></li>
<li><code>LIKE</code></li>
<li><code>NLIKE</code></li>
</ul>
<p>üìù Note</p>
<p>You can use the <em><code>IN</code></em>¬†and¬†<em><code>NIN</code></em>¬†operators as an alternative for <code>=</code> and <code>!=</code> with a single value.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#group-by-clause
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-group-by-clause
group_tracking_ids: docs-userguide-create-a-custom-query
<p>Create a Custom Query: Elements that Define a Custom Query: Group By Clause</p>
<p>You can use the <code>Group By</code> clause to divide the results of your query based on the property you specify. The following example diagram shows how a <code>Group By</code> clause aggregates the same metric into separate groups, each group representing an AWS region:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Faggregation_by_region.webp&amp;w=3840&amp;q=75" alt="Group By Clause" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#legend
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-legend
group_tracking_ids: docs-userguide-create-a-custom-query
<p>Create a Custom Query: Elements that Define a Custom Query: Legend</p>
<p>Use the <strong>Legend</strong> text box to specify a legend name for your time series.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/create-a-custom-query/#sample-example-to-create-custom-queries
tag_set: userguide, create-a-custom-query
image_urls: 
tracking_id: docs-userguide-create-a-custom-query-sample-example-to-create-custom-queries
group_tracking_ids: docs-userguide-create-a-custom-query
<p>Create a Custom Query: Elements that Define a Custom Query: Sample Example to Create Custom Queries</p>
<p>Checkout the <a href="https://signoz.io/docs/userguide/query-builder/">Query Builder</a> documentation for examples on how to create custom queries using Query Builder.</p>
<p>On this page</p>
<p><a href="#elements-that-define-a-custom-query">Elements that Define a Custom Query</a></p>
<p><a href="#aggregation-function">Aggregation Function</a></p>
<p><a href="#temporal-aggregation">Temporal Aggregation</a></p>
<p><a href="#spatial-aggregation">Spatial Aggregation</a></p>
<p><a href="#supported-aggregation-functions">Supported Aggregation Functions</a></p>
<p><a href="#how-do-metrics-work-in-signoz">How do Metrics work in SigNoz?</a></p>
<p><a href="#group-by-clause">Group By Clause</a></p>
<p><a href="#legend">Legend</a></p>
<p><a href="#sample-example-to-create-custom-queries">Sample Example to Create Custom Queries</a></p>
<p>Docs</p>
<p><a href="/docs/">Introduction</a>
<a href="/docs/contributing/">Contributing</a></p>
<p><a href="https://knowledgebase.signoz.io/kb">Knowledge Base</a></p>
<p><a href="/api_reference/">SigNoz API</a></p>
<p>Community</p>
<p><a href="/support/">Support</a></p>
<p><a href="https://signoz.io/slack">Slack</a></p>
<p><a href="https://twitter.com/SigNozHQ">Twitter</a></p>
<p><a href="https://community-chat.signoz.io/">Community Archive</a></p>
<p><a href="/changelog/">Changelog</a></p>
<p>More</p>
<p><a href="/product-comparison/signoz-vs-datadog/">SigNoz vs Datadog</a>
<a href="/product-comparison/signoz-vs-newrelic/">SigNoz vs New Relic</a>
<a href="/product-comparison/signoz-vs-grafana/">SigNoz vs Grafana</a>
<a href="/product-comparison/signoz-vs-dynatrace/">SigNoz vs Dynatrace</a></p>
<p><a href="https://jobs.gem.com/signoz">Careers</a></p>
<p><a href="/about-us/">About</a>
<a href="/terms-of-service/">Terms</a>
<a href="/privacy/">Privacy</a>
<a href="https://trust.signoz.io/">Security &amp; Compliance</a></p>
<p><img src="https://signoz.io/img/SigNozLogo-orange.svg" alt="" /></p>
<p>SigNoz</p>
<p>All systems operational</p>
<p><a href="https://github.com/SigNoz"></a>
<a href="https://www.linkedin.com/company/signozio/"></a>
<a href="https://signoz.io/slack"></a>
<a href="https://twitter.com/SigNozHQ"></a>
<a href="https://www.youtube.com/@signoz"></a></p>
<p><img src="https://signoz.io/svgs/icons/SOC-2.svg" alt="" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/setup-alerts-notification/
tag_set: setup-alerts-notification
image_urls: 
tracking_id: docs-setup-alerts-notification
group_tracking_ids: docs-setup-alerts-notification
<h2>Setup Alerts Notifications Channel</h2>
<p>You can setup notification channel for sending the generated alerts to other applications. Currently, the following channels are supported.</p>
<p><a href="/docs/alerts-management/notification-channel/slack">üìÑÔ∏è Slack</a>
<a href="/docs/alerts-management/notification-channel/webhook">üìÑÔ∏è Webhook</a>
<a href="/docs/alerts-management/notification-channel/pagerduty">üìÑÔ∏è PagerDuty</a>
<a href="/docs/alerts-management/notification-channel/opsgenie">üìÑÔ∏è Opsgenie</a>
<a href="/docs/alerts-management/notification-channel/ms-teams">üìÑÔ∏è MS Teams</a>
<a href="/docs/alerts-management/notification-channel/email">üìÑÔ∏è Email</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz</h2>
<p>This page shows how you can use distributed tracing to retrieve detailed telemetry data and see how your applications are performing. You‚Äôll learn the following:</p>
<ul>
<li>What is distributed tracing</li>
<li>How to visualize aggregate metrics from traces</li>
<li>How to filter your spans</li>
<li>How to inspect a span</li>
</ul>
<p>This section uses the¬†<a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a> sample application that comes preinstalled with SigNoz and generates sample data that you can query. You can apply the concepts and techniques you‚Äôll learn to monitor your own applications.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#prerequisites
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-prerequisites
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Prerequisites</h2>
<ul>
<li>This section assumes that your application is already instrumented. For details about how you can instrument your application, see the <a href="/docs/instrumentation/">Instrument Your Application</a> section.</li>
<li>This section assumes that you are familiar with the basics of monitoring applications.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#what-is-distributed-tracing
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-what-is-distributed-tracing
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: What is Distributed Tracing?</h2>
<p>Transaction tracing is a method of monitoring a request across various components of your application. Each request is assigned a unique identifier that is used to track it across every component of your application, allowing you to see which particular instance of a function is slow or failing.</p>
<p>In distributed architectures, a service can run on different virtual machines or containers. Distributed tracing is a method that allows you to monitor applications across different virtual machines or containers.</p>
<p>A distributed trace starts when a user initiates an action. For example, when a user adds an item to their cart, a unique identifier named ‚Äúparent span‚Äù is assigned to that request. As adding an item to the cart is comprised of several individual steps such as querying the database or making external API calls, each of these steps retains the parent identifier and is also assigned a unique identifier named ‚Äúchild span‚Äù. For more details about distributed tracing, see the <a href="https://signoz.io/blog/distributed-tracing-span/">Spans - a key concept of distributed tracing</a> and <a href="https://signoz.io/blog/context-propagation-in-distributed-tracing/">What is Context Propagation in Distributed Tracing?</a> blog posts.</p>
<p>On the <strong>Traces</strong> page, you can view and analyze requests as they propagate through various components of your application and get visibility into the experience of your users.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#open-the-traces-section
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-open-the-traces-section
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Open the Traces Section</h2>
<p>From the sidebar, select <strong>Traces</strong>:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fopen-traces-section-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Open the Traces section" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#visualize-aggregate-metrics-from-traces
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-visualize-aggregate-metrics-from-traces
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Visualize Aggregate Metrics from Traces</h2>
<p>SigNoz allows you to see aggregates of your filtered traces. The following functions are available for calculating aggregate values:</p>
<ul>
<li>Count</li>
<li>Rate per second</li>
<li>Sum</li>
<li>Average</li>
<li>Max</li>
<li>Min</li>
<li>50th percentile</li>
<li>90th percentile</li>
<li>95th percentile</li>
<li>99th percentile</li>
</ul>
<p>To specify the function you wish to graph and indicate how the system should group data, you must use the dropdown list to select a function from the list, and then use the¬†Group By clause to break the results into separate data series. The following example screenshot shows how you can plot the number of invocations for each of your services:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fcustomize-the-graph-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Customize the graph" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#filter-spans-by-tagsattributes
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-filter-spans-by-tagsattributes
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Filter Spans by Tags/Attributes</h2>
<p>Tags/Attributes are key-value pairs that allow you to filter spans by their characteristics. SigNoz supports multi-selection criterion. And the characteristics include, but are not limited to HTTP headers, DB systems, and Messaging destinations etc. For exhaustive list please refer to OpenTelemetry‚Äôs semantic conventions described on the <a href="https://github.com/open-telemetry/semantic-conventions/blob/main/docs/general/trace.md">Trace Semantic Conventions</a> page of the OpenTelemetry specification.</p>
<p>There are two ways in which you can filter traces by tags:</p>
<ul>
<li>You can enter plain text in the <strong>Quick Filter</strong> input box and then select the <strong>Run</strong> button at the far right. Note that text is interpreted as case-sensitive.</li>
<li>You can use the expression builder. To access the expression builder, select the <strong>Add Tags Filter</strong> button. Then, use the dropdown list to select a tag from the list of supported tags, specify an operator and enter a value. When you‚Äôve finished, select the <strong>Run Query</strong> button.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#advanced-filtering-feature
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-advanced-filtering-feature
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Advanced Filtering Feature</h2>
<p>You can use the <strong>Advanced Filtering</strong> pane that contains multiple filter criteria that you can apply to your spans:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fadvanced-filtering-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Advanced filtering" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#sort-spans-by-various-tags
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-sort-spans-by-various-tags
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Sort Spans by various tags</h2>
<p>Select a column heading to sort the list by the values in that column. Select the column heading again to reverse the sort order or to cancel sorting:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fsort-spans-v0.6.2.webp&amp;w=3840&amp;q=75" alt="Sort spans" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#inspect-a-span
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-inspect-a-span
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Inspect a Span</h2>
<p>To further troubleshoot your application, you can select a span from the list to view its details. For details, see the <a href="/docs/userguide/span-details/">Span Details</a> page.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#missing-spans
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-missing-spans
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Missing Spans</h2>
<p>If you are seeing missing spans in your traces, it could be due to the following reasons:</p>
<ul>
<li>
<p>One of the service is instrumented but not sending spans to SigNoz. For example, the service might not be exporting spans to SigNoz.</p>
</li>
<li>
<p>The spans are not being sent to SigNoz. For example, the spans might be dropped due to network issues or the spans might be dropped due to sampling.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/traces/#span-gaps
tag_set: userguide, traces
image_urls: 
tracking_id: docs-userguide-traces-span-gaps
group_tracking_ids: docs-userguide-traces
<h2>View Traces in SigNoz: Span Gaps</h2>
<p>Sometimes it's possible that there are gaps between consequent spans. This happens when some process/code are not traced. For Example, by default OpenTelemetry auto instrumentation libraries do not trace the custom functions/methods. This can be fixed by adding manual instrumentation to the code.</p>
<p>Example of span gaps:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fspan-gap.png&amp;w=3840&amp;q=75" alt="Span Gap" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/sso-authentication/
tag_set: userguide, sso-authentication
image_urls: 
tracking_id: docs-userguide-sso-authentication
group_tracking_ids: docs-userguide-sso-authentication
<h2>Single Sign-on Authentication</h2>
<p>Single Sign-on Settings can be configured through <code>Settings &gt; Organization Settings &gt; Authenticated Domains (section)</code>. You can use SSO settings to let your team log in through an Identity Providers (like Google Workspace, Okta, etc) instead of using passwords.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/sso-authentication/#google-workspace
tag_set: userguide, sso-authentication
image_urls: 
tracking_id: docs-userguide-sso-authentication-google-workspace
group_tracking_ids: docs-userguide-sso-authentication
<h2>Single Sign-on Authentication: Google Workspace</h2>
<p>Google Workspace single sign-on (SSO) provides password-free access to the invited members of your workspace to SigNoz.</p>
<h3>## Who can use this feature?</h3>
<ul>
<li>Google Workspace Owners and Org Owners</li>
<li>Available in <code>Enterprise</code> and <code>Teams</code> plan.</li>
</ul>
<h3>## Steps to configure Google OAuth 2.0</h3>
<p>Google Workspace single sign-on (SSO) lets all members of your workspace sign in to SigNoz using their Google accounts. If they don‚Äôt have a account in SigNoz yet, they will have to be invited by Admin from <code>Settings &gt; Organization Settings &gt; Invite Members</code>.</p>
<ol>
<li>
<p>Register your signoz instance with your Google org by visiting the <a href="https://console.cloud.google.com/apis/credentials">cloud console</a>
. You must <a href="https://redash.io/help/open-source/admin-guide/google-developer-account-setup">create a developers project</a> if you have not already. Then follow the Create Credentials flow.</p>
</li>
<li>
<p>Set the Authorized Redirect URL(s) to <code>http(s)://${SIGNOZ_BASEURL}/api/v1/complete/google</code></p>
</li>
<li>
<p>During the setup you will obtain a client id and a client secret. Note it down as you will need them while setting up google auth in SigNoz.</p>
</li>
<li>
<p>Go to <code>Settings &gt; Organization Settings &gt; Authenticated Domains</code>. Click <code>Add a Domain</code>. Enter your domain name (e.g. <code>user@your-email-domain.com</code>).</p>
</li>
<li>
<p>After domain is created, click on <code>Configure SSO</code>. Choose <code>Google Authentication</code> from the list.</p>
</li>
<li>
<p>Now, enter the client id and secret you obtained in step 3. Click <code>Save Settings</code>.</p>
</li>
<li>
<p>Click on <code>Enforce SSO</code> (next to your domain in Authenticated Domains) to enable google SSO login. When you enforce SSO, all users with user name format <code>&lt;username&gt;@your-email-domain.com</code> will be forced to log in through Google.</p>
</li>
<li>
<p>To test your setup, we recommend you to log in from a new browser window in Incognito mode.</p>
</li>
<li>
<p>If you face issue signining in, review the query service logs. To log into SigNoz for correcting SSO settings, admins may use this special URL to use password based login: <code>http(s)://${SIGNOZ_BASEURL}/login?password=Y</code></p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/sso-authentication/#saml-based-authentication
tag_set: userguide, sso-authentication
image_urls: 
tracking_id: docs-userguide-sso-authentication-saml-based-authentication
group_tracking_ids: docs-userguide-sso-authentication
<h2>Single Sign-on Authentication: SAML based Authentication: SAML based Authentication</h2>
<p>Integrating SAML with SigNoz lets your users access SigNoz without re-authenticating. Configuring SAML is a two step process. First, you would have to configure your IdP (Identity Provider like Okta, Azure AD) with details of your SigNoz app. When the first step is complete, you would need to enter the information (like Entity ID, etc) available in your IdP into SigNoz settings (<code>Settings &gt;&gt; Organization Settings &gt;&gt; Authentication Domains</code>)</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/sso-authentication/#who-can-use-this-feature-1
tag_set: userguide, sso-authentication
image_urls: 
tracking_id: docs-userguide-sso-authentication-who-can-use-this-feature-1
group_tracking_ids: docs-userguide-sso-authentication
<p>Single Sign-on Authentication: SAML based Authentication: Who can use this feature?</p>
<ul>
<li>Available in <code>Enterprise</code> and <code>Teams</code> plan.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/sso-authentication/#steps-to-be-performed-in-microsoft-entra-id-or-azure-active-directory---ad
tag_set: userguide, sso-authentication
image_urls: 
tracking_id: docs-userguide-sso-authentication-steps-to-be-performed-in-microsoft-entra-id-or-azure-active-directory--ad
group_tracking_ids: docs-userguide-sso-authentication
<p>Single Sign-on Authentication: SAML based Authentication: ### ## SAML authentication with Microsoft Entra ID - Steps to be performed in Microsoft Entra ID (Or Azure Active Directory - AD)</p>
<ol>
<li>
<p>Go to the <code>Microsoft Entra ID</code> and click on <code>Enterprise Applications</code>.</p>
</li>
<li>
<p>Click on <code>+ New Application</code> in the top bar of the <em>All Applications</em> page.</p>
</li>
<li>
<p>In the next page, click on <code>+Create your own application</code>. Enter your application name as <em>SigNoz</em>, Select <strong>Integrate with other Applications (Non-Gallery)</strong> option and create.</p>
</li>
<li>
<p>Once the application is created, go to <code>Single Sign-On</code> from left side bar and click on <code>SAML</code> card option</p>
</li>
<li>
<p>When the next page appears, you will see an card for <code>Basic SAML Configuration</code>. Click on edit icon button in this card</p>
</li>
<li>
<p>Fill out the following details and click <code>Save</code>:</p>
<ul>
<li>Entity Identifier (Entity ID): Set Base URL (host and port - if any) of your SigNoz app. (e.g. <code>test.in.signoz.cloud</code>)</li>
<li>Reply URL(Assertion Consumer Service URL): Set the reply URL using this format - <code>http(s)://${SIGNOZ_BASEURL}/api/v1/complete/saml</code> (e.g. <code>https://test.in.signoz.cloud/api/v1/complete/saml</code>)</li>
<li>Sign on URL: Set the sign on URL using this format - <code>http(s)://${SIGNOZ_BASEURL}/login</code> (e.g. <code>https://test.in.signoz.cloud/login</code>)</li>
</ul>
</li>
<li>
<p>Now we need to capture SSO information required to configure SAML in SigNoz. In the page, locate <em>App Federation Metadata URL</em>. Preferably, open this metadata page in a new tab. Once there, locate and copy these two field values from XML into a separate notepad:</p>
<ul>
<li>
<p>At the top of page, locate XML tag <code>EntityDescriptor</code> and copy the <code>entityID</code> value</p>
</li>
<li>
<p>Locate <code>X509Data</code> tag and copy the entity content (value). This is certificate (<code>Certificate Data</code>) that SigNoz needs to validate response from IdP.</p>
</li>
<li>
<p>Locate <code>Location</code> at the bottom of the page and copy its value. This is the ACS URL that SigNoz needs to send SAML response to.</p>
</li>
</ul>
</li>
</ol>
<p>For more details on the metadata page, click <a href="https://learn.microsoft.com/en-us/entra/identity-platform/federation-metadata">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/sso-authentication/#steps-to-be-performed-in-signoz
tag_set: userguide, sso-authentication
image_urls: 
tracking_id: docs-userguide-sso-authentication-steps-to-be-performed-in-signoz
group_tracking_ids: docs-userguide-sso-authentication
<p>Single Sign-on Authentication: SAML based Authentication: Steps to be performed in SigNoz</p>
<ol>
<li>Go to <code>Settings</code>. Click on <code>Organization Settings</code> tab and locate <code>Authenticated Domains</code> in the page</li>
<li>Click <code>Add Domain</code></li>
<li>Enter the domain that your users would login with. For example, if your user names or emails are in format such as _<a href="mailto:john@example.com">john@example.com</a>
_ then you would have to enter <em>example.com</em> here.</li>
<li>After domain is added, Click on <code>Configure SSO</code> and choose <code>SAML Authentication</code> option</li>
<li>Enter values of tags <code>entity ID</code>, <code>Certificate Data</code> and <code>Location(ACS URL)</code> that you acquired from the metadata page (step 7 above)</li>
<li>Save the settings and log in from an incognito tab to test the setup. If you face difficulties signing in, review the query service logs. Also if you are admin and are unable to login because of faulty setup, then you may login with password using this URL: <code>http(s)://${SIGNOZ_BASEURL}/login?password=Y</code></li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/sso-authentication/#steps-to-be-performed-in-okta
tag_set: userguide, sso-authentication
image_urls: 
tracking_id: docs-userguide-sso-authentication-steps-to-be-performed-in-okta
group_tracking_ids: docs-userguide-sso-authentication
<p>Single Sign-on Authentication: SAML based Authentication: ### ## SAML Authentication with Okta - Steps to be performed in Okta</p>
<p><img src="https://signoz.io/img/docs/okta-sso-saml-integration.gif" alt="Okta SSO SAML Integration" /></p>
<p><em>Okta SSO SAML Integration</em></p>
<ol>
<li>
<p>Log in to <strong>Okta</strong></p>
</li>
<li>
<p>From the <strong>Admin</strong> page, go to <strong>Applications</strong> &gt; <strong>Applications</strong> &gt; <strong>Create App Integration</strong></p>
</li>
<li>
<p>Select <strong>SAML 2.0</strong>, and hit <strong>Next</strong></p>
</li>
<li>
<p>On the <strong>SAML Integration</strong> page, enter the following:</p>
<ul>
<li><strong>Application Name</strong>: <code>SigNoz</code></li>
<li><strong>Single Sign-on URL</strong>: <code>http(s)://${SIGNOZ_BASEURL}/api/v1/complete/saml</code></li>
<li><strong>Audience URI (SP Entity ID)</strong>: <code>http(s)://${SIGNOZ_BASEURL}</code></li>
<li><strong>Default RelayState</strong>: <code>https://${SIGNOZ_BASEURL}?domainName=companydomain.com</code> - replace <code>companydomain.com</code> with your company domain used for SSO login</li>
</ul>
</li>
<li>
<p>Save the application integration</p>
</li>
<li>
<p>Now, we can visit the <strong>Metadata URL</strong> in Okta to get the following information:</p>
<ul>
<li>
<p><strong>Entity ID</strong>: At the top of page, locate XML tag <code>EntityDescriptor</code> and you will get the <code>entityID</code> value</p>
</li>
<li>
<p><strong>X509Certificate Data</strong>: Locate <code>X509Certificate</code> tag and copy the entity content (value). This is certificate (<code>Certificate Data</code>) that SigNoz needs to validate response from IdP. When copying this value, make sure that you copy the entire certificate and remove any whitespace</p>
<pre><code>        ...
</code></pre>
</li>
<li>
<p><strong>Location (ACS URL)</strong>: Locate <code>Location</code> at the bottom of the page and copy its value. This is the ACS URL that SigNoz needs to send SAML response to</p>
</li>
</ul>
</li>
<li>
<p>At last, assign the <strong>People</strong> or <strong>Groups</strong> to SigNoz application</p>
<ul>
<li>From <strong>Admin</strong> page, go to <strong>Directories</strong> &gt; <strong>People</strong> or <strong>Groups</strong></li>
<li>Select the specific people or groups</li>
<li>Go to <strong>Applications</strong> &gt; <strong>Assign applications</strong></li>
<li>Select <strong>SigNoz</strong> application &gt; <strong>Assign</strong> &gt; Hit <strong>Done</strong></li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/sso-authentication/#steps-to-be-performed-in-signoz-1
tag_set: userguide, sso-authentication
image_urls: 
tracking_id: docs-userguide-sso-authentication-steps-to-be-performed-in-signoz-1
group_tracking_ids: docs-userguide-sso-authentication
<p>Single Sign-on Authentication: SAML based Authentication: Steps to be performed in SigNoz</p>
<p><img src="https://signoz.io/img/docs/signoz-okta-sso-enable.gif" alt="Enable Okta SSO SAML in SigNoz" /></p>
<p><em>Enable Okta SSO SAML in SigNoz</em></p>
<ol>
<li>Go to <code>Settings</code>. Click on <code>Organization Settings</code> tab and locate <code>Authenticated Domains</code> in the page</li>
<li>Click <code>Add Domain</code></li>
<li>Enter the domain that your users would login with. For example, if your user names or emails are in format such as _<a href="mailto:john@example.com">john@example.com</a>
_ then you would have to enter <em>example.com</em> here.</li>
<li>After domain is added, Click on <code>Configure SSO</code> and choose <code>SAML Authentication</code> option</li>
<li>Enter values of tags <code>entity ID</code>, <code>Certificate Data</code> and <code>Location (ACS URL)</code> that you acquired from the metadata page (step 6 above)</li>
<li>Save the settings and log in from an incognito tab to test the setup. If you face difficulties signing in, review the query service logs. Also if you are admin and are unable to login because of faulty setup, then you may login with password using this URL: <code>http(s)://${SIGNOZ_BASEURL}/login?password=Y</code></li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#overview
tag_set: integrations, nginx
image_urls: 
tracking_id: docs-integrations-nginx-overview
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Nignx Logs - Overview</h2>
<p>This integration helps you to monitor Nginx server logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#prerequisites
tag_set: integrations, nginx
image_urls: 
tracking_id: docs-integrations-nginx-prerequisites
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>
<p>An Nginx server is running version newer than 1.0.0</p>
</li>
<li>
<p>An OpenTelemetry (OTEL) Collector with access to the Nginx server</p>
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the OTEL Collector</a>
(v0.88.0 or newer) if not done already</li>
<li>Ensure you can provide config files to the collector and set environment variables</li>
<li>The collector must be able to read the Nginx server log files</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#collecting-nginx-logs
tag_set: integrations, nginx
image_urls: 
tracking_id: docs-integrations-nginx-collecting-nginx-logs
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Collecting Nginx Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>nginx-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/nginx-access-logs:
    include: [&quot;${env:NGINX_ACCESS_LOG_FILE}&quot;]
    operators:
      # Parse the default nginx access log format. Nginx defaults to the &quot;combined&quot; log format
      # $remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;
      # For more details, see https://nginx.org/en/docs/http/ngx_http_log_module.html
      - type: regex_parser
        if: body matches '^(?P&lt;remote_addr&gt;[0-9\\.]+) - (?P&lt;remote_user&gt;[^\\s]+) \\[(?P&lt;ts&gt;.+)\\] &quot;(?P&lt;request_method&gt;\\w+?) (?P&lt;request_path&gt;.+?)&quot; (?P&lt;status&gt;[0-9]+) (?P&lt;body_bytes_sent&gt;[0-9]+) &quot;(?P&lt;http_referrer&gt;.+?)&quot; &quot;(?P&lt;http_user_agent&gt;.+?)&quot;$'
        parse_from: body
        parse_to: attributes
        regex: '^(?P&lt;remote_addr&gt;[0-9\.]+) - (?P&lt;remote_user&gt;[^\s]+) \[(?P&lt;ts&gt;.+)\] &quot;(?P&lt;request_method&gt;\w+?) (?P&lt;request_path&gt;.+?)&quot; (?P&lt;status&gt;[0-9]+) (?P&lt;body_bytes_sent&gt;[0-9]+) &quot;(?P&lt;http_referrer&gt;.+?)&quot; &quot;(?P&lt;http_user_agent&gt;.+?)&quot;$'
        timestamp:
          parse_from: attributes.ts
          layout: &quot;02/Jan/2006:15:04:05 -0700&quot;
          layout_type: gotime
        severity:
          parse_from: attributes.status
          overwrite_text: true
          mapping:
            debug: &quot;1xx&quot;
            info:
              - &quot;2xx&quot;
              - &quot;3xx&quot;
            warn: &quot;4xx&quot;
            error: &quot;5xx&quot;
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: add
        field: attributes.source
        value: nginx

  filelog/nginx-error-logs:
    include: [&quot;${env:NGINX_ERROR_LOG_FILE}&quot;]
    operators:
      # Parse the default nginx error log format.
      # YYYY/MM/DD HH:MM:SS [LEVEL] PID#TID: *CID MESSAGE
      # For more details, see https://github.com/phusion/nginx/blob/master/src/core/ngx_log.c
      - type: regex_parser
        if: body matches '^(?P&lt;ts&gt;.+?) \\[(?P&lt;log_level&gt;\\w+)\\] (?P&lt;pid&gt;\\d+)#(?P&lt;tid&gt;\\d+). \\*(?P&lt;cid&gt;\\d+) (?P&lt;message&gt;.+)$'
        parse_from: body
        parse_to: attributes
        regex: '^(?P&lt;ts&gt;.+?) \[(?P&lt;log_level&gt;\w+)\] (?P&lt;pid&gt;\d+)#(?P&lt;tid&gt;\d+). \*(?P&lt;cid&gt;\d+) (?P&lt;message&gt;.+)$'
        timestamp:
          parse_from: attributes.ts
          layout: &quot;2006/01/02 15:04:05&quot;
          layout_type: gotime
        severity:
          parse_from: attributes.log_level
          overwrite_text: true
          mapping:
            debug: &quot;debug&quot;
            info:
              - &quot;info&quot;
              - &quot;notice&quot;
            warn: &quot;warn&quot;
            error:
              - &quot;error&quot;
              - &quot;crit&quot;
              - &quot;alert&quot;
            fatal: &quot;emerg&quot;
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: move
        if: attributes.message  nil
        from: attributes.message
        to: body
      - type: add
        field: attributes.source
        value: nginx

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/nginx-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/nginx-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    logs/nginx:
      receivers: [filelog/nginx-access-logs, filelog/nginx-error-logs]
      processors: [batch]
      exporters: [otlp/nginx-logs]
</code></pre>
<p><strong>Note:</strong> If you are using a <a href="https://docs.nginx.com/nginx/admin-guide/monitoring/logging/#setting-up-the-access-log">custom Nginx log format</a>
, adjust the regex used for parsing logs in the receivers named <code>filelog/nginx-access-logs</code> and <code>filelog/nginx-error-logs</code> in the collector config.</p>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># path of Nginx access log file. must be accessible by the otel collector
# typically found at /usr/local/var/log/nginx/access.log on macOS
export NGINX_ACCESS_LOG_FILE=/var/log/nginx/access.log

# path of Nginx error log file. must be accessible by the otel collector
# typically found at /usr/local/var/log/nginx/error.log on macOS
export NGINX_ERROR_LOG_FILE=/var/log/nginx/error.log

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config nginx-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags in the collector run command.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#connect-nginx
tag_set: integrations, nginx
image_urls: https://signoz.io/img/docs/integrations/nginx/nginx-integration-search.webp, https://signoz.io/img/docs/integrations/nginx/nginx-integration-connect.webp, https://signoz.io/img/docs/integrations/nginx/nginx-integration-listening.webp
tracking_id: docs-integrations-nginx-connect-nginx
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Connect Nginx</h2>
<p>Once you're done with setting up Nginx for collecting logs, head over to the intergrations tab in SigNoz and search for the Nginx integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/nginx/nginx-integration-search.webp" alt="Search for Nginx in Integrations tab" /></p>
<p><em>Search for Nginx in Integrations tab</em></p>
<p>Click on the <code>Connect Nginx</code> Button, and select <strong>I have already configured</strong>, this will start listening for data from your Nginx server. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/nginx/nginx-integration-connect.webp" alt="Connect Nginx" /></p>
<p><em>Connect Nginx</em></p>
<p><img src="https://signoz.io/img/docs/integrations/nginx/nginx-integration-listening.webp" alt="Listening for data from Nginx" /></p>
<p><em>Listening for data from Nginx</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/nginx/#data-collected
tag_set: integrations, nginx
image_urls: https://signoz.io/img/docs/integrations/nginx/nginx-integration-data-collected.webp
tracking_id: docs-integrations-nginx-data-collected
group_tracking_ids: docs-integrations-nginx
<h2>Nignx Logs: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your Nginx Integrations, it shows you details about the different log attributes that you can monitor for Nginx. The tables below gives you a list of the different log attributes available.</p>
<p><img src="https://signoz.io/img/docs/integrations/nginx/nginx-integration-data-collected.webp" alt="Log attributes for Nginx" /></p>
<p><em>Log attributes details for Nginx</em></p>
<h3>## Nginx log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
<tr>
<td>Body Bytes Sent</td>
<td>attributes.body_bytes_sent</td>
<td>string</td>
</tr>
<tr>
<td>Referrer</td>
<td>attributes.http_referrer</td>
<td>string</td>
</tr>
<tr>
<td>User Agent</td>
<td>attributes.http_user_agent</td>
<td>string</td>
</tr>
<tr>
<td>Request Method</td>
<td>attributes.request_method</td>
<td>string</td>
</tr>
<tr>
<td>Request Path</td>
<td>attributes.request_path</td>
<td>string</td>
</tr>
<tr>
<td>Response Status Code</td>
<td>attributes.status</td>
<td>string</td>
</tr>
<tr>
<td>Remote Address</td>
<td>attributes.remote_addr</td>
<td>string</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/otlp-http-enable-cors/
tag_set: userguide, otlp-http-enable-cors
image_urls: https://signoz.io/img/blog/common/join_slack_cta.webp
tracking_id: docs-userguide-otlp-http-enable-cors
group_tracking_ids: docs-userguide-otlp-http-enable-cors
<h2>CORS in OTLP HTTP Receiver</h2>
<p>Enabling CORS (Cross-Origin Resource Sharing) in OTLP (OpenTelemetry Protocol) HTTP receiver is necessary when we want to send requests to our OTLP HTTP receiver from a different domain using browsers. Without enabling CORS, browsers will block these requests for security reasons.</p>
<p>By default, SigNoz does not enable CORS for OTLP HTTP receiver, which means it will block all requests sent via browsers due to security reasons. To change that, we will have to alter the CORS configurations in the OTLP HTTP receiver.</p>
<h2>Enable CORS in OTLP HTTP Receiver</h2>
<hr />
<p>To enable CORS, there are mainly three steps involved:</p>
<ol>
<li>
<p>Open SigNoz OtelCollector configuration and locate <code>http</code> receiver section. It should look similar to the following:</p>
<p>receivers:
otlp:
protocols:
http:</p>
</li>
<li>
<p>Add the CORS configuration under <code>http</code> receiver section.</p>
<p>http:
cors:
allowed_origins:
- https://netflix.com  # URL of your Frontend application</p>
</li>
</ol>
<p>Update the above example origin i.e. <code>https://netflix.com</code> with your frontend application URL. If you are experimenting on your local machine on port <code>3000</code>, you can use <code>http://localhost:3000</code> instead.</p>
<pre><code>http:
  cors:
    allowed_origins:
      - http://localhost:3000 
</code></pre>
<p>To allow any origin to make requests, you can use <code>*</code> wildcard character instead. However, it is not recommended to set allow all origins due to security risks.</p>
<pre><code>http:
  cors:
    allowed_origins:
      - &quot;*&quot;
</code></pre>
<ol start="3">
<li>Save the updated configurations and restart SigNoz OtelCollector.</li>
</ol>
<h3>## Docker</h3>
<p>In case of docker, update <code>otel-collector.yaml</code> to include CORS configuration inside <code>deploy/docker/clickhouse-setup</code> folder as instructed above.</p>
<p>Followed by clean removal of SigNoz OtelCollector to guarantee updatation of configuration and restart the SigNoz OtelCollector using <code>docker compose</code>:</p>
<pre><code># clean remove SigNoz OtelCollector
docker stop signoz-otel-collector
docker rm signoz-otel-collector

# restart SigNoz OtelCollector using `docker compose`
cd deploy
docker compose -f docker/clickhouse-setup/docker-compose.yaml up -d
</code></pre>
<h3>## Kubernetes</h3>
<p>In case of helm charts, copy the <code>otelCollector.config</code> section from <code>values.yaml</code> to <code>override-values.yaml</code> and include CORS configuration under <code>otlp.protocols.http</code>.</p>
<p>Followed by <code>helm upgrade</code> command below to restart OtelCollector:</p>
<pre><code>helm upgrade --namespace platform my-release signoz/signoz -f override-values.yaml
</code></pre>
<ul>
<li>
<p><a href="https://signoz.io/docs/instrumentation/angular/">Angular Intrumentation guide</a></p>
</li>
<li>
<p><a href="https://signoz.io/blog/opentelemetry-react/">React Instrumentation guide</a></p>
</li>
</ul>
<hr />
<p>If you have any feedback or facing issues, feel free to join our slack community to get help!</p>
<p><a href="https://signoz.io/slack"><img src="https://signoz.io/img/blog/common/join_slack_cta.webp" alt="SigNoz Slack community" /></a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#overview
tag_set: alerts-management, planned-maintenance
image_urls: 
tracking_id: docs-alerts-management-planned-maintenance-overview
group_tracking_ids: docs-alerts-management-planned-maintenance
<h2>Planned Maintenance/Downtime: Planned Maintenance/Downtime - Overview</h2>
<p>Use planned maintenance to schedule maintenance windows for your services. This helps you to avoid alerting during maintenance windows and to resume alerts after the maintenance window is over. The alert evaluation is paused during the maintenance window.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#create-a-maintenance-window
tag_set: alerts-management, planned-maintenance
image_urls: 
tracking_id: docs-alerts-management-planned-maintenance-create-a-maintenance-window
group_tracking_ids: docs-alerts-management-planned-maintenance
<h2>Planned Maintenance/Downtime: Create a Maintenance Window: Create a Maintenance Window</h2>
<p>To create a maintenance window, navigate to the <strong>Configuration</strong> tab in the <strong>Alerts</strong> section. Click on the <strong><code>+ New downtime</code></strong> button at the top right corner and fill in the details.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#types-of-maintenance-windows
tag_set: alerts-management, planned-maintenance
image_urls: 
tracking_id: docs-alerts-management-planned-maintenance-types-of-maintenance-windows
group_tracking_ids: docs-alerts-management-planned-maintenance
<p>Planned Maintenance/Downtime: Create a Maintenance Window: Types of maintenance windows</p>
<p>SigNoz supports two types of maintenance schedules:</p>
<ul>
<li><strong>One-time</strong>: A maintenance window that is scheduled at a specific time for a fixed duration.</li>
<li><strong>Recurring</strong>: A maintenance window that is scheduled at a specific time and repeats at a regular interval. Optionally, you can specify an end date for the maintenance window.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#one-time-maintenance-window
tag_set: alerts-management, planned-maintenance
image_urls: https://signoz.io/img/docs/product-features/alerts/one-time.png
tracking_id: docs-alerts-management-planned-maintenance-one-time-maintenance-window
group_tracking_ids: docs-alerts-management-planned-maintenance
<p>Planned Maintenance/Downtime: Create a Maintenance Window: One-time Maintenance Window</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/one-time.png" alt="one-time maintenance window" /></p>
<p><em>One-time maintenance window</em></p>
<p>Create a one-time maintenance window by clicking on the <strong><code>+ New downtime</code></strong> button at the top right corner. Since this is a one-time maintenance window, leave the <em>Repeats every</em> field with default value <code>Does not repeat</code>.</p>
<p>The following fields are required to create a one-time maintenance window:</p>
<ul>
<li><strong>Name</strong>: A descriptive name for the maintenance window.</li>
<li><strong>Start Time</strong>: The start time of the maintenance window in the selected timezone.</li>
<li><strong>End Time</strong>: The end time of the maintenance window in the selected timezone.</li>
<li><strong>Timezone</strong>: The timezone of the maintenance window. This is the timezone in which the maintenance window is scheduled.</li>
</ul>
<p>Example:</p>
<pre><code>Start time: 2024-08-01 22:00:00
End time: 2024-08-01 23:00:00
Timezone: America/New_York
</code></pre>
<p>This means that the maintenance window is scheduled for 22:00:00 to 23:00:00 in the America/New_York timezone.</p>
<p>The following fields are optional:</p>
<ul>
<li><strong>Silence Alerts</strong>: The alerts that are silenced during the maintenance window. If nothing is selected, all alerts are silenced.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/planned-maintenance/#recurring-maintenance-window
tag_set: alerts-management, planned-maintenance
image_urls: https://signoz.io/img/docs/product-features/alerts/recurring-daily.png
tracking_id: docs-alerts-management-planned-maintenance-recurring-maintenance-window
group_tracking_ids: docs-alerts-management-planned-maintenance
<p>Planned Maintenance/Downtime: Create a Maintenance Window: Recurring Maintenance Window</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/recurring-daily.png" alt="recurring maintenance window" /></p>
<p><em>Recurring maintenance window</em></p>
<p>Create a recurring maintenance window by clicking on the <strong><code>+ New downtime</code></strong> button at the top right corner. Since this is a recurring maintenance window, select the <em>Repeats every</em> field with the desired repetition type.</p>
<p>There are three types of repetition types available:</p>
<ul>
<li><strong>Daily</strong>: The maintenance window repeats every day.</li>
<li><strong>Weekly</strong>: The maintenance window repeats every week on a specific day(s) of the week. The weekly repetition type allows you to select the days of the week on which the maintenance window repeats.</li>
<li><strong>Monthly</strong>: The maintenance window repeats every month on a specific day of the month.</li>
</ul>
<p>The following fields are required to create a recurring maintenance window:</p>
<ul>
<li><strong>Name</strong>: A descriptive name for the maintenance window.</li>
<li><strong>Start Time</strong>: The start time of the maintenance window.</li>
<li><strong>Duration</strong>: The duration of the maintenance window.</li>
<li><strong>Timezone</strong>: The timezone of the maintenance window.</li>
<li><strong>Repeats every</strong>: The type of repetition.</li>
</ul>
<p>The following fields are optional:</p>
<ul>
<li><strong>Ends</strong>: The end date of the maintenance window.</li>
<li><strong>Silence Alerts</strong>: The alerts that are silenced during the maintenance window. If nothing is selected, all alerts are silenced.</li>
</ul>
<p>Examples:</p>
<ol>
<li>
<p>Daily maintenance window that repeats every day for 1 hour starting from 22:00:00 in the America/New_York timezone.</p>
<p>Start time: 2024-08-01 22:00:00
Repeats every: Daily
Duration: 1 hour
Timezone: America/New_York</p>
</li>
<li>
<p>Weekly maintenance window that repeats every week on Monday for 1 hour starting from 22:00:00 in the America/New_York timezone.</p>
<p>Start time: 2024-08-01 22:00:00
Repeats every: Weekly
Weekly Occurence: [Monday]
Duration: 1 hour
Timezone: America/New_York</p>
</li>
<li>
<p>Monthly maintenance window that repeats every month on the 12th day of the month for 1 hour starting from 22:00:00 in the America/New_York timezone.</p>
<p>Start time: 2024-08-12 22:00:00
Repeats every: Monthly
Duration: 1 hour
Timezone: America/New_York</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/slack/
tag_set: alerts-management, notification-channel, slack
image_urls: 
tracking_id: docs-alerts-management-notification-channel-slack
group_tracking_ids: docs-alerts-management-notification-channel-slack
<h2>Configure Slack Channel</h2>
<h2>Prerequisites</h2>
<hr />
<p>Before configuring Slack as a notification channel in SigNoz, ensure that you have:</p>
<ul>
<li><strong>Incoming Webhook</strong>: Follow the steps outlined in <a href="https://api.slack.com/messaging/webhooks">sending messages to slack using Incoming Webhook</a> to set up an Incoming Webhook in your Slack workspace.</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.5.0">v0.5.0</a> or later</li>
</ul>
<h2>Accessing Alert Channels</h2>
<hr />
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
<h2>Creating a new Notification channel</h2>
<hr />
<p>To create a new Slack notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Slack as the channel type.</li>
<li><strong>Webhook URL</strong>: Paste the Incoming Webhook URL generated in Slack.</li>
<li><strong>Recipient</strong>: Specify channel or user, use #channel-name, @username (has to be all lowercase, no whitespace)</li>
</ul>
<p>üìù Note</p>
<p>You can use <a href="https://prometheus.io/docs/alerting/latest/notifications/">Go templates</a> for customizing the title and description.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fnew-notification-channel.webp&amp;w=3840&amp;q=75" alt="new-notification-channel" /></p>
<p><strong>Test Configuration</strong>: Click the Test button to send a test alert to the configured Slack channel. This verifies that SigNoz can communicate with your Slack webhook.</p>
<h2>Editing a Notification channel</h2>
<hr />
<p>To edit an existing Slack notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the slack webhook URL and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fedit-notification-channel.webp&amp;w=3840&amp;q=75" alt="edit-notification-channel" /></p>
<h2>Receive Alert in Slack</h2>
<hr />
<p>Once the configuration is set up correctly, you will receive alerts in the configured Slack channel whenever monitored metrics exceed the specified thresholds in alert rules. This ensures you are promptly notified of any issues in your applications or infrastructure components.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts-in-slack.webp&amp;w=1920&amp;q=75" alt="alerts-in-slack" /></p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter issues:</p>
<ul>
<li><strong>Check the Webhook URL</strong>: Ensure the webhook URL is correctly entered in SigNoz.</li>
<li><strong>Verify Slack Permissions</strong>: Confirm that the webhook has permissions to post to the desired channel.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#overview
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-overview
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Clickhouse Metrics and Logs - Overview</h2>
<p>This integration helps you monitor key Clickhouse metrics and logs, view them with an out-of-the-box dashboard, and collect query logs for better querying and aggregation.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#prerequisites
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-prerequisites
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Prerequisites</h2>
<p>Before you begin, ensure you have:</p>
<ol>
<li>A Clickhouse server running version 23 or newer</li>
<li>An OpenTelemetry (OTEL) Collector with access to the Clickhouse server
<ul>
<li><a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Install the SigNoz OTEL Collector</a>
(v0.88.0 or newer, v0.88.23+ for query log collection) if not done already</li>
<li>Ensure you can provide config files and set environment variables for the collector</li>
<li>Ensure the OTEL collector has access to the Clickhouse server for metrics collection and can read the Clickhouse server log file for logs collection</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#preparing-clickhouse-server
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-preparing-clickhouse-server
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Preparing Clickhouse Server</h2>
<h3>## Check Clickhouse Version</h3>
<p>Ensure your Clickhouse server is running a supported version (v23 or newer):</p>
<pre><code>SELECT version();
</code></pre>
<h3>## Configure Prometheus Metrics Export</h3>
<p>If collecting metrics, ensure that Clickhouse is configured to export Prometheus metrics. Follow the Clickhouse Prometheus configuration guide if needed.</p>
<h3>## Create Monitoring User</h3>
<p>If collecting query logs, create a monitoring user with required permissions:</p>
<pre><code>CREATE USER monitoring IDENTIFIED BY 'monitoring_password';
GRANT SELECT ON system.query_log to monitoring;
-- If monitoring a clustered deployment, also grant privilege for executing remote queries
GRANT REMOTE ON *.* TO 'monitoring' on CLUSTER 'cluster_name';
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#collecting-clickhouse-metrics
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-collecting-clickhouse-metrics
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Collecting Clickhouse Metrics</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>clickhouse-metrics-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  prometheus/clickhouse:
    config:
      global:
        scrape_interval: 60s
      scrape_configs:
        - job_name: clickhouse
          static_configs:
            - targets:
                - ${env:CLICKHOUSE_PROM_METRICS_ENDPOINT}
          metrics_path: ${env:CLICKHOUSE_PROM_METRICS_PATH}

processors:
  # enriches the data with additional host information
  # see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#resource-detection-processor
  resourcedetection/system:
    # add additional detectors if needed
    detectors: [&quot;system&quot;]
    system:
      hostname_sources: [&quot;os&quot;]

exporters:
  # export to SigNoz cloud
  otlp/clickhouse:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/clickhouse:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    metrics/clickhouse:
      receivers: [prometheus/clickhouse]
      # note: remove this processor if the collector host is not running on the same host as the clickhouse instance
      processors: [resourcedetection/system]
      exporters: [otlp/clickhouse]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># Prometheus metrics endpoint on the clickhouse server
export CLICKHOUSE_PROM_METRICS_ENDPOINT=&quot;clickhouse:9363&quot;

# Prometheus metrics path on the clickhouse server
export CLICKHOUSE_PROM_METRICS_PATH=&quot;/metrics&quot;

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.us.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config clickhouse-metrics-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#collecting-clickhouse-server-logs
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-collecting-clickhouse-server-logs
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Collecting Clickhouse Server Logs</h2>
<p>You can configure Clickhouse server logs collection by providing the required collector config to your collector.</p>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>clickhouse-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  filelog/clickhouse:
    include: [&quot;${env:CLICKHOUSE_LOG_FILE}&quot;]
    operators:
      # Parse default clickhouse text log format.
      # See https://github.com/ClickHouse/ClickHouse/blob/master/src/Loggers/OwnPatternFormatter.cpp
      - type: recombine
        source_identifier: attributes[&quot;log.file.name&quot;]
        is_first_entry: body matches '^\\d{4}\\.\\d{2}\\.\\d{2}\\s+'
        combine_field: body
        overwrite_with: oldest
      - type: regex_parser
        parse_from: body
        if: body matches '^(?P&lt;ts&gt;\\d{4}\\.\\d{2}\\.\\d{2} \\d{2}:\\d{2}:\\d{2}.?[0-9]*)\\s+\\[\\s+(\\x1b.*?m)?(?P&lt;thread_id&gt;\\d*)(\\x1b.*?m)?\\s+\\]\\s+{((\\x1b.*?m)?(?P&lt;query_id&gt;[0-9a-zA-Z-_]*)(\\x1b.*?m)?)?}\\s+&lt;(\\x1b.*?m)?(?P&lt;log_level&gt;\\w*)(\\x1b.*?m)?&gt;\\s+((\\x1b.*?m)?(?P&lt;clickhouse_component&gt;[a-zA-Z0-9_]+)(\\x1b.*?m)?:)?\\s+(?s)(?P&lt;message&gt;.*)$'
        regex: '^(?P&lt;ts&gt;\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}.?[0-9]*)\s+\[\s+(\x1b.*?m)?(?P&lt;thread_id&gt;\d*)(\x1b.*?m)?\s+\]\s+{((\x1b.*?m)?(?P&lt;query_id&gt;[0-9a-zA-Z-_]*)(\x1b.*?m)?)?}\s+&lt;(\x1b.*?m)?(?P&lt;log_level&gt;\w*)(\x1b.*?m)?&gt;\s+((\x1b.*?m)?(?P&lt;clickhouse_component&gt;[a-zA-Z0-9_]+)(\x1b.*?m)?:)?\s+(?s)(?P&lt;message&gt;.*)$'
      - type: time_parser
        if: attributes.ts  nil
        parse_from: attributes.ts
        layout_type: gotime
        layout: 2006.01.02 15:04:05.999999
        location: ${env:CLICKHOUSE_TIMEZONE}
      - type: remove
        if: attributes.ts  nil
        field: attributes.ts
      - type: severity_parser
        if: attributes.log_level  nil
        parse_from: attributes.log_level
        overwrite_text: true
        # For mapping details, see getPriorityName defined in https://github.com/ClickHouse/ClickHouse/blob/master/src/Interpreters/InternalTextLogsQueue.cpp
        mapping:
          trace:
            - Trace
            - Test
          debug: Debug
          info:
            - Information
            - Notice
          warn: Warning
          error: Error
          fatal:
            - Fatal
            - Critical
      - type: remove
        if: attributes.log_level  nil
        field: attributes.log_level
      - type: move
        if: attributes.message  nil
        from: attributes.message
        to: body
      - type: add
        field: attributes.source
        value: clickhouse

processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s

exporters:
  # export to SigNoz cloud
  otlp/clickhouse-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/clickhouse-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    logs/clickhouse:
      receivers: [filelog/clickhouse]
      processors: [batch]
      exporters: [otlp/clickhouse-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># Path of Clickhouse server log file
export CLICKHOUSE_LOG_FILE=&quot;/var/log/clickhouse-server/server.log&quot;

# Timezone of the Clickhouse server
export CLICKHOUSE_TIMEZONE=&quot;Etc/UTC&quot;

# Region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# Your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config clickhouse-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong> The collector can use multiple config files by specifying multiple <code>--config</code> flags.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#collecting-clickhouse-query-logs
tag_set: integrations, clickhouse
image_urls: 
tracking_id: docs-integrations-clickhouse-collecting-clickhouse-query-logs
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Collecting Clickhouse Query Logs</h2>
<h3>## Step 1: Create the Collector Config File</h3>
<p>Create a file named <code>clickhouse-query-logs-collection-config.yaml</code> with the following content:</p>
<pre><code>receivers:
  clickhousesystemtablesreceiver/query_log:
    dsn: &quot;${env:CLICKHOUSE_MONITORING_DSN}&quot;
    cluster_name: &quot;${env:CLICKHOUSE_CLUSTER_NAME}&quot;
    query_log_scrape_config:
      scrape_interval_seconds: ${env:QUERY_LOG_SCRAPE_INTERVAL_SECONDS}
      min_scrape_delay_seconds: ${env:QUERY_LOG_SCRAPE_DELAY_SECONDS}

exporters:
  # export to SigNoz cloud
  otlp/clickhouse-query-logs:
    endpoint: &quot;${env:OTLP_DESTINATION_ENDPOINT}&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;${env:SIGNOZ_INGESTION_KEY}&quot;

  # export to local collector
  # otlp/clickhouse-query-logs:
  #   endpoint: &quot;localhost:4317&quot;
  #   tls:
  #     insecure: true

service:
  pipelines:
    logs/clickhouse-query-logs:
      receivers: [clickhousesystemtablesreceiver/query_log]
      processors: []
      exporters: [otlp/clickhouse-query-logs]
</code></pre>
<h3>## Step 2: Set Environment Variables</h3>
<p>Set the following environment variables:</p>
<pre><code># DSN for connecting to clickhouse with the monitoring user
export CLICKHOUSE_MONITORING_DSN=&quot;tcp://monitoring:monitoring_password@clickhouse:9000/&quot;

# If collecting query logs from a clustered deployment, specify a non-empty cluster name
export CLICKHOUSE_CLUSTER_NAME=&quot;&quot;

# Rows from query_log table will be collected periodically based on this setting
export QUERY_LOG_SCRAPE_INTERVAL_SECONDS=20

# Must be greater than flush_interval_milliseconds setting for query_log
export QUERY_LOG_SCRAPE_DELAY_SECONDS=8

# region specific SigNoz cloud ingestion endpoint
export OTLP_DESTINATION_ENDPOINT=&quot;ingest.{REGION}.signoz.cloud:443&quot;

# your SigNoz ingestion key
export SIGNOZ_INGESTION_KEY=&quot;signoz-ingestion-key&quot;
</code></pre>
<p>You can find more details about ingestion keys and Regions <a href="https://signoz.io/docs/ingestion/signoz-cloud/overview/">here</a></p>
<h3>## Step 3: Use the Collector Config File</h3>
<p>Add the following flag to your collector run command:</p>
<pre><code>--config clickhouse-query-logs-collection-config.yaml
</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>Only one collector instance should be configured to collect <code>query_logs</code>. Using multiple instances or replicas will lead to duplicate logs.</li>
<li>The collector can use multiple config files by specifying multiple <code>--config</code> flags.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#connect-clickhouse
tag_set: integrations, clickhouse
image_urls: https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-search.webp, https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-connect.webp, https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-listening.webp
tracking_id: docs-integrations-clickhouse-connect-clickhouse
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Connect Clickhouse</h2>
<p>Once you've set up Clickhouse for collecting metrics and logs, go to the integrations tab in SigNoz and search for the Clickhouse integration.</p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-search.webp" alt="Search for Clickhouse in Integrations tab" /></p>
<p><em>Search for Clickhouse in Integrations tab</em></p>
<p>Click on the <code>Connect Clickhouse</code> Button, and select <strong>I have already configured</strong>. This will start listening for data from your Clickhouse instance. To stop this, you can select the <code>Remove from SigNoz</code> button.</p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-connect.webp" alt="Connect Clickhouse" /></p>
<p><em>Connect Clickhouse</em></p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-listening.webp" alt="Listening for data from Clickhouse" /></p>
<p><em>Listening for data from Clickhouse</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#clickhouse-dashboard
tag_set: integrations, clickhouse
image_urls: https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-dashboard.webp
tracking_id: docs-integrations-clickhouse-clickhouse-dashboard
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Clickhouse dashboard</h2>
<p>Once SigNoz has started listening to your Clickhouse data, go to the Dashboards tab and search for Clickhouse. This will show you a newly created dashboard which displays various Clickhouse metrics and query log information.</p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-dashboard.webp" alt="Dashboard for monitoring Clickhouse Metrics" /></p>
<p><em>Dashboard for monitoring Clickhouse Metrics</em></p>
<h3>## Dashboard asset</h3>
<p>You can also manually create the above Clickhouse Dashboard by importing the JSON file available <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/clickhouse/assets/dashboards/overview.json">here</a>. To learn how to create Dashboards, checkout this <a href="https://signoz.io/docs/userguide/manage-dashboards/#steps-to-create-a-custom-dashboard">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/integrations/clickhouse/#data-collected
tag_set: integrations, clickhouse
image_urls: https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-data-collected.webp
tracking_id: docs-integrations-clickhouse-data-collected
group_tracking_ids: docs-integrations-clickhouse
<h2>Clickhouse Metrics and Logs: Data Collected</h2>
<p>When you switch to the <strong>Data Collected</strong> tab of your Clickhouse Integration, it shows you details about the different metrics types, server log attributes, and query log attributes that you can monitor for your Clickhouse instance.</p>
<p><img src="https://signoz.io/img/docs/integrations/clickhouse/clickhouse-integration-data-collected.webp" alt="Log attributes and metrics details for Clickhouse" /></p>
<p><em>Log attributes and metrics details for Clickhouse</em></p>
<h3>## Clickhouse log attributes</h3>
<ul>
<li><strong>Name</strong>: The name of the log attribute.</li>
<li><strong>Path</strong>: The specific location or attribute within a log entry where the corresponding data can be found.</li>
<li><strong>Type</strong>: The data type of the log attribute.</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Path</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timestamp</td>
<td>timestamp</td>
<td>timestamp</td>
</tr>
<tr>
<td>Severity Text</td>
<td>severity_text</td>
<td>string</td>
</tr>
<tr>
<td>Severity Number</td>
<td>severity_number</td>
<td>number</td>
</tr>
<tr>
<td>Thread ID</td>
<td>attributes.thread_id</td>
<td>string</td>
</tr>
<tr>
<td>Query ID</td>
<td>attributes.query_id</td>
<td>string</td>
</tr>
<tr>
<td>Clickhouse Component</td>
<td>attributes.clickhouse_component</td>
<td>string</td>
</tr>
</tbody>
</table>
<h3>## Clickhouse metrics</h3>
<ul>
<li><strong>Name</strong>: The name of the metric.</li>
<li><strong>Type</strong>: The type of the metric (e.g., Sum, Gauge).</li>
<li><strong>Unit</strong>: The unit of measurement for the metric.</li>
<li><strong>Description</strong>: A brief description of what the metric represents.</li>
</ul>
<p>To find a complete list of metrics you can checkout this <a href="https://github.com/SigNoz/signoz/blob/a5d58008713c23431fba26200a68ce7d9fc49e1e/pkg/query-service/app/integrations/builtin_integrations/clickhouse/data-collected.json">link</a> or the <strong>Data Collected</strong> tab.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/
tag_set: instrumentation
image_urls: 
tracking_id: docs-instrumentation
group_tracking_ids: docs-instrumentation
<h2>Instrument your Application</h2>
<p>To instrument your applications and send data to SigNoz, follow the instructions in the sections below.</p>
<ul>
<li><a href="/docs/instrumentation/python">üìÑÔ∏è Python: Send events from you Python application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/java">üìÑÔ∏è Java: Send events from you Java application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/javascript">üìÑÔ∏è Javascript: Send events from you Javascript application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/golang">üìÑÔ∏è Golang (Go): Send events from you Golang (Go) application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/php">üìÑÔ∏è PHP: Send events from you PHP application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/dotnet">üìÑÔ∏è .NET: Send events from you .NET application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/ruby-on-rails">üìÑÔ∏è Ruby on Rails: Send events from you Ruby on Rails application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/elixir">üìÑÔ∏è Elixir: Send events from you Elixir application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/rust">üìÑÔ∏è Rust: Send events from you Rust application to SigNoz.</a></li>
<li><a href="/docs/instrumentation/swift">üìÑÔ∏è Swift: Send events from you Swift application to SigNoz.</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/webhook/
tag_set: alerts-management, notification-channel, webhook
image_urls: 
tracking_id: docs-alerts-management-notification-channel-webhook
group_tracking_ids: docs-alerts-management-notification-channel-webhook
<h2>Configure Webhook Channel</h2>
<h2>Prerequisites</h2>
<hr />
<p>Before configuring a Webhook channel in SigNoz, ensure that you have:</p>
<ul>
<li>
<p><strong>Webhook Application</strong>: Have an application ready to accept webhook messages.</p>
</li>
<li>
<p><strong>Webhook URL</strong>: Obtain a valid webhook URL reachable from SigNoz Alert Manager.</p>
</li>
<li>
<p><strong>SigNoz Version</strong>: Ensure you are using SigNoz version Webhook <a href="https://github.com/SigNoz/signoz/releases/tag/v0.7.4">v0.7.4</a> or later</p>
</li>
</ul>
<h2>Accessing Alert Channels</h2>
<hr />
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
<h2>Creating a new Webhook channel</h2>
<hr />
<p>To create a new Webhook notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Webhook as the channel type.</li>
<li><strong>Webhook URL</strong>: Enter the Webhook URL endpoint.</li>
<li><strong>Username and Password</strong> (Optional): Provide the necessary credentials for authentication.</li>
</ul>
<p><strong>Test Configuration</strong>: Click the Test button to test the connection with your application.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fwebhook-1.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<h2>Editing a Webhook channel</h2>
<hr />
<p>To edit an existing webhook notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the webhook URL and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fwebhook-2.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<h2>Receive Alert through Webhook</h2>
<hr />
<p>Once the configuration is set up correctly, you will receive alerts in your application through the configured Webhook channel whenever monitored metrics exceed the specified thresholds in alert rules.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fwebhook-3.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<h2>Sample Webhook message</h2>
<hr />
<p>A webhook message may contain multiple alerts. By default, the SigNoz alert manager groups alerts by the alert name and delivers grouped messages every 5 minutes.</p>
<p>For resolved alerts, the alert manager will send the time of resolution in <em>endsAt</em>. You can also use the fingerprint property to identify and process updates sent by the alert manager.</p>
<pre><code>{
   &quot;receiver&quot;:&quot;w1&quot;,
   &quot;status&quot;:&quot;firing&quot;,
   &quot;alerts&quot;:[\
      {\
         &quot;status&quot;:&quot;firing&quot;,\
         &quot;labels&quot;:{\
            &quot;alertname&quot;:&quot;DiskRunningFull&quot;,\
            &quot;dev&quot;:&quot;sda3&quot;,\
            &quot;instance&quot;:&quot;example3&quot;,\
            &quot;severity&quot;:&quot;critical&quot;\
         },\
         &quot;annotations&quot;:{\
            &quot;info&quot;:&quot;The disk sda3 is running full&quot;,\
            &quot;summary&quot;:&quot;please check the instance example1&quot;\
         },\
         &quot;startsAt&quot;:&quot;2022-04-25T14:35:19.490146+05:30&quot;,\
         &quot;endsAt&quot;:&quot;0001-01-01T00:00:00Z&quot;,\
         &quot;generatorURL&quot;:&quot;&quot;,\
         &quot;fingerprint&quot;:&quot;ad592b0afcbe2e79&quot;\
      }\
   ],
   &quot;groupLabels&quot;:{
      &quot;alertname&quot;:&quot;DiskRunningFull&quot;
   },
   &quot;commonLabels&quot;:{
      &quot;alertname&quot;:&quot;DiskRunningFull&quot;,
      &quot;dev&quot;:&quot;sda3&quot;,
      &quot;instance&quot;:&quot;example3&quot;,
      &quot;severity&quot;:&quot;critical&quot;
   },
   &quot;commonAnnotations&quot;:{
      &quot;info&quot;:&quot;The disk sda3 is running full&quot;,
      &quot;summary&quot;:&quot;please check the instance example1&quot;
   },
   &quot;externalURL&quot;:&quot;http://Apples-MacBook-Pro-3.local:9093&quot;,
   &quot;version&quot;:&quot;4&quot;,
   &quot;groupKey&quot;:&quot;{}/{}:{alertname=\&quot;DiskRunningFull\&quot;}&quot;,
   &quot;truncatedAlerts&quot;:0
}
</code></pre>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter issues:</p>
<ul>
<li>
<p><strong>Check the Webhook URL</strong>: Ensure the webhook URL is correctly entered in SigNoz.</p>
</li>
<li>
<p><strong>Verify Webhook Permissions</strong>: Confirm that the webhook has permissions to post alerts to the desired endpoint.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/faqs/troubleshooting/
tag_set: faqs, troubleshooting
image_urls: 
tracking_id: docs-faqs-troubleshooting
group_tracking_ids: docs-faqs-troubleshooting
<h2>Troubleshooting - FAQs</h2>
<h3>## How to run SigNoz in debug mode?</h3>
<p>You might want to follow our troubleshooting docs.</p>
<p>Refer here:</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/install/troubleshooting/">SigNoz Troubleshooting Docs</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/troubleshoot">SigNoz Troubleshoot Github Repository</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=Y7OkvmuTRQ8">SigNoz YouTube Video on Troubleshooting</a></p>
</li>
</ul>
<h3>## How do I know if SigNoz is accessible from my Application?</h3>
<p>We have a troubleshooting guide to check if SigNoz is accessible from your application or not or, is the instrumentation not working or the application is not instrumented in the first place?</p>
<p>Set¬†<code>OTEL_TRACES_EXPORTER=console</code>¬†and observe. If it doesn‚Äôt output the traces to the stdout, the instrumentation is not working or your application isn‚Äôt correctly instrumented in the first place.</p>
<p>Refer here:</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/install/troubleshooting/#kubernetes">SigNoz Troubleshooting Docs</a></p>
</li>
<li>
<p><a href="https://github.com/SigNoz/troubleshoot">SigNoz Troubleshoot Github Repository</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=Y7OkvmuTRQ8">SigNoz YouTube Video on Troubleshooting</a></p>
</li>
</ul>
<h3>## I have installed SigNoz on Windows Kubernetes, but I can't make it work.</h3>
<p>We don't support Microsoft Windows as of now.</p>
<h3>## I am not seeing all my services related to my application listed in the Services tab, what could be the potential reason?</h3>
<p>We rely on the semantic conventions provided by OpenTelemetry. Every unique¬†<code>service.name</code> ¬†configured and received is part of the service list. You can read more about it from our <a href="https://signoz.io/docs/userguide/metrics/#open-the-services-section">docs</a>.</p>
<h3>## My services are not showing up in the Service Map section (but present in the services and traces tab), what should I do?</h3>
<p>You might need to zoom out a bit to see your service. Also, if you‚Äôre getting started the service map shows services from the sample hotrod application. Since your services are not connected to the hotrod application, it will appear isolated.</p>
<h3>## I am trying to change the retention period of Traces but the process gets stuck everytime.</h3>
<p>The process to change the retention period is resource-intensive, especially if you have a large amount of data ingested. The TTL (Time-to-Live) status table in the Signoz SQLite database may be stuck, preventing the retention period change from completing.</p>
<p>If process is stuck then you clear TTL table to try again.</p>
<ol>
<li>You can connect to SQLite DB and clear TTL status table to allow updating retention setting. If you are using docker follow below steps:</li>
</ol>
<ul>
<li>
<p>Connect to query-service</p>
<p>docker exec -it query-service sh</p>
</li>
</ul>
<ol start="2">
<li>Run the following commands:</li>
</ol>
<ul>
<li>
<p>Install sqlite</p>
<p>apk update
apk add sqlite</p>
</li>
<li>
<p>Open sqlite with signoz.db</p>
<p>sqlite3 /var/lib/signoz/signoz.db</p>
</li>
<li>
<p>(sqlite shell) check existing ttl status</p>
<p>select * from ttl_status;</p>
</li>
<li>
<p>Delete all rows of ttl_status</p>
<p>DELETE FROM ttl_status;</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/faqs/instrumentation/
tag_set: faqs, instrumentation
image_urls: 
tracking_id: docs-faqs-instrumentation
group_tracking_ids: docs-faqs-instrumentation
<h2>Instrumentation - FAQs</h2>
<p>For instrumentation instructions, follow our docs <a href="https://signoz.io/docs/instrumentation/">here</a>.</p>
<h3>## What are all the ports that will be used by a running instance of SigNoz and its associated dependencies so that I can check with my application ports to avoid conflicts.</h3>
<p>Ensure that the ports¬†<code>8080,</code> <code>3301</code>,¬†<code>4317</code>and¬†<code>4318</code>are open on the machine where you install SigNoz.</p>
<h3>## Do I still use OpenTelemetry SDKs to instrument ourselves and just use SigNoz as an analysis backend? Do I have to use SigNoz for instrumentation too?</h3>
<p>You have to instrument your application using OpenTelemetry SDKs.</p>
<p>Link for Instrumentation using SigNoz - <a href="https://signoz.io/docs/instrumentation">https://signoz.io/docs/instrumentation</a></p>
<p>Once your application is instrumented, you can point your OpenTelemetry exporter to send data to SigNoz installation. By defualt, SigNoz listens on port <code>4317</code> and <code>4318</code> for incoming telemetry data.</p>
<h3>## Which all languages/tech stack is currently supported with SigNoz for instrumentation?</h3>
<p>Python, JavaScript, Java, .NET, Ruby, Rust, Go, Elixir/Erlang, PHP.</p>
<p>Find the documentation for them here - <a href="https://signoz.io/docs/instrumentation/">https://signoz.io/docs/instrumentation/</a></p>
<h3>## Can I use auto instrumentation for my application(s)?</h3>
<p>OpenTelemetry and thus SigNoz, currently supports JavaScript, Java, Ruby, .NET, and Python modules for auto instrumentation.</p>
<p>Refer here: <a href="https://opentelemetry.io"></a>
<a href="https://opentelemetry.io/">https://opentelemetry.io/</a></p>
<p>If your module is auto-instrumentation is supported, you don't need code level changes but just need to add some more libraries in your application.</p>
<h3>## I am confused about <code>&lt;IP of SigNoz&gt;</code> can you provide some examples?</h3>
<p>IP of SigNoz means the host IP OR the IP of the instance where you have installed SigNoz, it could be either your local machine, the cloud providers, or VMs.</p>
<ul>
<li>IP is <code>localhost</code> or <code>127.0.0.1</code> - If it's installed on your local machine</li>
<li>IP is <code>xxx.xx.xx.xx</code>, where x is the public IP address of your AWS, Azure, GCP, or other cloud providers.</li>
<li>IP is<code>http://example.com</code> if SigNoz is hosted on your custom domain.</li>
</ul>
<p>So, to summarize, the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> would look something like this.</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot;

OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://127.0.0.1:4317&quot;

OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://netflix.com:4317&quot;

OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://xxx.xx.xx.xx:4317&quot;
</code></pre>
<p>You can also refer to this <a href="https://signoz.io/docs/instrumentation/troubleshoot-instrumentation/">grid</a> for help.</p>
<h3>## Does SigNoz have some agents for other servers from where I might want to collect data?</h3>
<p>You need to use OpenTelemetry Collector in your application servers to send host metrics data to SigNoz.</p>
<p>Refer here: <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#prerequisites
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-prerequisites
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Configure PagerDuty Channel - Prerequisites</h2>
<p>Before setting up PagerDuty as a notification channel in SigNoz, ensure the following:</p>
<ul>
<li><strong>Valid Integration Key</strong>: You have a valid Integration Key (also known as Routing Key) from PagerDuty.</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.8.0">v0.8.0</a> or later</li>
</ul>
<p>There are two main ways to integrate with PagerDuty:</p>
<ul>
<li>**Global <a href="https://support.pagerduty.com/docs/event-orchestration">Event Orchestration</a>
**: Ideal for automating incident creation and management.</li>
<li>**Direct Integration on <a href="https://support.pagerduty.com/docs/services-and-integrations">PagerDuty service</a>
**: Suitable for specific service-level integrations.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#accessing-alert-channels
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-accessing-alert-channels
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Accessing Alert Channels</h2>
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>
<p>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#obtaining-integration-or-routing-key
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-obtaining-integration-or-routing-key
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Obtaining Integration or Routing key</h2>
<h3>## For Global Event Orchestration</h3>
<ol>
<li>From the <strong>Automation</strong> menu, select <strong>Event Orchestration</strong></li>
<li>Create a new orchestration</li>
<li>Click on <strong>Global Orchestration Key</strong> and copy your <strong>integration key</strong> and keep it safe for later use.</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-1.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<h3>## For PagerDuty Service Integration</h3>
<ol>
<li>Navigate to <strong>Services &gt; Service Directory</strong> and select the <strong>service</strong> where you‚Äôd like to add the integration.</li>
<li>Select <strong>Integration tab</strong> and click <strong>Add another integration</strong></li>
<li>Select <strong>Events API V2</strong> from the list</li>
<li>Click <strong>Add</strong></li>
<li>Locate your integration in the list and click down arrow to view and copy integration key.</li>
</ol>
<p>For more details on PagerDuty service setup, visit <a href="https://support.pagerduty.com/docs/services-and-integrations#add-integrations-to-an-existing-service">PagerDuty Service and Integrations</a>.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-2.webp&amp;w=1920&amp;q=75" alt="image" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#create-a-new-pagerduty-channel
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-create-a-new-pagerduty-channel
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Create a New PagerDuty Channel</h2>
<p>To create a new PagerDuty notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select PagerDuty as the channel type.</li>
<li><strong>Routing Key</strong>: Enter Routing Key (Integration Key) obtained from PagerDuty.</li>
<li><strong>More information</strong>: Enter more information as necessary. Refer to the <a href="https://developer.pagerduty.com/docs/ZG9jOjExMDI5NTgw-events-api-v2-overview">Events API V2 Overview</a> for more details.</li>
</ul>
<p>üìù Note</p>
<p>You can use <a href="https://prometheus.io/docs/alerting/latest/notifications/">Go templates</a> for customizing the title and description.</p>
<p><strong>Test Configuration</strong>: Click the Test button to test the connection with your application.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-3.webp&amp;w=3840&amp;q=75" alt="image" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#testing-the-pagerduty-channel
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-testing-the-pagerduty-channel
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Testing the PagerDuty channel</h2>
<ol>
<li>
<p>Let's create a simple alert rule that monitors average CPU performance for each host. Go to <strong>Alerts</strong> page in <strong>your SigNoz app</strong> and click <code>New Alert</code> button. When the new alert page opens, edit metric query as shown below. Feel free to choose any other metric, the idea is to pick a metric with sufficient data to raise an alert.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-4.webp&amp;w=1920&amp;q=75" alt="image" /></p>
</li>
<li>
<p>We can now <strong>review the graph</strong> to identify a threshold that will definitely cause an alert. Here, anything below 0.2 looks like a good condition for threshold.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-5.webp&amp;w=1920&amp;q=75" alt="image" /></p>
</li>
<li>
<p>Let's <strong>set threshold to 0.12</strong> to be sure that alert will be raised in next few minutes.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-6.webp&amp;w=1920&amp;q=75" alt="image" /></p>
</li>
<li>
<p><strong>Save the alert</strong> rule. Feel free to edit severity and labels as necessary.</p>
</li>
<li>
<p>Go to your <strong>PagerDuty Alerts Dashboard</strong> (<code>PagerDuty Home &gt;&gt; Incident &gt;&gt; Alerts</code>) and wait for a few minutes. If all goes well, you will <strong>see an incident</strong>. You may have to refresh the page few times to see the alert.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-7.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-8.webp&amp;w=1920&amp;q=75" alt="image" /></p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#troubleshooting
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-troubleshooting
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Troubleshooting</h2>
<p>If you encounter issues with the PagerDuty integration:</p>
<ul>
<li><strong>Check the Integration Key</strong>: Ensure that the Routing Key or Integration Key is correctly entered in SigNoz.</li>
<li><strong>Verify PagerDuty Configuration</strong>: Confirm that the PagerDuty service or global event orchestration is set up correctly.</li>
<li><strong>Test Connectivity</strong>: Use the Test button in SigNoz to verify connectivity with PagerDuty. If the test fails, review your network settings and integration key.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/opsgenie/
tag_set: alerts-management, notification-channel, opsgenie
image_urls: 
tracking_id: docs-alerts-management-notification-channel-opsgenie
group_tracking_ids: docs-alerts-management-notification-channel-opsgenie
<h2>Configure Opsgenie Channel</h2>
<h2>Prerequisites</h2>
<hr />
<p>Before setting up Opsgenie as a notification channel in SigNoz, ensure the following:</p>
<ul>
<li><strong>Create Integration and Obtain API Key</strong>: You need to create an integration in Opsgenie and obtain an API key. Follow the steps provided <a href="https://support.atlassian.com/opsgenie/docs/integrate-opsgenie-with-prometheus/">here</a> to create an integration and obtain the necessary API key.</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.28.0">v0.28.0</a> or later</li>
</ul>
<h2>Accessing Alert Channels</h2>
<hr />
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
<h2>Creating a new Notification channel</h2>
<hr />
<p>To create a new Opsgenie notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Opsgenie as the channel type.</li>
<li><strong>API Key</strong>: Enter the API Key obtained from Opsgenie.</li>
<li>Customize the message, description, and priority using <a href="https://prometheus.io/docs/alerting/latest/notifications/">go templates</a>
.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fopsgenie-new-channel.webp&amp;w=3840&amp;q=75" alt="new-notification-channel" /></p>
<p><strong>Test Configuration</strong>: Click the Test button to send a test alert to the configured Opsgenie channel. This verifies that SigNoz can communicate with your Opsgenie.</p>
<h2>Editing a Notification channel</h2>
<hr />
<p>To edit an existing Opsgenie notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the opsgenie API Key and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>
<h2>Receiving Alerts in Opsgenie</h2>
<hr />
<p>Once configured correctly, alerts from SigNoz will appear in Opsgenie Alerts whenever monitored metrics cross the thresholds specified in your alert rules.This ensures you are promptly notified of any issues in your applications or infrastructure components.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-in-opsgenie.webp&amp;w=3840&amp;q=75" alt="alert-in-opsgenie" /></p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter issues with the Opsgenie integration:</p>
<ul>
<li><strong>Check the API Key</strong>: Ensure that the API Key entered in SigNoz matches the one provided by Opsgenie.</li>
<li><strong>Verify Opsgenie Integration</strong>: Confirm that the integration in Opsgenie is correctly set up and active.</li>
<li><strong>Test Connectivity</strong>: Use the Test button in SigNoz to check connectivity with Opsgenie. If the test fails, review your network settings and API Key.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation
group_tracking_ids: docs-metrics-management-types-and-aggregation
<h2>Metric Types and Aggregation</h2>
<p>This page describes the different types of metrics that SigNoz supports and how they are aggregated.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#metric-types
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-metric-types
group_tracking_ids: docs-metrics-management-types-and-aggregation
<h2>Metric Types and Aggregation: Metric Types</h2>
<p>SigNoz supports the following types of metrics:</p>
<ul>
<li>
<p><strong>Counter</strong>: A counter is a metric that represents a single numerical value that monotonically increases over time. A counter resets to zero when the process restarts. Counters are used to track the number of events that occur in a system. For example, the number of requests received by a server.</p>
</li>
<li>
<p><strong>Gauge</strong>: A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage.</p>
</li>
<li>
<p><strong>Histogram</strong>: A histogram is a metric that represents the distribution of a set of values. Histograms are used to track the distribution of values over time. For example, the response time of a web service.</p>
</li>
<li>
<p><strong>Exponential Histogram</strong>: An exponential histogram is a metric that represents the distribution of a set of values on a logarithmic scale. Exponential histograms are used to track the distribution of values that span several orders of magnitude. For example, the latency of a network request.</p>
</li>
</ul>
<h3>## Temporality of Metrics</h3>
<p>The temporality of a metric determines how the measurement is reported over time. SigNoz supports the following types of temporality:</p>
<ul>
<li>
<p><strong>Cumulative</strong> metrics store the accumulated value of the metric since the start of the process ‚Äî for example, an odometer in a vehicle.</p>
</li>
<li>
<p><strong>Delta</strong> metrics store the change in the value of the metric since the last time it was reported ‚Äî for example, the number of requests received by a server. Each time the metric is reported, the value is reset to zero.</p>
</li>
</ul>
<p>A table representing the different types of metrics and their temporality support in SigNoz is shown below:</p>
<table>
<thead>
<tr>
<th>Metric Type</th>
<th>Cumulative</th>
<th>Delta</th>
</tr>
</thead>
<tbody>
<tr>
<td>Counter</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Gauge</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>Histogram</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Exponential Histogram</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p>The Gauge metric type does not have a temporality, as it represents a single value at a point in time.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#aggregation
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-aggregation
group_tracking_ids: docs-metrics-management-types-and-aggregation
<h2>Metric Types and Aggregation: Aggregation</h2>
<p>The aggregation of metrics is a two-step process:</p>
<ul>
<li>
<p><strong>Temporal Aggregation</strong>: Temporal aggregation is the process of aggregating the measurements over a specific period within the same time series.</p>
</li>
<li>
<p><strong>Spatial Aggregation</strong>: Spatial aggregation is the process of aggregating the measurements across multiple time series after temporal aggregation.</p>
</li>
</ul>
<p>A table representing the different types of metrics and their temporal and spatial aggregation supported by SigNoz is shown below:</p>
<table>
<thead>
<tr>
<th>Metric Type</th>
<th>Temporal Aggregation</th>
<th>Spatial Aggregation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Counter</td>
<td>Increase, Rate</td>
<td>Sum, Avg, Min, Max</td>
</tr>
<tr>
<td>Gauge</td>
<td>Latest, Sum, Avg, Min, Max, Count, Count Distinct</td>
<td>Sum, Avg, Min, Max</td>
</tr>
<tr>
<td>Histogram</td>
<td>-</td>
<td>P50, P75, P90, P95, P99</td>
</tr>
<tr>
<td>Exponential Histogram</td>
<td>-</td>
<td>P50, P75, P90, P95, P99</td>
</tr>
</tbody>
</table>
<p>The next few sections describe how SigNoz performs temporal and spatial aggregation for each metric type.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#gauge
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/memory_ts_raw.png
tracking_id: docs-metrics-management-types-and-aggregation-gauge
group_tracking_ids: docs-metrics-management-types-and-aggregation
<h2>Metric Types and Aggregation: Gauge: Gauge</h2>
<p>Let's take an example of a Gauge metric that represents the memory usage of a process. The following table shows the measurements of the memory usage over time (at minutes:seconds timestamp). The measurements are reported every 5 seconds. This example has 7 time series, each representing the memory usage of a process (Px) running on the server (Hx).</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/memory_ts_raw.png" alt="Raw memory usage measurements" /></p>
<p>_</p>
<p>Raw memory usage measurements</p>
<p>_</p>
<p>Let's say we want to calculate the memory usage per host. The following steps show how SigNoz performs temporal and spatial aggregation for the Gauge metric:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-1-find-the-aggregation-interval
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-step-1-find-the-aggregation-interval
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Step 1: Find the aggregation interval</p>
<p>The aggregation interval is the period over which the measurements are aggregated. It is dynamically calculated based on the time range of the query. This is done to ensure that the number of data points returned is manageable and does not overload the system. In this example, for illustration purposes, we will use an aggregation interval of <strong>15 seconds</strong>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-21-group-measurements-into-intervals
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/memory_ts_temporal.png
tracking_id: docs-metrics-management-types-and-aggregation-step-21-group-measurements-into-intervals
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: ### ## Step 2: Temporal Aggregation - Step 2.1: Group measurements into intervals</p>
<p>For each time series, the measurements are grouped into intervals of 15 seconds. The following table shows the grouped measurements for each time series:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/memory_ts_temporal.png" alt="Grouped memory usage measurements" /></p>
<p>_</p>
<p>Grouped memory usage measurements</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#what-temporal-aggregation-function-to-use
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/memory_ts_temporal_avg.png
tracking_id: docs-metrics-management-types-and-aggregation-what-temporal-aggregation-function-to-use
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: #### ## Step 2.2: Calculate the aggregated value for each interval - What temporal aggregation function to use?</p>
<p>The selection of the temporal aggregation function depends on the use case and the metric being measured. It's important to choose the right function to get meaningful insights from the data. Let's analyze what each temporal aggregation function represents in the context of memory usage:</p>
<ul>
<li>
<p><strong>Latest</strong>: The latest memory usage of the process in the interval. This might be fine if the last value can be a good representation of the memory usage.</p>
</li>
<li>
<p><strong>Sum</strong>: The sum of memory usage measurements of the process in the interval. It doesn't make much sense to sum memory usage for intervals. For example, if the memory usage for an interval is [100, 102, 101], the sum would be 303, it would be incorrect to say that the memory usage was 303 in the interval.</p>
</li>
<li>
<p><strong>Avg</strong>: The average memory usage of the process in the interval. This is a better representation of memory usage as it gives an idea of the average memory usage over the interval.</p>
</li>
<li>
<p><strong>Min</strong>: The minimum memory usage of the process in the interval. This can be useful to find the minimum memory usage of the process in the interval.</p>
</li>
<li>
<p><strong>Max</strong>: The maximum memory usage of the process in the interval. This can be useful to find the maximum memory usage of the process in the interval.</p>
</li>
<li>
<p><strong>Count</strong>: The number of memory usage measurements in the interval. This doesn't make much sense in the context of memory usage.</p>
</li>
<li>
<p><strong>Count Distinct</strong>: The number of distinct memory usage measurements in the interval. This doesn't make much sense in the context of memory usage.</p>
</li>
</ul>
<p>For each interval, the aggregated value is calculated based on the temporal aggregation function specified in the query. The result of this step is a single aggregated value for each interval within the time series. The number of time series remains the same after temporal aggregation.</p>
<p>The following table shows the average value for each interval:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/memory_ts_temporal_avg.png" alt="Aggregated memory usage measurements within series" /></p>
<p>_</p>
<p>Aggregated memory usage measurements within series</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-3-spatial-aggregation
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-step-3-spatial-aggregation
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Step 3: Spatial Aggregation</p>
<p>The spatial aggregation step aggregates the values across multiple time series. The result of this step is a single aggregated value for each interval for a group of time series. The grouping is done based on the group tags specified in the query. If no group tags are specified, all time series are considered as a single group. In this example, we want to calculate the memory usage per host. The group tag is the hostname.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-31-group-time-series-based-on-group-tags
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/memory_ts_spatial_sum_by_host_intermediate.png
tracking_id: docs-metrics-management-types-and-aggregation-step-31-group-time-series-based-on-group-tags
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Step 3.1: Group time series based on group tags</p>
<p>The time series are grouped based on the hostname. The following table shows the grouped time series:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/memory_ts_spatial_sum_by_host_intermediate.png" alt="Grouped memory usage measurements by host" /></p>
<p>_</p>
<p>Grouped memory usage measurements by host</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-32-calculate-the-aggregated-value-for-each-interval
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/memory_ts_spatial_sum_by_host.png
tracking_id: docs-metrics-management-types-and-aggregation-step-32-calculate-the-aggregated-value-for-each-interval
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Step 3.2: Calculate the aggregated value for each interval</p>
<p>For each interval, the aggregated value is calculated based on the spatial aggregation function specified in the query. The result of this step is a single aggregated value for each interval for each group of time series. The number of time series is reduced after spatial aggregation. Let's analyze what each spatial aggregation function represents in the context of memory usage:</p>
<ul>
<li>
<p><strong>Sum</strong>: The sum of memory usage measurements of all processes in the group in the interval. . This can be useful to find the total memory usage of all processes (i.e. memory usage per host) in the group in the interval.</p>
</li>
<li>
<p><strong>Avg</strong>: The average memory usage of all processes in the group in the interval. It doesn't make much sense to average memory usage of all processes in the group in the interval. For example, if the memory usage for processes (p1, p2, p3) in the host group is [20, 25, 32], the average would be 25.67, this doesn't help in understanding the memory usage of hosts.</p>
</li>
<li>
<p><strong>Min</strong>: The minimum memory usage of all processes in the group in the interval. This can be useful to find the minimum memory usage of all processes in the group in the interval. If the goal is to find the minimum memory usage of all processes in the group in the interval, this can be useful.</p>
</li>
<li>
<p><strong>Max</strong>: The maximum memory usage of all processes in the group in the interval. This can be useful to find the maximum memory usage of all processes in the group in the interval. If the goal is to find the maximum memory usage of all processes in the group in the interval, this can be useful.</p>
</li>
</ul>
<p>As we can see, the spatial aggregation function <strong>Sum</strong> is the most useful in this context as it gives the memory usage per host. The following table shows the sum of memory usage for each host:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/memory_ts_spatial_sum_by_host.png" alt="Aggregated memory usage measurements by host" /></p>
<p>_</p>
<p>Aggregated memory usage measurements by host</p>
<p>_</p>
<p>The final result is the memory usage per host for each interval.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#counter
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-counter
group_tracking_ids: docs-metrics-management-types-and-aggregation
<h2>Metric Types and Aggregation: Gauge: Counter: Counter</h2>
<p>The Counter metric can have a cumulative or delta temporality.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#cumulative-counter
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-cumulative-counter
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Cumulative Counter</p>
<p>Let's take an example of a Counter metric that represents the number of requests received by a service. The following tables show the measurements of the number of requests over time (at minutes:seconds timestamp). The measurements are reported every 5 seconds. This example has 7 time series, each representing the number of requests received by a service (Sx) running on the server (Hx).</p>
<p>The following table shows the measurements of the number of requests over time (Cumulative Counter):</p>
<p><img src="https://signoz.io/docs/metrics-management/types-and-aggregation/cumulative" alt="Raw request count measurements (cumulative)" /></p>
<p>_</p>
<p>Raw request count measurements (cumulative)</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-1-find-the-aggregation-interval-1
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-step-1-find-the-aggregation-interval-1
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Step 1: Find the aggregation interval</p>
<p>The aggregation interval is the period over which the measurements are aggregated. It is dynamically calculated based on the time range of the query. This is done to ensure that the number of data points returned is manageable and does not overload the system. In this example, for illustration purposes, we will use an aggregation interval of <strong>15 seconds</strong>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-21-group-measurements-into-intervals-1
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_temporal_increase_intermediate.png
tracking_id: docs-metrics-management-types-and-aggregation-step-21-group-measurements-into-intervals-1
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: ### ## Step 2: Temporal Aggregation - Step 2.1: Group measurements into intervals</p>
<p>For each time series, the measurements are grouped into intervals of 15 seconds. The following table shows the grouped measurements for each time series:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_temporal_increase_intermediate.png" alt="Aggregated request count measurements within series" /></p>
<p>_</p>
<p>Aggregated request count measurements within series</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-22-calculate-the-aggregated-value-for-each-interval-1
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_temporal_increase.png
tracking_id: docs-metrics-management-types-and-aggregation-step-22-calculate-the-aggregated-value-for-each-interval-1
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Step 2.2: Calculate the aggregated value for each interval</p>
<p>For each interval, the aggregated value is calculated based on the temporal aggregation function specified in the query. The result of this step is a single aggregated value for each interval within the time series. The number of time series remains the same after temporal aggregation. Let's analyze what each temporal aggregation function represents in the context of request count:</p>
<ul>
<li>
<p><strong>Increase</strong>: The increase in the metric being measured in the interval. This is useful to find the total number of events that occurred in the interval.</p>
</li>
<li>
<p><strong>Rate</strong>: The rate of increase of the metric being measured in the interval. It is calculated as the increase in the metric divided by the interval duration. This is useful to find the rate at which events are occurring in the interval.</p>
</li>
</ul>
<p>The following table shows the increase in the number of requests received in each interval:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_temporal_increase.png" alt="Aggregated request count measurements within series" /></p>
<p>_</p>
<p>Aggregated request count measurements within series</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-3-spatial-aggregation-1
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-step-3-spatial-aggregation-1
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Step 3: Spatial Aggregation</p>
<p>The spatial aggregation step aggregates the values across multiple time series. The result of this step is a single aggregated value for each interval for a group of time series. The grouping is done based on the group tags specified in the query. If no group tags are specified, all time series are considered as a single group. In this example, we want to calculate the number of requests received per service. The group tag is the service name.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-31-group-time-series-based-on-group-tags-1
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_spatial_sum_by_service_intermediate.png
tracking_id: docs-metrics-management-types-and-aggregation-step-31-group-time-series-based-on-group-tags-1
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Step 3.1: Group time series based on group tags</p>
<p>The time series are grouped based on the service name. The following table shows the grouped time series:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_spatial_sum_by_service_intermediate.png" alt="Grouped request count measurements by service" /></p>
<p>_</p>
<p>Grouped request count measurements by service</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-32-calculate-the-aggregated-value-for-each-interval-1
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_spatial_sum_by_service.png
tracking_id: docs-metrics-management-types-and-aggregation-step-32-calculate-the-aggregated-value-for-each-interval-1
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Step 3.2: Calculate the aggregated value for each interval</p>
<p>For each interval, the aggregated value is calculated based on the spatial aggregation function specified in the query. The result of this step is a single aggregated value for each interval for each group of time series. The number of time series is reduced after spatial aggregation. Let's analyze what each spatial aggregation function represents in the context of request count:</p>
<ul>
<li>
<p><strong>Sum</strong>: The sum of the number of requests received by all services in the group in the interval. This can be useful to find the total number of requests received by all services in the group in the interval.</p>
</li>
<li>
<p><strong>Avg</strong>: The average number of requests received by all services in the group in the interval. It doesn't make much sense to average the number of requests received by all services in the group in the interval.</p>
</li>
<li>
<p><strong>Min</strong>: The minimum number of requests received by all services in the group in the interval. This can be useful to find the minimum number of requests received by all services in the group in the interval.</p>
</li>
<li>
<p><strong>Max</strong>: The maximum number of requests received by all services in the group in the interval. This can be useful to find the maximum number of requests received by all services in the group in the interval.</p>
</li>
</ul>
<p>As we can see, the spatial aggregation function <strong>Sum</strong> is the most useful in this context as it gives the number of requests received per service. The following table shows the sum of the number of requests received by each service:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_spatial_sum_by_service.png" alt="Aggregated request count measurements by service" /></p>
<p>_</p>
<p>Aggregated request count measurements by service</p>
<p>_</p>
<p>The final result is the number of requests received per service for each interval.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#delta-counter
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-delta-counter
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Delta Counter</p>
<p>Let's take an example of a Counter metric that represents the number of requests received by a service. The following tables show the measurements of the number of requests since the last time it was reported (Delta Counter). The measurements are reported every 5 seconds. This example has 7 time series, each representing the number of requests received by a service (Sx) running on the server (Hx).</p>
<p>The following table shows the measurements of the number of requests since the last time it was reported (Delta Counter):</p>
<p><img src="https://signoz.io/docs/metrics-management/types-and-aggregation/delta" alt="Raw request count measurements (delta)" /></p>
<p>_</p>
<p>Raw request count measurements (delta)</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-1-find-the-aggregation-interval-2
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-step-1-find-the-aggregation-interval-2
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Step 1: Find the aggregation interval</p>
<p>This step is the same as the Cumulative Counter.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-21-group-measurements-into-intervals-2
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/requests_count_delta_temporal_increase_intermediate.png
tracking_id: docs-metrics-management-types-and-aggregation-step-21-group-measurements-into-intervals-2
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: ### ## Step 2: Temporal Aggregation - Step 2.1: Group measurements into intervals</p>
<p>For each time series, the measurements are grouped into intervals of 15 seconds. The following table shows the grouped measurements for each time series:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/requests_count_delta_temporal_increase_intermediate.png" alt="Aggregated request count measurements within series" /></p>
<p>_</p>
<p>Aggregated request count measurements within series</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-22-calculate-the-aggregated-value-for-each-interval-2
tag_set: metrics-management, types-and-aggregation
image_urls: https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_spatial_sum_by_service_intermediate.png
tracking_id: docs-metrics-management-types-and-aggregation-step-22-calculate-the-aggregated-value-for-each-interval-2
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Step 2.2: Calculate the aggregated value for each interval</p>
<p>The following table shows the increase in the number of requests received in each interval:</p>
<p><img src="https://signoz.io/img/docs/metrics/aggregation/requests_count_cumulative_spatial_sum_by_service_intermediate.png" alt="Aggregated request count measurements within series" /></p>
<p>_</p>
<p>Aggregated request count measurements within series</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#step-3-spatial-aggregation-2
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-step-3-spatial-aggregation-2
group_tracking_ids: docs-metrics-management-types-and-aggregation
<p>Metric Types and Aggregation: Gauge: Counter: Step 3: Spatial Aggregation</p>
<p>This step is the same as the Cumulative Counter.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/metrics-management/types-and-aggregation/#histogram--exponential-histogram
tag_set: metrics-management, types-and-aggregation
image_urls: 
tracking_id: docs-metrics-management-types-and-aggregation-histogram-exponential-histogram
group_tracking_ids: docs-metrics-management-types-and-aggregation
<h2>Metric Types and Aggregation: Gauge: Counter: Histogram &amp; Exponential Histogram</h2>
<p>The aggregation of Histogram metrics is similar to that of Counter metrics. The difference is that Histogram metrics store the distribution of a set of values. There is no configuration required for temporal aggregation as the values get aggregated into buckets based on the histogram configuration. The result of the temporal aggregation is a single aggregated histogram for each interval within the time series.</p>
<p>The spatial aggregation step aggregates the values across multiple time series. The result of this step is a single numerical value indicating the percentile value for each interval for a group of time series. The grouping is done based on the group tags specified in the query. If no group tags are specified, all time series are considered as a single group.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/
tag_set: alerts-management, exceptions-based-alerts
image_urls: 
tracking_id: docs-alerts-management-exceptions-based-alerts
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts</h2>
<p>An Exceptions-based alert in SigNoz allows you to define conditions based on exception data, triggering alerts when these conditions are met. Here's a breakdown of the various sections and options available when configuring an Exceptions-based alert:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/#step-1-define-the-metric-using-clickhouse-query
tag_set: alerts-management, exceptions-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-exceptions-based-1.webp
tracking_id: docs-alerts-management-exceptions-based-alerts-step-1-define-the-metric-using-clickhouse-query
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts: Step 1: Define the Metric Using Clickhouse Query</h2>
<p>In this step, you define the Clickhouse query to retrieve the exception data and set conditions for triggering the alert. The following elements are available:</p>
<ul>
<li>
<p><strong>Clickhouse Query</strong>: A field to write a Clickhouse SQL query that selects and aggregates exception data. The query should define the exception type, time range, and other necessary conditions.</p>
</li>
<li>
<p><strong><a href="https://signoz.io/docs/userguide/query-builder/#legend-format">Legend Format</a></strong>: An optional field to define the format for the legend in the visual representation of the alert.</p>
</li>
<li>
<p><strong>Having</strong>: Apply conditions to filter the results further based on aggregate value.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-exceptions-based-1.webp" alt="Using Clickhouse Query to define metrics" /></p>
<p><em>Using Clickhouse Query to define metrics</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/#step-2-define-alert-conditions
tag_set: alerts-management, exceptions-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-exceptions-based-2.webp
tracking_id: docs-alerts-management-exceptions-based-alerts-step-2-define-alert-conditions
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts: Step 2: Define Alert Conditions</h2>
<p>This step is for setting the specific conditions for triggering the alert and determining the frequency of checking those conditions:</p>
<ul>
<li>
<p><strong>Send a notification when [A] is [above/below] the threshold in total during the last [X] mins</strong>: A template to set the threshold and define when the alert condition should be checked.</p>
</li>
<li>
<p><strong>Alert Threshold</strong>: A field to specify the threshold value for the alert condition.</p>
</li>
<li>
<p><strong>More Options</strong> :</p>
<ul>
<li>
<p><strong>Run alert every [X mins]</strong>: This option determines the frequency at which the alert condition is checked and notifications are sent.</p>
</li>
<li>
<p><strong>Send a notification if data is missing for [X] mins</strong>: A field to specify if a notification should be sent when data is missing for a certain period.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-exceptions-based-2.webp" alt="Define the alert conditions" /></p>
<p><em>Define the alert conditions</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/#step-3-alert-configuration
tag_set: alerts-management, exceptions-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-exceptions-based-3.webp
tracking_id: docs-alerts-management-exceptions-based-alerts-step-3-alert-configuration
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts: Step 3: Alert Configuration</h2>
<p>In this step, you set the alert's metadata, including severity, name, and description:</p>
<h3>## Severity</h3>
<p>Set the severity level for the alert (e.g., &quot;Warning&quot; or &quot;Critical&quot;).</p>
<h3>## Alert Name</h3>
<p>A field to name the alert for easy identification.</p>
<h3>## Alert Description</h3>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>You can incorporate result labels in the alert descriptions to make the alerts more informative:</p>
<p><strong>Syntax</strong>: Use <code>{{.Labels.&lt;label-name&gt;}}</code> to insert label values. Label values can be any attribute used in group by. Ensure that all <code>.</code> (dots) in attribute are converted to <code>_</code></p>
<p><strong>Example</strong>: If you have a query that has the label <code>service.name</code> then to use it in the alert description, you will use <code>{{.Labels.service_name}}</code>which creates an alert that is specific to the particular service.</p>
<h3>## Labels</h3>
<p>A field to add labels or tags for categorization. Labels should be added in key value pairs. First enter key (avoid space in key) and set value.</p>
<h3>## Notification channels</h3>
<p>A field to choose the <a href="https://signoz.io/docs/setup-alerts-notification">notification channels</a> from those configured in the Alert Channel settings.</p>
<h3>## Test Notification</h3>
<p>A button to test the alert to ensure that it works as expected.</p>
<p><img src="https://signoz.io/img/docs/alerts/alerts-exceptions-based-3.webp" alt="Configure the alert" /></p>
<p><em>Setting the alert metadata</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/exceptions-based-alerts/#examples
tag_set: alerts-management, exceptions-based-alerts
image_urls: 
tracking_id: docs-alerts-management-exceptions-based-alerts-examples
group_tracking_ids: docs-alerts-management-exceptions-based-alerts
<h2>Exceptions based alerts: Examples</h2>
<h3>## 1. Alert when exception of type <code>ConnectionError</code> occurs</h3>
<h4>## Here's a video tutorial for creating this alert:</h4>
<ul>
<li>
<p><strong>ClickHouse Query</strong>: Counts occurrences of 'ConnectionError' exceptions within one-minute intervals, grouped by service name. The ClickHouse Query would look like:</p>
<pre><code>SELECT 
    count() as value,
    toStartOfInterval(timestamp, toIntervalMinute(1)) AS interval,
    serviceName
FROM signoz_traces.distributed_signoz_error_index_v2
WHERE exceptionType !='ConnectionError'
AND timestamp BETWEEN {{.start_datetime}} AND {{.end_datetime}}
GROUP BY serviceName, interval;
</code></pre>
</li>
<li>
<p><strong>Alert Threshold</strong>: Set to <strong>0</strong></p>
</li>
<li>
<p><strong>Alert Name</strong>: &quot;Exceptions Alert&quot;</p>
</li>
<li>
<p><strong>Severity</strong>: &quot;Warning&quot;</p>
</li>
<li>
<p><strong>Notification Channels</strong>: signoz-slack-alerts (Slack channel)</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/alerts/alerts-exceptions-based.gif" alt="A gif of Exceptions Based alerts example in SigNoz" /></p>
<p><em>Exceptions Based Alert Example</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/ms-teams/
tag_set: alerts-management, notification-channel, ms-teams
image_urls: 
tracking_id: docs-alerts-management-notification-channel-ms-teams
group_tracking_ids: docs-alerts-management-notification-channel-ms-teams
<h2>Configure Microsoft Teams Channel</h2>
<h2>Prerequisite</h2>
<hr />
<p>Before configuring Ms Teams as a notification channel in SigNoz, ensure that you have:</p>
<ul>
<li><strong>Incoming Webhook</strong>: Follow the steps outlined in <a href="https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook">Microsoft Teams documentation</a> to create an incoming webhook and obtain the necessary webhook URL.</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.26.0">v0.26.0</a> or later.</li>
</ul>
<p>‚úÖ Info</p>
<p>Please note that the MS Teams notification option is only available for <strong>SigNoz paid plans</strong>. Feel free to explore the available plans and their features <a href="https://signoz.io/pricing/">here</a>.</p>
<h2>Accessing Alert Channels</h2>
<hr />
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
<h2>Creating a new Notification channel</h2>
<hr />
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Ms Teams as the channel type.</li>
<li><strong>Webhook URL</strong>: Paste the Incoming Webhook URL obtained from Microsoft Teams.</li>
</ul>
<p>üìù Note</p>
<p>You can use <a href="https://prometheus.io/docs/alerting/latest/notifications/">Go templates</a> for customizing the title and description.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fms-teams-new-channel.webp&amp;w=3840&amp;q=75" alt="new-notification-channel" /></p>
<p><strong>Test Configuration</strong>: Verify the configuration by clicking the Test button. This sends a test alert to Microsoft Teams, ensuring that SigNoz alert manager can communicate with your MS Teams setup.</p>
<h2>Editing a Notification channel</h2>
<hr />
<p>To edit an existing MS Teams notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the MS Teams webhook URL and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>
<h2>Receive Alert in MS Teams</h2>
<hr />
<p>Once configured correctly, alerts from SigNoz will appear in Microsoft Teams whenever monitored metrics cross the thresholds specified in your alert rules. This setup ensures that you are promptly alerted to any issues in your applications or infrastructure components.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-in-ms-teams.webp&amp;w=3840&amp;q=75" alt="alert-in-ms-teams" /></p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter issues with the Microsoft Teams integration:</p>
<ul>
<li><strong>Check the Webhook URL</strong>: Ensure that the webhook URL entered in SigNoz matches the one provided by Microsoft Teams.</li>
<li><strong>Verify MS Teams Configuration</strong>: Confirm that the incoming webhook in Microsoft Teams is correctly set up and active.</li>
<li><strong>Test Connectivity</strong>: Use the Test button in SigNoz to check connectivity with Microsoft Teams. If the test fails, review your network settings and webhook URL.</li>
</ul>
<p>üí° Tip</p>
<p>If you encounter any unexpected challenges during the use of this integration, please contact SigNoz Support at <a href="mailto:support@signoz.io">support@signoz.io</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#prerequisites
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-prerequisites
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Configure Email Channel - Prerequisites</h2>
<p>Before setting up Email as a notification channel in SigNoz, ensure the following:</p>
<ul>
<li><strong>SMTP Host</strong>: You need to have a SMTP host running through which emails are sent (Not needed for SigNoz Cloud).</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.41.0">v0.41.0</a> or later</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#accessing-alert-channels
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-accessing-alert-channels
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Accessing Alert Channels</h2>
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#creating-a-new-notification-channel
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-creating-a-new-notification-channel
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Creating a new Notification channel</h2>
<p>To create a new Email notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Email as the channel type.</li>
<li><strong>To</strong>: Enter the email address to which the alerts are sent. This is a comma separated list of email addresses.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Femail-new-channel.png&amp;w=3840&amp;q=75" alt="new-notification-channel" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#configuring-alertmanager
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-configuring-alertmanager
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Configuring Alertmanager</h2>
<p>The following environment variables need to be set for alertmanager to send emails:</p>
<ul>
<li><strong>ALERTMANAGER_SMTP_FROM</strong>: The email address from which the alerts are sent.</li>
<li><strong>ALERTMANAGER_SMTP_HOST</strong>: The SMTP host obtained from your email provider.</li>
<li><strong>ALERTMANAGER_SMTP_PORT</strong>: The SMTP port obtained from your email provider.</li>
<li><strong>ALERTMANAGER_SMTP_AUTH_USERNAME</strong>: The SMTP user obtained from your email provider.</li>
<li><strong>ALERTMANAGER_SMTP_AUTH_PASSWORD</strong>: The SMTP password obtained from your email provider.</li>
</ul>
<p>‚úÖ Info</p>
<p>This section is only required for <strong>Self-Hosted</strong> users. Cloud users don't need to follow this step.</p>
<h3>## Docker</h3>
<p>Based on your Docker installation, you can include the following section in your Docker Compose YAML file to configure alertmanager.</p>
<ul>
<li>
<p>Docker Standalone: <code>deploy/docker/clickhouse-setup/docker-compose.yaml</code></p>
</li>
<li>
<p>Docker Swarm: <code>deploy/docker-swarm/clickhouse-setup/docker-compose.yaml</code></p>
<p>services:
alertmanager:
environment:
- ALERTMANAGER_SMTP_FROM=&lt;email address&gt;
- ALERTMANAGER_SMTP_HOST=&lt;smtp host&gt;
- ALERTMANAGER_SMTP_PORT=&lt;smtp port&gt;
- ALERTMANAGER_SMTP_AUTH_USERNAME=&lt;smtp user&gt;
- ALERTMANAGER_SMTP_AUTH_PASSWORD=&lt;smtp password&gt;</p>
</li>
</ul>
<h3>## Kubernetes</h3>
<p>You can include the following section in your Helm override values YAML file.</p>
<pre><code>alertmanager:
  additionalEnvs:
    ALERTMANAGER_SMTP_FROM: &lt;email address&gt;
    ALERTMANAGER_SMTP_HOST: &lt;smtp host&gt;
    ALERTMANAGER_SMTP_PORT: &lt;smtp port&gt;
    ALERTMANAGER_SMTP_AUTH_USERNAME: &lt;smtp user&gt;
    ALERTMANAGER_SMTP_AUTH_PASSWORD: &lt;smtp password&gt;
</code></pre>
<p><strong>Test Configuration</strong>: Click the Test button to send a test alert to the configured email addresses. This verifies that SigNoz can communicate with your email provider.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#editing-a-notification-channel
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-editing-a-notification-channel
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Editing a Notification channel</h2>
<p>To edit an existing email notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the email addresses and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#receiving-alerts-in-email
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-receiving-alerts-in-email
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Receiving Alerts in Email</h2>
<p>Once configured correctly, alerts from SigNoz will be sent to Email whenever monitored metrics cross the thresholds specified in your alert rules.This ensures you are promptly notified of any issues in your applications or infrastructure components.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts-in-email.png&amp;w=3840&amp;q=75" alt="alert-in-email" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#troubleshooting
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-troubleshooting
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Troubleshooting</h2>
<p>If you encounter issues with the Email integration:</p>
<ul>
<li><strong>Check the SMTP Server info</strong>: Ensure that the SMTP Host, Port, User and Password entered in SigNoz matches the one provided by your email provider.</li>
<li><strong>Verify Email Addresses</strong>: Confirm that the email addresses entered in SigNoz are valid and active.</li>
<li><strong>Test Connectivity</strong>: Use the Test button in SigNoz to check connectivity with your email provider. If the test fails, review your network settings and SMTP Server info.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/
tag_set: alerts-management, metrics-based-alerts
image_urls: 
tracking_id: docs-alerts-management-metrics-based-alerts
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<h2>Metrics based alerts</h2>
<p>A Metric-based alert in SigNoz allows you to define conditions based on metric data and trigger alerts when these conditions are met. Here's a breakdown of the various sections and options available when configuring a Metric-based alert:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-1-define-the-metric
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-metrics-based-1.webp
tracking_id: docs-alerts-management-metrics-based-alerts-step-1-define-the-metric
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<h2>Metrics based alerts: Step 1: Define the Metric</h2>
<p>In this step, you use the <a href="https://signoz.io/docs/userguide/query-builder/#metrics-query-builder">Metrics Query Builder</a> to choose the metric to monitor. The following fields that are available in Metrics Query Builder includes:</p>
<ul>
<li>
<p><strong>Metric</strong>: A field to select the specific metric you want to monitor (e.g., CPU usage, memory utilization).</p>
</li>
<li>
<p><strong>Time aggregation</strong>: A field to select the time aggregation function to use for the metric. Learn more about <a href="../../metrics-management/types-and-aggregation/#aggregation">time aggregation</a></p>
</li>
<li>
<p><strong>WHERE</strong>: A filter field to define specific conditions for the metric. You can apply logical operators like &quot;IN,&quot; &quot;NOT IN&quot;.</p>
</li>
<li>
<p><strong>Space aggregation</strong>: A field to select the space aggregation function to use for the metric. Learn more about <a href="../../metrics-management/types-and-aggregation/#aggregation">space aggregation</a></p>
</li>
<li>
<p><strong><a href="https://signoz.io/docs/userguide/query-builder/#legend-format">Legend Format</a></strong>: An optional field to customize the legend's format in the visual representation of the alert.</p>
</li>
<li>
<p><strong>Having</strong>: Apply conditions to filter the results further based on aggregate value.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-metrics-based-1.webp" alt="Using Query Builder to the metric to monitor" /></p>
<p><em>Using Query Builder to define the metric to monitor</em></p>
<p>To know more about the functionalities of the Query Builder, checkout the <a href="https://signoz.io/docs/userguide/query-builder/">documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-2-define-alert-conditions
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-metrics-based-2.webp
tracking_id: docs-alerts-management-metrics-based-alerts-step-2-define-alert-conditions
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<h2>Metrics based alerts: Step 2: Define Alert Conditions</h2>
<p>In this step, you define the specific conditions that trigger the alert and the notification frequency. The following fields are available:</p>
<ul>
<li>
<p><strong>Condition</strong>: Specify when the metric should trigger the notification</p>
<ul>
<li>Greater than (<code>&gt;</code>)</li>
<li>Less than (<code>&lt;</code>)</li>
<li>Equal to (<code>=</code>)</li>
<li>Not equal to (<code>!=</code>)</li>
</ul>
</li>
<li>
<p><strong>Occurrence</strong>: Specify how condition should be evaluated</p>
<ul>
<li>At least once</li>
<li>Every time</li>
<li>On average</li>
<li>In total</li>
</ul>
</li>
<li>
<p><strong>Evaluation window</strong>: Specify the rolling time window for the condition evaluation. The following look back options are available:</p>
<ul>
<li>Last 5 minutes</li>
<li>Last 10 minutes</li>
<li>Last 15 minutes</li>
<li>Last 1 hour</li>
<li>Last 4 hours</li>
<li>Last 1 day</li>
</ul>
</li>
<li>
<p><strong>Alert Threshold</strong>: A field to set the threshold for the alert condition.</p>
</li>
<li>
<p><strong>Threshold Unit</strong>: The unit of the threshold value. This is convenient when you want to set the threshold in a different unit than the metric. For example, the memory usage metric might be in bytes, but you might want to monitor it in MBs. Please make sure to set the Y-axis unit to indicate the unit of the metric.</p>
</li>
<li>
<p><strong>More Options</strong> :</p>
<ul>
<li>
<p><strong>Run alert every [X mins]</strong>: This option determines the frequency at which the alert is evaluated.</p>
</li>
<li>
<p><strong>Send a notification if data is missing for [X] mins</strong>: A field to specify if a notification should be sent when data is missing for a certain period.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-metrics-based-2.webp" alt="Define the alert conditions" /></p>
<p><em>Define the alert conditions</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-3-alert-configuration
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-metrics-based-3.webp
tracking_id: docs-alerts-management-metrics-based-alerts-step-3-alert-configuration
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<h2>Metrics based alerts: Step 3: Alert Configuration</h2>
<p>In this step, you set the alert's metadata, including severity, name, and description:</p>
<h3>## Severity</h3>
<p>Set the severity level for the alert (e.g., &quot;Warning&quot; or &quot;Critical&quot;).</p>
<h3>## Alert Name</h3>
<p>A field to name the alert for easy identification.</p>
<h3>## Alert Description</h3>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>You can incorporate result labels in the alert descriptions to make the alerts more informative:</p>
<p><strong>Syntax</strong>: Use <code>{{.Labels.&lt;label-name&gt;}}</code> to insert label values. Label values can be any attribute used in group by. Ensure that all <code>.</code> (dots) in attribute are converted to <code>_</code></p>
<p><strong>Example</strong>: If you have a query that has the label <code>service.name</code> then to use it in the alert description, you will use <code>{{.Labels.service_name}}</code>which creates an alert that is specific to the particular service.</p>
<h3>## Labels</h3>
<p>A field to add labels or tags for categorization. Labels should be added in key value pairs. First enter key (avoid space in key) and set value.</p>
<h3>## Notification channels</h3>
<p>A field to choose the <a href="https://signoz.io/docs/setup-alerts-notification">notification channels</a> from those configured in the Alert Channel settings.</p>
<h3>## Test Notification</h3>
<p>A button to test the alert to ensure that it works as expected.</p>
<p><img src="https://signoz.io/img/docs/alerts/alerts-metrics-based-3.webp" alt="Configure the alert" /></p>
<p><em>Setting the alert metadata</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-1-write-query-builder-query-to-define-alert-metric
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-builder.png
tracking_id: docs-alerts-management-metrics-based-alerts-step-1-write-query-builder-query-to-define-alert-metric
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: #### ## Here's a video tutorial for creating this alert: - Step 1: Write Query Builder query to define alert metric</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-builder.png" alt="metrics builder query for memory usage" /></p>
<p><em>Memory usage metric builder query</em></p>
<p>The <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver"><code>hostmetricsreceiver</code></a> creates several host system metrics, including <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/memoryscraper/documentation.md#systemmemoryusage"><code>system_memory_usage</code></a>
, which contains the memory usage for each <a href="https://www.kernel.org/doc/Documentation/filesystems/proc.txt">state</a> from <code>/proc/meminfo</code>. The states can be <code>free</code>, <code>used</code>, <code>cached</code>, etc. We want to alert when the total memory usage of a host exceeds the threshold, so the WHERE clause excludes the <code>free</code> state. We calculate the average value for each state and then sum them up by host to get the per-host memory usage.</p>
<p>‚úÖ Info</p>
<p>Remember to set the unit of the <code>y-axis</code> to bytes, as that is the unit of the mentioned metric.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#step-2-set-alert-conditions
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-builder-condition.png
tracking_id: docs-alerts-management-metrics-based-alerts-step-2-set-alert-conditions
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: Step 2: Set alert conditions</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-builder-condition.png" alt="metrics builder query for memory usage" /></p>
<p><em>Memory usage alert condition</em></p>
<p>The condition is set to trigger a notification if the per-minute memory usage exceeds the threshold of 400 MB at least once in the last five minutes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#2-alert-when-memory-usage-for-host-goes-above-70
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-percentage-builder.png, https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-percentage-builder-condition.png
tracking_id: docs-alerts-management-metrics-based-alerts-2-alert-when-memory-usage-for-host-goes-above-70
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: 2. Alert when memory usage for host goes above 70%</p>
<p>You might want to alert based on the percentage rather than a fixed threshold. There are two ways to get the percentage: the convenient option is when the usage percentage is reported directly by the source, or when the source only sends the exact usage in bytes and you need to derive the percentage yourself. This example demonstrates how to derive the percentage from the original bytes metric.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-percentage-builder.png" alt="metrics builder query for memory usage" /></p>
<p><em>Memory usage percentage query</em></p>
<p>We use a formula to derive the percentage value from the exact memory usage in bytes. In the example, query <code>A</code> calculates the per-host memory usage, while query <code>B</code>, as shown in the image, doesn't have any WHERE clause filter, thus providing the total memory available. The formula for <code>A</code>/<code>B</code> is interpreted as (memory usage in bytes) / (total memory available in bytes). We set the unit of the y-axis to Percent (0.0 - 1.0) to match the result of the formula.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/mem-usage-alert-percentage-builder-condition.png" alt="metrics builder query for memory usage" /></p>
<p><em>Memory usage percentage condition</em></p>
<p>The condition is set to trigger a notification if the per-minute memory usage exceeds the threshold of 70% all the times in the last five minutes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#3-alert-when-the-error-percentage-for-an-endpoint-exceeds-5
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/error-percentage-builder.png, https://signoz.io/img/docs/product-features/alerts/metrics/error-percentage-builder-condition.png
tracking_id: docs-alerts-management-metrics-based-alerts-3-alert-when-the-error-percentage-for-an-endpoint-exceeds-5
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: 3. Alert when the error percentage for an endpoint exceeds 5%</p>
<p>SigNoz creates a metric <code>signoz_calls_total</code> from the trace data. The default attributes of the metric are <code>service_name</code>, <code>operation</code>, <code>span_kind</code>, <code>status_code</code>, and <code>http_status_code</code>. There is no separate metric for counting errors; instead, the <code>status_code</code> attribute is used to determine if a request counts as an error. This example demonstrates how to calculate the error percentage and alert on it.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/error-percentage-builder.png" alt="metrics builder query for error percentage" /></p>
<p><em>Error percentage query</em></p>
<p>We use a formula to derive the error percentage from the total calls metric. In the example, query <code>A</code> calculates the per-endpoint error rate, while query <code>B</code>, as shown in the image, doesn't have any WHERE clause filter for <code>status_code</code>, thus providing the per-endpoint total request rate. The formula for <code>A</code>/<code>B</code> is interpreted as (error request rate) / (total request rate), which gives the error percentage per endpoint. We set the unit of the y-axis to Percent (0.0 - 1.0) to match the result of the formula.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/error-percentage-builder-condition.png" alt="metrics builder query for error percentage" /></p>
<p><em>Error percentage condition</em></p>
<p>The condition is set to trigger a notification if the per-minute error percentage exceeds the threshold of 5% all the times in the last five minutes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/metrics-based-alerts/#4-alert-when-p95-latency-for-an-endpoint-is-above-1200-ms
tag_set: alerts-management, metrics-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/metrics/p95-latency-builder.png, https://signoz.io/img/docs/product-features/alerts/metrics/p95-latency-builder-condition.png
tracking_id: docs-alerts-management-metrics-based-alerts-4-alert-when-p95-latency-for-an-endpoint-is-above-1200-ms
group_tracking_ids: docs-alerts-management-metrics-based-alerts
<p>Metrics based alerts: Examples: 4. Alert when P95 latency for an endpoint is above 1200 ms</p>
<p>SigNoz creates a metric <code>signoz_latency_bucket</code> from the trace data. The default attributes of the metric are <code>service_name</code>, <code>operation</code>, <code>span_kind</code>, <code>status_code</code>, and <code>http_status_code</code>. This example demonstrates how to calculate the P95 latency for an endpoint and alert on it.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/p95-latency-builder.png" alt="metrics builder query for latency" /></p>
<p><em>Endpoint latency query</em></p>
<p>We use the P95 aggregation, which gives the 95th-percentile request latency per endpoint. We set the unit of the y-axis to milliseconds to match the unit of the metric.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/metrics/p95-latency-builder-condition.png" alt="metrics builder query for latency" /></p>
<p><em>Endpoint latency condition</em></p>
<p>The condition is set to trigger a notification if the per-minute P95 latency exceeds the threshold of 1200 ms at any time in the last five minutes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/query-builder/#introduction
tag_set: userguide, query-builder
image_urls: 
tracking_id: docs-userguide-query-builder-introduction
group_tracking_ids: docs-userguide-query-builder
<h2>Query Builder: Query Builder - Introduction</h2>
<p>Query Builder in SigNoz simplifies the process of filtering, aggregating, and visualizing data, making complex queries accessible to all users. This documentation will walk you through features of the Query Builder, showing you how to create detailed queries, visualize results, and understand your data better.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/query-builder/#filtering
tag_set: userguide, query-builder
image_urls: 
tracking_id: docs-userguide-query-builder-filtering
group_tracking_ids: docs-userguide-query-builder
<h2>Query Builder: ## Logs and Traces Query Builder - Filtering</h2>
<p>The Query Builder in SigNoz allows users to apply filtering based on various attributes of logs or traces.</p>
<h3>## Using the Filtering Feature</h3>
<ul>
<li><strong>Access</strong>: Navigate to the Query Builder.</li>
<li><strong>Open Filter Options</strong>: Click on the Search Filter field to open a dropdown list of available attributes.</li>
<li><strong>Select Attributes</strong>: Choose the attribute you want to filter by (e.g., <code>service.name</code>, <code>level</code>, <code>status</code>). Start typing to filter the list.</li>
<li><strong>Apply Operators</strong>: Choose the operator (e.g., <code>=</code>, <code>!=</code>, <code>IN</code>, <code>NOT_IN</code>) for the selected attribute.</li>
<li><strong>Input Values</strong>: Input the value(s) you want to filter by.</li>
<li><strong>Combine Filters</strong>: You can add multiple filters by repeating the above steps. Multiple filters are combined with <code>AND</code>.</li>
<li><strong>View Results</strong>: Click the Stage &amp; Run Query Button to display the logs or traces that match the applied filters.</li>
</ul>
<h3>## Example</h3>
<p>Suppose you want to filter logs from a specific service with a particular severity level. You can achieve this using the following filters:</p>
<ul>
<li>selecting the service name. <code>service.name = demo-app</code></li>
<li>selecting the severity text, <code>severity_text = DEBUG</code></li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/query-builder-filtering.gif" alt="A gif explaining the Filtering feature in SigNoz Query Builder" /></p>
<p><em>Example showing the filtering feature of SigNoz Query Builder</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/query-builder/#aggregation-and-grouping
tag_set: userguide, query-builder
image_urls: 
tracking_id: docs-userguide-query-builder-aggregation-and-grouping
group_tracking_ids: docs-userguide-query-builder
<h2>Query Builder: Aggregation and Grouping</h2>
<p>The Query Builder's Aggregation and Grouping feature enables you to combine data points and categorize them for complex insights.</p>
<h3>## Aggregation</h3>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/aggregation.gif" alt="A gif explaining the Aggregation feature in SigNoz Query Builder" /></p>
<p><em>Aggregation feature of SigNoz Query Builder</em></p>
<p>Aggregation allows you to perform calculations on a set of values to return a single value. With the Query Builder, you can:</p>
<ul>
<li><strong>Count</strong>: Count the number of times a particular event occurs.</li>
<li><strong>Count Distinct</strong>: Count unique occurrences of a specified attribute.</li>
<li><strong>Sum</strong>: Calculate the total for a numerical attribute.</li>
<li><strong>Avg</strong>: Find the average value of a numerical attribute.</li>
<li><strong>Max/Min</strong>: Determine the maximum or minimum value of a numerical attribute in an aggregation interval.</li>
<li><strong>Percentiles (P05, P10, P90, etc.)</strong>: Understand the distribution of your data with percentile calculations.</li>
<li><strong>Rate</strong>: Measure the frequency of occurrence within a given time frame. It is often expressed as a ratio, where the numerator represents the number of occurrences, and the denominator represents the length of the time period.</li>
<li><strong>Rate Sum</strong>: The rate of change in the sum of the selected attribute over the specified rate period.</li>
<li><strong>Rate Avg</strong>: The rate of change in the average of the selected attribute over the specified rate period.</li>
<li><strong>Rate Min/Max</strong>: The rate of change in the minimum or maximum value of the selected attribute over the specified rate period.</li>
</ul>
<h3>## Grouping</h3>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/group_by.gif" alt="A gif explaining the Group by feature in SigNoz Query Builder" /></p>
<p><em>Group by feature of SigNoz Query Builder</em></p>
<p>Grouping allows you to segment your data based on chosen attributes, facilitating comparative analysis across different categories of the selected attributes:</p>
<ul>
<li>You can group data by attributes like <code>service.name</code> or <code>method</code> to analyze patterns per service or HTTP method.</li>
<li>When combined with aggregation, grouping enables you to, for example, find the average response time per service or count errors per endpoint.</li>
</ul>
<h3>## Using Aggregation and Grouping Together</h3>
<ul>
<li>Select an aggregation function from the dropdown, such as <code>Avg</code>.</li>
<li>Then, choose an attribute to apply it to, like <code>response_time</code>.</li>
<li>To group the data, you would then specify an attribute in the <code>Group by</code> field.</li>
<li>After running the query, you'll see the average response times for each group, allowing you to identify areas that may need optimization.</li>
</ul>
<h3>## Example</h3>
<p>To analyze the average response time of services within a system, we can do so in the following way:</p>
<ul>
<li>In the aggregation dropdown, select <code>Avg</code>.</li>
<li>For the attribute to aggregate, choose <code>durationNano</code>.</li>
<li>To see the average duration for each of the service, enter <code>serviceName</code> in the <code>Group by</code> field.</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/query-builder-groupby-aggregation.gif" alt="A gif explaining the how to use Group by and Aggregation features together in SigNoz Query Builder" /></p>
<p><em>Example combining Group By and Aggregation features</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/query-builder/#result-manipulation
tag_set: userguide, query-builder
image_urls: 
tracking_id: docs-userguide-query-builder-result-manipulation
group_tracking_ids: docs-userguide-query-builder
<h2>Query Builder: Result Manipulation</h2>
<p>Result Manipulation is a set of features in the Query Builder that enables you to refine your query. The features include:</p>
<h3>## Order By</h3>
<p>Order your query results based on a specified attribute in either ascending or descending order. This can help in identifying the highest or lowest values in your data, such as the most frequently occurring errors.</p>
<h3>## Aggregate Every</h3>
<p>Define the interval over which to aggregate data. For example, you can aggregate count data in 60-second intervals to get a per-minute count of events.</p>
<h3>## Limit</h3>
<p>Set a limit on the number of results returned. This is useful when you only want to see the top N results, such as the top 10 most visited endpoints.</p>
<h3>## Having</h3>
<p>Apply conditions to filter the results further based on aggregate value.</p>
<h3>## Legend Format</h3>
<p>Customize the legend in your query's visual output to give more clarity, by formatting how grouped data will be labeled in your charts or graphs. We use the double curly braces - <code>{{}}</code> format to show the attribute.</p>
<p>For example, if you have grouped by <code>serviceName</code> attribute then you can write <code>{{serviceName}}</code> in your legend. You can also add text along with the attribute like <code>{{serviceName}}</code> - This is a service** will show be shown as <code>sampleService - This is a service name</code>.</p>
<h3>## Example</h3>
<p>Suppose you want to find the top 2 endpoints with an average response time greater than 500 ms. The query is configured as follows:</p>
<ul>
<li><strong>Group by</strong>: <code>serviceName</code></li>
<li><strong>Aggregate</strong>: <code>Avg</code> on <code>durationNano</code></li>
<li><strong>HAVING</strong>: <code>AVG(durationNano) &gt;= 500000000</code> to filter for endpoints with average response times exceeding 500 ms</li>
<li><strong>Order by</strong>: <code>avg(durationNano) desc</code> to list services with the highest response times first</li>
<li><strong>Limit</strong>: <code>2</code> to focus on the top 2 services with the longest response times</li>
<li><strong>Legend Format</strong>: <code>{{serviceName}}</code> to display the service name in the visualization legend</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/query-builder-result-manipulation.gif" alt="A gif explaining some more features  in SigNoz Query Builder" /></p>
<p><em>An example showcasing Order by, Limit, HAVING and Legend Format features of SigNoz Query Builder</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/query-builder/#multiple-queries-and-functions
tag_set: userguide, query-builder
image_urls: 
tracking_id: docs-userguide-query-builder-multiple-queries-and-functions
group_tracking_ids: docs-userguide-query-builder
<h2>Query Builder: Multiple Queries and Functions</h2>
<p>The SigNoz Query Builder allows you to run multiple queries simultaneously and perform functions on them. This feature facilitates analysis of complex data, such as comparing data or calculating ratios.</p>
<h3>## Multiple Queries</h3>
<p>Create and run multiple independent queries within the same view. Each query can have its own set of filters, aggregations, and groupings. This is particularly useful for analyzing different dimensions of your data in parallel.</p>
<h3>## Functions on Queries</h3>
<p>Apply mathematical functions to the results of your queries. This allows you to derive new insights by performing operations like addition, subtraction, multiplication, division, or more complex functions on your data.</p>
<h3>## List of supported functions</h3>
<ul>
<li>exp</li>
<li>log</li>
<li>ln</li>
<li>exp2</li>
<li>log2</li>
<li>exp10</li>
<li>log10</li>
<li>sqrt</li>
<li>cbrt</li>
<li>erf</li>
<li>erfc</li>
<li>lgamma</li>
<li>tgamma</li>
<li>sin</li>
<li>cos</li>
<li>tan</li>
<li>asin</li>
<li>acos</li>
<li>atan</li>
<li>degrees</li>
<li>radians</li>
<li>now</li>
</ul>
<h3>## Example</h3>
<p>Suppose you want to determiner the percentage of logs that contains errors relative to the total log count. You can achieve this using Multiple queries and a function as follows:</p>
<ul>
<li><strong>Query A</strong>: Filters logs to count the instances where the <code>body</code> contains the word <code>error</code>.</li>
<li><strong>Query B</strong>: Represents a baseline count of all logs.</li>
<li><strong>function <code>F1</code></strong> A**100/B</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/query-builder-multiple-queries.gif" alt="A gif explaining multiple queries and functions features in SigNoz Query Builder" /></p>
<p><em>Multiple Queries and Functions features of SigNoz Query Builder</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/query-builder/#metrics-query-builder
tag_set: userguide, query-builder
image_urls: 
tracking_id: docs-userguide-query-builder-metrics-query-builder
group_tracking_ids: docs-userguide-query-builder
<h2>Query Builder: Metrics Query Builder</h2>
<p>SigNoz's Metrics Query Builder has all the features mentioned above like filtering, aggregation, Order by, Multiple Queries etc. but it also has some additional functionalities which can enable users to delve deeper into metrics analysis with ease.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/query-builder/#spatial-and-temporal-aggregations
tag_set: userguide, query-builder
image_urls: https://signoz.io/img/docs/product-features/query-builder/temporal-spatial-aggregations.webp
tracking_id: docs-userguide-query-builder-spatial-and-temporal-aggregations
group_tracking_ids: docs-userguide-query-builder
<h2>Query Builder: Spatial and Temporal Aggregations</h2>
<p>Efficiently aggregate metrics data across time and various dimensions to gain comprehensive insights into your system's performance.</p>
<h3>## Temporal Aggregation</h3>
<p>Temporal aggregation simplifies metric analysis by consolidating data points over specific time periods. It's essential for managing data from applications that transmit metrics at regular intervals, helping to reduce the volume of data points into a more interpretable format. This aggregation is particularly crucial when dealing with long-term data visualization, where displaying each data point is neither practical nor informative due to limited screen space.</p>
<h3>## Spatial Aggregation</h3>
<p>Modern applications will have many time series across dimensions such as container names, service names, etc.</p>
<p>While temporal aggregations are meant for reducing data points across a single time axis, spatial aggregation refers to the technique of merging metric data across various dimensions or labels, such as container names, service names, or geographical regions.</p>
<h3>## Example</h3>
<p>If you select a counter metric called <code>signoz_calls_total</code>, you can apply temporal aggregations like <code>Rate</code> and <code>Increase</code> on it. Similarly, you can apply spatial aggregations across many dimensions, such as <code>service_name</code>, <code>operation</code>, <code>deployment_environment</code>, etc.</p>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/temporal-spatial-aggregations.webp" alt="Spatial &amp; temporal aggregations in the metrics query builder of SigNoz" /></p>
<p><em>How to apply temporal and spatial aggregations in metrics query builder of SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/query-builder/#functions-for-extended-data-analysis
tag_set: userguide, query-builder
image_urls: https://signoz.io/img/docs/product-features/query-builder/support-for-functions.webp, https://signoz.io/img/docs/product-features/query-builder/chain-two-functions.webp, https://signoz.io/img/docs/product-features/query-builder/time-shift.webp, https://signoz.io/img/docs/product-features/query-builder/plot-time-shift-charts.webp
tracking_id: docs-userguide-query-builder-functions-for-extended-data-analysis
group_tracking_ids: docs-userguide-query-builder
<h2>Query Builder: Functions for Extended Data Analysis</h2>
<p>To extend the data analysis capabilities of the Metrics Query Builder, you can use four different categories of functions.</p>
<h3>## Function Types:</h3>
<ul>
<li>
<p><strong>Exclusion Functions:</strong> These allow users to exclude data points based on certain conditions, such as values falling outside a specified range. This helps in focusing on the most relevant data. For example, <code>Cut Off Min</code> excludes data below a threshold, while <code>Cut Off Max</code> ignores values above a certain point.</p>
</li>
<li>
<p><strong>Arithmetic Functions:</strong> Users can perform mathematical operations on data points, enabling the customization of metric calculations to suit specific analysis needs. For example, functions like <code>Absolute</code> can return the absolute value of data points, and logarithmic functions such as <code>Log2</code> or <code>Log10</code> can transform data to a log scale for better comparison of wide-ranging numbers.</p>
</li>
<li>
<p><strong>Smoothing Functions:</strong> To deal with volatile data, smoothing functions can be applied, such as moving averages, which help in identifying underlying trends by reducing noise. For example, functions like EWMA 3/5/7 (Exponentially Weighted Moving Average) can smooth data fluctuations over 3, 5, or 7 periods.</p>
</li>
<li>
<p><strong>Time Shift Functions:</strong> These functions enable comparisons of data across different time periods, useful for analyzing trends, changes, or anomalies over time. For example, using the <code>Time Shift</code> function for shifting a series by a fixed amount of seconds to compare current data with past data.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/support-for-functions.webp" alt="Functions in metrics query builder" /></p>
<p><em>Functions in metrics query builder will help extend data analysis capabilities on metrics</em></p>
<h3>## Chain Functions:</h3>
<p>Combine two or more functions, like applying a logarithmic scale after setting a minimum cutoff, to tailor your metrics.</p>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/chain-two-functions.webp" alt="Chained functions" /></p>
<p><em>Chain two functions to get your required metrics</em></p>
<h3>## Example</h3>
<p>This example uses the <code>Time Shift</code> function to compare total calls for frontend service in an interval of 1hr.</p>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/time-shift.webp" alt="Time Shift function" /></p>
<p><em>Use the time shift function to compare data across different time periods</em></p>
<p>You can plot charts for the same service an hour apart.</p>
<p><img src="https://signoz.io/img/docs/product-features/query-builder/plot-time-shift-charts.webp" alt="Compare performance of services" /></p>
<p><em>Check the performance of frontend service an hour apart with the help of time shift function</em></p>
<p>Users can use the time shift function for very practical use cases, like comparing performance metrics before and after a deployment.</p>
<p>To know more details about the Metrics Query Builder, you can checkout these resources from the first SigNoz Launch Week:</p>
<ul>
<li>
<p>Article: <a href="https://signoz.io/newsroom/launch-week-1-day-2/">Feature in Spotlight: Metrics &amp; Query Builder [Day 2]</a></p>
</li>
<li>
<p>Video : <a href="https://www.youtube.com/watch?v=fl-z1YoSB_w&amp;t=1s">SigNoz Launch Week - Day 2 - Metrics &amp; Query Builder Improvements</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/
tag_set: alerts-management, log-based-alerts
image_urls: 
tracking_id: docs-alerts-management-log-based-alerts
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts</h2>
<p>A Log-based alert allows you to define conditions based on log data, triggering alerts when these conditions are met. Here's a breakdown of the various sections and options available when configuring a Log-based alert:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/#step-1-define-the-log-metric
tag_set: alerts-management, log-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-log-based-1.webp
tracking_id: docs-alerts-management-log-based-alerts-step-1-define-the-log-metric
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts: Step 1: Define the Log Metric</h2>
<p>In this step, you use the <a href="https://signoz.io/docs/userguide/query-builder/#logs-and-traces-query-builder">Logs Query Builder</a> to apply filters and operations on your logs to define conditions which triggers log based alert Some of the fields that are available in Logs Query Builder includes:</p>
<ul>
<li>
<p><strong>Logs</strong>: A field to filter the specific log data to monitor.</p>
</li>
<li>
<p><strong>Aggregate Attribute</strong>: Allows you to select how the log data should be aggregated (e.g., &quot;Count&quot;).</p>
</li>
<li>
<p><strong>Group by</strong>: Provides options to group log data by various attributes, such as &quot;service.name&quot;, &quot;method&quot; or custom attributes.</p>
</li>
<li>
<p><strong><a href="https://signoz.io/docs/userguide/query-builder/#legend-format">Legend Format</a></strong>: Lets you define the format for the legend in the visual representation of the alert.</p>
</li>
<li>
<p><strong>Having</strong>: Apply conditions to filter the results further based on aggregate value.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-log-based-1.webp" alt="Using Query Builder to perform operations on your logs" /></p>
<p><em>Using Query Builder to perform operations on your logs</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/#step-2-define-alert-conditions
tag_set: alerts-management, log-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-log-based-2.webp
tracking_id: docs-alerts-management-log-based-alerts-step-2-define-alert-conditions
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts: Step 2: Define Alert Conditions</h2>
<p>In this step, you define the specific conditions for triggering the alert, as well as the frequency of checking those conditions:</p>
<ul>
<li>
<p><strong>Send a notification when [A] is [above/below] the threshold [in total] during the last [X mins]</strong>: A template to set the threshold and define when the alert condition should be checked.</p>
</li>
<li>
<p><strong>Alert Threshold</strong>: A field to specify the threshold value for the alert condition.</p>
</li>
<li>
<p><strong>More Options</strong> :</p>
<ul>
<li>
<p><strong>Run alert every [X mins]</strong>: This option determines the frequency at which the alert condition is checked and notifications are sent.</p>
</li>
<li>
<p><strong>Send a notification if data is missing for [X] mins</strong>: A field to specify if a notification should be sent when data is missing for a certain period.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-log-based-2.webp" alt="Define the alert conditions" /></p>
<p><em>Define the alert conditions</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/#step-3-alert-configuration
tag_set: alerts-management, log-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-log-based-3.webp
tracking_id: docs-alerts-management-log-based-alerts-step-3-alert-configuration
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts: Step 3: Alert Configuration</h2>
<p>In this step, you set the alert's metadata, including severity, name, and description:</p>
<h3>## Severity</h3>
<p>Set the severity level for the alert (e.g., &quot;Warning&quot; or &quot;Critical&quot;).</p>
<h3>## Alert Name</h3>
<p>A field to name the alert for easy identification.</p>
<h3>## Alert Description</h3>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>You can incorporate result labels in the alert descriptions to make the alerts more informative:</p>
<p><strong>Syntax</strong>: Use <code>{{.Labels.&lt;label-name&gt;}}</code> to insert label values. Label values can be any attribute used in group by. Ensure that all <code>.</code> (dots) in attribute are converted to <code>_</code></p>
<p><strong>Example</strong>: If you have a query that has the label <code>service.name</code> then to use it in the alert description, you will use <code>{{.Labels.service_name}}</code>which creates an alert that is specific to the particular service.</p>
<h3>## Labels</h3>
<p>A field to add labels or tags for categorization. Labels should be added in key value pairs. First enter key (avoid space in key) and set value.</p>
<h3>## Notification channels</h3>
<p>A field to choose the <a href="https://signoz.io/docs/setup-alerts-notification">notification channels</a> from those configured in the Alert Channel settings.</p>
<h3>## Test Notification</h3>
<p>A button to test the alert to ensure that it works as expected.</p>
<p><img src="https://signoz.io/img/docs/alerts/alerts-log-based-3.webp" alt="Configure the alert" /></p>
<p><em>Setting the alert metadata</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/log-based-alerts/#examples
tag_set: alerts-management, log-based-alerts
image_urls: https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-metric.png, https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-chart.png, https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-conditions.png, https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-configuration.png
tracking_id: docs-alerts-management-log-based-alerts-examples
group_tracking_ids: docs-alerts-management-log-based-alerts
<h2>Log based alerts: Examples</h2>
<h3>## 1. Alert when percentage of <code>redis timeout</code> error logs greater than 7% in last 5 mins</h3>
<h4>## Here's a video tutorial for creating this alert:</h4>
<h4>## Step 1: Write Query Builder query to define alert metric</h4>
<p><img src="https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-metric.png" alt="logs builder query for redis timeout logs percentage" /></p>
<p><em>Redis timeout query</em></p>
<p>Here we write 2 queries to calculate error logs percent. First query to count logs which are <code>redis timeout</code> error logs. Second query to count total logs. Then we add a formula to calculate percentage.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-chart.png" alt="error logs percentage chart" /></p>
<p><em>Error log percentage chart</em></p>
<p>‚úÖ Info</p>
<p>Remember to select y-axis unit as Percent(0-100) as we want to apply threshold in percent.</p>
<h4>## Step 2: Set alert conditions</h4>
<p><img src="https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-conditions.png" alt="redis timeout alert condition" /></p>
<p><em>Error logs percentage alert condition</em></p>
<p>The condition is set to trigger a notification if the per-minute error logs percentage exceeds the threshold of 1 second on average in the last five minutes.</p>
<h4>## Step 3: Set alert configuration</h4>
<p><img src="https://signoz.io/img/docs/product-features/alerts/logs/too-many-redis-timeouts-configuration.png" alt="redis timeout alert configuration" /></p>
<p><em>Error logs percentage alert configuration</em></p>
<p>At last configure the alert as <code>Warning</code>, add a name and notification channel.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/
tag_set: install, kubernetes
image_urls: 
tracking_id: docs-install-kubernetes
group_tracking_ids: docs-install-kubernetes
<h2>Kubernetes</h2>
<p>Learn how to install SigNoz on Kubernetes with Helm.</p>
<ul>
<li><a href="/docs/install/kubernetes/aws">üìÑÔ∏è Deploying to AWS: First, we need to set up a Kubernetes cluster (see the official AWS documentation for more info).</a></li>
<li><a href="/docs/install/kubernetes/gcp">üìÑÔ∏è Deploying to GCP: First, we need to set up a Kubernetes cluster (see the official GCP documentation for more info).</a></li>
<li><a href="/docs/install/kubernetes/others">üìÑÔ∏è Deploying with Helm directly: Follow the steps on this page to install SigNoz on other Kubernetes Cloud Platform and bare-metal servers with Helm</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/
tag_set: alerts-management, trace-based-alerts
image_urls: 
tracking_id: docs-alerts-management-trace-based-alerts
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts</h2>
<p>A Trace-based alert in SigNoz allows you to define conditions based on trace data, triggering alerts when these conditions are met. Here's a breakdown of the various sections and options available when configuring a Trace-based alert:</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/#step-1-define-the-trace-metric
tag_set: alerts-management, trace-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-trace-based-1.webp
tracking_id: docs-alerts-management-trace-based-alerts-step-1-define-the-trace-metric
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts: Step 1: Define the Trace Metric</h2>
<p>In this step, you use the <a href="https://signoz.io/docs/userguide/query-builder/#logs-and-traces-query-builder">Traces Query Builder</a> to perform operations on your Traces to define conditions based on traces data. Some of the fields that are available in Traces Query Builder includes</p>
<ul>
<li>
<p><strong>Traces</strong>: A field to filter the trace data to monitor.</p>
</li>
<li>
<p><strong>Aggregate Attribute</strong>: Allows you to choose how the trace data should be aggregated. You can use functions like &quot;Count&quot;</p>
</li>
<li>
<p><strong>Group by</strong>: Lets you group trace data by different span/trace attributes, like &quot;serviceName&quot;, &quot;Status&quot; or other custom attributes.</p>
</li>
<li>
<p><strong><a href="https://signoz.io/docs/userguide/query-builder/#legend-format">Legend Format</a></strong>: An optional field to define the format for the legend in the visual representation of the alert.</p>
</li>
<li>
<p><strong>Having</strong>: Apply conditions to filter the results further based on aggregate value.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-trace-based-1.webp" alt="Using Query Builder to perform operations on your Traces" /></p>
<p><em>Using Query Builder to perform operations on your Traces</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/#step-2-define-alert-conditions
tag_set: alerts-management, trace-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-trace-based-2.webp
tracking_id: docs-alerts-management-trace-based-alerts-step-2-define-alert-conditions
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts: Step 2: Define Alert Conditions</h2>
<p>In this step, you set specific conditions for triggering the alert and determine the frequency of checking these conditions:</p>
<ul>
<li>
<p><strong>Send a notification when [A] is [above/below] the threshold in total during the last [X] mins</strong>: A template to set the threshold for the alert, allowing you to define when the alert condition should be checked.</p>
</li>
<li>
<p><strong>Alert Threshold</strong>: A field to specify the threshold value for the alert condition.</p>
</li>
<li>
<p><strong>More Options</strong> :</p>
<ul>
<li>
<p><strong>Run alert every [X mins]</strong>: This option determines the frequency at which the alert condition is checked and notifications are sent.</p>
</li>
<li>
<p><strong>Send a notification if data is missing for [X] mins</strong>: A field to specify if a notification should be sent when data is missing for a certain period.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/alerts/alerts-trace-based-2.webp" alt="Define the alert conditions" /></p>
<p><em>Define the alert conditions</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/#step-3-alert-configuration
tag_set: alerts-management, trace-based-alerts
image_urls: https://signoz.io/img/docs/alerts/alerts-trace-based-3.webp
tracking_id: docs-alerts-management-trace-based-alerts-step-3-alert-configuration
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts: Step 3: Alert Configuration</h2>
<p>In this step, you set the alert's metadata, including severity, name, and description:</p>
<h3>## Severity</h3>
<p>Set the severity level for the alert (e.g., &quot;Warning&quot; or &quot;Critical&quot;).</p>
<h3>## Alert Name</h3>
<p>A field to name the alert for easy identification.</p>
<h3>## Alert Description</h3>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>Add a detailed description for the alert, explaining its purpose and trigger conditions.</p>
<p>You can incorporate result labels in the alert descriptions to make the alerts more informative:</p>
<h3>## Test Notification</h3>
<p>A button to test the alert to ensure that it works as expected.</p>
<p><img src="https://signoz.io/img/docs/alerts/alerts-trace-based-3.webp" alt="Configure the alert" /></p>
<p><em>Setting the alert metadata</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/trace-based-alerts/#examples
tag_set: alerts-management, trace-based-alerts
image_urls: 
tracking_id: docs-alerts-management-trace-based-alerts-examples
group_tracking_ids: docs-alerts-management-trace-based-alerts
<h2>Trace based alerts: Examples</h2>
<h3>## 1. Alert when external API latency (P90) is over 1 second for last 5 mins</h3>
<h4>## Here's a video tutorial for creating this alert:</h4>
<h4>## Step 1: Write Query Builder query to define alert metric</h4>
<p><img src="https://signoz.io/docs/alerts-management/trace-based-alerts/p90" alt="traces builder query for external API latency(p90)" /></p>
<p><em>External API latency (P90) query</em></p>
<p>Using <code>externalHttpUrl</code> attribute we can filter specific external API endpoint and then set aggregation attribute to durationNano with P90 aggregation operation to plot a chart which measures 90th percentile latency. You can also choose <code>Avg</code> or anyother operation as aggregate operation depending on your needs.</p>
<p><img src="https://signoz.io/docs/alerts-management/trace-based-alerts/p90" alt="external API latency(p90) chart" /></p>
<p><em>External API latency (P90) chart</em></p>
<p>‚úÖ Info</p>
<p>Remember to select y-axis unit as nanoseconds as our aggregate key is durationNano.</p>
<h4>## Step 2: Set alert conditions</h4>
<p><img src="https://signoz.io/docs/alerts-management/trace-based-alerts/p90" alt="external API latency(p90) condition" /></p>
<p><em>External API latency (P90) alert condition</em></p>
<p>The condition is set to trigger a notification if the per-minute external API latency (P90) exceeds the threshold of 1 second all the time in the last five minutes.</p>
<h4>## Step 3: Set alert configuration</h4>
<p><img src="https://signoz.io/docs/alerts-management/trace-based-alerts/p90" alt="external API latency(p90) alert configuration" /></p>
<p><em>External API latency (P90) alert configuration</em></p>
<p>At last configure the alert as <code>Warning</code>, add a name and notification channel.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/slack/
tag_set: alerts-management, notification-channel, slack
image_urls: 
tracking_id: docs-alerts-management-notification-channel-slack
group_tracking_ids: docs-alerts-management-notification-channel-slack
<h2>Configure Slack Channel</h2>
<h2>Prerequisites</h2>
<hr />
<p>Before configuring Slack as a notification channel in SigNoz, ensure that you have:</p>
<ul>
<li><strong>Incoming Webhook</strong>: Follow the steps outlined in <a href="https://api.slack.com/messaging/webhooks">sending messages to slack using Incoming Webhook</a> to set up an Incoming Webhook in your Slack workspace.</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.5.0">v0.5.0</a> or later</li>
</ul>
<h2>Accessing Alert Channels</h2>
<hr />
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
<h2>Creating a new Notification channel</h2>
<hr />
<p>To create a new Slack notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Slack as the channel type.</li>
<li><strong>Webhook URL</strong>: Paste the Incoming Webhook URL generated in Slack.</li>
<li><strong>Recipient</strong>: Specify channel or user, use #channel-name, @username (has to be all lowercase, no whitespace)</li>
</ul>
<p>üìù Note</p>
<p>You can use <a href="https://prometheus.io/docs/alerting/latest/notifications/">Go templates</a> for customizing the title and description.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fnew-notification-channel.webp&amp;w=3840&amp;q=75" alt="new-notification-channel" /></p>
<p><strong>Test Configuration</strong>: Click the Test button to send a test alert to the configured Slack channel. This verifies that SigNoz can communicate with your Slack webhook.</p>
<h2>Editing a Notification channel</h2>
<hr />
<p>To edit an existing Slack notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the slack webhook URL and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fedit-notification-channel.webp&amp;w=3840&amp;q=75" alt="edit-notification-channel" /></p>
<h2>Receive Alert in Slack</h2>
<hr />
<p>Once the configuration is set up correctly, you will receive alerts in the configured Slack channel whenever monitored metrics exceed the specified thresholds in alert rules. This ensures you are promptly notified of any issues in your applications or infrastructure components.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts-in-slack.webp&amp;w=1920&amp;q=75" alt="alerts-in-slack" /></p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter issues:</p>
<ul>
<li><strong>Check the Webhook URL</strong>: Ensure the webhook URL is correctly entered in SigNoz.</li>
<li><strong>Verify Slack Permissions</strong>: Confirm that the webhook has permissions to post to the desired channel.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/telemetry/
tag_set: telemetry
image_urls: 
tracking_id: docs-telemetry
group_tracking_ids: docs-telemetry
<h2>Telemetry</h2>
<p>We collect telemetry data during the installation process to identify errors.</p>
<p>In addition we ask you to send us regular usage telemetry during the setup process to understand which part of the product is being used. This would help us provide better user experience and improve on the product. Read on to understand more.</p>
<h3>## Docker Installation</h3>
<p>During the installation process, we collect stats of which installations were successful, which were unsuccessful and need our help. This is used to analyze the</p>
<ul>
<li>Installation Started</li>
<li>Installation Success</li>
<li>Installation Errors</li>
<li>Installation Support</li>
</ul>
<p>Sample Event</p>
<pre><code>{
  &quot;anonymousId&quot;: &quot;57348733c8f3310a27ca6e07149d3a64d17e66ac&quot;,
  &quot;event&quot;: &quot;Installation Success&quot;,
  &quot;integrations&quot;: {},
  &quot;messageId&quot;: &quot;api-25CYlWEssuKD6V2H9f393QQ515h&quot;,
  &quot;properties&quot;: {
    &quot;os&quot;: &quot;Mac&quot;,
    &quot;setup_type&quot;: &quot;clickhouse&quot;
  },
  &quot;receivedAt&quot;: &quot;2022-02-16T17:31:26.365Z&quot;,
  &quot;timestamp&quot;: &quot;2022-02-16T17:31:26.365Z&quot;,
  &quot;type&quot;: &quot;track&quot;
}
</code></pre>
<h3>## SigNoz Server</h3>
<p>The SigNoz backend server sends a keep-alive ping every 6 hours to indicate that it is still running without any errors. We also send some metrics around usage like number of spans and timeseries to understand the scale at which signoz is being deployed and hence helps us to proactively run benchmarks on similar scale and fix any performance issues that users might be facing.</p>
<p>Sample event</p>
<pre><code>{
  &quot;event&quot;: &quot;Heart Beat&quot;,
  &quot;integrations&quot;: {},
  &quot;messageId&quot;: &quot;61b423f7-3faa-4ed8-9be0-2b1f58b5bfde&quot;,
  &quot;originalTimestamp&quot;: &quot;2022-02-16T17:26:21.680714337Z&quot;,
  &quot;properties&quot;: {
      &quot;deploymentType&quot;: &quot;kubernetes-helm&quot;,
      &quot;getSamplesInfoInLastHeartBeatInterval&quot;: 94297,
      &quot;maxTS&quot;: 2527,
      &quot;spansInLastHeartBeatInterval&quot;: 774662,
      &quot;totalSpans&quot;: 9328025,
      &quot;totalTS&quot;: 3702,
      &quot;version&quot;: &quot;v0.9.2&quot;
  },
  &quot;receivedAt&quot;: &quot;2022-02-16T17:26:26.665Z&quot;,
  &quot;sentAt&quot;: &quot;2022-02-16T17:26:25.490Z&quot;,
  &quot;timestamp&quot;: &quot;2022-02-16T17:26:22.855Z&quot;,
  &quot;type&quot;: &quot;track&quot;,
  &quot;userId&quot;: &quot;49.207.216.180&quot;,
}
</code></pre>
<h3>## SigNoz UI</h3>
<p>You can anonymise the following data during the set up process.</p>
<p>SigNoz UI captures behavioural data around navigation, clicks and total number of services. No data about your services or payload is captured.</p>
<p>Sample Event</p>
<pre><code>{
  &quot;event&quot;: &quot;API Call&quot;,
  &quot;integrations&quot;: {},
  &quot;messageId&quot;: &quot;d7c5d487-798f-4116-bdfc-804bffbd679c&quot;,
  &quot;originalTimestamp&quot;: &quot;2022-02-11T18:11:07.195662884Z&quot;,
  &quot;properties&quot;: {
    &quot;path&quot;: &quot;/api/v1/rules&quot;,
    &quot;version&quot;: &quot;v0.6.1&quot;
  },
  &quot;receivedAt&quot;: &quot;2022-02-11T18:11:10.044Z&quot;,
  &quot;sentAt&quot;: &quot;2022-02-11T18:11:09.971Z&quot;,
  &quot;timestamp&quot;: &quot;2022-02-11T18:11:07.267Z&quot;,
  &quot;type&quot;: &quot;track&quot;,
  &quot;userId&quot;: &quot;73.188.32.135&quot;
}
</code></pre>
<h4>## Disable Telemetry</h4>
<p>Set below env variable in query-service in docker-compose.yaml or helm chart and restart query-service</p>
<pre><code>TELEMETRY_ENABLED=false
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<h2>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.</h2>
<p>At SigNoz we store our data on ClickHouse. In this documentation, we will go through the schema of the logs table and see how we can write clickhouse queries to create different dashboard panels from Logs Data.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#logs-schema
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-logs-schema
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<h2>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Logs Schema</h2>
<p>If we check the schema of the logs table in clickhouse this is what it looks like. The table was created with respect to the <a href="https://opentelemetry.io/docs/specs/otel/logs/data-model/">OpenTelemetry Logs Data Model</a></p>
<pre><code>(
    `timestamp` UInt64 CODEC(DoubleDelta, LZ4),
    `observed_timestamp` UInt64 CODEC(DoubleDelta, LZ4),
    `id` String CODEC(ZSTD(1)),
    `trace_id` String CODEC(ZSTD(1)),
    `span_id` String CODEC(ZSTD(1)),
    `trace_flags` UInt32,
    `severity_text` LowCardinality(String) CODEC(ZSTD(1)),
    `severity_number` UInt8,
    `body` String CODEC(ZSTD(2)),
    `resources_string_key` Array(String) CODEC(ZSTD(1)),
    `resources_string_value` Array(String) CODEC(ZSTD(1)),
    `attributes_string_key` Array(String) CODEC(ZSTD(1)),
    `attributes_string_value` Array(String) CODEC(ZSTD(1)),
    `attributes_int64_key` Array(String) CODEC(ZSTD(1)),
    `attributes_int64_value` Array(Int64) CODEC(ZSTD(1)),
    `attributes_float64_key` Array(String) CODEC(ZSTD(1)),
    `attributes_float64_value` Array(Float64) CODEC(ZSTD(1)),
    `attributes_bool_key` Array(String) CODEC(ZSTD(1)),
    `attributes_bool_value` Array(Bool) CODEC(ZSTD(1)),
    INDEX body_idx body TYPE tokenbf_v1(10240, 3, 0) GRANULARITY 4,
    INDEX id_minmax id TYPE minmax GRANULARITY 1,
    INDEX severity_number_idx severity_number TYPE set(25) GRANULARITY 4,
    INDEX severity_text_idx severity_text TYPE set(25) GRANULARITY 4,
    INDEX trace_flags_idx trace_flags TYPE bloom_filter GRANULARITY 4
)
</code></pre>
<p>There is a distributed logs table which references the above table in each shard. The name of the table is <code>distributed_logs</code>. The schema is same as above.</p>
<p>üìù Note</p>
<p>Any query that we write will be written for the <code>distributed_logs</code> table.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#columns-in-the-logs-table
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-columns-in-the-logs-table
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<h2>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table</h2>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>timestamp</strong></td>
<td>Time when the log line was generated at the source. The default value is the time at which it is received and it can be changed using the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/time_parser.md">time parser</a>&lt;br&gt;.</td>
</tr>
<tr>
<td><strong>observed_timestamp</strong></td>
<td>Time when the log line is observed at the collection system. It is automatically added by the collector.</td>
</tr>
<tr>
<td><strong>id</strong></td>
<td>It is a <a href="https://github.com/segmentio/ksuid">ksuid</a>&lt;br&gt;, it helps us in paginating and sorting log lines. It is automatically added by the collector.</td>
</tr>
<tr>
<td><strong>trace_id</strong></td>
<td>Trace ID of the log line. <a href="https://www.w3.org/TR/trace-context/#trace-id">W3C Trace Context</a>&lt;br&gt;. It can be filled using <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/trace_parser.md">trace parser</a>&lt;br&gt;.</td>
</tr>
<tr>
<td><strong>span_id</strong></td>
<td>Span ID for the log line or set of log lines that are part of a particular processing span. It can be filled using <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/trace_parser.md">trace parser</a>&lt;br&gt;.</td>
</tr>
<tr>
<td><strong>trace_flags</strong></td>
<td>Trace Flag of the log line. <a href="https://www.w3.org/TR/trace-context/#trace-flags">W3C Trace Context</a>&lt;br&gt;. It can be filled using <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/trace_parser.md">trace parser</a>&lt;br&gt;.</td>
</tr>
<tr>
<td><strong>severity_text</strong></td>
<td>It is the log level. eg:- <code>info</code> . It can be filled using <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/severity_parser.md">severity parser</a>&lt;br&gt;.</td>
</tr>
<tr>
<td><strong>severity_number</strong></td>
<td>Numerical value of the <a href="https://opentelemetry.io/docs/specs/otel/logs/data-model/#field-severitynumber">severity_text</a>&lt;br&gt;. It can be filled using <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/severity_parser.md">severity parser</a>&lt;br&gt;.</td>
</tr>
<tr>
<td><strong>body</strong></td>
<td>The body/message of the log record.</td>
</tr>
<tr>
<td><strong>resources_string_key</strong></td>
<td>If we have a resource named <code>source: nginx</code> . Then <code>source</code> is stored in this column as an array value.</td>
</tr>
<tr>
<td><strong>resource_string_value</strong></td>
<td>If we have a resource named <code>source: nginx</code> . Then <code>nginx</code> is stored in this column as an array value and the index will be same as the corresponding key in <code>resources_string_key</code>.</td>
</tr>
<tr>
<td><strong>attributes_string_key</strong></td>
<td>If we have a string attribute named <code>method: GET</code> . Then <code>method</code> is stored in this column as an array value.</td>
</tr>
<tr>
<td><strong>attributes_string_value</strong></td>
<td>If we have a string attribute named <code>method: GET</code> . Then <code>GET</code> is stored in this column as an array value and the index will be same as the corresponding key in <code>attributes_string_key</code>.</td>
</tr>
<tr>
<td><strong>attributes_int64_key</strong></td>
<td>If we have a integer attribute named <code>bytes: 100</code> . Then <code>bytes</code> is stored in this column as an array value.</td>
</tr>
<tr>
<td><strong>attributes_int64_value</strong></td>
<td>If we have a integer attribute named <code>bytes: 100</code> . Then <code>100</code> is stored in this column as an array value and the index will be same as the corresponding key in <code>attributes_int64_key</code>.</td>
</tr>
<tr>
<td><strong>attributes_float64_key</strong></td>
<td>If we have a floating attribute named <code>delay: 10.0</code> . Then <code>delay</code> is stored in this column as an array value.</td>
</tr>
<tr>
<td><strong>attributes_float64_value</strong></td>
<td>If we have a floating attribute named <code>dealy: 10.0</code> . Then <code>10.0</code> is stored in this column as an array value and the index will be same as the corresponding key in <code>attributes_float64_key</code>.</td>
</tr>
<tr>
<td><strong>attributes_bool_key</strong></td>
<td>If we have a boolean attribute named <code>success: true</code> . Then <code>success</code> is stored in this column as an array value.</td>
</tr>
<tr>
<td><strong>attributes_bool_value</strong></td>
<td>If we have a boolean attribute named <code>success: true</code> . Then <code>true</code> is stored in this column as an array value and the index will be same as the corresponding key in <code>attributes_bool_key</code>.</td>
</tr>
</tbody>
</table>
<p>The attributes and resources can be added and transformed using different processors and operators. You can read more about them <a href="/docs/userguide/logs/#operators-for-parsing-and-manipulating-logs">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#selected-attributesresources-
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-selected-attributesresources
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<h2>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Selected Attributes/Resources:-</h2>
<p>When an attribute/resource field is converted to <a href="/docs/userguide/logs_fields/#selected-log-fields">selected(indexed) field</a>. Then two new columns are added.</p>
<p>Ex: if our attribute name is <code>method</code> which is present in <code>attributes_string_key</code> and it's value is present in <code>attributes_string_value</code> then the corresponding columns that will be created are <code>attribute_string_method</code> and <code>attribute_string_method_exists</code>. It will look like following in the logs schema.</p>
<pre><code>`attribute_string_method` String MATERIALIZED attributes_string_value[indexOf(attributes_string_key, 'xyz')] CODEC(ZSTD(1)),
`attribute_string_method_exists` Bool MATERIALIZED if(indexOf(attributes_string_key, 'xyz') != 0, true, false) CODEC(ZSTD(1)),
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#writing-clickhouse-queries-for-dashboard-panels
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-writing-clickhouse-queries-for-dashboard-panels
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<h2>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Writing Clickhouse Queries for Dashboard Panels</h2>
<p>While writing queries for logs table, if you want to use an attribute/resource attribute in your query you will have to reference it in the following format <code>&lt;type&gt;_&lt;dataType&gt;_value[indexOf(&lt;type&gt;_&lt;dataType&gt;_key, &lt;keyname&gt;)]</code></p>
<p>where <code>type</code> can be <code>attributes/resources</code> , <code>dataType</code> can be <code>int64/float64/string</code> and <code>keyname</code> is the name of the key.</p>
<p>Eg: If your <code>keyname</code> is <code>status</code> of <code>dataType</code> <code>string</code> and <code>type</code> <code>attribute</code>, it needs to be referenced as <code>attributes_string_value[indexOf(attributes_string_key, 'status')]</code></p>
<p>üìù Note</p>
<p>In the above example, if <code>status</code> is an <a href="#selected-attributesresources-">selected field</a> , then it can be referenced as <code>attribute_string_status</code></p>
<p>We will use two variables i.e <code>{{.start_timestamp_nano}}</code> and <code>{{.end_timestamp_nano}}</code> while writing our queries to specify the time range.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#timeseries
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-timeseries
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Timeseries</p>
<p>This panel is used when you want to view your aggregated data in a timeseries.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#count-of-log-lines-per-minute
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-count-of-log-lines-per-minute
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: #### ## Examples - Count of log lines per minute</p>
<pre><code>SELECT 
    toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS ts,
    toFloat64(count()) AS value 
FROM 
    signoz_logs.distributed_logs  
WHERE 
    (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}})
GROUP BY ts 
ORDER BY ts ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#count-of-log-lines-per-minute-group-by-container-name
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-count-of-log-lines-per-minute-group-by-container-name
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Count of log lines per minute group by container name</p>
<pre><code>SELECT 
    toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS ts,
    attributes_string_value[indexOf(attributes_string_key, 'container_name')] as container_name,
    toFloat64(count()) AS value 
FROM 
    signoz_logs.distributed_logs  
WHERE 
    (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}}) AND 
    indexOf(attributes_string_key, 'container_name') &gt; 0
GROUP BY container_name, ts
ORDER BY ts ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#count-of-log-lines-per-minute-where-severity_text--info
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-count-of-log-lines-per-minute-where-severity_text-info
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Count of log lines per minute where <code>severity_text = 'INFO'</code></p>
<pre><code>SELECT 
    toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS ts, 
    toFloat64(count()) AS value 
FROM 
    signoz_logs.distributed_logs  
WHERE 
    (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}}) AND 
    severity_text='INFO'
GROUP BY ts 
ORDER BY ts ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#count-of-log-lines-per-minute-where-severity_text--info---method--get--service_name--demo-here-method-is-an-attribute-while-service_name-is-a-resource-attribute
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-count-of-log-lines-per-minute-where-severity_text-info--method-get-service_name-demo-here-method-is-an-attribute-while-service_name-is-a-resource-attribute
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Count of log lines per minute where <code>severity_text = 'INFO'</code> , <code>method = 'GET'</code> , <code>service_name = 'demo'</code>. Here <code>method</code> is an attribute while <code>service_name</code> is a resource attribute.</p>
<pre><code>SELECT 
    toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS ts, 
    toFloat64(count()) AS value 
FROM 
    signoz_logs.distributed_logs  
WHERE 
    (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}}) AND
    severity_text='INFO' AND
    attributes_string_value[indexOf(attributes_string_key, 'method')] = 'GET' AND 
    resources_string_value[indexOf(resources_string_key, 'service_name')] = 'demo'
GROUP BY ts 
ORDER BY ts ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#count-of-log-lines-per-minute-where-severity_text--info---method--get--service_name--demo-here-method-is-an-attribute-while-service_name-is-a-resource-attribute-and-both-method-and-service_name-is-selected-field
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-count-of-log-lines-per-minute-where-severity_text-info--method-get-service_name-demo-here-method-is-an-attribute-while-service_name-is-a-resource-attribute-and-both-method-and-service_name-is-selected-field
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Count of log lines per minute where <code>severity_text = 'INFO'</code> , <code>method = 'GET'</code> , <code>service_name = 'demo'</code>. Here <code>method</code> is an attribute while <code>service_name</code> is a resource attribute and both <code>method</code> and <code>service_name</code> is <a href="#selected-attributesresources-">selected field</a></p>
<pre><code>SELECT 
    toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS ts, 
    toFloat64(count()) AS value 
FROM 
    signoz_logs.distributed_logs  
WHERE 
    (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}}) AND 
    severity_text='INFO' AND 
    attribute_string_method = 'GET' AND 
    resource_string_service_name = 'demo'
GROUP BY ts 
ORDER BY ts ASC;
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#value
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-value
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Value</p>
<p>For the value type panel, the overall query will be similar to timeseries, just that you will have to get the absolute value at the end. You can reduce your end result to either average, latest, sum, min, or max.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#average-count-of-log-lines-where-severity_text--info---method--get--service_name--demo-here-method-is-an-attribute-while-service_name-is-a-resource-attribute
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-average-count-of-log-lines-where-severity_text-info--method-get-service_name-demo-here-method-is-an-attribute-while-service_name-is-a-resource-attribute
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: #### ## Examples - Average count of log lines where <code>severity_text = 'INFO'</code> , <code>method = 'GET'</code> , <code>service_name = 'demo'</code>. Here <code>method</code> is an attribute while <code>service_name</code> is a resource attribute.</p>
<pre><code>SELECT 
    avg(value) as value, 
    any(ts) as ts FROM (
        SELECT 
            toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS ts, 
            toFloat64(count()) AS value 
        FROM 
            signoz_logs.distributed_logs  
        WHERE 
            (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}}) AND
            severity_text='INFO' AND
            attributes_string_value[indexOf(attributes_string_key, 'method')] = 'GET' AND 
            resources_string_value[indexOf(resources_string_key, 'service_name')] = 'demo'
        GROUP BY ts 
        ORDER BY ts ASC
    )
</code></pre>
<p>üìù Note</p>
<p><code>attributes_string_value[indexOf(attributes_string_key, 'method')]</code> will change to <code>attribute_string_method</code> if <code>method</code> is a <a href="#selected-attributesresources-">selected field</a> and <code>resources_string_value[indexOf(resources_string_key, 'service_name')]</code> will change to <code>resource_string_service_name</code> if <code>service_name</code> is a <a href="#selected-attributesresources-">selected field</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#table
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-table
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Table</p>
<p>This is used when you want to view the timeseries data in a tabular format.</p>
<p>The query is similar to timeseries query but instead of using time interval we use just use <code>now() as ts</code> in select.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#count-of-log-lines-where-severity_text--info---method--get--group-by--service_name-here-method-is-an-attribute-while-service_name-is-a-resource-attribute
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-count-of-log-lines-where-severity_text-info--method-get-group-by-service_name-here-method-is-an-attribute-while-service_name-is-a-resource-attribute
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<p>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: #### ## Examples - Count of log lines where <code>severity_text = 'INFO'</code> , <code>method = 'GET'</code> group by <code>service_name</code>. Here <code>method</code> is an attribute while <code>service_name</code> is a resource attribute.</p>
<pre><code>SELECT 
    now() as ts,
    resources_string_value[indexOf(resources_string_key, 'service_name')] as service_name,
    toFloat64(count()) AS value 
FROM 
    signoz_logs.distributed_logs  
WHERE 
    (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}}) AND
    severity_text='INFO' AND
    attributes_string_value[indexOf(attributes_string_key, 'method')] = 'GET' 
GROUP BY service_name, ts 
ORDER BY ts ASC;
</code></pre>
<p>üìù Note</p>
<p><code>attributes_string_value[indexOf(attributes_string_key, 'method')]</code> will change to <code>attribute_string_method</code> if <code>method</code> is a <a href="#selected-attributesresources-">selected field</a> and <code>resources_string_value[indexOf(resources_string_key, 'service_name')]</code> will change to <code>resource_string_service_name</code> if <code>service_name</code> is a <a href="#selected-attributesresources-">selected field</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#real-life-use-cases-example
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-real-life-use-cases-example
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<h2>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Real Life Use Cases Example</h2>
<h3>## Number of log lines generated by each kubernetes cluster</h3>
<pre><code>SELECT 
    toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS ts,
    resources_string_value[indexOf(resources_string_key, 'k8s_cluster_name')] as k8s_cluster_name,
    toFloat64(count()) AS value 
FROM 
    signoz_logs.distributed_logs  
WHERE 
    (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}}) AND 
    indexOf(resources_string_key, 'k8s_cluster_name') &gt; 0
GROUP BY k8s_cluster_name, ts
ORDER BY ts ASC;
</code></pre>
<p>üìù Note</p>
<p><code>resources_string_value[indexOf(resources_string_key, 'k8s_cluster_name')]</code> will change to <code>resource_string_k8s_cluster_name</code> if <code>k8s_cluster_name</code> is a <a href="#selected-attributesresources-">selected field</a> and <code>indexOf(resources_string_key, 'k8s_cluster_name') &gt; 0</code> will change to <code>resource_string_k8s_cluster_name_exists = true</code></p>
<h3>## Number of error logs generated by each service</h3>
<pre><code>SELECT 
    toStartOfInterval(fromUnixTimestamp64Nano(timestamp), INTERVAL 1 MINUTE) AS ts,
    resources_string_value[indexOf(resources_string_key, 'service_name')] as service_name,
    toFloat64(count()) AS value 
FROM 
    signoz_logs.distributed_logs  
WHERE 
    (timestamp &gt;= {{.start_timestamp_nano}} AND timestamp &lt;= {{.end_timestamp_nano}}) AND 
    severity_text='ERROR' AND
    indexOf(resources_string_key, 'service_name') &gt; 0
GROUP BY service_name,ts 
ORDER BY ts ASC;
</code></pre>
<p>üìù Note</p>
<p><code>resources_string_value[indexOf(resources_string_key, 'service_name')]</code> will change to <code>resource_string_service_name</code> if <code>service_name</code> is a <a href="#selected-attributesresources-">selected field</a> and <code>indexOf(resources_string_key, 'service_name') &gt; 0</code> will change to <code>resource_string_service_name_exists = true</code></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_clickhouse_queries/#panel-time-preference
tag_set: userguide, logs_clickhouse_queries
image_urls: 
tracking_id: docs-userguide-logs_clickhouse_queries-panel-time-preference
group_tracking_ids: docs-userguide-logs_clickhouse_queries
<h2>Logs Schema and Writing ClickHouse Queries for Building Dashboard Panels.: Columns in the Logs Table: Writing Clickhouse Queries for Dashboard Panels: Panel Time preference</h2>
<p>Using the <code>Panel Time Preference</code> present on the right you can select a custom time range for your panel. When you open the dashboard the specific panel will render for the time specified for that panel.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/guides/apm-metrics/
tag_set: traces-management, guides, apm-metrics
image_urls: 
tracking_id: docs-traces-management-guides-apm-metrics
group_tracking_ids: docs-traces-management-guides-apm-metrics
<h2>Guide to APM metrics</h2>
<p>SigNoz processes span data and generates metrics from it. This guide will help you understand how SigNoz generates metrics from span data. The metrics generated by SigNoz are the following:</p>
<p>Aggregates Request, Error and Duration (R.E.D) metrics from span data.</p>
<p>Metrics generated by this processor:</p>
<ul>
<li>signoz_calls_total</li>
<li>signoz_latency_sum</li>
<li>signoz_latency_count</li>
<li>signoz_latency_bucket</li>
<li>signoz_db_latency_sum</li>
<li>signoz_db_latency_count</li>
<li>signoz_external_call_latency_sum</li>
<li>signoz_external_call_latency_count</li>
</ul>
<p><strong>Request</strong> counts are computed as the number of spans seen per unique set of dimensions, including Errors. For example, the following metric shows 142 calls:</p>
<pre><code>signoz_calls_total{http_status_code=&quot;200&quot;,operation=&quot;/Address&quot;,service_name=&quot;shippingservice&quot;,span_kind=&quot;SPAN_KIND_SERVER&quot;,status_code=&quot;STATUS_CODE_UNSET&quot;} 142
</code></pre>
<p>Multiple metrics can be aggregated if, for instance, a user wishes to view call counts just on <code>service_name</code> and <code>operation</code>.</p>
<p><strong>Error</strong> counts are computed from the Request counts which have an &quot;Error&quot; Status Code metric dimension. For example, the following metric indicates 220 errors:</p>
<pre><code>signoz_calls_total{http_status_code=&quot;503&quot;,operation=&quot;/checkout&quot;,service_name=&quot;frontend&quot;,span_kind=&quot;SPAN_KIND_CLIENT&quot;,status_code=&quot;STATUS_CODE_ERROR&quot;} 220
</code></pre>
<p><strong>Duration</strong> is computed from the difference between the span start and end times and inserted into the relevant latency histogram time bucket for each unique set dimensions. For example, the following latency buckets indicate the vast majority of spans (9K) have a 100ms latency:</p>
<pre><code>signoz_latency_bucket{http_status_code=&quot;200&quot;,label1=&quot;value1&quot;,operation=&quot;/Address&quot;,service_name=&quot;shippingservice&quot;,span_kind=&quot;SPAN_KIND_SERVER&quot;,status_code=&quot;STATUS_CODE_UNSET&quot;,le=&quot;2&quot;} 327
signoz_latency_bucket{http_status_code=&quot;200&quot;,label1=&quot;value1&quot;,operation=&quot;/Address&quot;,service_name=&quot;shippingservice&quot;,span_kind=&quot;SPAN_KIND_SERVER&quot;,status_code=&quot;STATUS_CODE_UNSET&quot;,le=&quot;6&quot;} 751
signoz_latency_bucket{http_status_code=&quot;200&quot;,label1=&quot;value1&quot;,operation=&quot;/Address&quot;,service_name=&quot;shippingservice&quot;,span_kind=&quot;SPAN_KIND_SERVER&quot;,status_code=&quot;STATUS_CODE_UNSET&quot;,le=&quot;10&quot;} 1195
signoz_latency_bucket{http_status_code=&quot;200&quot;,label1=&quot;value1&quot;,operation=&quot;/Address&quot;,service_name=&quot;shippingservice&quot;,span_kind=&quot;SPAN_KIND_SERVER&quot;,status_code=&quot;STATUS_CODE_UNSET&quot;,le=&quot;100&quot;} 10180
signoz_latency_bucket{http_status_code=&quot;200&quot;,label1=&quot;value1&quot;,operation=&quot;/Address&quot;,service_name=&quot;shippingservice&quot;,span_kind=&quot;SPAN_KIND_SERVER&quot;,status_code=&quot;STATUS_CODE_UNSET&quot;,le=&quot;250&quot;} 10180
...
</code></pre>
<p>Each metric will have the following dimensions because they are common across all spans:</p>
<ul>
<li>Service name</li>
<li>Operation</li>
<li>Span kind</li>
<li>Status code</li>
<li>HTTP status code</li>
</ul>
<p><strong>DB Latency</strong> is computed from the difference between the span start and end times and inserted into the relevant latency histogram time bucket for each unique set dimensions. The average DB latency is computed by dividing the sum of the DB latency by the count of the DB latency.</p>
<p>Each metric will have the following dimensions because they are common across all spans:</p>
<ul>
<li>Service name</li>
<li>Operation</li>
<li>Span kind</li>
<li>Status code</li>
<li>Database system</li>
<li>Database name</li>
</ul>
<p><strong>External Call Latency</strong> is computed from the difference between the span start and end times and inserted into the relevant latency histogram time bucket for each unique set dimensions. The average external call latency is computed by dividing the sum of the external call latency by the count of the external call latency.</p>
<p>Each metric will have the following dimensions because they are common across all spans:</p>
<ul>
<li>Service name</li>
<li>Operation</li>
<li>Span kind</li>
<li>Status code</li>
<li>External service address</li>
<li>HTTP status code</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#introduction
tag_set: product-features, trace-explorer
image_urls: 
tracking_id: docs-product-features-trace-explorer-introduction
group_tracking_ids: docs-product-features-trace-explorer
<h2>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: Traces Explorer in SigNoz - Introduction</h2>
<p>The Traces Explorer page in SigNoz enables developers to filter, examine, and analyze traces. There are four different views available in Traces Explorer which include:</p>
<ul>
<li>
<p><a href="#list-view">List View</a></p>
</li>
<li>
<p><a href="#traces-view">Trace View</a></p>
</li>
<li>
<p><a href="#time-series-view">Time Series View</a></p>
</li>
<li>
<p><a href="#table-view">Table View</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#list-view
tag_set: product-features, trace-explorer
image_urls: https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-list-view.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-list-view-1.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-list-view-2.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-list-view-3.webp
tracking_id: docs-product-features-trace-explorer-list-view
group_tracking_ids: docs-product-features-trace-explorer
<p>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: List View</p>
<p>The List View is the default view on the Traces Explorer page.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-list-view.webp" alt="List View in the Trace Explorer" /></p>
<p><em>List View in Trace Explorer</em></p>
<p>You can perform the following operations in List view:</p>
<p><strong>Apply Filters</strong></p>
<ul>
<li>
<p><strong>Filtering</strong> - Narrow down the trace data based on specific filters such as <code>responseStatusCode</code> .</p>
</li>
<li>
<p><strong>Order By</strong> - Use <code>Order By</code> to filter the data in a particular order.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-list-view-1.webp" alt="Filter and Order By in List View in the Traces Explorer" /></p>
<p><em>Filter and Order By in List View in Trace Explorer</em></p>
<p>üìù Note</p>
<ul>
<li>The <strong>Group By</strong>, <strong>Add a query</strong> and <strong>Add a function</strong> features are not supported in List View.</li>
</ul>
<p><strong>Customize Columns</strong></p>
<p>Add or delete columns using the <code>Options</code> button. For example, you can add the <code>httpMethod</code> column to see the request methods alongside each trace.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-list-view-2.webp" alt="Add or Delete column in List View of the Traces Explorer" /></p>
<p><em>Add or Delete column in List View</em></p>
<p>üìù Note</p>
<ul>
<li>Timestamp column can't be removed.</li>
</ul>
<p><strong>Number of Spans to display</strong></p>
<p>You can increase the number of spans displayed upto 200/page using the dropdown shown in image below.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-list-view-3.webp" alt="List View in the Trace Explorer" /></p>
<p><em>Increase the number of spans displayed in List View</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#traces-view
tag_set: product-features, trace-explorer
image_urls: https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-traces-view.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-traces-view-1.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-traces-view-2.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-traces-view-3.webp
tracking_id: docs-product-features-trace-explorer-traces-view
group_tracking_ids: docs-product-features-trace-explorer
<p>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: Traces View</p>
<p>The Traces View focuses on analysis of traces related to root span. A root span also know as a Parent span is the starting point of a trace.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-traces-view.webp" alt="Traces View in the Trace Explorer" /></p>
<p><em>Traces View in Trace Explorer</em></p>
<p>You can perform the following operations in Traces view:</p>
<p><strong>Apply Filters</strong></p>
<p>Filters applied in this view will only be applied on the root spans, not the entire traces.</p>
<p>For example,</p>
<p>When you apply filters for <code>serviceName</code> or <code>durationNano</code>(For the Root span duration) these will be applied on the Root Span only.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-traces-view-1.webp" alt="Applying Filter in Traces View in Trace Explorer" /></p>
<p><em>Applying Filter in Traces View in Trace Explorer</em></p>
<p>üìù Note</p>
<ul>
<li>The <strong>Group By</strong>, <strong>Add a query</strong> and <strong>Add a function</strong> features are not supported in List View.</li>
<li>Right now there is no support to filter by <code>No. of Spans</code></li>
</ul>
<p><strong>Root Duration</strong></p>
<p>Traces are sorted by descending order of duration which helps in identifying the longest running trace.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-traces-view-2.webp" alt="Root Durations in Traces View in Trace Explorer" /></p>
<p><em>Root Duration in Traces View</em></p>
<p><strong>Number of Traces to display</strong></p>
<p>You can increase the number of spans upto 200/page using the dropdown shown in image below.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-traces-view-3.webp" alt=" Increase the number of spans displayed in Traces View" /></p>
<p><em>Increase the number of spans displayed in Traces View</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#time-series-view
tag_set: product-features, trace-explorer
image_urls: https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-timeseries-view.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-timeseries-view-1.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-timeseries-view-2.webp
tracking_id: docs-product-features-trace-explorer-time-series-view
group_tracking_ids: docs-product-features-trace-explorer
<p>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: Time Series View</p>
<p>The Time Series View provides a graphical representation of trace data over time.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-timeseries-view.webp" alt="Timeseries View in the Trace Explorer" /></p>
<p><em>Timeseries View in Trace Explorer</em></p>
<p>The Time Series View has the following components:</p>
<p><strong>Query Builder</strong></p>
<p>Query builder helps in filtering, aggregation and grouping of your traces data. Refer to this <a href="https://signoz.io/docs/userguide/query-builder/">documentation</a> to read more about the capabilities of the Query Builder.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-timeseries-view-1.webp" alt="Query Builder in Timeseries View" /></p>
<p><em>Query Builder in Timeseries View</em></p>
<p><strong>Graph View</strong></p>
<p>This provides you with a graphical representation of the output of Query made using the Query Builder.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-timeseries-view-2.webp" alt="Graph View in Timeseries View" /></p>
<p><em>Graph View in Timeseries View</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#table-view
tag_set: product-features, trace-explorer
image_urls: https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-table-view.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-table-view-1.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-table-view-2.webp
tracking_id: docs-product-features-trace-explorer-table-view
group_tracking_ids: docs-product-features-trace-explorer
<p>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: Table View</p>
<p>The Table View provides a tabular representation of the trace data.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-table-view.webp" alt="Table View in the Trace Explorer" /></p>
<p><em>Table View in Trace Explorer</em></p>
<p><strong>Query Builder</strong></p>
<p>Just like Time Series View, the Query builder in Table view also helps in filtering, aggregation and grouping of your traces data. Refer to this <a href="https://signoz.io/docs/userguide/query-builder/">documentation</a> to read more about the capabilities of the Query Builder.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-table-view-1.webp" alt="Query Builder in Table View" /></p>
<p><em>Query Builder in Table View</em></p>
<p><strong>Table</strong></p>
<p>This provides you with a tabular representation of the output of Query made using the Query Builder.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-table-view-2.webp" alt="Table in Table View" /></p>
<p><em>Table in Table View</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#quick-filters
tag_set: product-features, trace-explorer
image_urls: https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-quick-filters.webp
tracking_id: docs-product-features-trace-explorer-quick-filters
group_tracking_ids: docs-product-features-trace-explorer
<h2>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: Quick Filters: Quick Filters</h2>
<p>The Traces Explorer in SigNoz provides a feature for quick filtering of spans using various parameters. This functionality allows users to quickly search their specific spans, making it easier to analyze and debug applications. Here's a detailed explanation of the available filters and how to use them.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-quick-filters.webp" alt="Quick Filters in Traces Explorer" /></p>
<p><em>Quick Filters in Traces Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#available-filters
tag_set: product-features, trace-explorer
image_urls: 
tracking_id: docs-product-features-trace-explorer-available-filters
group_tracking_ids: docs-product-features-trace-explorer
<p>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: Quick Filters: Available Filters</p>
<ol>
<li>
<p><strong>Duration</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans based on their duration.</li>
<li><strong>Usage</strong>: Specify a minimum and maximum duration in milliseconds to find spans that fall within this time range. This is useful for identifying spans that are unusually long or short.</li>
</ul>
</li>
<li>
<p><strong>Status</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans by their status.</li>
<li><strong>Usage</strong>: Options can include <code>Error</code> and <code>Ok</code>. Use this filter to quickly identify spans that resulted in errors or were successful.</li>
</ul>
</li>
<li>
<p><strong>Service Name</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans by the name of the service that generated them.</li>
<li><strong>Usage</strong>: Select from a list of service names to isolate spans from a specific service. This can be helpful in microservices architectures where multiple services generate spans.</li>
</ul>
</li>
<li>
<p><strong>Operation / Name</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans based on the operation or name.</li>
<li><strong>Usage</strong>: Select specific operations, such as HTTP GET requests or SQL SELECT queries, to focus on particular types of operations within your spans.</li>
</ul>
</li>
<li>
<p><strong>RPC Method</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans by the RPC method used.</li>
<li><strong>Usage</strong>: Useful for isolating spans based on the remote procedure call methods.</li>
</ul>
</li>
<li>
<p><strong>Status Code</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans by their HTTP status code.</li>
<li><strong>Usage</strong>: Select from common status codes like <code>200</code>, <code>404</code>, <code>500</code>, etc., to find spans that resulted in specific HTTP responses.</li>
</ul>
</li>
<li>
<p><strong>HTTP Host</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans by the HTTP host.</li>
<li><strong>Usage</strong>: Specify the host to find spans related to particular HTTP hosts.</li>
</ul>
</li>
<li>
<p><strong>HTTP Method</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans by the HTTP method.</li>
<li><strong>Usage</strong>: Choose HTTP methods such as <code>GET</code>, <code>POST</code>, etc., to filter spans based on the type of HTTP request.</li>
</ul>
</li>
<li>
<p><strong>HTTP Route</strong>:</p>
<ul>
<li><strong>Description</strong>: Filter spans by the HTTP route.</li>
<li><strong>Usage</strong>: Specify routes to find spans related to specific paths in your application.</li>
</ul>
</li>
<li>
<p><strong>HTTP URL</strong>:</p>
</li>
</ol>
<pre><code>*   **Description**: Filter spans by the full HTTP URL.
*   **Usage**: Useful for isolating spans related to specific URLs, providing a more granular filter compared to HTTP routes.
</code></pre>
<ol start="11">
<li><strong>Trace ID</strong>:</li>
</ol>
<pre><code>*   **Description**: Filter spans by their unique Trace ID.
*   **Usage**: Enter a specific Trace ID to find and analyze an individual trace. This is particularly useful for deep dives into specific transactions.
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#how-to-use-quick-filters
tag_set: product-features, trace-explorer
image_urls: 
tracking_id: docs-product-features-trace-explorer-how-to-use-quick-filters
group_tracking_ids: docs-product-features-trace-explorer
<p>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: Quick Filters: How to Use Quick Filters</p>
<ol>
<li><strong>Accessing Filters</strong>: Open the Traces Explorer and locate the Filters panel on the left side.</li>
<li><strong>Selecting Filters</strong>: Click on the desired filter category to expand it. For example, click on &quot;Duration&quot; to set a time range.</li>
<li><strong>Setting Filter Values</strong>: Enter or select the appropriate values for the filter. For instance, you can specify a minimum and maximum duration or select specific status codes.</li>
<li><strong>Applying Filters</strong>: Once you set the filter values, the Traces Explorer will automatically update the displayed spans based on the selected filters.</li>
<li><strong>Clearing Filters</strong>: If you need to reset a filter, click on the &quot;Clear All&quot; option next to the filter category. You can also clear all applied filters at once by clicking on a Reset button available at the top.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/trace-explorer/#bottom-bar
tag_set: product-features, trace-explorer
image_urls: https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-bottom-bar.webp, https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-bottom-bar-save-view.webp
tracking_id: docs-product-features-trace-explorer-bottom-bar
group_tracking_ids: docs-product-features-trace-explorer
<h2>Traces Explorer in SigNoz: Traces Explorer in SigNoz - Introduction: Quick Filters: Bottom Bar</h2>
<p>The bar present at the Bottom of each view in Traces Explorer has multiple helpful features.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-bottom-bar.webp" alt="Bottom Bar of Traces Explorer" /></p>
<p><em>Bottom Bar in Traces Explorer</em></p>
<h3>## Save View</h3>
<p>At the Bottom of each View available in the Traces Explorer, we have an option to <strong>Save this View</strong> . What this does is, it saves the specific settings that you have applied for the particular View as a template which can be accessed later from the <code>Views Tab</code> available at the top or from <code>Select a View</code> dropdown. To know more details about Save View, please refer to this <a href="https://signoz.io/docs/product-features/saved-view/">documentation</a>.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-bottom-bar-save-view.webp" alt="Save view" /></p>
<p><em>Save view</em></p>
<h3>## Create an Alert</h3>
<p>Just next to Save this view, we have the <strong>Create an Alert</strong> button which can be used to instantly create a <a href="https://signoz.io/docs/alerts-management/metrics-based-alerts/">Trace based Alert</a> according to the filters applied in our current view.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-bottom-bar-create-an-alert.gif" alt="Create an Alert" /></p>
<p><em>Create an Alert</em></p>
<h3>## Add to Dashboard</h3>
<p>Next to Create an Alert, we have the <strong>Add to Dashboard</strong> button using which you can add the current panel to a New or any other existing dashboard.</p>
<p><img src="https://signoz.io/img/docs/product-features/trace-explorer/trace-explorer-bottom-bar-add-to-dashboard.gif" alt="Add to Dashboard" /></p>
<p><em>Add to Dashboard</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/docker-swarm/
tag_set: install, docker-swarm
image_urls: 
tracking_id: docs-install-docker-swarm
group_tracking_ids: docs-install-docker-swarm
<h2>Docker Swarm</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>This section provides information on installing SigNoz on Docker Swarm.</p>
<h2>Prerequisites</h2>
<hr />
<ul>
<li>
<p>A Linux or macOS machine. Microsoft Windows is not officially supported.</p>
</li>
<li>
<p><a href="https://docs.docker.com/get-docker/">Docker Engine</a>
. A minimum of 4GB of memory must be allocated to each Docker node.</p>
</li>
<li>
<p><a href="https://docs.docker.com/compose/install/">Docker Compose</a></p>
</li>
<li>
<p><a href="https://desktop.github.com/">Git client</a></p>
</li>
</ul>
<h2>Install SigNoz on Docker Swarm</h2>
<hr />
<ol>
<li>
<p>In a directory of your choosing, clone the SigNoz repository and 'cd' into the <code>signoz/deploy</code> directory by entering the following commands:</p>
<pre><code>git clone -b main https://github.com/SigNoz/signoz.git &amp;&amp; cd signoz/deploy/
</code></pre>
</li>
<li>
<p>Initialize a single-node swarm by entering the following command:</p>
<pre><code>docker swarm init
</code></pre>
<p>The output should look similar as shown below:</p>
<pre><code>Swarm initialized: current node (6muco3j7jjuo6k4rbiq8yr8fw) is now a manager.

To add a worker to this swarm, run the following command:

docker swarm join --token SWMTKN-1-6ak6diq1lbrwemx17up9c1ph039h64z0dxksjxv647qnqrd290-4tt6q22dd462p4lf2n6bqbnt4 192.168.65.3:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
</code></pre>
</li>
<li>
<p><em>(Optional)</em> You can use the <code>docker swarm join</code> command to add more nodes to the swarm. Note that the node you added in the previous step is the manager. For details, see the <a href="https://docs.docker.com/engine/reference/commandline/swarm_join/">Docker Swarm Join</a> page of the Docker documentation.</p>
</li>
<li>
<p>Deploy SigNoz by entering the <code>docker stack deploy command</code> and specifying the following:</p>
<ul>
<li>
<p><code>-c</code> and the path to the Compose file (<code>docker-swarm/clickhouse-setup/docker-compose.yaml</code>)</p>
</li>
<li>
<p>The name of the stack (<code>signoz</code>)</p>
<p>docker stack deploy -c docker-swarm/clickhouse-setup/docker-compose.yaml signoz</p>
</li>
</ul>
<p>The output should look similar to the following:</p>
<pre><code>Creating network signoz_default
Creating service signoz_query-service
Creating service signoz_frontend
Creating service signoz_otel-collector
Creating service signoz_hotrod
Creating service signoz_load-hotrod
Creating service signoz_clickhouse
Creating service signoz_alertmanager
</code></pre>
</li>
<li>
<p><em>(Optional)</em> By default, the instructions in this document create three replicas, and each replica can handle 50K spans per second. To handle an increased load, perform the steps in the <a href="/docs/operate/docker-swarm/#scale-up">Scale Up</a> section of the <a href="/docs/operate/docker-swarm/">Operate on Docker Swarm</a>
page.</p>
</li>
</ol>
<h2>Verify the Installation</h2>
<hr />
<ol>
<li>
<p>Using the <code>docker stack services</code> command, monitor the SigNoz deployment process. Wait until all SigNoz services and replicas are created:</p>
<pre><code>docker stack services signoz
</code></pre>
<p>You should see the following output:</p>
<pre><code>ID             NAME                            MODE         REPLICAS   IMAGE                                          PORTS
6b67m0nuzf40   signoz_alertmanager             replicated   1/1        signoz/alertmanager:0.23.0-0.1
zgateenyifwv   signoz_clickhouse               replicated   1/1        yandex/clickhouse-server:21.12.3.32
vzc1gdx86f0w   signoz_frontend                 replicated   1/1        signoz/frontend:0.8.0                          *:3301-&gt;3301/tcp
dgisjp0vhv8m   signoz_hotrod                   replicated   1/1        jaegertracing/example-hotrod:1.30
336omtkvwukm   signoz_load-hotrod              replicated   1/1        grubykarol/locust:1.2.3-python3.9-alpine3.12
av5iggw983b5   signoz_otel-collector           replicated   3/3        signoz/otelcontribcol:0.43.0-0.1               *:4317-4318-&gt;4317-4318/tcp
hw28zb1hozu5   signoz_query-service            replicated   1/1        signoz/query-service:0.8.0                     *:8080-&gt;8080/tcp
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/docker-swarm/">Docker Swarm Operate</a> section for detailed instructions.</p>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>
<h2>Next Steps</h2>
<hr />
<ul>
<li>
<p><a href="/docs/instrumentation/overview/">Instrument Your Application</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/ecs-monitoring/
tag_set: ecs-monitoring
image_urls: 
tracking_id: docs-ecs-monitoring
group_tracking_ids: docs-ecs-monitoring
<h2>ECS Monitoring</h2>
<ul>
<li><a href="/docs/aws-monitoring/ecs-ec2-external/">üìÑÔ∏è EC2 / External: To monitor your ECS EC2 or external service...</a></li>
<li><a href="/docs/aws-monitoring/ecs-fargate/">üìÑÔ∏è Fargate: To monitor your ECS Fargate service, check out...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#overview
tag_set: aws-monitoring, elb-logs
image_urls: 
tracking_id: docs-aws-monitoring-elb-logs-overview
group_tracking_ids: docs-aws-monitoring-elb-logs
<h2>Send your ELB logs to SigNoz: Send your ELB logs to SigNoz - Overview</h2>
<p>This documentation provides a detailed walkthrough on how to set up an AWS Lambda function to collect Elastic Load Balancer (ELB) logs stored in an AWS S3 bucket and forward them to SigNoz. By the end of this guide, you will have a setup that automatically sends your ELB logs to SigNoz, enabling you to visualize and monitor your application's load balancing performance and health.</p>
<p><strong>Here‚Äôs a quick summary of what we‚Äôll be doing in this detailed article.</strong></p>
<ul>
<li>
<p><a href="#creating--configuring-your-s3-bucket">Creating / Configuring your S3 bucket</a></p>
</li>
<li>
<p><a href="#understanding-how-lambda-function-work">Understanding how lambda function work</a></p>
</li>
<li>
<p><a href="#creating-a-lambda-function">Creating a lambda function</a></p>
</li>
<li>
<p><a href="#the-lambda-function">The Lambda Function Code</a></p>
</li>
<li>
<p><a href="#visualize-the-logs-in-signoz">Visualize the logs in SigNoz</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#prerequisites
tag_set: aws-monitoring, elb-logs
image_urls: 
tracking_id: docs-aws-monitoring-elb-logs-prerequisites
group_tracking_ids: docs-aws-monitoring-elb-logs
<h2>Send your ELB logs to SigNoz: Prerequisites</h2>
<ul>
<li>AWS account with administrative privilege.</li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#creating--configuring-your-s3-bucket
tag_set: aws-monitoring, elb-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-create_bucket_2.webp
tracking_id: docs-aws-monitoring-elb-logs-creating-configuring-your-s3-bucket
group_tracking_ids: docs-aws-monitoring-elb-logs
<h2>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket</h2>
<p>To accomplish the task described, please follow these steps:</p>
<ol>
<li><strong>Creating an S3 Bucket:</strong>
<ul>
<li>Sign in to the AWS Management Console.</li>
<li>Navigate to the Amazon S3 service.</li>
<li>Click on <strong>Create bucket</strong>.</li>
<li>Enter a unique bucket name, select the region, and configure any additional settings if needed (such as versioning, logging, etc.).</li>
<li>Click <strong>Create bucket</strong> to finalize the creation process.</li>
</ul>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-create_bucket_2.webp" alt="" /></p>
<p><em>Fill up bucket details</em></p>
<ol start="2">
<li>
<p><strong>Uploading Data to the S3 Bucket:</strong></p>
<ul>
<li>After creating the bucket, navigate to the S3 Management Console.</li>
<li>Click on the <strong>Upload</strong> button.</li>
<li>Select the files you wish to upload from your local machine.</li>
<li>Optionally, configure settings like encryption, permissions, etc.</li>
<li>Click <strong>Upload</strong> to upload the selected files to the bucket.</li>
</ul>
</li>
<li>
<p><strong>File Formats:</strong></p>
<ul>
<li>You can upload files in various formats such as .json, .csv, .log, .gz, .zip, etc.</li>
<li>If you have configured Elastic Load Balancer (ELB) logging, ELB logs are automatically saved in <code>.gzip</code> format in your S3 bucket.</li>
</ul>
</li>
</ol>
<p>‚úÖ Info</p>
<p>Ensure that you have appropriate permissions and follow AWS best practices for security and cost optimization when creating and using S3 buckets. Refer to <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/enable-access-logging.html#enable-access-logs">this link</a> to know more about setting up your S3 bucket to automatically collect ELB logs.</p>
<p>üìù Note</p>
<p>Please be advised that all logs will undergo conversion to JSON format before transmission. Consequently, it may be necessary to perform supplementary preprocessing of the logs as part of this conversion process. Here preprocessing of the logs means getting them from S3 bucket, separating each log line based on the delimiter (&quot;,&quot; or based on quotes) and assigning them to respective keys. Thus making a key, value pair before sending to SigNoz.</p>
<p>To move forward, we assume that you already have some data in your S3 bucket.</p>
<p>‚úÖ Info</p>
<p>For the scope of this documentation, we assume that all the data in S3 bucket is in the same format. For example, if one file is in <code>.csv</code> format, then all files within the bucket will be in <code>.csv</code> format. For files in different formats, you will have to use different parsing functions for each format or update the existing function accordingly.</p>
<p>The general header(table) format of ELB logs is:</p>
<pre><code>elb_headers= [&quot;type&quot;,&quot;time&quot;,&quot;elb&quot;,&quot;client_port&quot;,&quot;target_port&quot;,\
&quot;request_processing_time&quot;,&quot;target_processing_time&quot;,&quot;response_processing_time&quot;,\
&quot;elb_status_code&quot;,&quot;target_status_code&quot;,&quot;received_bytes&quot;,&quot;sent_bytes&quot;,&quot;request&quot;,\
&quot;user_agent&quot;,&quot;ssl_cipher&quot;,&quot;ssl_protocol&quot;,&quot;target_group_arn&quot;,&quot;trace_idd&quot;,\
&quot;domain_name&quot;,&quot;chosen_cert_arn&quot;,&quot;matched_rule_priority&quot;,\
&quot;request_creation_time&quot;,&quot;actions_executed&quot;,&quot;redirect_url&quot;,\
&quot;error_reason&quot;,&quot;target_port_list&quot;,&quot;target_status_code_list&quot;,\
&quot;classification&quot;,&quot;classification_reason&quot;]
</code></pre>
<p>‚úÖ Info</p>
<p>Note that these headers are just for name sake, you can change them if you wish to, but it is not advisable. You might have noticed in the <code>elb_headers</code>, We have mentioned <code>trace_idd</code> with 2 dd's and not <code>trace_id</code>. This is not a typo, by default, ELB logs have <code>trace_id</code> as the header name but payload structure SigNoz recommends is shown below:</p>
<pre><code>[\
	{\
		&quot;timestamp&quot;: ,\
		&quot;trace_id&quot;: ,\
		&quot;span_id&quot;: ,\
		&quot;trace_flags&quot;: \
		&quot;severity_text&quot;: ,\
		&quot;severity_number&quot;: ,\
		&quot;attributes&quot;: ,\
		&quot;resources&quot;: ,\
		&quot;body&quot;: ,\
	}\
]
</code></pre>
<p>Source - <a href="https://signoz.io/docs/userguide/send-logs-http/">https://signoz.io/docs/userguide/send-logs-http/</a></p>
<p><code>trace_id</code> here is of the format <code>&lt;hex string&gt;</code> and because we‚Äôll sending everything as string json for simplicity and less data processing and parsing, hence the change in field name from trace_id to trace_idd. [If you use <code>trace_id</code> and send the trace_id as a string, you‚Äôll get 400 error, this can resolved by further logs formatting]</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#understanding-how-lambda-function-work
tag_set: aws-monitoring, elb-logs
image_urls: 
tracking_id: docs-aws-monitoring-elb-logs-understanding-how-lambda-function-work
group_tracking_ids: docs-aws-monitoring-elb-logs
<h2>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: Understanding how lambda function work</h2>
<p>When you successfully attach your lambda function with the S3 bucket and configure it correctly, any new addition / deletion / copy / PUT etc, requests made to the S3 bucket will trigger the lambda function and the code written in the lambda function will get executed.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#creating-a-lambda-function
tag_set: aws-monitoring, elb-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_1.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_2.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_3.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_4.webp
tracking_id: docs-aws-monitoring-elb-logs-creating-a-lambda-function
group_tracking_ids: docs-aws-monitoring-elb-logs
<p>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: Creating a lambda function</p>
<p>Follow these steps to create the lambda function:</p>
<p><strong>Step 1:</strong> Go to your AWS console and search for AWS Lambda, go to <strong>Functions</strong> and click on <strong>Create Function</strong>.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_1.webp" alt="" /></p>
<p><em>Create Lambda function from AWS Console</em></p>
<p><strong>Step 2:</strong> Choose the <code>Author from scratch</code> checkbox and proceed to fill in the function name.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_2.webp" alt="" /></p>
<p><em>Choose the Author from scratch and fill up other details</em></p>
<p><strong>Step 3:</strong> Choose <code>Python 3.x</code> as the Runtime version, <code>x86_64</code> as Architecture (preferably), and keep other settings as default. Select <code>Create a new role with basic Lambda permissions</code>for now, we‚Äôll requiring more permissions down the lane. So for now, select this option.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_3.webp" alt="" /></p>
<p><em>Choose Create a new role here</em></p>
<p><strong>Step 4:</strong> Once you are done configuring the lambda function, you Lambda function is created.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_4.webp" alt="" /></p>
<p><em>Your barebones Lambda function is created now</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#configuring-policies-for-lambda-function
tag_set: aws-monitoring, elb-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_5.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_6.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_7.webp, https://signoz.io/img/docs/elb/elb-logs-policies-used.webp
tracking_id: docs-aws-monitoring-elb-logs-configuring-policies-for-lambda-function
group_tracking_ids: docs-aws-monitoring-elb-logs
<p>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: Configuring Policies for Lambda function</p>
<p>As said in Step 4 previously, we need extra permissions in order to access the S3 Bucket for execution of our Lambda code, follow along to set it up.</p>
<p><strong>Step 1:</strong> Scroll down from your Lambda page, you‚Äôll see a few tabs there. Go to <code>Configurations</code> and select <code>Permissions</code> from the left sidebar.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_5.webp" alt="" /></p>
<p><em>Choosing execution role from Configurations tab</em></p>
<p><strong>Step 2:</strong> Click on the <code>Execution Role name</code> link just under Role name, it will take us to AWS IAM page. Here we will add policies to get full S3 access. Once here, click on the <code>Add permissions</code> button and select <code>Attach policies</code> from the drop down list.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_6.webp" alt="" /></p>
<p><em>Attach policies to your Lambda function</em></p>
<p><strong>Step 3:</strong> Search ‚ÄúS3‚Äù and you‚Äôll a policy named <code>AmazonS3FullAccess</code>select that and proceed.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_7.webp" alt="" /></p>
<p><em>Policies you'll need to run your Lambda function</em></p>
<p>‚ö†Ô∏è Warning</p>
<p>It's advisable to proceed with caution when granting full S3 access, particularly in a production environment. Before deploying your Lambda function with such extensive permissions, it's essential to consult with your system administrator or designated authority to ensure compliance with security protocols and best practices. This step helps mitigate potential risks and ensures that access permissions align with organizational guidelines and requirements.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-policies-used.webp" alt="" /></p>
<p><em>Extra policies you might require to execute your Lambda function</em></p>
<p>Please refer to the image above as a comprehensive guide to the policy names that you may consider adding to your Lambda function. Failure to include these policies could result in insufficient privileges, potentially hindering the function's ability to perform necessary operations within the AWS environment.</p>
<p>Congrats, you are just done with one of the major hurdle in running your code. Now, let‚Äôs add a trigger.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#adding-triggers
tag_set: aws-monitoring, elb-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_8.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_9.webp
tracking_id: docs-aws-monitoring-elb-logs-adding-triggers
group_tracking_ids: docs-aws-monitoring-elb-logs
<p>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: Adding Triggers</p>
<p>You need to use the Lambda console to build a trigger so that your function can be called immediately by another AWS service (S3, in our case). A trigger is a resource you set up to enable your function to be called by another AWS service upon the occurrence of specific events or conditions.</p>
<p>A function may have more than one trigger. Every trigger functions as a client, independently calling your method, and Lambda transfers data from a single trigger to each event it passes to your function.</p>
<p>To setup the trigger, follow these steps:</p>
<p><strong>Step 1:</strong> Click on the <code>+ Add trigger</code> button from the Lambda console.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_8.webp" alt="" /></p>
<p><em>Add a trigger to the function.</em></p>
<p><strong>Step 2:</strong> Select <code>S3</code> from the first drop down of AWS services list. Pick your S3 bucket for the second field.</p>
<p><strong>Step 3:</strong> For the Event types field, you can select any number of options you wish. The trigger will occur depending upon what option(s) you choose here. By default, the <code>All object create events</code> will be selected.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_9.webp" alt="" /></p>
<p><em>Choose event types from the drop down menu</em></p>
<p>Verify the settings and click on <code>Add</code> button at bottom right to add this trigger.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#adding-request-layer
tag_set: aws-monitoring, elb-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-layer_3.webp, https://signoz.io/img/docs/elb/elb-logs-layer_4.webp, https://signoz.io/img/docs/elb/elb-logs-layer_5.webp, https://signoz.io/img/docs/elb/elb-logs-layer_6.webp
tracking_id: docs-aws-monitoring-elb-logs-adding-request-layer
group_tracking_ids: docs-aws-monitoring-elb-logs
<p>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: Adding Request Layer</p>
<p>We will be using python's <code>request</code> module which is <a href="https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/">not included by default</a> in Lambda.</p>
<p>To utilize Python's <code>requests</code> module within a Lambda function, it's necessary to explicitly add it as a layer. While there may be alternative approaches, it's advisable to adhere to established practices that have been thoroughly tested and proven effective. Therefore, we will proceed with adding <code>requests</code> as a layer to ensure reliable functionality within the Lambda environment.</p>
<p><strong>Step 1:</strong> Follow the steps below to create a zip of the request module and add it as a layer to make it work on AWS lambda.</p>
<p>The commands you‚Äôd need:</p>
<pre><code># make a new directory
mkdir python
# move into that directory
cd python

# install requests module
pip install --target . requests
# zip the contents under the name dependencies.zip
zip -r dependencies.zip ../python 
</code></pre>
<p><strong>Step 2:</strong> To upload your zip file, go to AWS Lambda &gt; Layers and click on <code>Create Layer</code>. [Not inside your specific Lambda function, just the landing page of AWS Lambda].</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_3.webp" alt="" /></p>
<p><em>Creating a new Layer</em></p>
<p><strong>Step 3:</strong> You‚Äôll be redirected to Layer configurations page, here, give a name to your layer, an optional description, select <code>Upload a .zip file</code> , click on <code>Upload</code> and locate the requirements.zip file.</p>
<p><strong>Step 4:</strong> Select your desired architecture and pick <code>Python 3.x</code> as your runtime. Hit <code>Create</code>. Your layer has now been created. Now lets connect it to our Lambda function which we created to send logs to SigNoz.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_4.webp" alt="" /></p>
<p><em>Uploading the requirements.zip file to later add it as a layer</em></p>
<p><strong>Step 5:</strong> Go to your Lambda function, scroll down to Layers section and on the right of it, you‚Äôll find a button that says <code>Add a layer</code> to click on.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_5.webp" alt="" /></p>
<p><em>Add a layer to your Lambda function</em></p>
<p><strong>Step 6:</strong> Pick <code>Custom layers</code> from the checkbox and select your custom layer from the given drop down below and then click on the button <code>Add</code>.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_6.webp" alt="" /></p>
<p><em>Choose your layer name</em></p>
<p>Congratulations, the <code>requests</code> module has been successfully integrated into your code area. By adding this layer, you have resolved the 'request module not found error' that would have otherwise occurred.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#the-lambda-function
tag_set: aws-monitoring, elb-logs
image_urls: 
tracking_id: docs-aws-monitoring-elb-logs-the-lambda-function
group_tracking_ids: docs-aws-monitoring-elb-logs
<p>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: The Lambda Function: The Lambda Function: The Lambda Function</p>
<p>Now, we come to the pivotal section of this document: the code implementation.</p>
<p>The Python script's primary function revolves around retrieving gzipped log files stored within an Amazon S3 bucket. Subsequently, it decompresses these files, transforms individual log entries into JSON objects, and transmits the resultant JSON data to a predetermined HTTP endpoint.</p>
<p>Below is the comprehensive code along with detailed comments for clarity:</p>
<pre><code>import json
import gzip
import boto3 
import requests
import shlex

# Create an S3 client
s3 = boto3.client('s3')

# Function to convert a log line into a JSON object
def convert_log_line_to_json(line):
    # Define the headers to be used for the JSON keys
    headers = [&quot;type&quot;, &quot;time&quot;, &quot;elb&quot;, &quot;client:port&quot;, &quot;target:port&quot;, &quot;request_processing_time&quot;, &quot;target_processing_time&quot;, &quot;response_processing_time&quot;, &quot;elb_status_code&quot;, &quot;target_status_code&quot;, &quot;received_bytes&quot;, &quot;sent_bytes&quot;, &quot;request&quot;,&quot;user_agent&quot;, &quot;ssl_cipher&quot;, &quot;ssl_protocol&quot;, &quot;target_group_arn&quot;, &quot;trace_idd&quot;, &quot;domain_name&quot;, &quot;chosen_cert_arn&quot;, &quot;matched_rule_priority&quot;, &quot;request_creation_time&quot;, &quot;actions_executed&quot;, &quot;redirect_url&quot;, &quot;error_reason&quot;, &quot;target:port_list&quot;, &quot;target_status_code_list&quot;, &quot;classification&quot;, &quot;classification_reason&quot;]

    # Split the log line using shell-like syntax (keeping quotes, etc.)
    res = shlex.split(line, posix=False)

    # Create a dictionary by zipping headers and log line parts
    return dict(zip(headers, res))

# Lambda function handler
def lambda_handler(event, context):
    # S3 bucket name
    bucket_name = '&lt;name_of_your_bucket&gt;'

    # List all objects in the specified S3 bucket
    response = s3.list_objects_v2(Bucket=bucket_name)

    # Iterate through each object in the bucket
    for obj in response['Contents']:
        # Check if the object is a gzipped log file
        if obj['Key'].endswith('.log.gz'):
            file_key = obj['Key']

            # Download the gzipped file content
            file_obj = s3.get_object(Bucket=bucket_name, Key=file_key)
            file_content = file_obj['Body'].read()

            # Decompress the gzipped content
            decompressed_content = gzip.decompress(file_content)

            # Convert bytes to string
            json_data = str(decompressed_content, encoding='utf-8')

            # Split the string into lines
            lines = json_data.strip().split('\n')

            # Convert the list of strings into a JSON-formatted string
            result = json.dumps(lines, indent=2)

            # Load the JSON-formatted string into a list of strings
            list_of_strings = json.loads(result)

            # Convert each log line string into a JSON object
            json_data = [convert_log_line_to_json(line) for line in list_of_strings]
            
            req_headers = {
                     'signoz-access-token': '&lt;SIGNOZ_INGESTION_KEY&gt;',
                     'Content-Type': 'application/json'
                }
            # Specify the HTTP endpoint for sending the data
            http_url = 'https://ingest.in.signoz.cloud:443/logs/json'  # Replace with your actual URL

            # Send the JSON data to the specified HTTP endpoint
            response = requests.post(http_url, json=json_data, headers=req_headers)

            # Print information about the sent data and the response received
            print(f&quot;Sent data to {http_url}. Response: {response.status_code}&quot;)
</code></pre>
<p>Here‚Äôs how a raw, unprocessed ELB log line looks like:</p>
<pre><code>https 2024-01-01T23:58:03.391277Z app/abc-prod-alb/0b46e552ds5b44da 35.244.22.76:41802 192.1.0.114:80 0.000 1.077 0.000 200 200 1430 923 &quot;POST https://api.abcs.com:4463/suporodv2/v1/get-result/ HTTP/1.1&quot; &quot;SFDC-Callout/59.0&quot; ECDHE-RSA-AES128-GCM-SHA256 TLSv1.2 arn:aws:elasticloadbalancing:ap-south-1:8429181216651:targetgroup/ecs-Sabc-P-Private-SU-API/02f1623fsddec2691ce &quot;Root=1-65343518a-72123e913f71cb2e20213a3ea9&quot; &quot;api.example-sabcs.com&quot; &quot;session-reused&quot; 98 2024-01-01T23:58:02.313000Z &quot;forward&quot; &quot;-&quot; &quot;-&quot; &quot;192.1.1.114:80‚Äù &quot;200&quot; &quot;-&quot; &quot;-&quot;
</code></pre>
<p>In the code, each field corresponds to a header. The purpose of the code above is to transmit Elastic Load Balancer (ELB) logs to the SigNoz endpoint.</p>
<p>‚ö†Ô∏è Warning</p>
<p>The provided code is functional, but exercise caution when copying and pasting it in its entirety. Incorrect configuration could result in the unintentional ingestion of a large volume of data.</p>
<p>Other than the above explanation and the code comments, in a nutshell, what the this code does is:</p>
<p>Sends the parsable content of <strong>ENTIRE</strong> S3 bucket whenever the lambda function gets triggered. It gets triggered by the condition you set above. Let‚Äôs mention that again here.</p>
<blockquote>
<p><strong>Step 3:</strong> For the Event types field, you can select any number of options you wish. The trigger will occur depending upon what option(s) you choose here. By default, the <code>All object create events</code> will be selected.</p>
</blockquote>
<p>Lets say you add something to your S3 bucket, it may / may not trigger this lambda function or if you have setup your s3 as if it automatically stores all your ELB/VPC logs, segregated in different folders, so whenever any new log gets added, the function will get triggered and send all the S3 data.</p>
<p>This is obviously not what everyone expects, ideal case would be to have a mass log transfer once the first connection is made to SigNoz otel-collector (which then they later get stored in gp2/gp3 storageClass of EBS), and then send logs lines of only the recently logged one.</p>
<p>To achieve this functionality, you need to add few conditions to the code.</p>
<ol>
<li>Assuming all standard log lines have a timestamp field.</li>
<li>Parse and select the timestamp field from the log line and add it before the <code>response = requests.post(http_url, json=json_data)</code> line as a if else condition to only send logs which are x days older (say 3 days).</li>
</ol>
<p>So, the function now will first check the log timestamp and only send those logs which are 3 days older (say) or even a few hours old.</p>
<p>Let‚Äôs consider the below pseudo code for better understanding:</p>
<pre><code>   from datetime import datetime, timedelta

   # Your given timestamp
   given_timestamp_str = &quot;2024-01-01T23:58:02.231919Z&quot;
   given_timestamp = datetime.fromisoformat(given_timestamp_str.replace('Z', '+00:00'))

   # Current time
   current_time = datetime.utcnow()

   # Calculate the time difference
   time_difference = current_time - given_timestamp

   # Check if the time difference is less than 3 days
   if time_difference &lt; timedelta(days=3):
       # Run your specific function here
       print(&quot;Running the specific function.&quot;)
   # ADD THE response = requests.post(http_url, json=json_data) LINE HERE

   else:
       print(&quot;Time difference exceeds 3 days. Function will not run.&quot;)
</code></pre>
<p>Feel free to modify any part of the code according to your requirements.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#running-the-code-locally
tag_set: aws-monitoring, elb-logs
image_urls: 
tracking_id: docs-aws-monitoring-elb-logs-running-the-code-locally
group_tracking_ids: docs-aws-monitoring-elb-logs
<p>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: The Lambda Function: Running the code locally.</p>
<p>If you want to run the entire setup locally in your laptop for testing purposes. Here‚Äôs the reference code for you:</p>
<pre><code>import os
import gzip
import json
import requests
import shlex

def convert_log_line_to_json(line):
    headers= [&quot;type&quot;,&quot;time&quot;,&quot;elb&quot;,&quot;client_port&quot;,&quot;target_port&quot;,&quot;request_processing_time&quot;,&quot;target_processing_time&quot;,&quot;response_processing_time&quot;,&quot;elb_status_code&quot;,&quot;target_status_code&quot;,&quot;received_bytes&quot;,&quot;sent_bytes&quot;,&quot;request&quot;,&quot;user_agent&quot;,&quot;ssl_cipher&quot;,&quot;ssl_protocol&quot;,&quot;target_group_arn&quot;,&quot;trace_idd&quot;,&quot;domain_name&quot;,&quot;chosen_cert_arn&quot;,&quot;matched_rule_priority&quot;,&quot;request_creation_time&quot;,&quot;actions_executed&quot;,&quot;redirect_url&quot;,&quot;error_reason&quot;,&quot;target_port_list&quot;,&quot;target_status_code_list&quot;,&quot;classification&quot;,&quot;classification_reason&quot;]
    res = shlex.split(line, posix = False)
    return dict(zip(headers, res))

def process_log_file(file_path):
    with gzip.open(file_path, 'r') as f:
        log_data = f.read().decode('utf-8') 
        lines = log_data.strip().split('\n')
        result = json.dumps(lines, indent=2)
        list_of_strings = json.loads(result)
        json_data = [convert_log_line_to_json(line) for line in list_of_strings]
        req_headers = {
                     'signoz-access-token': '&lt;SIGNOZ_INGESTION_KEY&gt;',
                     'Content-Type': 'application/json'
                }
        http_url = 'https://ingest.in.signoz.cloud:443/logs/json'  # Replace with your actual URL
        response = requests.post(http_url, json=json_data, headers=req_headers)

def main():
    root_folder = '&lt;folder_name&gt;'

    for root, _, files in os.walk(root_folder):
        for file in files:
            if file.endswith('.log.gz'):
                file_path = os.path.join(root, file)
                process_log_file(file_path)

if __name__ == '__main__':
    main()
</code></pre>
<p>‚úÖ Info</p>
<p>To incorporate log files into the folder, including nested directories, the code systematically examines all sub-folders to identify files ending with the <code>.log.gz</code> extension. Should you wish to target different file types, you have the flexibility to modify the code accordingly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#testing-your-lambda-function
tag_set: aws-monitoring, elb-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-timeout_increase.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_test_event.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_10.webp
tracking_id: docs-aws-monitoring-elb-logs-testing-your-lambda-function
group_tracking_ids: docs-aws-monitoring-elb-logs
<p>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: The Lambda Function: Testing your Lambda function</p>
<p>Once you've finished writing your code, it's crucial to deploy it and conduct thorough testing to ensure its functionality. Before proceeding with deployment and testing, it's important to consider adjusting the timeout setting for your Lambda function. This adjustment is necessary because the process of transferring data from S3 to an external endpoint may take several minutes, exceeding the default Lambda timeout of 3 seconds.</p>
<p>To extend the timeout duration, follow these steps:</p>
<ol>
<li>Navigate to the Lambda function configuration.</li>
<li>Access the &quot;General Configuration&quot; section.</li>
<li>Click on the &quot;Edit&quot; button to modify settings.</li>
<li>Increase the timeout value to a duration exceeding 10 minutes. Typically, the code execution completes within 1-4 minutes at most.</li>
</ol>
<p>By adjusting the timeout setting, you ensure that your Lambda function has sufficient time to complete the data transfer process without encountering timeouts. This proactive measure enhances the reliability and effectiveness of your deployed solution.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-timeout_increase.webp" alt="" /></p>
<p><em>Configuring execution timeout of Lambda function</em></p>
<p>Once you've finished adjusting the timeout setting, navigate to the code editor for your Lambda function. Locate the 'test' button, and from the dropdown menu, select the option labeled 'Configure test events.' Create a new test case by specifying it as an S3 PUT event, then save your configuration.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_test_event.webp" alt="" /></p>
<p><em>Creating Sample event</em></p>
<p>You're now prepared to proceed. Whenever you make alterations to the code and wish to evaluate them, follow these steps: Deploy the code first (equivalent to pressing 'Save'), and once it's fully deployed, proceed to click on the 'Test' button.</p>
<p>Below is an image showing the process of transmitting VPC logs (excluding ELB logs) to the SigNoz endpoint.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_10.webp" alt="" /></p>
<p><em>Visual representation of code in Lambda function</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#test-case-and-output
tag_set: aws-monitoring, elb-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_output.webp
tracking_id: docs-aws-monitoring-elb-logs-test-case-and-output
group_tracking_ids: docs-aws-monitoring-elb-logs
<p>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: The Lambda Function: Test Case and Output</p>
<p>If the logs are sent successfully, here's how they'll be transmitted. The following output displays the JSON-formatted data as we've printed it to visualize the sent information.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_output.webp" alt="" /></p>
<p><em>Visual representation of code output of Lambda function</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/elb-logs/#visualize-the-logs-in-signoz
tag_set: aws-monitoring, elb-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_12.webp
tracking_id: docs-aws-monitoring-elb-logs-visualize-the-logs-in-signoz
group_tracking_ids: docs-aws-monitoring-elb-logs
<h2>Send your ELB logs to SigNoz: Creating / Configuring your S3 bucket: Understanding how lambda function work: Visualize the logs in SigNoz</h2>
<p>Upon accessing the SigNoz logs section, you will notice a considerable influx of logs. You have the option to seamlessly transition to live monitoring of logs as well. Simply click on any log line to view its detailed information.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_12.webp" alt="A sample log line of the logs sent from AWS Lambda" /></p>
<p><em>A sample log line of the logs sent from AWS Lambda</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#overview
tag_set: aws-monitoring, vpc-logs
image_urls: 
tracking_id: docs-aws-monitoring-vpc-logs-overview
group_tracking_ids: docs-aws-monitoring-vpc-logs
<h2>Send your VPC logs to SigNoz: Send your VPC logs to SigNoz - Overview</h2>
<p>This documentation provides a detailed walkthrough on how to set up an AWS Lambda function to collect Virtual Private Cloud (VPC) logs stored in an AWS S3 bucket and forward them to SigNoz. By the end of this guide, you will have a setup that automatically sends your VPC logs to SigNoz, enabling you to visualize and monitor your application's load balancing performance and health.</p>
<p><strong>Here‚Äôs a quick summary of what we‚Äôll be doing in this detailed article.</strong></p>
<ul>
<li>
<p><a href="#creating--configuring-your-s3-bucket">Creating / Configuring your S3 bucket</a></p>
</li>
<li>
<p><a href="#understanding-how-lambda-function-work">Understanding how lambda function work</a></p>
</li>
<li>
<p><a href="#creating-a-lambda-function">Creating a lambda function</a></p>
</li>
<li>
<p><a href="#the-lambda-function">The Lambda Function Code</a></p>
</li>
<li>
<p><a href="#visualize-the-logs-in-signoz">Visualize the logs in SigNoz</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#prerequisites
tag_set: aws-monitoring, vpc-logs
image_urls: 
tracking_id: docs-aws-monitoring-vpc-logs-prerequisites
group_tracking_ids: docs-aws-monitoring-vpc-logs
<h2>Send your VPC logs to SigNoz: Prerequisites</h2>
<ul>
<li>AWS account with administrative privilege.</li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#creating--configuring-your-s3-bucket
tag_set: aws-monitoring, vpc-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-create_bucket_2.webp
tracking_id: docs-aws-monitoring-vpc-logs-creating-configuring-your-s3-bucket
group_tracking_ids: docs-aws-monitoring-vpc-logs
<h2>Send your VPC logs to SigNoz: Creating / Configuring your S3 bucket</h2>
<p>To accomplish the task described, please follow these steps:</p>
<ol>
<li><strong>Creating an S3 Bucket:</strong>
<ul>
<li>Sign in to the AWS Management Console.</li>
<li>Navigate to the Amazon S3 service.</li>
<li>Click on <strong>Create bucket</strong>.</li>
<li>Enter a unique bucket name, select the region, and configure any additional settings if needed (such as versioning, logging, etc.).</li>
<li>Click <strong>Create bucket</strong> to finalize the creation process.</li>
</ul>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-create_bucket_2.webp" alt="" /></p>
<p><em>Fill up bucket details</em></p>
<ol start="2">
<li>
<p><strong>Uploading Data to the S3 Bucket:</strong></p>
<ul>
<li>After creating the bucket, navigate to the S3 Management Console.</li>
<li>Click on the <strong>Upload</strong> button.</li>
<li>Select the files you wish to upload from your local machine.</li>
<li>Optionally, configure settings like encryption, permissions, etc.</li>
<li>Click <strong>Upload</strong> to upload the selected files to the bucket.</li>
</ul>
</li>
<li>
<p><strong>File Formats:</strong></p>
<ul>
<li>You can upload files in various formats such as .json, .csv, .log, .gz, .zip, etc.</li>
<li>If you have configured Virtual Private Cloud (VPC) logging, VPC logs are automatically saved in <code>.gzip</code> format in your S3 bucket.</li>
</ul>
</li>
</ol>
<p>‚úÖ Info</p>
<p>Ensure that you have appropriate permissions and follow AWS best practices for security and cost optimization when creating and using S3 buckets. Refer to <a href="https://docs.aws.amazon.com/vpc/latest/tgw/flow-logs-s3.html">this link</a> to know more about setting automatic VPC flowlogs collection to S3 bucket.</p>
<p>üìù Note</p>
<p>Please be advised that all logs will undergo conversion to JSON format before transmission. Consequently, it may be necessary to perform supplementary preprocessing of the logs as part of this conversion process. Here preprocessing of the logs means getting them from S3 bucket, separating each log line based on the delimiter (&quot;,&quot; or based on quotes) and assigning them to respective keys. Thus making a key, value pair before sending to SigNoz.</p>
<p>To move forward, we assume that you already have some data in your S3 bucket.</p>
<p>‚úÖ Info</p>
<p>For the scope of this documentation, we assume that all the data in S3 bucket is in the same format. For example, if one file is in <code>.csv</code> format, then all files within the bucket will be in <code>.csv</code> format. For files in different formats, you will have to use different parsing functions for each format or update the existing function accordingly.</p>
<p>The general header(table) format of VPC logs is:</p>
<pre><code>vpc_headers= [&quot;version&quot;,&quot;account-id&quot;,&quot;interface-id&quot;,&quot;srcaddr&quot;,&quot;dstaddr&quot;,&quot;srcport&quot;,\
&quot;dstport&quot;,&quot;protocol&quot;,&quot;packets&quot;,&quot;bytes&quot;,&quot;start&quot;,&quot;end&quot;,&quot;action&quot;,&quot;log-status&quot;]
</code></pre>
<p>‚úÖ Info</p>
<p>Note that these headers are just for name sake, you can change them if you wish to, but it is not advisable.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#understanding-how-lambda-function-work
tag_set: aws-monitoring, vpc-logs
image_urls: 
tracking_id: docs-aws-monitoring-vpc-logs-understanding-how-lambda-function-work
group_tracking_ids: docs-aws-monitoring-vpc-logs
<h2>Send your VPC logs to SigNoz: Understanding how lambda function work: Understanding how lambda function work</h2>
<p>When you successfully attach your lambda function with the S3 bucket and configure it correctly, any new addition / deletion / copy / PUT etc, requests made to the S3 bucket will trigger the lambda function and the code written in the lambda function will get executed.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#creating-a-lambda-function
tag_set: aws-monitoring, vpc-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_1.webp, https://signoz.io/img/docs/vpc-docs/create_function.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_3.webp, https://signoz.io/img/docs/vpc-docs/lambda_function.webp
tracking_id: docs-aws-monitoring-vpc-logs-creating-a-lambda-function
group_tracking_ids: docs-aws-monitoring-vpc-logs
<p>Send your VPC logs to SigNoz: Understanding how lambda function work: Creating a lambda function</p>
<p>Follow these steps to create the lambda function:</p>
<p><strong>Step 1:</strong> Go to your AWS console and search for AWS Lambda, go to <strong>Functions</strong> and click on <strong>Create Function</strong>.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_1.webp" alt="" /></p>
<p><em>Create Lambda function from AWS Console</em></p>
<p><strong>Step 2:</strong> Choose the <code>Author from scratch</code> checkbox and proceed to fill in the function name.</p>
<p><img src="https://signoz.io/img/docs/vpc-docs/create_function.webp" alt="" /></p>
<p><em>Choose the Author from scratch and fill up other details</em></p>
<p><strong>Step 3:</strong> Choose <code>Python 3.x</code> as the Runtime version, <code>x86_64</code> as Architecture (preferably), and keep other settings as default. Select <code>Create a new role with basic Lambda permissions</code>for now, we‚Äôll requiring more permissions down the lane. So for now, select this option.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_3.webp" alt="" /></p>
<p><em>Choose Create a new role here</em></p>
<p><strong>Step 4:</strong> Once you are done configuring the lambda function, you Lambda function is created.</p>
<p><img src="https://signoz.io/img/docs/vpc-docs/lambda_function.webp" alt="" /></p>
<p><em>Your barebones Lambda function is created now</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#configuring-policies-for-lambda-function
tag_set: aws-monitoring, vpc-logs
image_urls: https://signoz.io/img/docs/vpc-docs/execution_role.webp, https://signoz.io/img/docs/vpc-docs/add_policy.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_7.webp, https://signoz.io/img/docs/elb/elb-logs-policies-used.webp
tracking_id: docs-aws-monitoring-vpc-logs-configuring-policies-for-lambda-function
group_tracking_ids: docs-aws-monitoring-vpc-logs
<p>Send your VPC logs to SigNoz: Understanding how lambda function work: Configuring Policies for Lambda function</p>
<p>As said in Step 3 previously, we need extra permissions in order to access the S3 Bucket for execution of our Lambda code, follow along to set it up.</p>
<p><strong>Step 1:</strong> Scroll down from your Lambda page, you‚Äôll see a few tabs there. Go to <code>Configurations</code> and select <code>Permissions</code> from the left sidebar.</p>
<p><img src="https://signoz.io/img/docs/vpc-docs/execution_role.webp" alt="" /></p>
<p><em>Choosing execution role from Configurations tab</em></p>
<p><strong>Step 2:</strong> Click on the <code>Execution Role name</code> link just under Role name, it will take us to AWS IAM page. Here we will add policies to get full S3 access. Once here, click on the <code>Add permissions</code> button and select <code>Attach policies</code> from the drop down list.</p>
<p><img src="https://signoz.io/img/docs/vpc-docs/add_policy.webp" alt="" /></p>
<p><em>Attach policies to your Lambda function</em></p>
<p><strong>Step 3:</strong> Search ‚ÄúS3‚Äù and you‚Äôll a policy named <code>AmazonS3FullAccess</code>select that and proceed.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_7.webp" alt="" /></p>
<p><em>Policies you'll need to run your Lambda function</em></p>
<p>‚ö†Ô∏è Warning</p>
<p>It's advisable to proceed with caution when granting full S3 access, particularly in a production environment. Before deploying your Lambda function with such extensive permissions, it's essential to consult with your system administrator or designated authority to ensure compliance with security protocols and best practices. This step helps mitigate potential risks and ensures that access permissions align with organizational guidelines and requirements.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-policies-used.webp" alt="" /></p>
<p><em>Extra policies you might require to execute your Lambda function</em></p>
<p>Please refer to the image above as a comprehensive guide to the policy names that you may consider adding to your Lambda function. Failure to include these policies could result in insufficient privileges, potentially hindering the function's ability to perform necessary operations within the AWS environment.</p>
<p>Congrats, you are just done with one of the major hurdle in running your code. Now, let‚Äôs add a trigger.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#adding-triggers
tag_set: aws-monitoring, vpc-logs
image_urls: https://signoz.io/img/docs/vpc-docs/add_trigger.webp, https://signoz.io/img/docs/vpc-docs/add_trigger_2.webp
tracking_id: docs-aws-monitoring-vpc-logs-adding-triggers
group_tracking_ids: docs-aws-monitoring-vpc-logs
<p>Send your VPC logs to SigNoz: Understanding how lambda function work: Adding Triggers</p>
<p>You need to use the Lambda console to build a trigger so that your function can be called immediately by another AWS service (S3, in our case). A trigger is a resource you set up to enable your function to be called by another AWS service upon the occurrence of specific events or conditions.</p>
<p>A function may have more than one trigger. Every trigger functions as a client, independently calling your method, and Lambda transfers data from a single trigger to each event it passes to your function.</p>
<p>To setup the trigger, follow these steps:</p>
<p><strong>Step 1:</strong> Click on the <code>+ Add trigger</code> button from the Lambda console.</p>
<p><img src="https://signoz.io/img/docs/vpc-docs/add_trigger.webp" alt="" /></p>
<p><em>Add a trigger to the function.</em></p>
<p><strong>Step 2:</strong> Select <code>S3</code> from the first drop down of AWS services list. Pick your S3 bucket for the second field.</p>
<p><strong>Step 3:</strong> For the Event types field, you can select any number of options you wish. The trigger will occur depending upon what option(s) you choose here. By default, the <code>All object create events</code> will be selected.</p>
<p><img src="https://signoz.io/img/docs/vpc-docs/add_trigger_2.webp" alt="" /></p>
<p><em>Choose event types from the drop down menu</em></p>
<p>Verify the settings and click on <code>Add</code> button at bottom right to add this trigger.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#adding-request-layer
tag_set: aws-monitoring, vpc-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-layer_3.webp, https://signoz.io/img/docs/elb/elb-logs-layer_4.webp, https://signoz.io/img/docs/elb/elb-logs-layer_5.webp, https://signoz.io/img/docs/elb/elb-logs-layer_6.webp
tracking_id: docs-aws-monitoring-vpc-logs-adding-request-layer
group_tracking_ids: docs-aws-monitoring-vpc-logs
<p>Send your VPC logs to SigNoz: Understanding how lambda function work: Adding Request Layer</p>
<p>We will be using python's <code>request</code> module which is <a href="https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/">not included by default</a> in Lambda.</p>
<p>To utilize Python's <code>requests</code> module within a Lambda function, it's necessary to explicitly add it as a layer. While there may be alternative approaches, it's advisable to adhere to established practices that have been thoroughly tested and proven effective. Therefore, we will proceed with adding <code>requests</code> as a layer to ensure reliable functionality within the Lambda environment.</p>
<p><strong>Step 1:</strong> Follow the steps below to create a zip of the request module and add it as a layer to make it work on AWS lambda.</p>
<p>The commands you‚Äôd need:</p>
<pre><code># make a new directory
mkdir python
# move into that directory
cd python

# install requests module
pip install --target . requests
# zip the contents under the name dependencies.zip
zip -r dependencies.zip ../python 
</code></pre>
<p><strong>Step 2:</strong> To upload your zip file, go to AWS Lambda &gt; Layers and click on <code>Create Layer</code>. [Not inside your specific Lambda function, just the landing page of AWS Lambda].</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_3.webp" alt="" /></p>
<p><em>Creating a new Layer</em></p>
<p><strong>Step 3:</strong> You‚Äôll be redirected to Layer configurations page, here, give a name to your layer, an optional description, select <code>Upload a .zip file</code> , click on <code>Upload</code> and locate the requirements.zip file.</p>
<p><strong>Step 4:</strong> Select your desired architecture and pick <code>Python 3.x</code> as your runtime. Hit <code>Create</code>. Your layer has now been created. Now lets connect it to our Lambda function which we created to send logs to SigNoz.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_4.webp" alt="" /></p>
<p><em>Uploading the requirements.zip file to later add it as a layer</em></p>
<p><strong>Step 5:</strong> Go to your Lambda function, scroll down to Layers section and on the right of it, you‚Äôll find a button that says <code>Add a layer</code> to click on.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_5.webp" alt="" /></p>
<p><em>Add a layer to your Lambda function</em></p>
<p><strong>Step 6:</strong> Pick <code>Custom layers</code> from the checkbox and select your custom layer from the given drop down below and then click on the button <code>Add</code>.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_6.webp" alt="" /></p>
<p><em>Choose your layer name</em></p>
<p>Congratulations, the <code>requests</code> module has been successfully integrated into your code area. By adding this layer, you have resolved the <code>request module not found error</code> that would have otherwise occurred.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#the-lambda-function
tag_set: aws-monitoring, vpc-logs
image_urls: 
tracking_id: docs-aws-monitoring-vpc-logs-the-lambda-function
group_tracking_ids: docs-aws-monitoring-vpc-logs
<p>Send your VPC logs to SigNoz: Understanding how lambda function work: The Lambda Function: The Lambda Function: The Lambda Function</p>
<p>Now, we come to the pivotal section of this document: the code implementation.</p>
<p>The Python script's primary function revolves around retrieving gzipped log files stored within an Amazon S3 bucket. Subsequently, it decompresses these files, transforms individual log entries into JSON objects, and transmits the resultant JSON data to a predetermined HTTP endpoint.</p>
<p>Below is the comprehensive code along with detailed comments for clarity:</p>
<pre><code>import json
import gzip
import boto3 
import requests
import shlex

# Create an S3 client
s3 = boto3.client('s3')

# Function to convert a log line into a JSON object
def convert_log_line_to_json(line):
    # Define the headers to be used for the JSON keys
    headers = [&quot;version&quot;,&quot;account-id&quot;,&quot;interface-id&quot;,&quot;srcaddr&quot;,&quot;dstaddr&quot;,&quot;srcport&quot;,&quot;dstport&quot;,&quot;protocol&quot;,&quot;packets&quot;,&quot;bytes&quot;,&quot;start&quot;,&quot;end&quot;,&quot;action&quot;,&quot;log-status&quot;]

    # Split the log line using shell-like syntax (keeping quotes, etc.)
    res = shlex.split(line, posix=False)

    # Create a dictionary by zipping headers and log line parts
    return dict(zip(headers, res))

# Lambda function handler
def lambda_handler(event, context):
    # S3 bucket name
    bucket_name = '&lt;name_of_your_bucket&gt;'

    # List all objects in the specified S3 bucket
    response = s3.list_objects_v2(Bucket=bucket_name)

    # Iterate through each object in the bucket
    for obj in response['Contents']:
        # Check if the object is a gzipped log file
        if obj['Key'].endswith('.log.gz'):
            file_key = obj['Key']

            # Download the gzipped file content
            file_obj = s3.get_object(Bucket=bucket_name, Key=file_key)
            file_content = file_obj['Body'].read()

            # Decompress the gzipped content
            decompressed_content = gzip.decompress(file_content)

            # Convert bytes to string
            json_data = str(decompressed_content, encoding='utf-8')

            # Split the string into lines
            lines = json_data.strip().split('\n')

            # Convert the list of strings into a JSON-formatted string
            result = json.dumps(lines, indent=2)

            # Load the JSON-formatted string into a list of strings
            list_of_strings = json.loads(result)

            # Convert each log line string into a JSON object
            json_data = [convert_log_line_to_json(line) for line in list_of_strings]
            
            req_headers = {
                     'signoz-access-token': '&lt;SIGNOZ_INGESTION_KEY&gt;',
                     'Content-Type': 'application/json'
                }
            # Specify the HTTP endpoint for sending the data
            http_url = 'https://ingest.in.signoz.cloud:443/logs/json'  # Replace with your actual URL

            # Send the JSON data to the specified HTTP endpoint
            response = requests.post(http_url, json=json_data, headers=req_headers)

            # Print information about the sent data and the response received
            print(f&quot;Sent data to {http_url}. Response: {response.status_code}&quot;)
</code></pre>
<p>Here‚Äôs how a raw, unprocessed VPC log line looks like:</p>
<pre><code>2 842928376651 eni-0660f9815e9e8d140 192.3.0.225 192.3.0.154 43276 6319 6 1 52 1701285848 1701215902 ACCEPT OK
</code></pre>
<p>In the code, each field corresponds to a header. The purpose of the code above is to transmit Virtual Private Cloud (VPC) logs to the SigNoz endpoint.</p>
<p>‚ö†Ô∏è Warning</p>
<p>The provided code is functional, but exercise caution when copying and pasting it in its entirety. Incorrect configuration could result in the unintentional ingestion of a large volume of data.</p>
<p>Other than the above explanation and the code comments, in a nutshell, what the this code does is:</p>
<p>Sends the parsable content of <strong>ENTIRE</strong> S3 bucket whenever the lambda function gets triggered. It gets triggered by the condition you set above. Let‚Äôs mention that again here.</p>
<blockquote>
<p><strong>Step 3:</strong> For the Event types field, you can select any number of options you wish. The trigger will occur depending upon what option(s) you choose here. By default, the <code>All object create events</code> will be selected.</p>
</blockquote>
<p>Lets say you add something to your S3 bucket, it may / may not trigger this lambda function or if you have setup your s3 as if it automatically stores all your ELB/VPC logs, segregated in different folders, so whenever any new log gets added, the function will get triggered and send all the S3 data.</p>
<p>This is obviously not what everyone expects, ideal case would be to have a mass log transfer once the first connection is made to SigNoz otel-collector (which then they later get stored in gp2/gp3 storageClass of EBS), and then send logs lines of only the recently logged one.</p>
<p>To achieve this functionality, you need to add few conditions to the code.</p>
<ol>
<li>Assuming all standard log lines have a timestamp field.</li>
<li>Parse and select the timestamp field from the log line and add it before the <code>response = requests.post(http_url, json=json_data)</code> line as a if else condition to only send logs which are x days older (say 3 days).</li>
</ol>
<p>So, the function now will first check the log timestamp and only send those logs which are 3 days older (say) or even a few hours old.</p>
<p>Let‚Äôs consider the below pseudo code for better understanding:</p>
<pre><code>   from datetime import datetime, timedelta

   # Your given timestamp
   given_timestamp_str = &quot;2024-01-01T23:58:02.231919Z&quot;
   given_timestamp = datetime.fromisoformat(given_timestamp_str.replace('Z', '+00:00'))

   # Current time
   current_time = datetime.utcnow()

   # Calculate the time difference
   time_difference = current_time - given_timestamp

   # Check if the time difference is less than 3 days
   if time_difference &lt; timedelta(days=3):
       # Run your specific function here
       print(&quot;Running the specific function.&quot;)
   # ADD THE response = requests.post(http_url, json=json_data) LINE HERE

   else:
       print(&quot;Time difference exceeds 3 days. Function will not run.&quot;)
</code></pre>
<p>Feel free to modify any part of the code according to your requirements.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#running-the-code-locally
tag_set: aws-monitoring, vpc-logs
image_urls: 
tracking_id: docs-aws-monitoring-vpc-logs-running-the-code-locally
group_tracking_ids: docs-aws-monitoring-vpc-logs
<p>Send your VPC logs to SigNoz: Understanding how lambda function work: The Lambda Function: Running the code locally.</p>
<p>If you want to run the entire setup locally in your laptop for testing purposes. Here‚Äôs the reference code for you:</p>
<pre><code>import os
import gzip
import json
import requests
import shlex

def convert_log_line_to_json(line):
    headers= [&quot;version&quot;,&quot;account-id&quot;,&quot;interface-id&quot;,&quot;srcaddr&quot;,&quot;dstaddr&quot;,&quot;srcport&quot;,&quot;dstport&quot;,&quot;protocol&quot;,&quot;packets&quot;,&quot;bytes&quot;,&quot;start&quot;,&quot;end&quot;,&quot;action&quot;,&quot;log-status&quot;]
    res = shlex.split(line, posix = False)
    return dict(zip(headers, res))

def process_log_file(file_path):
    with gzip.open(file_path, 'r') as f:
        log_data = f.read().decode('utf-8') 
        lines = log_data.strip().split('\n')
        result = json.dumps(lines, indent=2)
        list_of_strings = json.loads(result)
        json_data = [convert_log_line_to_json(line) for line in list_of_strings]
        req_headers = {
                     'signoz-access-token': '&lt;SIGNOZ_INGESTION_KEY&gt;',
                     'Content-Type': 'application/json'
                }
        http_url = 'https://ingest.in.signoz.cloud:443/logs/json'  # Replace with your actual URL
        response = requests.post(http_url, json=json_data, headers=req_headers)

def main():
    root_folder = '&lt;folder_name&gt;'

    for root, _, files in os.walk(root_folder):
        for file in files:
            if file.endswith('.log.gz'):
                file_path = os.path.join(root, file)
                process_log_file(file_path)

if __name__ == '__main__':
    main()
</code></pre>
<p>‚úÖ Info</p>
<p>To incorporate log files into the folder, including nested directories, the code systematically examines all sub-folders to identify files ending with the <code>.log.gz</code> extension. Should you wish to target different file types, you have the flexibility to modify the code accordingly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#testing-your-lambda-function
tag_set: aws-monitoring, vpc-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-timeout_increase.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_test_event.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_10.webp
tracking_id: docs-aws-monitoring-vpc-logs-testing-your-lambda-function
group_tracking_ids: docs-aws-monitoring-vpc-logs
<p>Send your VPC logs to SigNoz: Understanding how lambda function work: The Lambda Function: Testing your Lambda function</p>
<p>Once you've finished writing your code, it's crucial to deploy it and conduct thorough testing to ensure its functionality. Before proceeding with deployment and testing, it's important to consider adjusting the timeout setting for your Lambda function. This adjustment is necessary because the process of transferring data from S3 to an external endpoint may take several minutes, exceeding the default Lambda timeout of 3 seconds.</p>
<p>To extend the timeout duration, follow these steps:</p>
<ol>
<li>Navigate to the Lambda function configuration.</li>
<li>Access the &quot;General Configuration&quot; section.</li>
<li>Click on the &quot;Edit&quot; button to modify settings.</li>
<li>Increase the timeout value to a duration exceeding 10 minutes. Typically, the code execution completes within 1-4 minutes at most.</li>
</ol>
<p>By adjusting the timeout setting, you ensure that your Lambda function has sufficient time to complete the data transfer process without encountering timeouts. This proactive measure enhances the reliability and effectiveness of your deployed solution.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-timeout_increase.webp" alt="" /></p>
<p><em>Configuring execution timeout of Lambda function</em></p>
<p>Once you've finished adjusting the timeout setting, navigate to the code editor for your Lambda function. Locate the 'test' button, and from the dropdown menu, select the option labeled 'Configure test events.' Create a new test case by specifying it as an S3 PUT event, then save your configuration.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_test_event.webp" alt="" /></p>
<p><em>Creating Sample event</em></p>
<p>You're now prepared to proceed. Whenever you make alterations to the code and wish to evaluate them, follow these steps: Deploy the code first (equivalent to pressing 'Save'), and once it's fully deployed, proceed to click on the 'Test' button.</p>
<p>Below is an image showing the process of transmitting VPC logs to the SigNoz endpoint.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_10.webp" alt="" /></p>
<p><em>Visual representation of code in Lambda function</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#test-case-and-output
tag_set: aws-monitoring, vpc-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_output.webp
tracking_id: docs-aws-monitoring-vpc-logs-test-case-and-output
group_tracking_ids: docs-aws-monitoring-vpc-logs
<p>Send your VPC logs to SigNoz: Understanding how lambda function work: The Lambda Function: Test Case and Output</p>
<p>If the logs are sent successfully, here's how they'll be transmitted. The following output displays the JSON-formatted data as we've printed it to visualize the sent information.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_output.webp" alt="" /></p>
<p><em>Visual representation of code output of Lambda function</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/vpc-logs/#visualize-the-logs-in-signoz
tag_set: aws-monitoring, vpc-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_12.webp
tracking_id: docs-aws-monitoring-vpc-logs-visualize-the-logs-in-signoz
group_tracking_ids: docs-aws-monitoring-vpc-logs
<h2>Send your VPC logs to SigNoz: Understanding how lambda function work: Visualize the logs in SigNoz</h2>
<p>Upon accessing the SigNoz logs section, you will notice a considerable influx of logs. You have the option to seamlessly transition to live monitoring of logs as well. Simply click on any log line to view its detailed information.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_12.webp" alt="A sample log line of the logs sent from AWS Lambda" /></p>
<p><em>A sample log line of the logs sent from AWS Lambda</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#overview
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-overview
group_tracking_ids: docs-aws-monitoring-rds-logs
<h2>Send your RDS logs to SigNoz: Send your RDS logs to SigNoz - Overview</h2>
<p>This documentation provides a detailed walkthrough on how to set up an AWS Lambda function to collect Relational Database Service (RDS) logs stored in an AWS S3 bucket and forward them to SigNoz. By the end of this guide, you will have a setup that automatically sends your RDS logs to SigNoz, enabling you to visualize and monitor your database performance and health.</p>
<p><strong>Here‚Äôs a quick summary of what we‚Äôll be doing in this detailed article.</strong></p>
<ul>
<li>
<p><a href="#creating--configuring-your-s3-bucket">Creating / Configuring your S3 bucket</a></p>
</li>
<li>
<p><a href="#understanding-how-lambda-function-work">Understanding how lambda function work</a></p>
</li>
<li>
<p><a href="#creating-a-lambda-function">Creating a lambda function</a></p>
</li>
<li>
<p><a href="#the-lambda-function">The Lambda Function Code</a></p>
</li>
<li>
<p><a href="#visualize-the-logs-in-signoz">Visualize the logs in SigNoz</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#prerequisites
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-prerequisites
group_tracking_ids: docs-aws-monitoring-rds-logs
<h2>Send your RDS logs to SigNoz: Prerequisites</h2>
<ul>
<li>AWS account with administrative privilege.</li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></li>
</ul>
<p>Before we dive into creating and configuring the S3 bucket and Lambda function, let's talk a little about AWS RDS () supported databases and all the expected types of logs that you will be dealing with.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#introduction-to-database-logging-in-aws-rds
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-introduction-to-database-logging-in-aws-rds
group_tracking_ids: docs-aws-monitoring-rds-logs
<h2>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Introduction to Database Logging in AWS RDS</h2>
<p>Amazon Web Services (AWS) Relational Database Service (RDS) offers a managed database solution for various relational database engines, catering to the diverse needs of businesses and applications. AWS RDS supports several popular database engines, each offering its unique set of logging capabilities to monitor and troubleshoot database activities effectively.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#supported-database-engines
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-supported-database-engines
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Supported Database Engines</p>
<p>AWS RDS supports a wide range of relational database engines, including:</p>
<ul>
<li>PostgreSQL,</li>
<li>Oracle,</li>
<li>MySQL,</li>
<li>MS SQL Server, and</li>
<li>MariaDB.</li>
</ul>
<p>Each of these database engines comes with its strengths and capabilities, tailored to specific use cases and requirements.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#postgresql
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-postgresql
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: PostgreSQL:</p>
<p>PostgreSQL provides comprehensive logging capabilities covering various aspects of database operation. They include:</p>
<ul>
<li>Startup Logs</li>
<li>Server Logs</li>
<li>Query Logs</li>
<li>Transaction Logs</li>
<li>Connection Logs</li>
<li>Error Logs</li>
<li>Audit Logs</li>
</ul>
<p><strong>General header format for postgres logs:</strong></p>
<pre><code>{&quot;timestamp&quot;, &quot;user&quot;, &quot;dbname&quot;, &quot;pid&quot;, &quot;remote_host&quot;, &quot;remote_port&quot;, &quot;session_id&quot;, &quot;line_num&quot;, &quot;ps&quot;, &quot;session_start&quot;, &quot;vxid&quot;, &quot;txid&quot;, &quot;error_severity&quot;, &quot;state_code&quot;, &quot;message&quot;, &quot;detail&quot;, &quot;hint&quot;, &quot;internal_query&quot;, &quot;internal_position&quot;, &quot;context&quot;, &quot;statement&quot;, &quot;cursor_position&quot;, &quot;func_name&quot;, &quot;file_name&quot;, &quot;file_line_num&quot;, &quot;application_name&quot;, &quot;backend_type&quot;, &quot;leader_pid&quot;, &quot;query_id&quot;}
</code></pre>
<p><strong>Sample log line:</strong></p>
<pre><code>2017-06-12 19:09:49 UTC:...:rds_test@postgres:[11701]:LOG: AUDIT: OBJECT,1,1,READ,SELECT,TABLE,public.t1,select * from t1;
</code></pre>
<p><a href="https://www.postgresql.org/docs/current/runtime-config-logging.html">Know More</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#oracle
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-oracle
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Oracle:</p>
<p>Oracle databases offer several log types catering to different aspects of database management. They include:</p>
<ul>
<li>Alert Logs</li>
<li>Audit Files</li>
<li>Trace Files</li>
<li>Listener Logs</li>
<li>Management Agent</li>
</ul>
<p><strong>General header format for oracle DB logs:</strong></p>
<pre><code>{&quot;TIME&quot;, &quot;DATA&quot;, &quot;MODULENAME&quot;, &quot;DOMAIN&quot;, &quot;LOGLEVEL&quot;, &quot;LOGINID&quot;, &quot;IPADDR&quot;, &quot;LOGGEDBY&quot;, &quot;HOSTNAME&quot;, &quot;MESSAGEID&quot;, &quot;CONTEXTID&quot;}
</code></pre>
<p><a href="https://docs.oracle.com/cd/E19681-01/820-3740/adrdp/index.html">Know more</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#mysql
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-mysql
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: MySQL:</p>
<p>MySQL databases support various log types to aid in monitoring and troubleshooting. They include:</p>
<ul>
<li>Error Log</li>
<li>Slow Query Log</li>
<li>General Query Log</li>
<li>Audit Log</li>
<li>Binary Log</li>
<li>Relay Log</li>
<li>DDL Log</li>
</ul>
<p><strong>Example¬†error_log¬†contents for MYSQL db:</strong></p>
<pre><code>{&quot;LOGGED&quot;, &quot;THREAD_ID&quot;, &quot;PRIO&quot;, &quot;ERROR_CODE&quot;, &quot;SUBSYSTEM&quot;, &quot;DATA&quot;}
</code></pre>
<p><strong>Sample Audit logs for MySQL:</strong></p>
<pre><code>  2019-10-03T14:09:38 UTC
  6_2019-10-03T14:06:33
  Query
  5
  0
  0
  root[root] @ localhost [127.0.0.1]
  
  localhost
  127.0.0.1
  drop_table
  DROP TABLE IF EXISTS t
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/8.0/en/error-log-configuration.html">Know more</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#ms-sql-server
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-ms-sql-server
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: MS SQL Server:</p>
<p>MS SQL Server databases maintain several types of logs to facilitate monitoring and troubleshooting. They include:</p>
<ul>
<li>Error Logs</li>
<li>Agent Logs</li>
<li>Trace Files</li>
<li>Dump Files</li>
</ul>
<p><strong>Audit Log header format:</strong></p>
<pre><code>{&quot;action_id&quot;, &quot;action_name&quot;, &quot;additional_information&quot;, &quot;affected_rows&quot;, &quot;application_name&quot;, &quot;audit_schema_version&quot;, &quot;class_type&quot;, &quot;class_type_desc&quot;, &quot;client_ip&quot;, &quot;connection_id&quot;, &quot;data_sensitivity_information&quot;, &quot;database_name&quot;, &quot;database_principal_id&quot;, &quot;database_principal_name&quot;, &quot;duration_milliseconds&quot;, &quot;event_time&quot;, &quot;host_name&quot;, &quot;is_column_permission&quot;, &quot;is_server_level_audit&quot;, &quot;object_id&quot;, &quot;object_name&quot;, &quot;obo_middle_tier_app_id&quot;, &quot;permission_bitmask&quot;, &quot;response_rows&quot;, &quot;schema_name&quot;, &quot;securable_class_type&quot;, &quot;sequence_group_id&quot;, &quot;sequence_number&quot;, &quot;server_instance_name&quot;, &quot;server_principal_id&quot;, &quot;server_principal_name&quot;, &quot;server_principal_sid&quot;, &quot;session_id&quot;, &quot;session_server_principal_name&quot;, &quot;statement&quot;, &quot;succeeded&quot;, &quot;target_database_principal_id&quot;, &quot;target_database_principal_name&quot;, &quot;target_server_principal_id&quot;, &quot;target_server_principal_name&quot;, &quot;target_server_principal_sid&quot;, &quot;transaction_id&quot;, &quot;user_defined_event_id&quot;, &quot;user_defined_information&quot;}
</code></pre>
<p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.SQLServer.html">Know more</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#mariadb
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-mariadb
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: MariaDB:</p>
<p>MariaDB databases offer logging capabilities covering error handling and query performance monitoring. They include:</p>
<ul>
<li>Error Log</li>
<li>Slow Query Log</li>
<li>General Log</li>
</ul>
<p><strong>Slow Query Log format:</strong></p>
<pre><code>{&quot;start_time&quot;, &quot;user_host&quot;, &quot;query_time&quot;, &quot;lock_time&quot;, &quot;rows_sent&quot;, &quot;rows_examined&quot;, &quot;db&quot;, &quot;last_insert_id&quot;, &quot;insert_id&quot;, &quot;server_id&quot;, &quot;sql_text&quot;, &quot;thread_id&quot;, &quot;rows_affected&quot;}
</code></pre>
<p><strong>Error Log format:</strong> Until MariaDB 10.1.4, the format only consisted of the date (yymmdd) and time, followed by the type of error (Note, Warning, or Error) and the error message.</p>
<p><strong>Sample error log:</strong></p>
<pre><code>2018-11-27  2:46:46 140278181563136 [Warning] mysqld: Disk is full writing '/var/lib/mariadb-bin.00001' (Errcode: 28 &quot;No space left on device&quot;). Waiting for someone to free space... (Expect up to 60 secs delay for server to continue after freeing disk space)
</code></pre>
<p><strong>General Log format:</strong></p>
<pre><code>{&quot;event_time&quot;, &quot;user_host&quot;, &quot;thread_id&quot;, &quot;server_id&quot;, &quot;command_type&quot;, &quot;argument&quot;}
</code></pre>
<p><a href="https://mariadb.com/kb/en/mysqlslow_log-table/">Know More</a></p>
<p>‚ö†Ô∏è Warning</p>
<p>The screenshots shown below were taken to send Elastic Load Balancer logs to SigNoz, rest assured, the steps for for RDS logs to SigNoz cloud endpoint remain same. Please name the appropriate name changes against what is shown below in the screenshots (e.g. - bucket names, name fields, etc)</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#creating--configuring-your-s3-bucket
tag_set: aws-monitoring, rds-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-create_bucket_2.webp
tracking_id: docs-aws-monitoring-rds-logs-creating-configuring-your-s3-bucket
group_tracking_ids: docs-aws-monitoring-rds-logs
<h2>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Creating / Configuring your S3 bucket</h2>
<p>To accomplish the task described, please follow these steps:</p>
<ol>
<li><strong>Creating an S3 Bucket:</strong>
<ul>
<li>Sign in to the AWS Management Console.</li>
<li>Navigate to the Amazon S3 service.</li>
<li>Click on <strong>Create bucket</strong>.</li>
<li>Enter a unique bucket name, select the region, and configure any additional settings if needed (such as versioning, logging, etc.).</li>
<li>Click <strong>Create bucket</strong> to finalize the creation process.</li>
</ul>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-create_bucket_2.webp" alt="" /></p>
<p><em>Fill up bucket details</em></p>
<ol start="2">
<li>
<p><strong>Uploading Data to the S3 Bucket:</strong></p>
<ul>
<li>After creating the bucket, navigate to the S3 Management Console.</li>
<li>Click on the <strong>Upload</strong> button.</li>
<li>Select the files you wish to upload from your local machine.</li>
<li>Optionally, configure settings like encryption, permissions, etc.</li>
<li>Click <strong>Upload</strong> to upload the selected files to the bucket.</li>
</ul>
</li>
<li>
<p><strong>File Formats:</strong></p>
<ul>
<li>You can upload files in various formats such as .json, .csv, .log, .gz, .zip, etc.</li>
<li>If you have configured RDS logging, RDS logs for various DB's are either saved in <code>.gzip</code> (compressed) format or any of the format mentioned in the previous point in your S3 bucket.</li>
</ul>
</li>
</ol>
<p>üìù Note</p>
<p>Please be advised that all logs will undergo conversion to JSON format before transmission. Consequently, it may be necessary to perform supplementary preprocessing of the logs as part of this conversion process. Here preprocessing of the logs means getting them from S3 bucket, separating each log line based on the delimiter (&quot;,&quot; or based on quotes or whitespaces) and assigning them to respective keys. Thus making a key, value pair before sending to SigNoz.</p>
<p>To move forward, we assume that you already have some data in your S3 bucket.</p>
<p>‚úÖ Info</p>
<p>For the scope of this documentation, we assume that all the data in S3 bucket is in the same format. For example, if one file is in <code>.csv</code> format, then all files within the bucket will be in <code>.csv</code> format. For files in different formats, you will have to use different parsing functions for each format or update the existing function accordingly.</p>
<p>The general header(table) format of all types of RDS logs are described above, in our code, we'll use the headers for which you wish to send the logs to SigNoz.</p>
<p>‚úÖ Info</p>
<p>Note that these headers are just for name sake to represent the log row values, you can change them if you wish to, but it is not advisable.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#understanding-how-lambda-function-work
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-understanding-how-lambda-function-work
group_tracking_ids: docs-aws-monitoring-rds-logs
<h2>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: Understanding how lambda function work</h2>
<p>When you successfully attach your lambda function with the S3 bucket and configure it correctly, any new addition / deletion / copy / PUT etc, requests made to the S3 bucket will trigger the lambda function and the code written in the lambda function will get executed.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#creating-a-lambda-function
tag_set: aws-monitoring, rds-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_1.webp, https://signoz.io/img/docs/rds-docs/create_function.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_3.webp, https://signoz.io/img/docs/rds-docs/lambda_function.webp
tracking_id: docs-aws-monitoring-rds-logs-creating-a-lambda-function
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: Creating a lambda function</p>
<p>Follow these steps to create the lambda function:</p>
<p><strong>Step 1:</strong> Go to your AWS console and search for AWS Lambda, go to <strong>Functions</strong> and click on <strong>Create Function</strong>.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_1.webp" alt="" /></p>
<p><em>Create Lambda function from AWS Console</em></p>
<p><strong>Step 2:</strong> Choose the <code>Author from scratch</code> checkbox and proceed to fill in the function name.</p>
<p><img src="https://signoz.io/img/docs/rds-docs/create_function.webp" alt="" /></p>
<p><em>Choose the Author from scratch and fill up other details</em></p>
<p><strong>Step 3:</strong> Choose <code>Python 3.x</code> as the Runtime version, <code>x86_64</code> as Architecture (preferably), and keep other settings as default. Select <code>Create a new role with basic Lambda permissions</code>for now, we‚Äôll requiring more permissions down the lane. So for now, select this option.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_3.webp" alt="" /></p>
<p><em>Choose Create a new role here</em></p>
<p><strong>Step 4:</strong> Once you are done configuring the lambda function, the function will get created.</p>
<p><img src="https://signoz.io/img/docs/rds-docs/lambda_function.webp" alt="" /></p>
<p><em>Your barebones Lambda function is created now</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#configuring-policies-for-lambda-function
tag_set: aws-monitoring, rds-logs
image_urls: https://signoz.io/img/docs/rds-docs/execution_role.webp, https://signoz.io/img/docs/rds-docs/attach_policy.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_7.webp, https://signoz.io/img/docs/elb/elb-logs-policies-used.webp
tracking_id: docs-aws-monitoring-rds-logs-configuring-policies-for-lambda-function
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: Configuring Policies for Lambda function</p>
<p>As said in Step 3 previously, we need extra permissions in order to access the S3 Bucket for execution of our Lambda code, follow along to set it up.</p>
<p><strong>Step 1:</strong> Scroll down from your Lambda page, you‚Äôll see a few tabs there. Go to <code>Configurations</code> and select <code>Permissions</code> from the left sidebar.</p>
<p><img src="https://signoz.io/img/docs/rds-docs/execution_role.webp" alt="" /></p>
<p><em>Choosing execution role from Configurations tab</em></p>
<p><strong>Step 2:</strong> Click on the <code>Execution Role name</code> link just under Role name, it will take us to AWS IAM page. Here we will add policies to get full S3 access. Once here, click on the <code>Add permissions</code> button and select <code>Attach policies</code> from the drop down list.</p>
<p><img src="https://signoz.io/img/docs/rds-docs/attach_policy.webp" alt="" /></p>
<p><em>Attach policies to your Lambda function</em></p>
<p><strong>Step 3:</strong> Search ‚ÄúS3‚Äù and you‚Äôll a policy named <code>AmazonS3FullAccess</code> select that and proceed.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_7.webp" alt="" /></p>
<p><em>Policies you'll need to run your Lambda function</em></p>
<p>‚ö†Ô∏è Warning</p>
<p>It's advisable to proceed with caution when granting full S3 access, particularly in a production environment. Before deploying your Lambda function with such extensive permissions, it's essential to consult with your system administrator or designated authority to ensure compliance with security protocols and best practices. This step helps mitigate potential risks and ensures that access permissions align with organizational guidelines and requirements.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-policies-used.webp" alt="" /></p>
<p><em>Extra policies you might require to execute your Lambda function</em></p>
<p>Please refer to the image above as a comprehensive guide to the policy names that you may consider adding to your Lambda function. Failure to include these policies could result in insufficient privileges, potentially hindering the function's ability to perform necessary operations within the AWS environment.</p>
<p>Congrats, you are just done with one of the major hurdle in running your code. Now, let‚Äôs add a trigger.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#adding-triggers
tag_set: aws-monitoring, rds-logs
image_urls: https://signoz.io/img/docs/rds-docs/add_trigger_2.webp, https://signoz.io/img/docs/rds-docs/add_trigger.webp
tracking_id: docs-aws-monitoring-rds-logs-adding-triggers
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: Adding Triggers</p>
<p>You need to use the Lambda console to build a trigger so that your function can be called immediately by another AWS service (S3, in our case). A trigger is a resource you set up to enable your function to be called by another AWS service upon the occurrence of specific events or conditions.</p>
<p>A function may have more than one trigger. Every trigger functions as a client, independently calling your method, and Lambda transfers data from a single trigger to each event it passes to your function.</p>
<p>To setup the trigger, follow these steps:</p>
<p><strong>Step 1:</strong> Click on the <code>+ Add trigger</code> button from the Lambda console.</p>
<p><img src="https://signoz.io/img/docs/rds-docs/add_trigger_2.webp" alt="" /></p>
<p><em>Add a trigger to the function.</em></p>
<p><strong>Step 2:</strong> Select <code>S3</code> from the first drop down of AWS services list. Pick your S3 bucket for the second field.</p>
<p><strong>Step 3:</strong> For the Event types field, you can select any number of options you wish. The trigger will occur depending upon what option(s) you choose here. By default, the <code>All object create events</code> will be selected.</p>
<p><img src="https://signoz.io/img/docs/rds-docs/add_trigger.webp" alt="" /></p>
<p><em>Choose event types from the drop down menu</em></p>
<p>Verify the settings and click on <code>Add</code> button at bottom right to add this trigger.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#adding-request-layer
tag_set: aws-monitoring, rds-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-layer_3.webp, https://signoz.io/img/docs/elb/elb-logs-layer_4.webp, https://signoz.io/img/docs/elb/elb-logs-layer_5.webp, https://signoz.io/img/docs/elb/elb-logs-layer_6.webp
tracking_id: docs-aws-monitoring-rds-logs-adding-request-layer
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: Adding Request Layer</p>
<p>We will be using python's <code>request</code> module which is <a href="https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/">not included by default</a> in Lambda.</p>
<p>To utilize Python's <code>requests</code> module within a Lambda function, it's necessary to explicitly add it as a layer. While there may be alternative approaches, it's advisable to adhere to established practices that have been thoroughly tested and proven effective. Therefore, we will proceed with adding <code>requests</code> as a layer to ensure reliable functionality within the Lambda environment.</p>
<p><strong>Step 1:</strong> Follow the steps below to create a zip of the request module and add it as a layer to make it work on AWS lambda.</p>
<p>The commands you‚Äôd need:</p>
<pre><code># make a new directory
mkdir python
# move into that directory
cd python

# install requests module
pip install --target . requests
# zip the contents under the name dependencies.zip
zip -r dependencies.zip ../python 
</code></pre>
<p><strong>Step 2:</strong> To upload your zip file, go to AWS Lambda &gt; Layers and click on <code>Create Layer</code>. [Not inside your specific Lambda function, just the landing page of AWS Lambda].</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_3.webp" alt="" /></p>
<p><em>Creating a new Layer</em></p>
<p><strong>Step 3:</strong> You‚Äôll be redirected to Layer configurations page, here, give a name to your layer, an optional description, select <code>Upload a .zip file</code> , click on <code>Upload</code> and locate the requirements.zip file.</p>
<p><strong>Step 4:</strong> Select your desired architecture and pick <code>Python 3.x</code> as your runtime. Hit <code>Create</code>. Your layer has now been created. Now lets connect it to our Lambda function which we created to send logs to SigNoz.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_4.webp" alt="" /></p>
<p><em>Uploading the requirements.zip file to later add it as a layer</em></p>
<p><strong>Step 5:</strong> Go to your Lambda function, scroll down to Layers section and on the right of it, you‚Äôll find a button that says <code>Add a layer</code> to click on.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_5.webp" alt="" /></p>
<p><em>Add a layer to your Lambda function</em></p>
<p><strong>Step 6:</strong> Pick <code>Custom layers</code> from the checkbox and select your custom layer from the given drop down below and then click on the button <code>Add</code>.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_6.webp" alt="" /></p>
<p><em>Choose your layer name</em></p>
<p>Congratulations, the <code>requests</code> module has been successfully integrated into your code area. By adding this layer, you have resolved the 'request module not found error' that would have otherwise occurred.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#the-lambda-function
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-the-lambda-function
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: The Lambda Function: The Lambda Function: The Lambda Function</p>
<p>Now, we come to the pivotal section of this document: the code implementation.</p>
<p>The Python script's primary function revolves around retrieving gzipped log files stored within an Amazon S3 bucket. Subsequently, it decompresses these files, transforms individual log entries into JSON objects, and transmits the resultant JSON data to a predetermined HTTP endpoint.</p>
<p>Below is the comprehensive code along with detailed comments for clarity:</p>
<p>‚úÖ Info</p>
<p>The below code assumes your log files are already in compressed form, if they are not, feel free to remove the decompressing of log files and match the file type you have for you logs (if not already <code>.log</code>).</p>
<pre><code>import json
import gzip
import boto3 
import requests
import shlex

# Create an S3 client
s3 = boto3.client('s3')

# Function to convert a log line into a JSON object
def convert_log_line_to_json(line):
    # Define the headers to be used for the JSON keys
    postgres_headers = [&quot;timestamp&quot;, &quot;user&quot;, &quot;dbname&quot;, &quot;pid&quot;, &quot;remote_host&quot;, &quot;remote_port&quot;, &quot;session_id&quot;, &quot;line_num&quot;, &quot;ps&quot;, &quot;session_start&quot;, &quot;vxid&quot;, &quot;txid&quot;, &quot;error_severity&quot;, &quot;state_code&quot;, &quot;message&quot;, &quot;detail&quot;, &quot;hint&quot;, &quot;internal_query&quot;, &quot;internal_position&quot;, &quot;context&quot;, &quot;statement&quot;, &quot;cursor_position&quot;, &quot;func_name&quot;, &quot;file_name&quot;, &quot;file_line_num&quot;, &quot;application_name&quot;, &quot;backend_type&quot;, &quot;leader_pid&quot;, &quot;query_id&quot;]

    # Split the log line using shell-like syntax (keeping quotes, etc.)
    res = shlex.split(line, posix=False)

    # Create a dictionary by zipping headers and log line parts
    return dict(zip(postgres_headers, res))

# Lambda function handler
def lambda_handler(event, context):
    # S3 bucket name
    bucket_name = '&lt;name_of_your_bucket&gt;'

    # List all objects in the specified S3 bucket
    response = s3.list_objects_v2(Bucket=bucket_name)

    # Iterate through each object in the bucket
    for obj in response['Contents']:
        # Check if the object is a gzipped log file
        if obj['Key'].endswith('.log.gz'):
            file_key = obj['Key']

            # Download the gzipped file content
            file_obj = s3.get_object(Bucket=bucket_name, Key=file_key)
            file_content = file_obj['Body'].read()

            # Decompress the gzipped content
            decompressed_content = gzip.decompress(file_content)

            # Convert bytes to string
            json_data = str(decompressed_content, encoding='utf-8')

            # Split the string into lines
            lines = json_data.strip().split('\n')

            # Convert the list of strings into a JSON-formatted string
            result = json.dumps(lines, indent=2)

            # Load the JSON-formatted string into a list of strings
            list_of_strings = json.loads(result)

            # Convert each log line string into a JSON object
            json_data = [convert_log_line_to_json(line) for line in list_of_strings]
            
            req_headers = {
                     'signoz-access-token': '&lt;SIGNOZ_INGESTION_KEY&gt;',
                     'Content-Type': 'application/json'
                }
            # Specify the HTTP endpoint for sending the data
            http_url = 'https://ingest.in.signoz.cloud:443/logs/json'  # Replace with your actual URL

            # Send the JSON data to the specified HTTP endpoint
            response = requests.post(http_url, json=json_data, headers=req_headers)

            # Print information about the sent data and the response received
            print(f&quot;Sent data to {http_url}. Response: {response.status_code}&quot;)
</code></pre>
<p>Here‚Äôs how a raw, unprocessed Postgres RDS log line looks like:</p>
<pre><code>2017-06-12 19:09:49 UTC:...:rds_test@postgres:[11701]:LOG: AUDIT: OBJECT,1,1,READ,SELECT,TABLE,public.t1,select * from t1;
</code></pre>
<p>In the code, each field corresponds to a header. The purpose of the code above is to transmit the logs to the SigNoz endpoint. Here, we have used <code>postgres_headers</code>, feel free to change that to any other RDS Database header format as described in the start of this document.</p>
<p>‚ö†Ô∏è Warning</p>
<p>The provided code is functional, but exercise caution when copying and pasting it in its entirety. Incorrect configuration could result in the unintentional ingestion of a large volume of data. If you have limited you log collection or configured it to collect less information, make the suitable changes to the headers as well to match the correct log values.</p>
<p>Other than the above explanation and the code comments, in a nutshell, what the this code does is:</p>
<p>Sends the parsable content of <strong>ENTIRE</strong> S3 bucket whenever the lambda function gets triggered. It gets triggered by the condition you set above. Let‚Äôs mention that again here.</p>
<blockquote>
<p><strong>Step 3:</strong> For the Event types field, you can select any number of options you wish. The trigger will occur depending upon what option(s) you choose here. By default, the <code>All object create events</code> will be selected.</p>
</blockquote>
<p>Lets say you add something to your S3 bucket, it may / may not trigger this lambda function or if you have setup your s3 as if it automatically stores all your RDS logs, segregated in different folders, so whenever any new log gets added, the function will get triggered and send all the S3 data.</p>
<p>This is obviously not what everyone expects, ideal case would be to have a mass log transfer once the first connection is made to SigNoz otel-collector (which then they later get stored in gp2/gp3 storageClass of EBS), and then send logs lines of only the recently logged one.</p>
<p>To achieve this functionality, you need to add few conditions to the code.</p>
<ol>
<li>Assuming all standard log lines have a timestamp field.</li>
<li>Parse and select the timestamp field from the log line and add it before the <code>response = requests.post(http_url, json=json_data)</code> line as a if else condition to only send logs which are x days older (say 3 days).</li>
</ol>
<p>So, the function now will first check the log timestamp and only send those logs which are 3 days older (say) or even a few hours old.</p>
<p>Let‚Äôs consider the below pseudo code for better understanding:</p>
<pre><code>   from datetime import datetime, timedelta

   # Your given timestamp
   given_timestamp_str = &quot;2024-01-01T23:58:02.231919Z&quot;
   given_timestamp = datetime.fromisoformat(given_timestamp_str.replace('Z', '+00:00'))

   # Current time
   current_time = datetime.utcnow()

   # Calculate the time difference
   time_difference = current_time - given_timestamp

   # Check if the time difference is less than 3 days
   if time_difference &lt; timedelta(days=3):
       # Run your specific function here
       print(&quot;Running the specific function.&quot;)
   # ADD THE response = requests.post(http_url, json=json_data) LINE HERE

   else:
       print(&quot;Time difference exceeds 3 days. Function will not run.&quot;)
</code></pre>
<p>Feel free to modify any part of the code according to your requirements.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#running-the-code-locally
tag_set: aws-monitoring, rds-logs
image_urls: 
tracking_id: docs-aws-monitoring-rds-logs-running-the-code-locally
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: The Lambda Function: Running the code locally.</p>
<p>If you want to run the entire setup locally in your laptop for testing purposes. Here‚Äôs the reference code for you:</p>
<pre><code>import os
import gzip
import json
import requests
import shlex

def convert_log_line_to_json(line):
    postgres_headers = [&quot;timestamp&quot;, &quot;user&quot;, &quot;dbname&quot;, &quot;pid&quot;, &quot;remote_host&quot;, &quot;remote_port&quot;, &quot;session_id&quot;, &quot;line_num&quot;, &quot;ps&quot;, &quot;session_start&quot;, &quot;vxid&quot;, &quot;txid&quot;, &quot;error_severity&quot;, &quot;state_code&quot;, &quot;message&quot;, &quot;detail&quot;, &quot;hint&quot;, &quot;internal_query&quot;, &quot;internal_position&quot;, &quot;context&quot;, &quot;statement&quot;, &quot;cursor_position&quot;, &quot;func_name&quot;, &quot;file_name&quot;, &quot;file_line_num&quot;, &quot;application_name&quot;, &quot;backend_type&quot;, &quot;leader_pid&quot;, &quot;query_id&quot;]
    res = shlex.split(line, posix = False)
    return dict(zip(headers, res))

def process_log_file(file_path):
    with gzip.open(file_path, 'r') as f:
        log_data = f.read().decode('utf-8') 
        lines = log_data.strip().split('\n')
        result = json.dumps(lines, indent=2)
        list_of_strings = json.loads(result)
        json_data = [convert_log_line_to_json(line) for line in list_of_strings]
        req_headers = {
                     'signoz-access-token': '&lt;SIGNOZ_INGESTION_KEY&gt;',
                     'Content-Type': 'application/json'
                }
        http_url = 'https://ingest.in.signoz.cloud:443/logs/json'  # Replace with your actual URL
        response = requests.post(http_url, json=json_data, headers=req_headers)

def main():
    root_folder = '&lt;folder_name&gt;'

    for root, _, files in os.walk(root_folder):
        for file in files:
            if file.endswith('.log.gz'):
                file_path = os.path.join(root, file)
                process_log_file(file_path)

if __name__ == '__main__':
    main()
</code></pre>
<p>‚úÖ Info</p>
<p>To incorporate log files into the folder, including nested directories, the code systematically examines all sub-folders to identify files ending with the <code>.log.gz</code> extension. Should you wish to target different file types, you have the flexibility to modify the code accordingly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#testing-your-lambda-function
tag_set: aws-monitoring, rds-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-timeout_increase.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_test_event.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_10.webp
tracking_id: docs-aws-monitoring-rds-logs-testing-your-lambda-function
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: The Lambda Function: Testing your Lambda function</p>
<p>Once you've finished writing your code, it's crucial to deploy it and conduct thorough testing to ensure its functionality. Before proceeding with deployment and testing, it's important to consider adjusting the timeout setting for your Lambda function. This adjustment is necessary because the process of transferring data from S3 to an external endpoint may take several minutes, exceeding the default Lambda timeout of 3 seconds.</p>
<p>To extend the timeout duration, follow these steps:</p>
<ol>
<li>Navigate to the Lambda function configuration.</li>
<li>Access the &quot;General Configuration&quot; section.</li>
<li>Click on the &quot;Edit&quot; button to modify settings.</li>
<li>Increase the timeout value to a duration exceeding 10 minutes. Typically, the code execution completes within 1-4 minutes at most.</li>
</ol>
<p>By adjusting the timeout setting, you ensure that your Lambda function has sufficient time to complete the data transfer process without encountering timeouts. This proactive measure enhances the reliability and effectiveness of your deployed solution.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-timeout_increase.webp" alt="" /></p>
<p><em>Configuring execution timeout of Lambda function</em></p>
<p>Once you've finished adjusting the timeout setting, navigate to the code editor for your Lambda function. Locate the 'test' button, and from the dropdown menu, select the option labeled 'Configure test events.' Create a new test case by specifying it as an S3 PUT event, then save your configuration.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_test_event.webp" alt="" /></p>
<p><em>Creating Sample event</em></p>
<p>You're now prepared to proceed. Whenever you make alterations to the code and wish to evaluate them, follow these steps: Deploy the code first (equivalent to pressing 'Save'), and once it's fully deployed, proceed to click on the 'Test' button.</p>
<p>Below is an image showing the process of transmitting the RDS logs to the SigNoz endpoint.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_10.webp" alt="" /></p>
<p><em>Visual representation of code in Lambda function</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#test-case-and-output
tag_set: aws-monitoring, rds-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_output.webp
tracking_id: docs-aws-monitoring-rds-logs-test-case-and-output
group_tracking_ids: docs-aws-monitoring-rds-logs
<p>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: The Lambda Function: Test Case and Output</p>
<p>If the logs are sent successfully, here's how they'll be transmitted. The following output displays the JSON-formatted data as we've printed it to visualize the sent information.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_output.webp" alt="" /></p>
<p><em>Visual representation of code output of Lambda function</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/rds-logs/#visualize-the-logs-in-signoz
tag_set: aws-monitoring, rds-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_12.webp
tracking_id: docs-aws-monitoring-rds-logs-visualize-the-logs-in-signoz
group_tracking_ids: docs-aws-monitoring-rds-logs
<h2>Send your RDS logs to SigNoz: Introduction to Database Logging in AWS RDS: Understanding how lambda function work: Visualize the logs in SigNoz</h2>
<p>Upon accessing the SigNoz logs section, you will notice a considerable influx of logs. You have the option to seamlessly transition to live monitoring of logs as well. Simply click on any log line to view its detailed information.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_12.webp" alt="A sample log line of the logs sent from AWS Lambda" /></p>
<p><em>A sample log line of the logs sent from AWS Lambda</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-blob-storage/logging/
tag_set: azure-monitoring, az-blob-storage, logging
image_urls: https://signoz.io/img/docs/azure-monitoring/blob-store-diag-settings.webp
tracking_id: docs-azure-monitoring-az-blob-storage-logging
group_tracking_ids: docs-azure-monitoring-az-blob-storage-logging
<h2>Azure Blob Storage Audit Logging</h2>
<h2>Overview</h2>
<hr />
<p>Blob Storage Audit Logging is a feature of Azure Blob Storage that allows you to track and monitor access to your blobs. It provides detailed information about who accessed your blobs, when, and what actions were performed. This feature can help you identify and respond to security incidents or unauthorized access to your data more effectively (SIEM).</p>
<p>The following categories of Logs are available to export to Storage Account or EventHub.</p>
<ul>
<li>Storage Read</li>
<li>Storage Write</li>
<li>Storage Delete</li>
</ul>
<h3>## Prerequisites</h3>
<ul>
<li>
<p><a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p><a href="../../bootstrapping/collector-setup">Central Collector Setup</a></p>
</li>
</ul>
<h2>Setup</h2>
<hr />
<ol>
<li>
<p>Navigate to the relevant Storage Account in the Azure portal</p>
</li>
<li>
<p>Search for &quot;Diagnostic settings&quot; in the left navigation menu</p>
</li>
<li>
<p>Click on <code>blob</code> under the storage account</p>
</li>
<li>
<p>Click on &quot;Add Diagnostic Setting&quot;</p>
</li>
<li>
<p>Select the desired log categories to export:</p>
<ul>
<li>Storage Read</li>
<li>Storage Write</li>
<li>Storage Delete</li>
</ul>
</li>
<li>
<p>Configure the destination details as &quot;<strong>Stream to an Event Hub</strong>&quot; and select the Event Hub namespace and Event Hub name created during the <a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p>Save the diagnostic settings</p>
</li>
</ol>
<p>That's it! You have successfully set up logging for your Azure Blob Storage.</p>
<p><img src="https://signoz.io/img/docs/azure-monitoring/blob-store-diag-settings.webp" alt="Blob Storage Diagnostic Settings" /></p>
<p>_</p>
<p>Blob Storage Diagnostic Settings</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#overview
tag_set: aws-monitoring, lambda-logs
image_urls: 
tracking_id: docs-aws-monitoring-lambda-logs-overview
group_tracking_ids: docs-aws-monitoring-lambda-logs
<h2>Send your AWS Lambda logs to SigNoz: Send your AWS Lambda logs to SigNoz - Overview</h2>
<p>This documentation provides a detailed walkthrough on how to set up an AWS Lambda function to collect AWS Lambda logs stored in an AWS S3 bucket and forward them to SigNoz. By the end of this guide, you will have a setup that automatically sends your Lambda logs to SigNoz, enabling you to visualize and monitor your application's load balancing performance and health.</p>
<p><strong>Here‚Äôs a quick summary of what we‚Äôll be doing in this detailed article.</strong></p>
<ul>
<li>
<p><a href="#creating--configuring-your-s3-bucket">Creating / Configuring your S3 bucket</a></p>
</li>
<li>
<p><a href="#understanding-how-lambda-function-work">Understanding how lambda function work</a></p>
</li>
<li>
<p><a href="#creating-a-lambda-function">Creating a lambda function</a></p>
</li>
<li>
<p><a href="#the-lambda-function">The Lambda Function Code</a></p>
</li>
<li>
<p><a href="#visualize-the-logs-in-signoz">Visualize the logs in SigNoz</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#prerequisites
tag_set: aws-monitoring, lambda-logs
image_urls: 
tracking_id: docs-aws-monitoring-lambda-logs-prerequisites
group_tracking_ids: docs-aws-monitoring-lambda-logs
<h2>Send your AWS Lambda logs to SigNoz: Prerequisites</h2>
<ul>
<li>AWS account with administrative privilege.</li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#creating--configuring-your-s3-bucket
tag_set: aws-monitoring, lambda-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-create_bucket_2.webp
tracking_id: docs-aws-monitoring-lambda-logs-creating-configuring-your-s3-bucket
group_tracking_ids: docs-aws-monitoring-lambda-logs
<h2>Send your AWS Lambda logs to SigNoz: Creating / Configuring your S3 bucket</h2>
<p>To accomplish the task described, please follow these steps:</p>
<ol>
<li><strong>Creating an S3 Bucket:</strong>
<ul>
<li>Sign in to the AWS Management Console.</li>
<li>Navigate to the Amazon S3 service.</li>
<li>Click on <strong>Create bucket</strong>.</li>
<li>Enter a unique bucket name, select the region, and configure any additional settings if needed (such as versioning, logging, etc.).</li>
<li>Click <strong>Create bucket</strong> to finalize the creation process.</li>
</ul>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-create_bucket_2.webp" alt="" /></p>
<p><em>Fill up bucket details</em></p>
<ol start="2">
<li><strong>Uploading Data to the S3 Bucket:</strong></li>
</ol>
<p>‚úÖ Info</p>
<p>Refer <a href="https://aws.amazon.com/blogs/compute/using-aws-lambda-extensions-to-send-logs-to-custom-destinations/">this link</a> to know how to automatically collect Lambda logs in S3.</p>
<ul>
<li>After creating the bucket, navigate to the S3 Management Console.</li>
<li>Click on the <strong>Upload</strong> button.</li>
<li>Select the files you wish to upload from your local machine.</li>
<li>Optionally, configure settings like encryption, permissions, etc.</li>
<li>Click <strong>Upload</strong> to upload the selected files to the bucket.</li>
</ul>
<ol start="3">
<li><strong>File Formats:</strong>
<ul>
<li>You can upload files in various formats such as .json, .csv, .log, .gz, .zip, etc.</li>
<li>If you have configured Lambda logging, Lambda logs are automatically saved in <code>.log</code> format in your S3 bucket.</li>
</ul>
</li>
</ol>
<p>To move forward, we assume that you already have some data in your S3 bucket.</p>
<p>‚úÖ Info</p>
<p>For the scope of this documentation, we assume that all the data in S3 bucket is in the same format. For example, if one file is in <code>.csv</code> format (say), then all files within the bucket will be in <code>.csv</code> format. For files in different formats, you will have to use different parsing functions for each format or update the existing function accordingly.</p>
<p>The general header(table) format of Lambda logs is:</p>
<pre><code>lambda_headers= [&quot;time&quot;,&quot;type&quot;,&quot;record&quot;]
</code></pre>
<p>There can be nested log data as described below. Modify the code <a href="#the-lambda-function">below</a> according to your needs to capture and send only the required info.</p>
<pre><code>// a single lambda log record
{
    &quot;time&quot;: &quot;2020-11-12T14:55:06.780Z&quot;,
    &quot;type&quot;: &quot;platform.report&quot;,
    &quot;record&quot;: {
      &quot;requestId&quot;: &quot;49e64413-fd42-47ef-b130-6fd16f30148d&quot;,
      &quot;metrics&quot;: {
        &quot;durationMs&quot;: 4.96,
        &quot;billedDurationMs&quot;: 100,
        &quot;memorySizeMB&quot;: 128,
        &quot;maxMemoryUsedMB&quot;: 87,
        &quot;initDurationMs&quot;: 792.41
      },
      &quot;tracing&quot;: {
        &quot;type&quot;: &quot;X-Amzn-Trace-Id&quot;,
        &quot;value&quot;: &quot;Root=1-5fad4cc9-70259536495de84a2a6282cd;Parent=67286c49275ac0ad;Sampled=1&quot;
      }
    }
  }
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#understanding-how-lambda-function-work
tag_set: aws-monitoring, lambda-logs
image_urls: 
tracking_id: docs-aws-monitoring-lambda-logs-understanding-how-lambda-function-work
group_tracking_ids: docs-aws-monitoring-lambda-logs
<h2>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: Understanding how lambda function work</h2>
<p>When you successfully attach your lambda function with the S3 bucket and configure it correctly, any new addition / deletion / copy / PUT etc, requests made to the S3 bucket will trigger the lambda function and the code written in the lambda function will get executed.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#creating-a-lambda-function
tag_set: aws-monitoring, lambda-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_1.webp, https://signoz.io/img/docs/lambda-docs/create_function.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_3.webp, https://signoz.io/img/docs/lambda-docs/lambda_logs_page.webp
tracking_id: docs-aws-monitoring-lambda-logs-creating-a-lambda-function
group_tracking_ids: docs-aws-monitoring-lambda-logs
<p>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: Creating a lambda function</p>
<p>üìù Note</p>
<p>The below attached screenshots are repurposed here (previously for load balancer logs), lambda function creation processes remains the same, adjust the name values according to your needs.</p>
<p>Follow these steps to create the lambda function:</p>
<p><strong>Step 1:</strong> Go to your AWS console and search for AWS Lambda, go to <strong>Functions</strong> and click on <strong>Create Function</strong>.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_1.webp" alt="" /></p>
<p><em>Create Lambda function from AWS Console</em></p>
<p><strong>Step 2:</strong> Choose the <code>Author from scratch</code> checkbox and proceed to fill in the function name.</p>
<p><img src="https://signoz.io/img/docs/lambda-docs/create_function.webp" alt="" /></p>
<p><em>Choose the Author from scratch and fill up other details</em></p>
<p><strong>Step 3:</strong> Choose <code>Python 3.x</code> as the Runtime version, <code>x86_64</code> as Architecture (preferably), and keep other settings as default. Select <code>Create a new role with basic Lambda permissions</code>for now, we‚Äôll requiring more permissions down the lane. So for now, select this option.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_3.webp" alt="" /></p>
<p><em>Choose Create a new role here</em></p>
<p><strong>Step 4:</strong> Once you are done configuring the lambda function, you Lambda function is created.</p>
<p><img src="https://signoz.io/img/docs/lambda-docs/lambda_logs_page.webp" alt="" /></p>
<p><em>Your barebones Lambda function is created now</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#configuring-policies-for-lambda-function
tag_set: aws-monitoring, lambda-logs
image_urls: https://signoz.io/img/docs/lambda-docs/execution_role.webp, https://signoz.io/img/docs/lambda-docs/lamba_roles.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_7.webp, https://signoz.io/img/docs/elb/elb-logs-policies-used.webp
tracking_id: docs-aws-monitoring-lambda-logs-configuring-policies-for-lambda-function
group_tracking_ids: docs-aws-monitoring-lambda-logs
<p>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: Configuring Policies for Lambda function</p>
<p>As said in Step 3 previously, we need extra permissions in order to access the S3 Bucket for execution of our Lambda code, follow along to set it up.</p>
<p><strong>Step 1:</strong> Scroll down from your Lambda page, you‚Äôll see a few tabs there. Go to <code>Configurations</code> and select <code>Permissions</code> from the left sidebar.</p>
<p><img src="https://signoz.io/img/docs/lambda-docs/execution_role.webp" alt="" /></p>
<p><em>Choosing execution role from Configurations tab</em></p>
<p><strong>Step 2:</strong> Click on the <code>Execution Role name</code> link just under Role name, it will take us to AWS IAM page. Here we will add policies to get full S3 access. Once here, click on the <code>Add permissions</code> button and select <code>Attach policies</code> from the drop down list.</p>
<p><img src="https://signoz.io/img/docs/lambda-docs/lamba_roles.webp" alt="" /></p>
<p><em>Attach policies to your Lambda function</em></p>
<p><strong>Step 3:</strong> Search ‚ÄúS3‚Äù and you‚Äôll a policy named <code>AmazonS3FullAccess</code>select that and proceed.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_7.webp" alt="" /></p>
<p><em>Policies you'll need to run your Lambda function</em></p>
<p>‚ö†Ô∏è Warning</p>
<p>It's advisable to proceed with caution when granting full S3 access, particularly in a production environment. Before deploying your Lambda function with such extensive permissions, it's essential to consult with your system administrator or designated authority to ensure compliance with security protocols and best practices. This step helps mitigate potential risks and ensures that access permissions align with organizational guidelines and requirements.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-policies-used.webp" alt="" /></p>
<p><em>Extra policies you might require to execute your Lambda function</em></p>
<p>‚úÖ Info</p>
<p>Please refer to the image above as a comprehensive guide to the policy names that you may consider adding to your Lambda function. Failure to include these policies could result in insufficient privileges, potentially hindering the function's ability to perform necessary operations within the AWS environment.</p>
<p>Congrats, you are just done with one of the major hurdle in running your code. Now, let‚Äôs add a trigger.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#adding-triggers
tag_set: aws-monitoring, lambda-logs
image_urls: https://signoz.io/img/docs/lambda-docs/add_trigger.webp, https://signoz.io/img/docs/lambda-docs/add_trigger_2.webp
tracking_id: docs-aws-monitoring-lambda-logs-adding-triggers
group_tracking_ids: docs-aws-monitoring-lambda-logs
<p>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: Adding Triggers</p>
<p>You need to use the Lambda console to build a trigger so that your function can be called immediately by another AWS service (S3, in our case). A trigger is a resource you set up to enable your function to be called by another AWS service upon the occurrence of specific events or conditions.</p>
<p>üìù Note</p>
<p>A function may have more than one trigger. Every trigger functions as a client, independently calling your method, and Lambda transfers data from a single trigger to each event it passes to your function.</p>
<p>To setup the trigger, follow these steps:</p>
<p><strong>Step 1:</strong> Click on the <code>+ Add trigger</code> button from the Lambda console.</p>
<p><img src="https://signoz.io/img/docs/lambda-docs/add_trigger.webp" alt="" /></p>
<p><em>Add a trigger to the function.</em></p>
<p><strong>Step 2:</strong> Select <code>S3</code> from the first drop down of AWS services list. Pick your S3 bucket for the second field.</p>
<p><strong>Step 3:</strong> For the Event types field, you can select any number of options you wish. The trigger will occur depending upon what option(s) you choose here. By default, the <code>All object create events</code> will be selected.</p>
<p><img src="https://signoz.io/img/docs/lambda-docs/add_trigger_2.webp" alt="" /></p>
<p><em>Choose event types from the drop down menu</em></p>
<p>Verify the settings and click on <code>Add</code> button at bottom right to add this trigger.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#adding-request-layer
tag_set: aws-monitoring, lambda-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-layer_3.webp, https://signoz.io/img/docs/elb/elb-logs-layer_4.webp, https://signoz.io/img/docs/elb/elb-logs-layer_5.webp, https://signoz.io/img/docs/elb/elb-logs-layer_6.webp
tracking_id: docs-aws-monitoring-lambda-logs-adding-request-layer
group_tracking_ids: docs-aws-monitoring-lambda-logs
<p>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: Adding Request Layer</p>
<p>We will be using python's <code>request</code> module which is <a href="https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/">not included by default</a> in Lambda.</p>
<p>To utilize Python's <code>requests</code> module within a Lambda function, it's necessary to explicitly add it as a layer. While there may be alternative approaches, it's advisable to adhere to established practices that have been thoroughly tested and proven effective. Therefore, we will proceed with adding <code>requests</code> as a layer to ensure reliable functionality within the Lambda environment.</p>
<p><strong>Step 1:</strong> Follow the steps below to create a zip of the request module and add it as a layer to make it work on AWS lambda.</p>
<p>The commands you‚Äôd need:</p>
<pre><code># make a new directory
mkdir python
# move into that directory
cd python

# install requests module
pip install --target . requests
# zip the contents under the name dependencies.zip
zip -r dependencies.zip ../python 
</code></pre>
<p><strong>Step 2:</strong> To upload your zip file, go to AWS Lambda &gt; Layers and click on <code>Create Layer</code>. [Not inside your specific Lambda function, just the landing page of AWS Lambda].</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_3.webp" alt="" /></p>
<p><em>Creating a new Layer</em></p>
<p><strong>Step 3:</strong> You‚Äôll be redirected to Layer configurations page, here, give a name to your layer, an optional description, select <code>Upload a .zip file</code> , click on <code>Upload</code> and locate the requirements.zip file.</p>
<p><strong>Step 4:</strong> Select your desired architecture and pick <code>Python 3.x</code> as your runtime. Hit <code>Create</code>. Your layer has now been created. Now lets connect it to our Lambda function which we created to send logs to SigNoz.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_4.webp" alt="" /></p>
<p><em>Uploading the requirements.zip file to later add it as a layer</em></p>
<p><strong>Step 5:</strong> Go to your Lambda function, scroll down to Layers section and on the right of it, you‚Äôll find a button that says <code>Add a layer</code> to click on.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_5.webp" alt="" /></p>
<p><em>Add a layer to your Lambda function</em></p>
<p><strong>Step 6:</strong> Pick <code>Custom layers</code> from the checkbox and select your custom layer from the given drop down below and then click on the button <code>Add</code>.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-layer_6.webp" alt="" /></p>
<p><em>Choose your layer name</em></p>
<p>Congratulations, the <code>requests</code> module has been successfully integrated into your code area. By adding this layer, you have resolved the <code>request module not found error</code> that would have otherwise occurred.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#the-lambda-function
tag_set: aws-monitoring, lambda-logs
image_urls: 
tracking_id: docs-aws-monitoring-lambda-logs-the-lambda-function
group_tracking_ids: docs-aws-monitoring-lambda-logs
<p>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: The Lambda Function: The Lambda Function: The Lambda Function</p>
<p>Now, we come to the pivotal section of this document: the code implementation.</p>
<p>The Python script's primary function revolves around retrieving log files stored within an Amazon S3 bucket and send data to a predetermined HTTP endpoint.</p>
<p>Below is the comprehensive code along with detailed comments for clarity:</p>
<pre><code>import boto3 
import requests

# Create an S3 client
s3 = boto3.client('s3')

# Lambda function handler
def lambda_handler(event, context):
    # S3 bucket name
    bucket_name = '&lt;name_of_your_bucket&gt;'

    # List all objects in the specified S3 bucket
    response = s3.list_objects_v2(Bucket=bucket_name)

    # Iterate through each object in the bucket
    for obj in response['Contents']:
        # Check if the object is a gzipped log file
        if obj['Key'].endswith('.log'):
            file_key = obj['Key']

            # Get the file content
            file_obj = s3.get_object(Bucket=bucket_name, Key=file_key)
            file_content = file_obj['Body'].read()
            log_data = data.replace(&quot;'&quot;, '&quot;')
            
            req_headers = {
                     'signoz-access-token': '&lt;SIGNOZ_INGESTION_KEY&gt;',
                     'Content-Type': 'application/json'
                }
            # Specify the HTTP endpoint for sending the data
            http_url = 'https://ingest.in.signoz.cloud:443/logs/json'  # Replace with your actual URL

            # Send the JSON data to the specified HTTP endpoint
            response = requests.post(http_url, data=log_data, headers=req_headers)

            # Print information about the sent data and the response received
            print(f&quot;Sent data to {http_url}. Response: {response.status_code}&quot;)
</code></pre>
<p>In the <code>http_url = 'https://ingest.in.signoz.cloud:443/logs/json'</code>, replace the <code>in</code> with your REGION name. You can find ingestion details (REGION and SIGNOZ_INGESTION_KEY) in the SigNoz dashboard.</p>
<p>‚ö†Ô∏è Warning</p>
<p>The provided code is functional, but exercise caution when copying and pasting it in its entirety. Incorrect configuration could result in the unintentional ingestion of a large volume of data.</p>
<p>Other than the above explanation and the code comments, in a nutshell, what the this code does is:</p>
<p>Sends the parsable content of <strong>ENTIRE</strong> S3 bucket whenever the lambda function gets triggered. It gets triggered by the condition you set above. Let‚Äôs mention that again here.</p>
<blockquote>
<p><strong>Step 3:</strong> For the Event types field, you can select any number of options you wish. The trigger will occur depending upon what option(s) you choose here. By default, the <code>All object create events</code> will be selected.</p>
</blockquote>
<p>Lets say you add something to your S3 bucket, it may / may not trigger this lambda function or if you have setup your s3 as if it automatically stores all your ELB/VPC/Lambda logs, segregated in different folders, so whenever any new log gets added, the function will get triggered and send all the S3 data.</p>
<p>This is obviously not what everyone expects, ideal case would be to have a mass log transfer once the first connection is made to SigNoz otel-collector (which then they later get stored in gp2/gp3 storageClass of EBS), and then send logs lines of only the recently logged one.</p>
<p>To achieve this functionality, you need to add few conditions to the code.</p>
<ol>
<li>Assuming all standard log lines have a timestamp field.</li>
<li>Parse and select the timestamp field from the log line and add it before the <code>response = requests.post(http_url, data=log_data, headers=req_headers)</code> line as a if else condition to only send logs which are x days older (say 3 days).</li>
</ol>
<p>So, the function now will first check the log timestamp and only send those logs which are 3 days older (say) or even a few hours old.</p>
<p>Let‚Äôs consider the below pseudo code for better understanding:</p>
<pre><code>   from datetime import datetime, timedelta

   # Your given timestamp
   given_timestamp_str = &quot;2024-01-01T23:58:02.231919Z&quot;
   given_timestamp = datetime.fromisoformat(given_timestamp_str.replace('Z', '+00:00'))

   # Current time
   current_time = datetime.utcnow()

   # Calculate the time difference
   time_difference = current_time - given_timestamp

   # Check if the time difference is less than 3 days
   if time_difference &lt; timedelta(days=3):
       # Run your specific function here
       print(&quot;Running the specific function.&quot;)
   # ADD THE response = requests.post(http_url, json=json_data) LINE HERE

   else:
       print(&quot;Time difference exceeds 3 days. Function will not run.&quot;)
</code></pre>
<p>Feel free to modify any part of the code according to your requirements.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#running-the-code-locally
tag_set: aws-monitoring, lambda-logs
image_urls: 
tracking_id: docs-aws-monitoring-lambda-logs-running-the-code-locally
group_tracking_ids: docs-aws-monitoring-lambda-logs
<p>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: The Lambda Function: Running the code locally</p>
<p>If you want to run the entire setup locally in your laptop for testing purposes. Here‚Äôs the reference code for you:</p>
<pre><code>import os
import requests

def send_log(file_path, url):
    with open(file_path, 'r') as file:
        data = file.read()
        log_data = data.replace(&quot;'&quot;, '&quot;')
        
    headers = {'Content-Type': 'application/json'}
    requests.post(url, data=log_data, headers=headers)

def main():
    root_folder = 'lambda'
    url = 'http://example.com/upload'  # Replace with your actual URL

    for root, _, files in os.walk(root_folder):
        for file in files:
            if file.endswith('.log'):
                file_path = os.path.join(root, file)
                send_log(file_path, url)

if __name__ == '__main__':
    main()
</code></pre>
<p>‚úÖ Info</p>
<p>To incorporate log files into the folder, including nested directories, the code systematically examines all sub-folders to identify files ending with the <code>.log</code> extension. Should you wish to target different file types, you have the flexibility to modify the code accordingly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#testing-your-lambda-function
tag_set: aws-monitoring, lambda-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-timeout_increase.webp, https://signoz.io/img/docs/elb/elb-logs-lambda_test_event.webp
tracking_id: docs-aws-monitoring-lambda-logs-testing-your-lambda-function
group_tracking_ids: docs-aws-monitoring-lambda-logs
<p>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: The Lambda Function: Testing your Lambda function</p>
<p>Once you've finished writing your code, it's crucial to deploy it and conduct thorough testing to ensure its functionality. Before proceeding with deployment and testing, it's important to consider adjusting the timeout setting for your Lambda function. This adjustment is necessary because the process of transferring data from S3 to an external endpoint may take several minutes, exceeding the default Lambda timeout of 3 seconds.</p>
<p>To extend the timeout duration, follow these steps:</p>
<ol>
<li>Navigate to the Lambda function configuration.</li>
<li>Access the &quot;General Configuration&quot; section.</li>
<li>Click on the &quot;Edit&quot; button to modify settings.</li>
<li>Increase the timeout value to a duration exceeding 10 minutes. Typically, the code execution completes within 1-4 minutes at most.</li>
</ol>
<p>By adjusting the timeout setting, you ensure that your Lambda function has sufficient time to complete the data transfer process without encountering timeouts. This proactive measure enhances the reliability and effectiveness of your deployed solution.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-timeout_increase.webp" alt="" /></p>
<p><em>Configuring execution timeout of Lambda function</em></p>
<p>Once you've finished adjusting the timeout setting, navigate to the code editor for your Lambda function. Locate the 'test' button, and from the dropdown menu, select the option labeled 'Configure test events.' Create a new test case by specifying it as an S3 PUT event, then save your configuration.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_test_event.webp" alt="" /></p>
<p><em>Creating Sample event</em></p>
<p>You're now prepared to proceed. Whenever you make alterations to the code and wish to evaluate them, follow these steps: Deploy the code first (equivalent to pressing 'Save'), and once it's fully deployed, proceed to click on the 'Test' button.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#test-case-and-output
tag_set: aws-monitoring, lambda-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_output.webp
tracking_id: docs-aws-monitoring-lambda-logs-test-case-and-output
group_tracking_ids: docs-aws-monitoring-lambda-logs
<p>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: The Lambda Function: Test Case and Output</p>
<p>If the logs are sent successfully, here's how they'll be transmitted. The following output displays the JSON-formatted data as we've printed it to visualize the sent information.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_output.webp" alt="" /></p>
<p><em>Visual representation of code output of Lambda function (SAMPLE ONLY)</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/lambda-logs/#visualize-the-logs-in-signoz
tag_set: aws-monitoring, lambda-logs
image_urls: https://signoz.io/img/docs/elb/elb-logs-lambda_12.webp
tracking_id: docs-aws-monitoring-lambda-logs-visualize-the-logs-in-signoz
group_tracking_ids: docs-aws-monitoring-lambda-logs
<h2>Send your AWS Lambda logs to SigNoz: Understanding how lambda function work: Visualize the logs in SigNoz</h2>
<p>Upon accessing the SigNoz logs section, you will notice a considerable influx of logs. You have the option to seamlessly transition to live monitoring of logs as well. Simply click on any log line to view its detailed information.</p>
<p><img src="https://signoz.io/img/docs/elb/elb-logs-lambda_12.webp" alt="A sample log line of the logs sent from AWS Lambda" /></p>
<p><em>A sample log line of the logs sent from AWS Lambda (SAMPLE ONLY)</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>First, we need to set up a Kubernetes cluster (see the <a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html">official AWS documentation</a> for more info). Follow the &quot;Managed nodes - Linux&quot; guide.</p>
<p>Follow the steps on this page to install SigNoz on Kubernetes with Helm.</p>
<p>The <a href="https://signoz.io/docs/install/kubernetes/">SigNoz Helm chart</a> will install the following components into your Kubernetes cluster:</p>
<ul>
<li>Query Service (backend service)</li>
<li>Web UI (frontend)</li>
<li>OpenTelemetry Collectors</li>
<li>Alertmanager</li>
<li>ClickHouse chart (datastore)</li>
<li>K8s-Infra chart (k8s infra metrics/logs collectors)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#prerequisites
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-prerequisites
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Prerequisites</h2>
<ul>
<li>
<p>Managed nodes - Linux. Fargate is not offically supported</p>
</li>
<li>
<p>You must have an EKS cluster</p>
</li>
<li>
<p>Kubernetes version &gt;= <code>1.22</code></p>
</li>
<li>
<p><code>x86-64</code>/<code>amd64</code> workloads as currently <code>arm64</code> architecture is not supported</p>
</li>
<li>
<p>Helm version &gt;= <code>3.8</code></p>
</li>
<li>
<p>You must have <code>kubectl</code> access to your cluster</p>
</li>
<li>
<p>The following table describes the hardware requirements that are needed to install SigNoz on Kubernetes:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Minimal Requirements</th>
<th>Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory</td>
<td>8 GB</td>
<td>16 GB</td>
</tr>
<tr>
<td>CPU</td>
<td>4 cores</td>
<td>8 cores</td>
</tr>
<tr>
<td>Storage</td>
<td>30 GB</td>
<td>80 GB</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Suggestion: In case you want to use your own custom storage class for PVCs, you can set <code>global.storageClass</code> configuration to desired storage class.</p>
</li>
<li>
<p>In case of K8s version <code>1.23</code> and above, you must install the Amazon EBS CSI driver and provide relevant volume permissions to the role assigned to the Amazon EKS cluster IAM role. To know more, refer to the <a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi-migration-faq.html">Amazon EBS CSI migration documentation</a>
.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#chart-configuration
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-chart-configuration
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Chart configuration</h2>
<p>Here's the minimal required <code>override-values.yaml</code> that we'll be using later. You can find an overview of the parameters that can be configured during installation under <a href="https://github.com/SigNoz/charts/tree/main/charts/signoz#configuration">chart configuration</a>.</p>
<pre><code>global:
  storageClass: gp2-resizable
  cloud: aws

clickhouse:
  installCustomStorageClass: true
</code></pre>
<p>‚úÖ Info</p>
<p>To list storage class in your Kubernetes cluster: <code>kubectl get storageclass</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#install-signoz-on-kubernetes-with-helm
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-install-signoz-on-kubernetes-with-helm
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Install SigNoz on Kubernetes with Helm</h2>
<ol>
<li>
<p>Add the SigNoz Helm repository to your client with name <code>signoz</code> by running the following command:</p>
<pre><code>helm repo add signoz https://charts.signoz.io
</code></pre>
</li>
<li>
<p>Verify that the repository is accessible to the Helm CLI by entering the following command:</p>
<pre><code>helm repo list
</code></pre>
</li>
<li>
<p>Use the <code>kubectl create ns</code> command to create a new namespace. SigNoz recommends you use <code>platform</code> for your new namespace:</p>
<pre><code>kubectl create ns platform
</code></pre>
</li>
<li>
<p>Run the following command to install the chart with the release name <code>my-release</code> and namespace <code>platform</code>:</p>
<p>helm --namespace platform install my-release signoz/signoz -f override-values.yaml</p>
</li>
</ol>
<p>Output:</p>
<pre><code>NAME: my-release
LAST DEPLOYED: Mon May 23 20:34:55 2022
NAMESPACE: platform
STATUS: deployed
REVISION: 1
NOTES:
1. You have just deployed SigNoz cluster:

- frontend version: '0.8.0'
- query-service version: '0.8.0'
- alertmanager version: '0.23.0-0.1'
- otel-collector version: '0.43.0-0.1'
- otel-collector-metrics version: '0.43.0-0.1'
</code></pre>
<p>*<em>Note that the above command installs the latest stable version of SigNoz.</em></p>
<p>(Optional) To install a different version, you can use the <code>--set</code> flag to specify the version you wish to install. The following example command installs SigNoz version <code>0.8.0</code>:</p>
<pre><code>helm --namespace platform install my-release signoz/signoz \
  --set frontend.image.tag=&quot;0.8.0&quot; \
  --set queryService.image.tag=&quot;0.8.0&quot;
</code></pre>
<p>‚úÖ Info</p>
<ul>
<li>If you use the <code>--set</code> flag, ensure that you specify the same versions for the <code>frontend</code> and <code>queryService</code> images. Specifying different versions could lead the SigNoz cluster to behave abnormally.</li>
<li>Do not use the <code>latest</code> or <code>develop</code> tags in a production environment. Specifying these tags could install different versions of SigNoz on your cluster and could lead to data loss.</li>
</ul>
<ol start="5">
<li>
<p>You can access SigNoz by setting up port forwarding and browsing to the specified port. The following <code>kubectl port-forward</code> example command forwards all connections made to <code>localhost:3301</code> to <code>&lt;signoz-frontend-service&gt;:3301</code>:</p>
<p>export SERVICE_NAME=$(kubectl get svc --namespace platform -l &quot;app.kubernetes.io/component=frontend&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)</p>
<p>kubectl --namespace platform port-forward svc/$SERVICE_NAME 3301:3301</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#verify-the-installation
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-verify-the-installation
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Verify the Installation</h2>
<p>Using the <code>kubectl -n platform get pods</code> command, monitor the SigNoz deployment process. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n platform get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                                                        READY   STATUS    RESTARTS   AGE
chi-signoz-cluster-0-0-0                                    1/1     Running   0          8m21s
clickhouse-operator-8cff468-n5s99                           2/2     Running   0          8m55s
my-release-signoz-alertmanager-0                            1/1     Running   0          8m54s
my-release-signoz-frontend-78774f44d7-wl87p                 1/1     Running   0          8m55s
my-release-signoz-otel-collector-66c8c7dc9d-d8v5c           1/1     Running   0          8m55s
my-release-signoz-otel-collector-metrics-68bcfd5556-9tkgh   1/1     Running   0          8m55s
my-release-signoz-query-service-0                           1/1     Running   0          8m54s
my-release-zookeeper-0                                      1/1     Running   0          8m54s
</code></pre>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#optional-install-a-sample-application-and-generate-tracing-data
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-optional-install-a-sample-application-and-generate-tracing-data
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: (Optional) Install a Sample Application and Generate Tracing Data</h2>
<p>Follow the steps in this section to install a sample application named <a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a>
, and generate tracing data.</p>
<ol>
<li>
<p>Use the HotROD install script below to create a <code>sample-application</code> namespace and deploy HotROD application on it:</p>
<pre><code>curl -sL https://github.com/SigNoz/signoz/raw/develop/sample-apps/hotrod/hotrod-install.sh \
  | HELM_RELEASE=my-release SIGNOZ_NAMESPACE=platform bash
</code></pre>
</li>
<li>
<p>Using the <code>kubectl -n sample-application get pods</code> command, monitor the sample application pods. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n sample-application get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                            READY   STATUS    RESTARTS   AGE
hotrod-55bd58cc8d-mzxq8         1/1     Running   0          2m
locust-master-b65744bbf-l7v7n   1/1     Running   0          2m
locust-slave-688c86bcb7-ngx7w   1/1     Running   0          2m
</code></pre>
</li>
<li>
<p>Use the following command to generate load:</p>
<pre><code>kubectl --namespace sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl -X POST -F \
  'user_count=6' -F 'spawn_rate=2' http://locust-master:8089/swarm
</code></pre>
</li>
<li>
<p>Browse to <code>http://localhost:3301</code> and see the metrics and traces for your sample application.</p>
</li>
<li>
<p>Use the following command to stop load generation:</p>
<pre><code>kubectl -n sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl \
  http://locust-master:8089/stop
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/kubernetes">Kubernetes Operate</a> section for detailed instructions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#next-steps
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-next-steps
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Next Steps</h2>
<ul>
<li>
<p><a href="/docs/instrumentation/overview">Instrument Your Application</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">Use OpenTelemetry Operator for automatic instrumentation (if your applications are in k8s)</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/product-features/alert-management/
tag_set: product-features, alert-management
image_urls: https://signoz.io/img/docs/product-features/alerts/product-features-alert-management.webp
tracking_id: docs-product-features-alert-management
group_tracking_ids: docs-product-features-alert-management
<h2>Alert Management in SigNoz</h2>
<h2>Introduction</h2>
<hr />
<p>Alerts in SigNoz can help you to define which data to monitor, set thresholds to detect potential problems, and specify who should be notified and how.</p>
<p><img src="https://signoz.io/img/docs/product-features/alerts/product-features-alert-management.webp" alt="Alert Management in SigNoz" /></p>
<p><em>Alert Management in SigNoz</em></p>
<h2>Alert Rules Management</h2>
<hr />
<ul>
<li><strong>Alert Rules Tab</strong>: Manage and overview alert rules with options to view, edit, sort, and filter based on various parameters like creation date, severity, labels etc.</li>
<li><strong>Triggered Alerts Tab</strong>: Provides a real-time view of active alerts.</li>
<li><strong>Creating New Alerts</strong>: Create new alerts across different signals such as Metrics, Logs, Traces, and Exceptions, triggered by threshold conditions.</li>
</ul>
<p>For detailed instructions on setting up and managing alerts in SigNoz, please refer to this <a href="https://signoz.io/docs/alerts/">documentation</a>.</p>
<p>Create alerts over different signals like Metrics, Logs, Traces and Exceptions triggered based on threshold conditions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/ec2-logs/
tag_set: aws-monitoring, ec2-logs
image_urls: https://signoz.io/img/docs/ec2-application-server-logs.webp
tracking_id: docs-aws-monitoring-ec2-logs
group_tracking_ids: docs-aws-monitoring-ec2-logs
<h2>Send Application/Server logs from EC2 to SigNoz</h2>
<h2>Introduction</h2>
<hr />
<p>This guide provides detailed instructions on how to send application and server logs from an EC2 instance to <strong>SigNoz Cloud</strong>. By integrating with SigNoz, you can efficiently collect, monitor, and analyze your logs for better insights into your applications and servers.</p>
<h2>Prerequisites</h2>
<hr />
<ul>
<li>A Linux-based EC2 instance</li>
<li>An active <a href="http://localhost:3000/teams/">SigNoz Cloud</a> account</li>
</ul>
<p>Sending your server/application logs to SigNoz Cloud broadly involves these two simple steps:</p>
<ul>
<li>Install OpenTelemetry Collector(OTel collector)</li>
<li>Configure filelog receiver</li>
</ul>
<h2>Install OpenTelemetry Collector</h2>
<hr />
<p>The OpenTelemetry collector provides a vendor-neutral way to collect, process, and export your telemetry data such as logs, metrics, and traces.</p>
<p>You can install OpenTelemetry collector as an agent on your Virtual Machine by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">guide</a>.</p>
<h2>Dummy log file</h2>
<hr />
<p>As an example, we can use a sample log file called <code>app.log</code> with the following dummy data:</p>
<pre><code>This is log line 1
This is log line 2
This is log line 3
</code></pre>
<p>This file represents a log file of your application/server.</p>
<h2>Configure filelog receiver</h2>
<hr />
<p>Receivers are used to get data into the collector. A filelog receiver collects logs from files. Modify the <code>config.yaml</code> file that you created while installing OTel collector in the previous step to include the filelog receiver. This involves specifying the path to your <code>app.log</code> file (or your log file) and setting the <code>start_at</code> parameter. For more fields that are available for filelog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">this link</a>.</p>
<pre><code>receivers:
  ...
  filelog/app:
    include: [ /tmp/app.log ] #include the full path to your log file
    start_at: end
...
</code></pre>
<p>üìù Note</p>
<p>The <code>start_at: end</code> configuration ensures that only newly added logs are transmitted. If you wish to include historical logs from the file, remember to modify <code>start_at</code> to <code>beginning</code>.</p>
<h2>Update pipeline configuration</h2>
<hr />
<p>Receivers must be enabled via pipelines within the service section of the collector config file. In the same <code>config.yaml</code> file mentioned above, update the pipeline settings to include the new filelog receiver. This step is crucial for ensuring that the logs are correctly processed and sent to SigNoz.</p>
<pre><code>    service:
        ....
        logs:
            receivers: [otlp, filelog/app]
            processors: [batch]
            exporters: [otlp]
</code></pre>
<p>Now restart the OTel collector so that new changes are applied. The steps to run the OTel collector can be found <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a></p>
<h2>Verifying the exported logs</h2>
<hr />
<p>The logs will be exported to SigNoz UI. If you add more entries to your app.log file they will also be visible in SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/ec2-application-server-logs.webp" alt="Logs of the dummy app.log file visible in SigNoz" /></p>
<p><em>Dummy log file data shown in SigNoz Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>First, we need to set up a Kubernetes cluster (see the <a href="https://cloud.google.com/kubernetes-engine/">official GCP documentation</a> for more info).</p>
<p>Follow the steps on this page to install SigNoz on Kubernetes with Helm.</p>
<p>The <a href="https://signoz.io/docs/install/kubernetes/">SigNoz Helm chart</a> will install the following components into your Kubernetes cluster:</p>
<ul>
<li>Query Service (backend service)</li>
<li>Web UI (frontend)</li>
<li>OpenTelemetry Collectors</li>
<li>Alertmanager</li>
<li>ClickHouse chart (datastore)</li>
<li>K8s-Infra chart (k8s infra metrics/logs collectors)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#prerequisites
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-prerequisites
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Prerequisites</h2>
<ul>
<li>
<p>You must have a GKE cluster. Both Standard and Autopilot are supported.</p>
</li>
<li>
<p>Kubernetes version &gt;= <code>1.22</code></p>
</li>
<li>
<p><code>x86-64</code>/<code>amd64</code> workloads as currently <code>arm64</code> architecture is not supported</p>
</li>
<li>
<p>Helm version &gt;= <code>3.8</code></p>
</li>
<li>
<p>You must have <code>kubectl</code> access to your cluster</p>
</li>
<li>
<p>The following table describes the hardware requirements that are needed to install SigNoz on Kubernetes:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Minimal Requirements</th>
<th>Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory</td>
<td>8 GB</td>
<td>16 GB</td>
</tr>
<tr>
<td>CPU</td>
<td>4 cores</td>
<td>8 cores</td>
</tr>
<tr>
<td>Storage</td>
<td>30 GB</td>
<td>80 GB</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Suggestion: In case you want to use your own custom storage class for PVCs, you can set <code>global.storageClass</code> configuration to desired storage class.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#chart-configuration
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-chart-configuration
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Chart configuration</h2>
<p>Here's the minimal required <code>override-values.yaml</code> that we'll be using later. You can find an overview of the parameters that can be configured during installation under <a href="https://github.com/SigNoz/charts/tree/main/charts/signoz#configuration">chart configuration</a>.</p>
<h3>## GKE Standard</h3>
<p>In GKE Standard, you can either install with the default configuration or make use of the following <code>override-values.yaml</code>:</p>
<pre><code>global:
  storageClass: gce-resizable
  cloud: gcp

clickhouse:
  installCustomStorageClass: true
</code></pre>
<h3>## GKE Autopilot</h3>
<p>In GKE Autopilot, you must set <code>cloud</code> to <code>gcp/autogke</code> as well as update <code>kubeletMetrics</code> to use read-only Kubelet endpoint as shown below in the <code>override-values.yaml</code>:</p>
<pre><code>global:
  storageClass: gce-resizable
  cloud: gcp/autogke

clickhouse:
  installCustomStorageClass: true

k8s-infra:
  presets:
    kubeletMetrics:
      authType: none
      endpoint: ${K8S_NODE_NAME}:10255
</code></pre>
<p>GKE Autopilot automatically overriddes resource requests/limits. In our case, all <code>signoz</code> chart components as well as components from <code>clickhouse</code> and <code>k8s-infra</code> charts, if enabled. Therefore, make sure to have enough resource quota for the region where the cluster is deployed. Read more about it <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-resource-requests#defaults">here</a>.</p>
<p>‚úÖ Info</p>
<p>To list storage class in your Kubernetes cluster: <code>kubectl get storageclass</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#install-signoz-on-kubernetes-with-helm
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-install-signoz-on-kubernetes-with-helm
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Install SigNoz on Kubernetes with Helm</h2>
<ol>
<li>
<p>Add the SigNoz Helm repository to your client with name <code>signoz</code> by running the following command:</p>
<pre><code>helm repo add signoz https://charts.signoz.io
</code></pre>
</li>
<li>
<p>Verify that the repository is accessible to the Helm CLI by entering the following command:</p>
<pre><code>helm repo list
</code></pre>
</li>
<li>
<p>Use the <code>kubectl create ns</code> command to create a new namespace. SigNoz recommends you use <code>platform</code> for your new namespace:</p>
<pre><code>kubectl create ns platform
</code></pre>
</li>
<li>
<p>Run the following command to install the chart with the release name <code>my-release</code> and namespace <code>platform</code>:</p>
<p>helm --namespace platform install my-release signoz/signoz -f override-values.yaml</p>
</li>
</ol>
<p>Output:</p>
<pre><code>NAME: my-release
LAST DEPLOYED: Mon May 23 20:34:55 2022
NAMESPACE: platform
STATUS: deployed
REVISION: 1
NOTES:
1. You have just deployed SigNoz cluster:

- frontend version: '0.8.0'
- query-service version: '0.8.0'
- alertmanager version: '0.23.0-0.1'
- otel-collector version: '0.43.0-0.1'
- otel-collector-metrics version: '0.43.0-0.1'
</code></pre>
<p>*<em>Note that the above command installs the latest stable version of SigNoz.</em></p>
<p>(Optional) To install a different version, you can use the <code>--set</code> flag to specify the version you wish to install. The following example command installs SigNoz version <code>0.8.0</code>:</p>
<pre><code>helm --namespace platform install my-release signoz/signoz \
  --set frontend.image.tag=&quot;0.8.0&quot; \
  --set queryService.image.tag=&quot;0.8.0&quot;
</code></pre>
<p>‚úÖ Info</p>
<ul>
<li>If you use the <code>--set</code> flag, ensure that you specify the same versions for the <code>frontend</code> and <code>queryService</code> images. Specifying different versions could lead the SigNoz cluster to behave abnormally.</li>
<li>Do not use the <code>latest</code> or <code>develop</code> tags in a production environment. Specifying these tags could install different versions of SigNoz on your cluster and could lead to data loss.</li>
</ul>
<ol start="5">
<li>
<p>You can access SigNoz by setting up port forwarding and browsing to the specified port. The following <code>kubectl port-forward</code> example command forwards all connections made to <code>localhost:3301</code> to <code>&lt;signoz-frontend-service&gt;:3301</code>:</p>
<p>export SERVICE_NAME=$(kubectl get svc --namespace platform -l &quot;app.kubernetes.io/component=frontend&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)</p>
<p>kubectl --namespace platform port-forward svc/$SERVICE_NAME 3301:3301</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#verify-the-installation
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-verify-the-installation
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Verify the Installation</h2>
<p>Using the <code>kubectl -n platform get pods</code> command, monitor the SigNoz deployment process. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n platform get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                                                        READY   STATUS    RESTARTS   AGE
chi-signoz-cluster-0-0-0                                    1/1     Running   0          8m21s
clickhouse-operator-8cff468-n5s99                           2/2     Running   0          8m55s
my-release-signoz-alertmanager-0                            1/1     Running   0          8m54s
my-release-signoz-frontend-78774f44d7-wl87p                 1/1     Running   0          8m55s
my-release-signoz-otel-collector-66c8c7dc9d-d8v5c           1/1     Running   0          8m55s
my-release-signoz-otel-collector-metrics-68bcfd5556-9tkgh   1/1     Running   0          8m55s
my-release-signoz-query-service-0                           1/1     Running   0          8m54s
my-release-zookeeper-0                                      1/1     Running   0          8m54s
</code></pre>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#optional-install-a-sample-application-and-generate-tracing-data
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-optional-install-a-sample-application-and-generate-tracing-data
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: (Optional) Install a Sample Application and Generate Tracing Data</h2>
<p>Follow the steps in this section to install a sample application named <a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a>
, and generate tracing data.</p>
<ol>
<li>
<p>Use the HotROD install script below to create a <code>sample-application</code> namespace and deploy HotROD application on it:</p>
<pre><code>curl -sL https://github.com/SigNoz/signoz/raw/develop/sample-apps/hotrod/hotrod-install.sh \
  | HELM_RELEASE=my-release SIGNOZ_NAMESPACE=platform bash
</code></pre>
</li>
<li>
<p>Using the <code>kubectl -n sample-application get pods</code> command, monitor the sample application pods. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n sample-application get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                            READY   STATUS    RESTARTS   AGE
hotrod-55bd58cc8d-mzxq8         1/1     Running   0          2m
locust-master-b65744bbf-l7v7n   1/1     Running   0          2m
locust-slave-688c86bcb7-ngx7w   1/1     Running   0          2m
</code></pre>
</li>
<li>
<p>Use the following command to generate load:</p>
<pre><code>kubectl --namespace sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl -X POST -F \
  'user_count=6' -F 'spawn_rate=2' http://locust-master:8089/swarm
</code></pre>
</li>
<li>
<p>Browse to <code>http://localhost:3301</code> and see the metrics and traces for your sample application.</p>
</li>
<li>
<p>Use the following command to stop load generation:</p>
<pre><code>kubectl -n sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl \
  http://locust-master:8089/stop
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/kubernetes">Kubernetes Operate</a> section for detailed instructions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#next-steps
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-next-steps
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Next Steps</h2>
<ul>
<li>
<p><a href="/docs/instrumentation/overview">Instrument Your Application</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">Use OpenTelemetry Operator for automatic instrumentation (if your applications are in k8s)</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/ec2-infra-metrics/
tag_set: aws-monitoring, ec2-infra-metrics
image_urls: https://signoz.io/img/ec2-infra.webp
tracking_id: docs-aws-monitoring-ec2-infra-metrics
group_tracking_ids: docs-aws-monitoring-ec2-infra-metrics
<h2>Infrastructure metrics of EC2 instance</h2>
<h3>## Overview</h3>
<p>This documentation guides you through integrating AWS EC2 infrastructure metrics into SigNoz using the Hostmetrics receiver in OpenTelemetry Collector. The Hostmetrics receiver is designed to collect metrics about the host system from various sources. It supports various scrapers for collecting different metrics, including CPU, disk, load, filesystem, memory, network, paging, and process metrics.</p>
<h3>## Prerequisites</h3>
<ul>
<li>An EC2 instance</li>
<li>A <a href="https://signoz.io/teams/">SigNoz Cloud</a> account</li>
</ul>
<h3>## Configuring Hostmetrics Receiver</h3>
<p>To see your infrastructure metrics in SigNoz, you need to configure the hostmetrics receiver and create a HostMetrics Dashboard. Follow <a href="https://signoz.io/docs/userguide/hostmetrics/">this documentation</a> to configure hostmetrics receiver and creating the Hostmetrics Dashboard.</p>
<h3>## Final Output</h3>
<p>After setting up your Hostmetrics Dashboard, here's what it might look like:</p>
<p><img src="https://signoz.io/img/ec2-infra.webp" alt="Hostmetrics Dashboard for EC2 instance" /></p>
<p>_</p>
<p>Hostmetrics Dashboard for AWS EC2 instance</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>Follow the steps on this page to install SigNoz on other Kubernetes Cloud Platform and bare-metal servers with Helm.</p>
<p>Follow the steps on this page to install SigNoz on Kubernetes with Helm.</p>
<p>The <a href="https://signoz.io/docs/install/kubernetes/">SigNoz Helm chart</a> will install the following components into your Kubernetes cluster:</p>
<ul>
<li>Query Service (backend service)</li>
<li>Web UI (frontend)</li>
<li>OpenTelemetry Collectors</li>
<li>Alertmanager</li>
<li>ClickHouse chart (datastore)</li>
<li>K8s-Infra chart (k8s infra metrics/logs collectors)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#prerequisites
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-prerequisites
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Prerequisites</h2>
<ul>
<li>
<p>You must have a Kubernetes cluster</p>
</li>
<li>
<p>Kubernetes version &gt;= <code>1.22</code></p>
</li>
<li>
<p><code>x86-64</code>/<code>amd64</code> workloads as currently <code>arm64</code> architecture is not supported</p>
</li>
<li>
<p>Helm version &gt;= <code>3.8</code></p>
</li>
<li>
<p>You must have <code>kubectl</code> access to your cluster</p>
</li>
<li>
<p>The following table describes the hardware requirements that are needed to install SigNoz on Kubernetes:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Minimal Requirements</th>
<th>Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory</td>
<td>8 GB</td>
<td>16 GB</td>
</tr>
<tr>
<td>CPU</td>
<td>4 cores</td>
<td>8 cores</td>
</tr>
<tr>
<td>Storage</td>
<td>30 GB</td>
<td>80 GB</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Suggestion: In case you want to use your own custom storage class for PVCs, you can set <code>global.storageClass</code> configuration to desired storage class.</p>
</li>
<li>
<p>Suggestion: In case you do not have any other storage class which supports volume expansion, you can patch default storage class definition by setting <code>allowVolumeExpansion</code> to <code>True</code> (this enables PVC resize).</p>
<pre><code>DEFAULT_STORAGE_CLASS=$(kubectl get storageclass -o=jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class==&quot;true&quot;)].metadata.name}')

kubectl patch storageclass &quot;$DEFAULT_STORAGE_CLASS&quot; -p '{&quot;allowVolumeExpansion&quot;: true}'
</code></pre>
<p>‚úÖ Info</p>
<p>To list storage class in your Kubernetes cluster: <code>kubectl get storageclass</code>.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#chart-configuration
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-chart-configuration
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Chart configuration</h2>
<p>You can find an overview of the parameters that can be configured during installation under <a href="https://github.com/SigNoz/charts/tree/main/charts/signoz#configuration">chart configuration</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#install-signoz-on-kubernetes-with-helm
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-install-signoz-on-kubernetes-with-helm
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Install SigNoz on Kubernetes with Helm</h2>
<ol>
<li>
<p>Add the SigNoz Helm repository to your client with name <code>signoz</code> by running the following command:</p>
<pre><code>helm repo add signoz https://charts.signoz.io
</code></pre>
</li>
<li>
<p>Verify that the repository is accessible to the Helm CLI by entering the following command:</p>
<pre><code>helm repo list
</code></pre>
</li>
<li>
<p>Use the <code>kubectl create ns</code> command to create a new namespace. SigNoz recommends you use <code>platform</code> for your new namespace:</p>
<pre><code>kubectl create ns platform
</code></pre>
</li>
<li>
<p>Run the following command to install the chart with the release name <code>my-release</code> and namespace <code>platform</code>:</p>
<p>helm --namespace platform install my-release signoz/signoz</p>
</li>
</ol>
<p>Output:</p>
<pre><code>NAME: my-release
LAST DEPLOYED: Mon May 23 20:34:55 2022
NAMESPACE: platform
STATUS: deployed
REVISION: 1
NOTES:
1. You have just deployed SigNoz cluster:

- frontend version: '0.8.0'
- query-service version: '0.8.0'
- alertmanager version: '0.23.0-0.1'
- otel-collector version: '0.43.0-0.1'
- otel-collector-metrics version: '0.43.0-0.1'
</code></pre>
<p>*<em>Note that the above command installs the latest stable version of SigNoz.</em></p>
<p>(Optional) To install a different version, you can use the <code>--set</code> flag to specify the version you wish to install. The following example command installs SigNoz version <code>0.8.0</code>:</p>
<pre><code>helm --namespace platform install my-release signoz/signoz \
  --set frontend.image.tag=&quot;0.8.0&quot; \
  --set queryService.image.tag=&quot;0.8.0&quot;
</code></pre>
<p>‚úÖ Info</p>
<ul>
<li>If you use the <code>--set</code> flag, ensure that you specify the same versions for the <code>frontend</code> and <code>queryService</code> images. Specifying different versions could lead the SigNoz cluster to behave abnormally.</li>
<li>Do not use the <code>latest</code> or <code>develop</code> tags in a production environment. Specifying these tags could install different versions of SigNoz on your cluster and could lead to data loss.</li>
</ul>
<ol start="5">
<li>
<p>You can access SigNoz by setting up port forwarding and browsing to the specified port. The following <code>kubectl port-forward</code> example command forwards all connections made to <code>localhost:3301</code> to <code>&lt;signoz-frontend-service&gt;:3301</code>:</p>
<p>export SERVICE_NAME=$(kubectl get svc --namespace platform -l &quot;app.kubernetes.io/component=frontend&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)</p>
<p>kubectl --namespace platform port-forward svc/$SERVICE_NAME 3301:3301</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#verify-the-installation
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-verify-the-installation
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Verify the Installation</h2>
<p>Using the <code>kubectl -n platform get pods</code> command, monitor the SigNoz deployment process. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n platform get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                                                        READY   STATUS    RESTARTS   AGE
chi-signoz-cluster-0-0-0                                    1/1     Running   0          8m21s
clickhouse-operator-8cff468-n5s99                           2/2     Running   0          8m55s
my-release-signoz-alertmanager-0                            1/1     Running   0          8m54s
my-release-signoz-frontend-78774f44d7-wl87p                 1/1     Running   0          8m55s
my-release-signoz-otel-collector-66c8c7dc9d-d8v5c           1/1     Running   0          8m55s
my-release-signoz-otel-collector-metrics-68bcfd5556-9tkgh   1/1     Running   0          8m55s
my-release-signoz-query-service-0                           1/1     Running   0          8m54s
my-release-zookeeper-0                                      1/1     Running   0          8m54s
</code></pre>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#optional-install-a-sample-application-and-generate-tracing-data
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-optional-install-a-sample-application-and-generate-tracing-data
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: (Optional) Install a Sample Application and Generate Tracing Data</h2>
<p>Follow the steps in this section to install a sample application named <a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a>
, and generate tracing data.</p>
<ol>
<li>
<p>Use the HotROD install script below to create a <code>sample-application</code> namespace and deploy HotROD application on it:</p>
<pre><code>curl -sL https://github.com/SigNoz/signoz/raw/develop/sample-apps/hotrod/hotrod-install.sh \
  | HELM_RELEASE=my-release SIGNOZ_NAMESPACE=platform bash
</code></pre>
</li>
<li>
<p>Using the <code>kubectl -n sample-application get pods</code> command, monitor the sample application pods. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n sample-application get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                            READY   STATUS    RESTARTS   AGE
hotrod-55bd58cc8d-mzxq8         1/1     Running   0          2m
locust-master-b65744bbf-l7v7n   1/1     Running   0          2m
locust-slave-688c86bcb7-ngx7w   1/1     Running   0          2m
</code></pre>
</li>
<li>
<p>Use the following command to generate load:</p>
<pre><code>kubectl --namespace sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl -X POST -F \
  'user_count=6' -F 'spawn_rate=2' http://locust-master:8089/swarm
</code></pre>
</li>
<li>
<p>Browse to <code>http://localhost:3301</code> and see the metrics and traces for your sample application.</p>
</li>
<li>
<p>Use the following command to stop load generation:</p>
<pre><code>kubectl -n sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl \
  http://locust-master:8089/stop
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/kubernetes">Kubernetes Operate</a> section for detailed instructions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/others/#next-steps
tag_set: install, kubernetes, others
image_urls: 
tracking_id: docs-install-kubernetes-others-next-steps
group_tracking_ids: docs-install-kubernetes-others
<h2>Deploying with Helm directly: Next Steps</h2>
<ul>
<li>
<p><a href="/docs/instrumentation/overview">Instrument Your Application</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">Use OpenTelemetry Operator for automatic instrumentation (if your applications are in k8s)</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>First, we need to set up a Kubernetes cluster (see the <a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html">official AWS documentation</a> for more info). Follow the &quot;Managed nodes - Linux&quot; guide.</p>
<p>Follow the steps on this page to install SigNoz on Kubernetes with Helm.</p>
<p>The <a href="https://signoz.io/docs/install/kubernetes/">SigNoz Helm chart</a> will install the following components into your Kubernetes cluster:</p>
<ul>
<li>Query Service (backend service)</li>
<li>Web UI (frontend)</li>
<li>OpenTelemetry Collectors</li>
<li>Alertmanager</li>
<li>ClickHouse chart (datastore)</li>
<li>K8s-Infra chart (k8s infra metrics/logs collectors)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#prerequisites
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-prerequisites
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Prerequisites</h2>
<ul>
<li>
<p>Managed nodes - Linux. Fargate is not offically supported</p>
</li>
<li>
<p>You must have an EKS cluster</p>
</li>
<li>
<p>Kubernetes version &gt;= <code>1.22</code></p>
</li>
<li>
<p><code>x86-64</code>/<code>amd64</code> workloads as currently <code>arm64</code> architecture is not supported</p>
</li>
<li>
<p>Helm version &gt;= <code>3.8</code></p>
</li>
<li>
<p>You must have <code>kubectl</code> access to your cluster</p>
</li>
<li>
<p>The following table describes the hardware requirements that are needed to install SigNoz on Kubernetes:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Minimal Requirements</th>
<th>Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory</td>
<td>8 GB</td>
<td>16 GB</td>
</tr>
<tr>
<td>CPU</td>
<td>4 cores</td>
<td>8 cores</td>
</tr>
<tr>
<td>Storage</td>
<td>30 GB</td>
<td>80 GB</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Suggestion: In case you want to use your own custom storage class for PVCs, you can set <code>global.storageClass</code> configuration to desired storage class.</p>
</li>
<li>
<p>In case of K8s version <code>1.23</code> and above, you must install the Amazon EBS CSI driver and provide relevant volume permissions to the role assigned to the Amazon EKS cluster IAM role. To know more, refer to the <a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi-migration-faq.html">Amazon EBS CSI migration documentation</a>
.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#chart-configuration
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-chart-configuration
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Chart configuration</h2>
<p>Here's the minimal required <code>override-values.yaml</code> that we'll be using later. You can find an overview of the parameters that can be configured during installation under <a href="https://github.com/SigNoz/charts/tree/main/charts/signoz#configuration">chart configuration</a>.</p>
<pre><code>global:
  storageClass: gp2-resizable
  cloud: aws

clickhouse:
  installCustomStorageClass: true
</code></pre>
<p>‚úÖ Info</p>
<p>To list storage class in your Kubernetes cluster: <code>kubectl get storageclass</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#install-signoz-on-kubernetes-with-helm
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-install-signoz-on-kubernetes-with-helm
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Install SigNoz on Kubernetes with Helm</h2>
<ol>
<li>
<p>Add the SigNoz Helm repository to your client with name <code>signoz</code> by running the following command:</p>
<pre><code>helm repo add signoz https://charts.signoz.io
</code></pre>
</li>
<li>
<p>Verify that the repository is accessible to the Helm CLI by entering the following command:</p>
<pre><code>helm repo list
</code></pre>
</li>
<li>
<p>Use the <code>kubectl create ns</code> command to create a new namespace. SigNoz recommends you use <code>platform</code> for your new namespace:</p>
<pre><code>kubectl create ns platform
</code></pre>
</li>
<li>
<p>Run the following command to install the chart with the release name <code>my-release</code> and namespace <code>platform</code>:</p>
<p>helm --namespace platform install my-release signoz/signoz -f override-values.yaml</p>
</li>
</ol>
<p>Output:</p>
<pre><code>NAME: my-release
LAST DEPLOYED: Mon May 23 20:34:55 2022
NAMESPACE: platform
STATUS: deployed
REVISION: 1
NOTES:
1. You have just deployed SigNoz cluster:

- frontend version: '0.8.0'
- query-service version: '0.8.0'
- alertmanager version: '0.23.0-0.1'
- otel-collector version: '0.43.0-0.1'
- otel-collector-metrics version: '0.43.0-0.1'
</code></pre>
<p>*<em>Note that the above command installs the latest stable version of SigNoz.</em></p>
<p>(Optional) To install a different version, you can use the <code>--set</code> flag to specify the version you wish to install. The following example command installs SigNoz version <code>0.8.0</code>:</p>
<pre><code>helm --namespace platform install my-release signoz/signoz \
  --set frontend.image.tag=&quot;0.8.0&quot; \
  --set queryService.image.tag=&quot;0.8.0&quot;
</code></pre>
<p>‚úÖ Info</p>
<ul>
<li>If you use the <code>--set</code> flag, ensure that you specify the same versions for the <code>frontend</code> and <code>queryService</code> images. Specifying different versions could lead the SigNoz cluster to behave abnormally.</li>
<li>Do not use the <code>latest</code> or <code>develop</code> tags in a production environment. Specifying these tags could install different versions of SigNoz on your cluster and could lead to data loss.</li>
</ul>
<ol start="5">
<li>
<p>You can access SigNoz by setting up port forwarding and browsing to the specified port. The following <code>kubectl port-forward</code> example command forwards all connections made to <code>localhost:3301</code> to <code>&lt;signoz-frontend-service&gt;:3301</code>:</p>
<p>export SERVICE_NAME=$(kubectl get svc --namespace platform -l &quot;app.kubernetes.io/component=frontend&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)</p>
<p>kubectl --namespace platform port-forward svc/$SERVICE_NAME 3301:3301</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#verify-the-installation
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-verify-the-installation
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Verify the Installation</h2>
<p>Using the <code>kubectl -n platform get pods</code> command, monitor the SigNoz deployment process. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n platform get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                                                        READY   STATUS    RESTARTS   AGE
chi-signoz-cluster-0-0-0                                    1/1     Running   0          8m21s
clickhouse-operator-8cff468-n5s99                           2/2     Running   0          8m55s
my-release-signoz-alertmanager-0                            1/1     Running   0          8m54s
my-release-signoz-frontend-78774f44d7-wl87p                 1/1     Running   0          8m55s
my-release-signoz-otel-collector-66c8c7dc9d-d8v5c           1/1     Running   0          8m55s
my-release-signoz-otel-collector-metrics-68bcfd5556-9tkgh   1/1     Running   0          8m55s
my-release-signoz-query-service-0                           1/1     Running   0          8m54s
my-release-zookeeper-0                                      1/1     Running   0          8m54s
</code></pre>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#optional-install-a-sample-application-and-generate-tracing-data
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-optional-install-a-sample-application-and-generate-tracing-data
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: (Optional) Install a Sample Application and Generate Tracing Data</h2>
<p>Follow the steps in this section to install a sample application named <a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a>
, and generate tracing data.</p>
<ol>
<li>
<p>Use the HotROD install script below to create a <code>sample-application</code> namespace and deploy HotROD application on it:</p>
<pre><code>curl -sL https://github.com/SigNoz/signoz/raw/develop/sample-apps/hotrod/hotrod-install.sh \
  | HELM_RELEASE=my-release SIGNOZ_NAMESPACE=platform bash
</code></pre>
</li>
<li>
<p>Using the <code>kubectl -n sample-application get pods</code> command, monitor the sample application pods. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n sample-application get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                            READY   STATUS    RESTARTS   AGE
hotrod-55bd58cc8d-mzxq8         1/1     Running   0          2m
locust-master-b65744bbf-l7v7n   1/1     Running   0          2m
locust-slave-688c86bcb7-ngx7w   1/1     Running   0          2m
</code></pre>
</li>
<li>
<p>Use the following command to generate load:</p>
<pre><code>kubectl --namespace sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl -X POST -F \
  'user_count=6' -F 'spawn_rate=2' http://locust-master:8089/swarm
</code></pre>
</li>
<li>
<p>Browse to <code>http://localhost:3301</code> and see the metrics and traces for your sample application.</p>
</li>
<li>
<p>Use the following command to stop load generation:</p>
<pre><code>kubectl -n sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl \
  http://locust-master:8089/stop
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/kubernetes">Kubernetes Operate</a> section for detailed instructions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/aws/#next-steps
tag_set: install, kubernetes, aws
image_urls: 
tracking_id: docs-install-kubernetes-aws-next-steps
group_tracking_ids: docs-install-kubernetes-aws
<h2>Deploying to AWS: Next Steps</h2>
<ul>
<li>
<p><a href="/docs/instrumentation/overview">Instrument Your Application</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">Use OpenTelemetry Operator for automatic instrumentation (if your applications are in k8s)</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#prerequisites
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-prerequisites
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Configure Email Channel - Prerequisites</h2>
<p>Before setting up Email as a notification channel in SigNoz, ensure the following:</p>
<ul>
<li><strong>SMTP Host</strong>: You need to have a SMTP host running through which emails are sent (Not needed for SigNoz Cloud).</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.41.0">v0.41.0</a> or later</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#accessing-alert-channels
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-accessing-alert-channels
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Accessing Alert Channels</h2>
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#creating-a-new-notification-channel
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-creating-a-new-notification-channel
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Creating a new Notification channel</h2>
<p>To create a new Email notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Email as the channel type.</li>
<li><strong>To</strong>: Enter the email address to which the alerts are sent. This is a comma separated list of email addresses.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Femail-new-channel.png&amp;w=3840&amp;q=75" alt="new-notification-channel" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#configuring-alertmanager
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-configuring-alertmanager
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Configuring Alertmanager</h2>
<p>The following environment variables need to be set for alertmanager to send emails:</p>
<ul>
<li><strong>ALERTMANAGER_SMTP_FROM</strong>: The email address from which the alerts are sent.</li>
<li><strong>ALERTMANAGER_SMTP_HOST</strong>: The SMTP host obtained from your email provider.</li>
<li><strong>ALERTMANAGER_SMTP_PORT</strong>: The SMTP port obtained from your email provider.</li>
<li><strong>ALERTMANAGER_SMTP_AUTH_USERNAME</strong>: The SMTP user obtained from your email provider.</li>
<li><strong>ALERTMANAGER_SMTP_AUTH_PASSWORD</strong>: The SMTP password obtained from your email provider.</li>
</ul>
<p>‚úÖ Info</p>
<p>This section is only required for <strong>Self-Hosted</strong> users. Cloud users don't need to follow this step.</p>
<h3>## Docker</h3>
<p>Based on your Docker installation, you can include the following section in your Docker Compose YAML file to configure alertmanager.</p>
<ul>
<li>
<p>Docker Standalone: <code>deploy/docker/clickhouse-setup/docker-compose.yaml</code></p>
</li>
<li>
<p>Docker Swarm: <code>deploy/docker-swarm/clickhouse-setup/docker-compose.yaml</code></p>
<p>services:
alertmanager:
environment:
- ALERTMANAGER_SMTP_FROM=&lt;email address&gt;
- ALERTMANAGER_SMTP_HOST=&lt;smtp host&gt;
- ALERTMANAGER_SMTP_PORT=&lt;smtp port&gt;
- ALERTMANAGER_SMTP_AUTH_USERNAME=&lt;smtp user&gt;
- ALERTMANAGER_SMTP_AUTH_PASSWORD=&lt;smtp password&gt;</p>
</li>
</ul>
<h3>## Kubernetes</h3>
<p>You can include the following section in your Helm override values YAML file.</p>
<pre><code>alertmanager:
  additionalEnvs:
    ALERTMANAGER_SMTP_FROM: &lt;email address&gt;
    ALERTMANAGER_SMTP_HOST: &lt;smtp host&gt;
    ALERTMANAGER_SMTP_PORT: &lt;smtp port&gt;
    ALERTMANAGER_SMTP_AUTH_USERNAME: &lt;smtp user&gt;
    ALERTMANAGER_SMTP_AUTH_PASSWORD: &lt;smtp password&gt;
</code></pre>
<p><strong>Test Configuration</strong>: Click the Test button to send a test alert to the configured email addresses. This verifies that SigNoz can communicate with your email provider.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#editing-a-notification-channel
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-editing-a-notification-channel
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Editing a Notification channel</h2>
<p>To edit an existing email notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the email addresses and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#receiving-alerts-in-email
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-receiving-alerts-in-email
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Receiving Alerts in Email</h2>
<p>Once configured correctly, alerts from SigNoz will be sent to Email whenever monitored metrics cross the thresholds specified in your alert rules.This ensures you are promptly notified of any issues in your applications or infrastructure components.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts-in-email.png&amp;w=3840&amp;q=75" alt="alert-in-email" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/email/#troubleshooting
tag_set: alerts-management, notification-channel, email
image_urls: 
tracking_id: docs-alerts-management-notification-channel-email-troubleshooting
group_tracking_ids: docs-alerts-management-notification-channel-email
<h2>Configure Email Channel: Troubleshooting</h2>
<p>If you encounter issues with the Email integration:</p>
<ul>
<li><strong>Check the SMTP Server info</strong>: Ensure that the SMTP Host, Port, User and Password entered in SigNoz matches the one provided by your email provider.</li>
<li><strong>Verify Email Addresses</strong>: Confirm that the email addresses entered in SigNoz are valid and active.</li>
<li><strong>Test Connectivity</strong>: Use the Test button in SigNoz to check connectivity with your email provider. If the test fails, review your network settings and SMTP Server info.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#overview
tag_set: userguide, logs
image_urls: https://signoz.io/img/blog/common/signoz_logs.webp
tracking_id: docs-userguide-logs-overview
group_tracking_ids: docs-userguide-logs
<h2>Logs: Logs - Overview</h2>
<p><img src="https://signoz.io/img/blog/common/signoz_logs.webp" alt="Log Management in SigNoz" /></p>
<p><em>Logs management in SigNoz</em></p>
<p>Of all telemetry signals, logs have probably the biggest legacy. Most programming languages have built-in logging capabilities or well-known, widely used logging libraries. SigNoz natively supports OpenTelemetry for collecting logs. OpenTelemetry is a open source standard for generating and collecting telemetry data.</p>
<p>OpenTelemetry logs support is added with the philosophy that it should support legacy logs and logging libraries as well as provide improvements and better integration with the rest of the observability world where possible.</p>
<p>At SigNoz we follow OpenTelemetry approach for logs. All the features that are present with OpenTelemetry for logs is supported by SigNoz. We have done optimizations on the collector side for adding different features for logs in SigNoz. <strong>But all the documentation for OpenTelemetry logs remains valid for SigNoz.</strong></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#collecting-logs-in-signoz-cloud
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-collecting-logs-in-signoz-cloud
group_tracking_ids: docs-userguide-logs
<h2>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in SigNoz Cloud</h2>
<p>Sending logs to the SigNoz cloud depends on what environment your application is running on. In most cases, you need to install OpenTelemetry Collector to collect and send logs to SigNoz.</p>
<p>If you‚Äôre using an <a href="https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_python/">OpenTelemetry SDK</a>
, you can send your logs directly to Signoz.</p>
<p>Let‚Äôs give you an overview of how OpenTelemetry Collector and OpenTelemetry SDK can collect and send logs to the SigNoz cloud.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#using-opentelemetry-collector-to-send-logs
tag_set: userguide, logs
image_urls: https://signoz.io/img/docs/saas-docs/logs-via-stdout-file.webp, https://signoz.io/img/docs/saas-docs/via-logging-agent.webp
tracking_id: docs-userguide-logs-using-opentelemetry-collector-to-send-logs
group_tracking_ids: docs-userguide-logs
<p>Logs: Collecting Logs in SigNoz Cloud: Using OpenTelemetry Collector to send logs</p>
<p>OpenTelemetry collector is a standalone service provided by OpenTelemetry to receive, process, and export telemetry data. You can use it for applications deployed on Kubernetes and VMs. You can also use it if you are using any log shipper like FluentD or Logstash.</p>
<p>Here are different setups with OpenTelemetry Collector that you might set up with your application:</p>
<ul>
<li><strong>Via File or Stdout Logs</strong><br />
Here, the logs of the application are directly collected by the OpenTelemetry receiver using collectors like <strong><a href="/docs/userguide/logs/#log-receivers">filelog receiver</a></strong> and operators and processors to parse them into the OTel model.</li>
</ul>
<p><img src="https://signoz.io/img/docs/saas-docs/logs-via-stdout-file.webp" alt="Logs collection via stdout, etc." /></p>
<ul>
<li><strong>Via a logging agent like FluentD, FluentBit, Logstash</strong><br />
If advanced parsing and collecting capabilities are needed which is not present in OpenTelemetry or something like FluentBit/LogStash etc. is already present, then the agents can push the logs to OpenTelemetry collector using protocols like¬†<strong>FluentForward/TCP/UDP,</strong>¬†etc.</li>
</ul>
<p><img src="https://signoz.io/img/docs/saas-docs/via-logging-agent.webp" alt="via logging agent" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#using-opentelemetry-sdk
tag_set: userguide, logs
image_urls: https://signoz.io/img/docs/saas-docs/otel-sdk-logs.webp
tracking_id: docs-userguide-logs-using-opentelemetry-sdk
group_tracking_ids: docs-userguide-logs
<p>Logs: Collecting Logs in SigNoz Cloud: Using OpenTelemetry SDK</p>
<p>In this approach, you can modify your logging library that is used by the application to use the logging SDK provided by OpenTelemetry and directly forward the logs from the application to OpenTelemetry. This approach removes any need for agents/intermediary medium but loses the simplicity of having the log file locally.</p>
<p>Currently, OpenTelemetry logging SDK is available for <a href="https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_python/">Python</a> and <a href="https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/">Java</a>.</p>
<p><img src="https://signoz.io/img/docs/saas-docs/otel-sdk-logs.webp" alt="Otel logs sdk" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#sending-logs-to-signoz-cloud-based-on-your-environment
tag_set: userguide, logs
image_urls: https://signoz.io/img/docs/saas-docs/multiple-vms-logs.webp
tracking_id: docs-userguide-logs-sending-logs-to-signoz-cloud-based-on-your-environment
group_tracking_ids: docs-userguide-logs
<p>Logs: Collecting Logs in SigNoz Cloud: Sending logs to SigNoz Cloud based on your environment</p>
<p>Based on your application environment (Kubernetes, VMs, etc.), you need to install and configure OTel Collectors accordingly to collect and send logs.</p>
<p>Please use this exporter for sending logs to SigNoz cloud.</p>
<pre><code>exporters:
 otlp:
   endpoint: &quot;ingest.{region}.signoz.cloud:443&quot;
   tls:
     insecure: false
   headers:
     &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;

...

pipeline:

....

	logs:
	     receivers: [otlp]
	     processors: [batch]
	     exporters: [otlp]
</code></pre>
<ul>
<li><code>SIGNOZ_INGESTION_KEY</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>VMKubernetes</p>
<p>For applications deployed on VMs, you can install <code>otel-binary</code> to collect and send logs to SigNoz. You can find the instructions <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a>.</p>
<p>The <code>otel-binary</code> collects logs from your application and parses them into the OTel model before sending it to the SigNoz cloud.</p>
<p>You can then configure the otlp endpoint for SigNoz cloud to forward logs from your VMs to SigNoz cloud.</p>
<p><img src="https://signoz.io/img/docs/saas-docs/multiple-vms-logs.webp" alt="Logs collection from application deployed on VMs" /></p>
<p><em>For applications on VMs, you need to install otel-binary to collect logs and send them to SigNoz Cloud</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#collecting-logs-in-self-hosted-signoz-using-opentelemetry
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-collecting-logs-in-self-hosted-signoz-using-opentelemetry
group_tracking_ids: docs-userguide-logs
<h2>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry</h2>
<p>SigNoz natively supports OpenTelemetry for collecting logs. OpenTelemetry provides various receivers and processors for collecting first-party and third-party logs directly via OpenTelemetry Collector or via existing agents such as FluentBit so that minimal changes are required to move to OpenTelemetry for logs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#collecting-legacy-first-party-application-logs
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-collecting-legacy-first-party-application-logs
group_tracking_ids: docs-userguide-logs
<p>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Collecting legacy first-party Application Logs</p>
<p>These applications are built in-house and use existing logging libraries. The logs from these applications can be pushed to OpenTelemetry with little to no changes in application code. If you don‚Äôt have request context like <strong>traceId</strong> and <strong>spanId</strong> in your logs, you might want to add them for easier correlation with application metrics and traces.</p>
<p>There are <strong>two</strong> ways to collect logs from these applications.</p>
<ul>
<li>
<p><strong>Via File or Stdout Logs</strong><br />
Here, the logs of the application are directly collected by the OpenTelemetry receiver using collectors like <strong><a href="/docs/userguide/logs/#log-receivers">filelog receiver</a></strong> and operators and processors to parse them into the OTel model.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Ffile_stdout.webp&amp;w=3840&amp;q=75" alt="Untitled" /></p>
<p>If advanced parsing and collecting capabilities are needed which is not present in OpenTelemetry or something like FluentBit/LogStash etc is already present then the agents can push the logs to OpenTelemetry collector using protocols like <strong>FluentForward/TCP/UDP</strong> etc.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fusing_agent.webp&amp;w=3840&amp;q=75" alt="Untitled" /></p>
</li>
<li>
<p><strong>Direct to collector</strong><br />
In this approach you can modify your logging library that is used by the application to use the logging SDK provided by OpenTelemetry and directly forward the logs from the application to OpenTelemetry. This approach removes any need for agents/intermediary medium but loses the simplicity of having the log file locally.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fdirect_to_collector.webp&amp;w=3840&amp;q=75" alt="Untitled" /></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#collecting-third-party-application-logs
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-collecting-third-party-application-logs
group_tracking_ids: docs-userguide-logs
<p>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Collecting third-party application logs</p>
<p>Logs emitted by third party applications running on the system are known as third party application logs. The logs are typically written to stdout, files or other specialized medium (e.g. Windows Event Logs for applications).</p>
<p>These logs can be collected using OpenTelemetry file receiver and then processed or can be collected by a logging agent like FluentD/FluentBit etc and then forward to OTEL collector. The examples of which is discussed in <strong>Collecting legacy first-party application logs</strong>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#collecting-system-logs
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-collecting-system-logs
group_tracking_ids: docs-userguide-logs
<p>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Collecting system logs</p>
<p>These are logs generated by the operating system and over which we have no control. We cannot change the format or affect what information is included. Examples of system format are Syslog and Windows Event Logs.</p>
<p>System logs are written at the host level (which may be physical, virtual or containerized) and have a predefined format and content (note that applications may also be able to write records to standard system logs: this case is covered below in the¬†<a href="https://opentelemetry.io/docs/specs/otel/logs/#third-party-application-logs">Third-Party Applications</a> section.</p>
<p>System operations recorded in the logs can be a result of a request execution. However system logs either do not include any data about the request context or if included it is highly idiosyncratic and thus difficult to identify, parse and use. This makes it nearly impossible to perform request context correlation for system logs. However we can and should automatically enrich system logs with the resource context - the information about the host that is available during collection. This can include the host name, IP address, container or pod name, etc. This information should be added to the Resource field of collected log data.</p>
<p>OpenTelemetry Collector can read system logs and automatically enrich them with Resource information using the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/master/processor/resourcedetectionprocessor">resourcedetection</a> processor.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#collecting-infrastructure-logs
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-collecting-infrastructure-logs
group_tracking_ids: docs-userguide-logs
<p>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Collecting Infrastructure Logs</p>
<p>Like system logs, the infrastructure logs produced by infrastructure components such as Docker and Kubernetes events lack a request context. It can be enriched by the resource context - information about the node, pod, container, etc.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#collecting-new-first-party-application-logs
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-collecting-new-first-party-application-logs
group_tracking_ids: docs-userguide-logs
<p>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Collecting new first-party Application Logs</p>
<p>It is currently in greenfield development, but OpenTelemetry aims to provide extensions for popular logging libraries that will enrich the logs with relevant context. The extensions will also support sending the logs using OTLP protocol to OpenTelemetry Collector.</p>
<p>Currently OpenTelemetry does‚Äôt define a new logging API or create new user-facing logging libraries. The initial goal is to enhance existing popular logging libraries as needed. This is how a typical new application uses OpenTelemetry API, SDK and the existing log libraries</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Ffirst_party_collection.webp&amp;w=3840&amp;q=75" alt="first_party" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#storing-logs-in-signoz
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-storing-logs-in-signoz
group_tracking_ids: docs-userguide-logs
<h2>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Storing logs in SigNoz</h2>
<p>SigNoz has developed its own distro of OpenTelemetry collector which has a custom ClickHouse exporter. This custom version just extends the upstream collector. Everything that works with the upstream collector will work with SigNoz OTEL collector.</p>
<p>This brings a requirement that the final collector should be SigNoz OTEL collector i.e</p>
<ul>
<li>
<p>If you have N number of OpenTelemetry collectors running in different places then they should process and send data to SigNoz OTEL collector using OTLP for it to be able to store in ClickHouse.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fn_collectors.webp&amp;w=3840&amp;q=75" alt="notelcollectors.webp" /></p>
</li>
<li>
<p>The other way is to use SigNoz OTEL collector everywhere which can directly write to ClickHouse.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#log-receivers
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-log-receivers
group_tracking_ids: docs-userguide-logs
<h2>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Log Receivers</h2>
<p>A log receiver is how logs data gets into the OpenTelemetry Collector. Different types of receivers supported by OpenTelemetry for logs:</p>
<ul>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/receiver/otlpreceiver/README.md">OTLP Receiver</a>
-</strong> This receiver receives logs over the OTLP protocol in a specified port. Any library which uses OTEL SDK can forward logs to this protocol. This protocol is also used when OTEL collector needs to forward logs to another OTEL collector.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">Filelog Receiver</a>
-</strong> This receiver can tail and parse files containing logs.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/fluentforwardreceiver">Fluent Forward Receiver</a>
-</strong> This receiver runs a TCP server that accepts events via Fluent Forward Protocol. FluentD and FluentBit can forward logs to this receiver.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/tcplogreceiver">TCP Receiver</a>
-</strong> This receiver runs a TCP server which can receive logs.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/udplogreceiver">UDP Receiver</a>
-</strong> This receiver runs a UDP server which can receive logs.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/syslogreceiver">Syslog Receiver</a>
-</strong> This receiver parses syslog received over TCP and UDP</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#operators-for-parsing-and-manipulating-logs
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-operators-for-parsing-and-manipulating-logs
group_tracking_ids: docs-userguide-logs
<h2>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Operators for parsing and manipulating logs</h2>
<p>An operator is the most basic unit of log processing. Each operator fulfills a single responsibility, such as adding an attribute to a log field or parsing JSON from a field. Operators are then chained together in a pipeline to achieve a desired result.</p>
<p>For example, a user may parse log lines using <code>regex_parser</code> and then use <code>trace_parser</code> to parse the <code>traceId</code> and <code>spanId</code> from the logs.</p>
<p>The receivers FluentForward and OTLP doesn‚Äôt have operators. But for parsing them we can use logprocessor.</p>
<ul>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md">csv_parser</a></strong> :- The¬†<code>csv_parser</code>¬†operator parses the string-type field selected by¬†<code>parse_from</code>¬†with the given header values.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md">json_parser</a>
:-</strong> The¬†<code>json_parser</code>¬†operator parses the string-type field selected by¬†<code>parse_from</code>¬†as JSON.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_parser.md">regex_parser</a>
:-</strong> The¬†<code>regex_parser</code>¬†operator parses the string-type field selected by¬†<code>parse_from</code>¬†with the given regular expression pattern. This operator makes use of¬†<a href="https://github.com/google/re2/wiki/Syntax">Go regular expression</a>
. When writing a regex, consider using a tool such as <a href="https://regex101.com/?flavor=golang">regex101</a></p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/syslog_parser.md">syslog_parser</a>
:-</strong> The¬†<code>syslog_parser</code>¬†operator parses the string-type field selected by¬†<code>parse_from</code>¬†as syslog. Timestamp parsing is handled automatically by this operator.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/severity_parser.md">severity_parse</a></strong> :- The¬†<code>severity_parser</code>¬†operator sets the severity on an entry by parsing a value from the body.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/time_parser.md">time_parser</a></strong> :- The¬†<code>time_parser</code>¬†operator sets the timestamp on an entry by parsing a value from the body.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/trace_parser.md">trace_parser</a></strong> :- The¬†<code>trace_parser</code>¬†operator sets the trace on an entry by parsing a value from the body.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/uri_parser.md">uri_parser</a></strong> :- The¬†<code>uri_parser</code> ¬†operator parses the string-type field selected by¬†<code>parse_from</code> ¬†as¬†<a href="https://tools.ietf.org/html/rfc3986">URI</a></p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md">add</a></strong> :- The¬†<code>add</code>¬†operator adds a value to an¬†<code>entry</code>'s¬†<code>body</code>,¬†<code>attributes</code>, or¬†<code>resource</code>.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md">copy</a></strong> :- The¬†<code>copy</code>¬†operator copies a value from one¬†<a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/field.md">field</a> to another.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/filter.md">filter</a></strong> :- The¬†<code>filter</code>¬†operator filters incoming entries that match an expression.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/flatten.md">flatten</a></strong> :- The¬†<code>flatten</code>¬†operator flattens a field by moving its children up to the same level as the field. The operator only flattens a single level deep.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/move.md">move</a></strong> :- The¬†<code>move</code>¬†operator moves (or renames) a field from one location to another.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md">recombine</a></strong> :- The¬†<code>recombine</code>¬†operator combines consecutive logs into single logs based on simple expression rules.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md">remove</a></strong> :- The¬†<code>remove</code>¬†operator removes a field from a record.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/retain.md">retain</a></strong> :- The¬†<code>retain</code>¬†operator keeps the specified list of fields, and removes the rest.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/router.md">router</a></strong> :- The¬†<code>router</code>¬†operator allows logs to be routed dynamically based on their content. The operator is configured with a list of routes, where each route has an associated expression. An entry sent to the router operator is forwarded to the first route in the list whose associated expression returns¬†<code>true</code>. An entry that does not match any of the routes is dropped and not processed further</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md">key_value_parser</a></strong> :- The <code>key_value_parser</code> operator parses the string-type field selected by parse_from into key value pairs. All values are of type string.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs/#processors-available-for-processing-logs
tag_set: userguide, logs
image_urls: 
tracking_id: docs-userguide-logs-processors-available-for-processing-logs
group_tracking_ids: docs-userguide-logs
<h2>Logs: Collecting Logs in SigNoz Cloud: Collecting Logs in Self-Hosted SigNoz using OpenTelemetry: Processors available for processing logs</h2>
<p>Processors are used at various stages of a pipeline. Generally, a processor pre-processes data before it is exported (e.g. modify attributes or sample) or helps ensure that data makes it through a pipeline successfully (e.g. batch/retry).</p>
<p>Process are also helpful when you have multiple recivers for logs and you want parse/transforms logs collected from all the recievers.</p>
<p><strong>We highly recommend users to use Batch and Memory Limiter Processor with logs</strong></p>
<ul>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md">Batch Processor</a></strong> :- The batch processor accepts <a href="https://signoz.io/blog/distributed-tracing-span/">spans</a>
, metrics, or logs and places them into batches. Batching helps better compress the data and reduce the number of outgoing connections required to transmit the data. This processor supports both size and time based batching.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md">Memory Limiter Processor</a></strong> :- The memory limiter processor is used to prevent out of memory situations on the collector. Given that the amount and type of data the collector processes is environment specific and resource utilization of the collector is also dependent on the configured processors, it is important to put checks in place regarding memory usage. The memory_limiter uses soft and hard memory limits. Hard limit is always above or equal the soft limit.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor">Attributes Processor</a></strong> :- This processor allows you to modify the attributes of a log.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor">Filter Processor</a></strong> :- The filter processor can be configured to include or exclude logs based on resource attributes using the strict or regexp match types</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/groupbyattrsprocessor">Group by Attributes processor</a></strong> :- This processor re-associates log records to a Resource that matches with the specified attributes. As a result, all log records with the same values for the specified attributes are &quot;grouped&quot; under the same Resource.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/logstransformprocessor">Logs Transform Processor</a></strong> :- The logs transform processor can be used to apply log operators to logs coming from any receiver.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor">Resource Detection Processor</a></strong> :- The resource detection processor can be used to detect resource information from the host, in a format that conforms to the OpenTelemetry resource semantic conventions, and append or override the resource value in telemetry data with this information.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor">Resource Processor</a></strong> :- The resource processor can be used to apply changes on resource attributes.</p>
</li>
<li>
<p><strong><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md">Transform Processor</a></strong> :- The transform processor modifies telemetry based on configuration using the Telemetry Query Language. The processor takes a list of queries for each signal type and executes the queries against the incoming telemetry in the order specified in the config. Each query can access and transform telemetry using functions and allow the use of a condition to help decide whether the function should be executed.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/bar/
tag_set: dashboards, panel-types, bar
image_urls: https://signoz.io/img/docs/dashboards/panel-types/bar-chart.png
tracking_id: docs-dashboards-panel-types-bar
group_tracking_ids: docs-dashboards-panel-types-bar
<h2>Bar Chart Panel Type</h2>
<h2>Bar Chart</h2>
<p>========================</p>
<p>A Bar chart is a plot chart that shows frequency of a single or a few different categories over time. This helps in directly comparing the values of different categories, which correspond to the length of the bar. This is particularly useful when you have discrete categorical data and you want to highlight differences.</p>
<h2>Data Formats</h2>
<hr />
<h3>## Supported signals</h3>
<ul>
<li>Logs</li>
<li>Traces</li>
<li>Metrics</li>
</ul>
<p>This panel type supports any time series data. The time series data can be from logs, traces, or metrics.</p>
<h3>## Examples</h3>
<p>The following graph shows the requests per second (req/s) for a service over a period of time in bar chart.</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/bar-chart.png" alt="Bar chart for req/s over period of time" /></p>
<p><em>Bar chart for req/s over period of time.</em></p>
<h2>Configuration</h2>
<hr />
<h3>## Y-axis Unit</h3>
<p>The unit of the y-axis. The default unit is <code>None</code>.</p>
<h3>## Soft Min Max</h3>
<p>The soft min max is used to adjust the y-axis scale for better visualization. By default, the soft min max is disabled and the y-axis range will be auto-adjusted based on the data. If the soft min max is enabled, the y-axis range will be adjusted to use the soft min max values. It is particularly useful when you want to prevent small values in the result from being magnified too much.</p>
<h3>## Thresholds</h3>
<p>Thresholds are used to draw a line on the y-axis to highlight the value. The thresholds are defined as a list of tuples. Each tuple contains the value and the color. The color is optional and if not provided, the default color will be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/list/
tag_set: dashboards, panel-types, list
image_urls: https://signoz.io/img/docs/dashboards/panel-types/list.png
tracking_id: docs-dashboards-panel-types-list
group_tracking_ids: docs-dashboards-panel-types-list
<h2>List Chart Panel Type</h2>
<h2>List Chart</h2>
<p>==========================</p>
<p>A List chart is a plot chart that shows a list of values. This is particularly useful when you want to show a list of values in a single panel. A typical use case is to show the error logs or the traces in a single panel. The panel supports infinite scrolling and search.</p>
<h2>Data Formats</h2>
<hr />
<h3>## Supported signals</h3>
<ul>
<li>Logs</li>
<li>Traces</li>
</ul>
<p>This panel type supports showing the logs lines and spans in a list.</p>
<h3>## Examples</h3>
<p>The following graph shows the spans over a period of time in a list.</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/list.png" alt="List chart for spans" /></p>
<p><em>List chart for spans.</em></p>
<h2>Configuration</h2>
<hr />
<p>This panel type doesn't support any configuration.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/service-map/
tag_set: userguide, service-map
image_urls: 
tracking_id: docs-userguide-service-map
group_tracking_ids: docs-userguide-service-map
<h2>Service Map</h2>
<p>Select Service Map tab in the left navigation panel to reach to the service map.</p>
<ul>
<li>
<p>This map gives an idea of the services in your infrastructure, and how one services calls other.</p>
</li>
<li>
<p>The P99 latency, error rate and RPS is shown by hovering above the edge in between services.</p>
</li>
<li>
<p>If an edge service map is shown as red, it means that it is having some 4xx error or high latency.</p>
</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fservice-map.webp&amp;w=3840&amp;q=75" alt="service-map" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Flask applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Flask application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#requirements
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-requirements
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Requirements</h2>
<ul>
<li>Python 3.8 or newer</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#send-traces-to-signoz-cloud
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetes</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-flask
<p>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud</p>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
<p><strong>Step 4.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
opentelemetry-instrument &lt;your_run_command&gt;
</code></pre>
<ul>
<li><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</li>
<li><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></li>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the api token provided by SigNoz. You can find it in the email sent by SigNoz with your cloud account details.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, if you use <code>export Flask_ENV=development</code>, it enables the reloader mode which breaks OpenTelemetry instrumentation.</p>
<p><strong>Step 5.</strong> Validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#send-traces-via-otel-collector-binary
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-flask
<p>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Python application.</p>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
<p><strong>Step 4.</strong> To run your application and send data to collector in same VM:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, if you use <code>export Flask_ENV=development</code>, it enables the reloader mode which breaks OpenTelemetry instrumentation.</p>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></p>
<p><em><code>http://localhost:4317</code></em> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively.</p>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4317 with <code>&lt;IP Address of the VM&gt;:4317</code>.</p>
<p><strong>Step 5.</strong> You can validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, flask
image_urls: https://signoz.io/img/docs/otel_python_instrumentation.webp
tracking_id: docs-instrumentation-flask-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Send Traces to Self-Hosted SigNoz</h2>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Flask application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/otel_python_instrumentation.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Python applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in Flask.</p>
<p>You can use OpenTelemetry to send your traces directly to SigNoz. OpenTelemetry provides a handy distro in Python that can help you get started with automatic instrumentation. We recommend using it to get started quickly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#steps-to-auto-instrument-flask-app-for-traces
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-steps-to-auto-instrument-flask-app-for-traces
group_tracking_ids: docs-instrumentation-flask
<p>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Flask app for traces</p>
<ol>
<li>
<p><strong>Create a virtual environment</strong></p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
</li>
<li>
<p><strong>Install the OpenTelemetry dependencies</strong></p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
</li>
<li>
<p><strong>Add automatic instrumentation</strong><br />
The below command inspects the dependencies of your application and installs the instrumentation packages relevant for your Flask application.</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
</li>
<li>
<p><strong>Run your application</strong><br />
In the final run command, you can configure environment variables and flags. Flags for exporters:</p>
<p>For running your application, there are a few things that you need to keep in mind. Below are the notes:</p>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation. For example, if you use <code>export Flask_ENV=development</code>, it enables the reloader mode which breaks OpenTelemetry instrumentation.</p>
<p>For running applications with application servers which are based on <a href="#running-applications-with-gunicorn-uwsgi">pre fork model</a>
, like Gunicorn, uWSGI you have to add a post_fork hook or a @postfork decorator in your configuration.</p>
<p>To start sending data to SigNoz, use the following run command:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></p>
<p><code>IP of SigNoz backend</code> is the IP of the machine where you installed SigNoz. If you have installed SigNoz on <code>localhost</code>, the endpoint will be <code>http://localhost:4317</code> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively. Remember to allow incoming requests to port¬†<strong>4317</strong>/<strong>4318</strong>¬†of machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, flask
image_urls: https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp
tracking_id: docs-instrumentation-flask-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp" alt="Python Application in the list of services being monitored in SigNoz" /></p>
<p><em>Python Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#database-instrumentation
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-database-instrumentation
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Database Instrumentation</h2>
<p>Make sure that the DB client library you are using has the corresponding instrumentation library, and the version of the DB client library is supported by OpenTelemetry.</p>
<h3>## MongoDB Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MongoDB database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MongoDB, the instrumentation library is <code>opentelemetry-instrumentation-pymongo</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Redis Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your Redis database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Redis, the instrumentation library is <code>opentelemetry-instrumentation-redis</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## MySQL Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MySQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MySQL, we have two isntrumentation libraries:</p>
<ul>
<li>opentelemetry-instrumentation-mysql</li>
<li>opentelemetry-instrumentation-pymysql</li>
</ul>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Postgres Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your PostgreSQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Postgres, the instrumentation library is <code>opentelemetry-instrumentation-psycopg2</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<p>üìù Note</p>
<p><code>psycopg2-binary</code> is not supported by opentelemetry auto instrumentation libraries as it is not recommended for production use. Please use <code>psycopg2</code> to see DB calls also in your trace data in SigNoz</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#running-applications-with-gunicorn-uwsgi
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-running-applications-with-gunicorn-uwsgi
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Running applications with Gunicorn, uWSGI</h2>
<p>For application servers which are based on pre fork model like Gunicorn, uWSGI you have to add a <code>post_fork</code> hook or a <code>@postfork</code> decorator in your configuration.</p>
<p>Check this <a href="https://opentelemetry-python.readthedocs.io/en/latest/examples/fork-process-model/README.html">documentation</a> from OpenTelemetry on how to set it up.</p>
<p><a href="https://github.com/SigNoz/opentelemetry-python/tree/main/docs/examples/fork-process-model">Here's</a> a working example where we have configured a gunicorn server with <code>post_fork</code> hook.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#troubleshooting-your-installation
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Troubleshooting your installation</h2>
<h4>## Spans are not being reported</h4>
<p>If spans are not being reported to SigNoz, try enabling debug exporter which writes the JSON formatted trace data to the console by setting env var OTEL_TRACES_EXPORTER=console.</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=python_app OTEL_TRACES_EXPORTER=console opentelemetry-instrument &lt;your run command&gt;


{
    &quot;name&quot;: &quot;alice&quot;,
    &quot;context&quot;: {
        &quot;trace_id&quot;: &quot;0xedb7caf0c8b082a9578460a201759193&quot;,
        &quot;span_id&quot;: &quot;0x57cf7eee198e1fed&quot;,
        &quot;trace_state&quot;: &quot;[]&quot;
    },
    &quot;kind&quot;: &quot;SpanKind.INTERNAL&quot;,
    &quot;parent_id&quot;: null,
    &quot;start_time&quot;: &quot;2022-03-27T14:55:18.804758Z&quot;,
    &quot;end_time&quot;: &quot;2022-03-27T14:55:18.804805Z&quot;,
    &quot;status&quot;: {
        &quot;status_code&quot;: &quot;UNSET&quot;
    },
    &quot;attributes&quot;: {},
    &quot;events&quot;: [],
    &quot;links&quot;: [],
    &quot;resource&quot;: {
        &quot;telemetry.sdk.language&quot;: &quot;python&quot;,
        &quot;telemetry.sdk.name&quot;: &quot;opentelemetry&quot;,
        &quot;telemetry.sdk.version&quot;: &quot;1.10.0&quot;,
        &quot;service.name&quot;: &quot;my-service&quot;
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#sample-flask-application
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-sample-flask-application
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Sample Flask Application</h2>
<ul>
<li>
<p><a href="https://github.com/SigNoz/opentelemetry-python/tree/main/docs/examples/fork-process-model">Working example</a> where we have configured a gunicorn server with <code>post_fork</code> hook.</p>
</li>
<li>
<p>We have included a sample Flask application with README.md at <a href="https://github.com/SigNoz/sample-flask-app">Sample Flask App Github Repo.</a></p>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/flask/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, flask
image_urls: 
tracking_id: docs-instrumentation-flask-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-flask
<h2>Flask OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation</h2>
<p>This document contains instructions on how to set up OpenTelemetry instrumentation in your Falcon applications. OpenTelemetry, also known as OTel for short, is an open source observability framework that can help you generate and collect telemetry data - traces, metrics, and logs from your Falcon application.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#requirements
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-requirements
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Requirements</h2>
<ul>
<li>Python 3.8 or newer</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#send-traces-to-signoz-cloud
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-send-traces-to-signoz-cloud
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to SigNoz Cloud</h2>
<p>Based on your application environment, you can choose the setup below to send traces to SigNoz Cloud.</p>
<p>VMKubernetes</p>
<p>From VMs, there are two ways to send data to SigNoz Cloud.</p>
<ul>
<li>
<p><a href="#send-traces-directly-to-signoz-cloud">Send traces directly to SigNoz Cloud</a></p>
</li>
<li>
<p><a href="#send-traces-via-otel-collector-binary">Send traces via OTel Collector binary</a>
(recommended)</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#send-traces-directly-to-signoz-cloud
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-send-traces-directly-to-signoz-cloud
group_tracking_ids: docs-instrumentation-falcon
<p>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces directly to SigNoz Cloud</p>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
<p><strong>Step 4.</strong> Run your application</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; \
OTEL_EXPORTER_OTLP_HEADERS=&quot;signoz-access-token=SIGNOZ_INGESTION_KEY&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
opentelemetry-instrument &lt;your_run_command&gt;
</code></pre>
<ul>
<li><em><code>&lt;service_name&gt;</code></em>¬†is the name of the service you want</li>
<li><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></li>
<li>Replace <code>SIGNOZ_INGESTION_KEY</code> with the api token provided by SigNoz. You can find it in the email sent by SigNoz with your cloud account details.</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the ingest endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation.</p>
<p><strong>Step 5.</strong> Validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#send-traces-via-otel-collector-binary
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-send-traces-via-otel-collector-binary
group_tracking_ids: docs-instrumentation-falcon
<p>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send traces via OTel Collector binary</p>
<p>OTel Collector binary helps to collect logs, hostmetrics, resource and infra attributes. It is recommended to install Otel Collector binary to collect and send traces to SigNoz cloud. You can correlate signals and have rich contextual data through this way.</p>
<p>You can find instructions to install OTel Collector binary <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a> in your VM. Once you are done setting up your OTel Collector binary, you can follow the below steps for instrumenting your Python application.</p>
<p><strong>Step 1.</strong> Create a virtual environment</p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
<p><strong>Step 2.</strong> Install the OpenTelemetry dependencies</p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
<p><strong>Step 3.</strong> Add automatic instrumentation</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
<p><strong>Step 4.</strong> To run your application and send data to collector in same VM:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; \
OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4317&quot; \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation.</p>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></p>
<p><em><code>http://localhost:4317</code></em> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively.</p>
<p>In case you have OtelCollector Agent in different VM, replace localhost:4317 with <code>&lt;IP Address of the VM&gt;:4317</code>.</p>
<p><strong>Step 5.</strong> You can validate if your application is sending traces to SigNoz cloud by following the instructions <a href="#validating-instrumentation-by-checking-for-traces">here</a>.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#send-traces-to-self-hosted-signoz
tag_set: instrumentation, falcon
image_urls: https://signoz.io/img/docs/otel_python_instrumentation.webp
tracking_id: docs-instrumentation-falcon-send-traces-to-self-hosted-signoz
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Send Traces to Self-Hosted SigNoz</h2>
<p>There are three major steps to using OpenTelemetry:</p>
<ul>
<li>Instrumenting your Falcon application with OpenTelemetry</li>
<li>Configuring exporter to send data to SigNoz</li>
<li>Validating that configuration to ensure that data is being sent as expected.</li>
</ul>
<p><img src="https://signoz.io/img/docs/otel_python_instrumentation.webp" alt="OpenTelemetry helps to generate and collect telemetry data from your application which is then sent to an observability backend like SigNoz" /></p>
<p><em>OpenTelemetry helps generate and collect telemetry data from Python applications which can then be sent to SigNoz for storage, visualization, and analysis.</em></p>
<p>Let‚Äôs understand how to download, install, and run OpenTelemetry in Falcon.</p>
<p>You can use OpenTelemetry to send your traces directly to SigNoz. OpenTelemetry provides a handy distro in Python that can help you get started with automatic instrumentation. We recommend using it to get started quickly.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#steps-to-auto-instrument-falcon-app-for-traces
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-steps-to-auto-instrument-falcon-app-for-traces
group_tracking_ids: docs-instrumentation-falcon
<p>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Steps to auto-instrument Falcon app for traces</p>
<ol>
<li>
<p><strong>Create a virtual environment</strong></p>
<pre><code>python3 -m venv .venv
source .venv/bin/activate
</code></pre>
</li>
<li>
<p><strong>Install the OpenTelemetry dependencies</strong></p>
<pre><code>pip install opentelemetry-distro==0.43b0
pip install opentelemetry-exporter-otlp==1.22.0
</code></pre>
<p>The dependencies included are briefly explained below:</p>
<p><code>opentelemetry-distro</code> - The distro provides a mechanism to automatically configure some of the more common options for users. It helps to get started with OpenTelemetry auto-instrumentation quickly.</p>
<p><code>opentelemetry-exporter-otlp</code> - This library provides a way to install all OTLP exporters. You will need an exporter to send the data to SigNoz.</p>
<p>üìù Note</p>
<p>üí° The¬†<code>opentelemetry-exporter-otlp</code>¬†is a convenience wrapper package to install all OTLP exporters. Currently, it installs:</p>
<ul>
<li>
<p>opentelemetry-exporter-otlp-proto-http</p>
</li>
<li>
<p>opentelemetry-exporter-otlp-proto-grpc</p>
</li>
<li>
<p>(soon) opentelemetry-exporter-otlp-json-http</p>
</li>
</ul>
<p>The <code>opentelemetry-exporter-otlp-proto-grpc</code> package installs the gRPC exporter which depends on the <code>grpcio</code> package. The installation of <code>grpcio</code> may fail on some platforms for various reasons. If you run into such issues, or you don't want to use gRPC, you can install the HTTP exporter instead by installing the <code>opentelemetry-exporter-otlp-proto-http</code> package. You need to set the <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> environment variable to <code>http/protobuf</code> to use the HTTP exporter.</p>
</li>
<li>
<p><strong>Add automatic instrumentation</strong><br />
The below command inspects the dependencies of your application and installs the instrumentation packages relevant for your Falcon application.</p>
<pre><code>opentelemetry-bootstrap --action=install
</code></pre>
<p>üìù Note</p>
<p>Please make sure that you have installed all the dependencies of your application before running the above command. The command will not install instrumentation for the dependencies which are not installed.</p>
</li>
<li>
<p><strong>Run your application</strong><br />
In the final run command, you can configure environment variables and flags. Flags for exporters:</p>
<p>For running your application, there are a few things that you need to keep in mind. Below are the notes:</p>
<p>üìù Note</p>
<p>Don‚Äôt run app in reloader/hot-reload mode as it breaks instrumentation.</p>
<p>For running applications with application servers which are based on <a href="#running-applications-with-gunicorn-uwsgi">pre fork model</a>
, like Gunicorn, uWSGI you have to add a post_fork hook or a @postfork decorator in your configuration.</p>
<p>To start sending data to SigNoz, use the following run command:</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt; OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_EXPORTER_OTLP_PROTOCOL=grpc opentelemetry-instrument &lt;your run command&gt;
</code></pre>
<p><em><code>&lt;service_name&gt;</code></em>¬†is the name of service you want</p>
<p><em><code>&lt;your_run_command&gt;</code></em>¬†can be¬†<code>python3 app.py</code>¬†or¬†<code>flask run</code></p>
<p><code>IP of SigNoz backend</code> is the IP of the machine where you installed SigNoz. If you have installed SigNoz on <code>localhost</code>, the endpoint will be <code>http://localhost:4317</code> for gRPC exporter and <code>http://localhost:4318</code> for HTTP exporter.</p>
<p>üìù Note</p>
<p>The port numbers are 4317 and 4318 for the gRPC and HTTP exporters respectively. Remember to allow incoming requests to port¬†<strong>4317</strong>/<strong>4318</strong>¬†of machine where SigNoz backend is hosted.</p>
<p>In case you encounter an issue where all applications do not get listed in the services section then please refer to the <a href="#troubleshooting-your-installation">troubleshooting section</a>
.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#validating-instrumentation-by-checking-for-traces
tag_set: instrumentation, falcon
image_urls: https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp
tracking_id: docs-instrumentation-falcon-validating-instrumentation-by-checking-for-traces
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Validating instrumentation by checking for traces</h2>
<p>With your application running, you can verify that you‚Äôve instrumented your application with OpenTelemetry correctly by confirming that tracing data is being reported to SigNoz.</p>
<p>To do this, you need to ensure that your application generates some data. Applications will not produce traces unless they are being interacted with, and OpenTelemetry will often buffer data before sending. So you need to interact with your application and wait for some time to see your tracing data in SigNoz.</p>
<p>Validate your traces in SigNoz:</p>
<ol>
<li>Trigger an action in your app that generates a web request. Hit the endpoint a number of times to generate some data. Then, wait for some time.</li>
<li>In SigNoz, open the¬†<code>Services</code>¬†tab. Hit the¬†<code>Refresh</code>¬†button on the top right corner, and your application should appear in the list of¬†<code>Applications</code>.</li>
<li>Go to the¬†<code>Traces</code>¬†tab, and apply relevant filters to see your application‚Äôs traces.</li>
</ol>
<p>You might see other dummy applications if you‚Äôre using SigNoz for the first time. You can remove it by following the docs¬†<a href="https://signoz.io/docs/operate/docker-standalone/#remove-the-sample-application">here</a>.</p>
<p><img src="https://signoz.io/img/docs/opentelemetry_python_app_instrumented.webp" alt="Python Application in the list of services being monitored in SigNoz" /></p>
<p><em>Python Application in the list of services being monitored in SigNoz</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#database-instrumentation
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-database-instrumentation
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Database Instrumentation</h2>
<p>Make sure that the DB client library you are using has the corresponding instrumentation library, and the version of the DB client library is supported by OpenTelemetry.</p>
<h3>## MongoDB Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MongoDB database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MongoDB, the instrumentation library is <code>opentelemetry-instrumentation-pymongo</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Redis Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your Redis database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Redis, the instrumentation library is <code>opentelemetry-instrumentation-redis</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## MySQL Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your MySQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For MySQL, we have two isntrumentation libraries:</p>
<ul>
<li>opentelemetry-instrumentation-mysql</li>
<li>opentelemetry-instrumentation-pymysql</li>
</ul>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<h3>## Postgres Database Instrumentation</h3>
<p>You can use <code>opentelemetry-distro</code> to initialize instrumentation for your PostgreSQL database calls. You need to ensure that the version of your DB client library is supported by OpenTelemetry. For Postgres, the instrumentation library is <code>opentelemetry-instrumentation-psycopg2</code>.</p>
<p>You can check the supported versions <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation">here</a>.</p>
<p>üìù Note</p>
<p><code>psycopg2-binary</code> is not supported by opentelemetry auto instrumentation libraries as it is not recommended for production use. Please use <code>psycopg2</code> to see DB calls also in your trace data in SigNoz</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#running-applications-with-gunicorn-uwsgi
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-running-applications-with-gunicorn-uwsgi
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Running applications with Gunicorn, uWSGI</h2>
<p>For application servers which are based on pre fork model like Gunicorn, uWSGI you have to add a <code>post_fork</code> hook or a <code>@postfork</code> decorator in your configuration.</p>
<p>Check this <a href="https://opentelemetry-python.readthedocs.io/en/latest/examples/fork-process-model/README.html">documentation</a> from OpenTelemetry on how to set it up.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-python/tree/main/docs/examples/fork-process-model">Here's</a> a working example a gunicorn server configured with <code>post_fork</code> hook.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#troubleshooting-your-installation
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-troubleshooting-your-installation
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Troubleshooting your installation</h2>
<h4>## Spans are not being reported</h4>
<p>If spans are not being reported to SigNoz, try enabling debug exporter which writes the JSON formatted trace data to the console by setting env var OTEL_TRACES_EXPORTER=console.</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=python_app OTEL_TRACES_EXPORTER=console opentelemetry-instrument &lt;your run command&gt;


{
    &quot;name&quot;: &quot;alice&quot;,
    &quot;context&quot;: {
        &quot;trace_id&quot;: &quot;0xedb7caf0c8b082a9578460a201759193&quot;,
        &quot;span_id&quot;: &quot;0x57cf7eee198e1fed&quot;,
        &quot;trace_state&quot;: &quot;[]&quot;
    },
    &quot;kind&quot;: &quot;SpanKind.INTERNAL&quot;,
    &quot;parent_id&quot;: null,
    &quot;start_time&quot;: &quot;2022-03-27T14:55:18.804758Z&quot;,
    &quot;end_time&quot;: &quot;2022-03-27T14:55:18.804805Z&quot;,
    &quot;status&quot;: {
        &quot;status_code&quot;: &quot;UNSET&quot;
    },
    &quot;attributes&quot;: {},
    &quot;events&quot;: [],
    &quot;links&quot;: [],
    &quot;resource&quot;: {
        &quot;telemetry.sdk.language&quot;: &quot;python&quot;,
        &quot;telemetry.sdk.name&quot;: &quot;opentelemetry&quot;,
        &quot;telemetry.sdk.version&quot;: &quot;1.10.0&quot;,
        &quot;service.name&quot;: &quot;my-service&quot;
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#sample-falcon-application
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-sample-falcon-application
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: Sample Falcon Application</h2>
<ul>
<li>
<p><a href="https://github.com/SigNoz/opentelemetry-python/tree/main/docs/examples/fork-process-model">Working example</a> where we have configured a gunicorn server with <code>post_fork</code> hook.</p>
</li>
<li>
<p>We have included a sample Falcon application with README.md at <a href="https://github.com/SigNoz/python-falcon-template">Sample Falcon App Github Repo.</a></p>
</li>
</ul>
<h3>## Frequently Asked Questions</h3>
<ol>
<li>
<p>How to find what to use in <code>IP of SigNoz</code> if I have installed SigNoz in Kubernetes cluster?</p>
<p>Based on where you have installed your application and where you have installed SigNoz, you need to find the right value for this. Please use <a href="/docs/instrumentation/troubleshoot-instrumentation/">this grid</a> to find the value you should use for <code>IP of SigNoz</code></p>
</li>
<li>
<p>I am sending data from my application to SigNoz, but I don't see any events or graphs in the SigNoz dashboard. What should I do?</p>
<p>This could be because of one of the following reasons:</p>
<ol>
<li>
<p><em>Your application is generating telemetry data, but not able to connect with SigNoz installation</em></p>
<p>Please use this <a href="/docs/install/troubleshooting/">troubleshooting guide</a> to find if your application is able to access SigNoz installation and send data to it.</p>
</li>
<li>
<p><em>Your application is not actually generating telemetry data</em></p>
<p>Please check if the application is generating telemetry data first. You can use <code>Console Exporter</code> to just print your telemetry data in console first. Join our <a href="https://signoz.io/slack/">Slack Community</a> if you need help on how to export your telemetry data in console</p>
</li>
<li>
<p><em>Your SigNoz installation is not running or behind a firewall</em></p>
<p>Please double check if the pods in SigNoz installation are running fine. <code>docker ps</code> or <code>kubectl get pods -n platform</code> are your friends for this.</p>
</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/falcon/#what-cloud-endpoint-should-i-use
tag_set: instrumentation, falcon
image_urls: 
tracking_id: docs-instrumentation-falcon-what-cloud-endpoint-should-i-use
group_tracking_ids: docs-instrumentation-falcon
<h2>Falcon OpenTelemetry Instrumentation: Send Traces to SigNoz Cloud: Send Traces to Self-Hosted SigNoz: What Cloud Endpoint Should I Use?</h2>
<p>The primary method for sending data to SigNoz Cloud is through OTLP exporters. You can either send the data directly from your application using the exporters available in SDKs/language agents or send the data to a collector agent, which batches/enriches telemetry and sends it to the Cloud.</p>
<h3>## My Collector Sends Data to SigNoz Cloud</h3>
<h4>## Using gRPC Exporter</h4>
<p>The endpoint should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the exporter endpoint doesn't require a scheme for the gRPC exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlp:
        endpoint: &quot;ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h4>## Using HTTP Exporter</h4>
<p>The endpoint should be <code>https://ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>. Note that the endpoint includes the scheme <code>https</code> for the HTTP exporter in the collector.</p>
<pre><code># Sample config with `us` region
exporters:
    otlphttp:
        endpoint: &quot;https://ingest.us.signoz.cloud:443&quot;
        tls:
            insecure: false
        headers:
            &quot;signoz-access-token&quot;: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<h3>## My Application Sends Data to SigNoz Cloud</h3>
<p>The endpoint should be configured either with environment variables or in the SDK setup code.</p>
<h4>## Using Environment Variables</h4>
<h5>## Using gRPC Exporter</h5>
<p>Examples with <code>us</code> region</p>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h5>## Using HTTP Exporter</h5>
<ul>
<li><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.us.signoz.cloud:443 OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code></li>
</ul>
<h4>## Configuring Endpoint in Code</h4>
<p>Please refer to the agent documentation.</p>
<h3>## Sending Data from a Third-Party Service</h3>
<p>The endpoint configuration here depends on the export protocol supported by the third-party service. They may support either gRPC, HTTP, or both. Generally, you will need to adjust the host and port. The host address should be <code>ingest.{region}.signoz.cloud:443</code>, where <code>{region}</code> should be replaced with <code>in</code>, <code>us</code>, or <code>eu</code>, and port <code>443</code> should be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/ms-teams/
tag_set: alerts-management, notification-channel, ms-teams
image_urls: 
tracking_id: docs-alerts-management-notification-channel-ms-teams
group_tracking_ids: docs-alerts-management-notification-channel-ms-teams
<h2>Configure Microsoft Teams Channel</h2>
<h2>Prerequisite</h2>
<hr />
<p>Before configuring Ms Teams as a notification channel in SigNoz, ensure that you have:</p>
<ul>
<li><strong>Incoming Webhook</strong>: Follow the steps outlined in <a href="https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook">Microsoft Teams documentation</a> to create an incoming webhook and obtain the necessary webhook URL.</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.26.0">v0.26.0</a> or later.</li>
</ul>
<p>‚úÖ Info</p>
<p>Please note that the MS Teams notification option is only available for <strong>SigNoz paid plans</strong>. Feel free to explore the available plans and their features <a href="https://signoz.io/pricing/">here</a>.</p>
<h2>Accessing Alert Channels</h2>
<hr />
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
<h2>Creating a new Notification channel</h2>
<hr />
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Ms Teams as the channel type.</li>
<li><strong>Webhook URL</strong>: Paste the Incoming Webhook URL obtained from Microsoft Teams.</li>
</ul>
<p>üìù Note</p>
<p>You can use <a href="https://prometheus.io/docs/alerting/latest/notifications/">Go templates</a> for customizing the title and description.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fms-teams-new-channel.webp&amp;w=3840&amp;q=75" alt="new-notification-channel" /></p>
<p><strong>Test Configuration</strong>: Verify the configuration by clicking the Test button. This sends a test alert to Microsoft Teams, ensuring that SigNoz alert manager can communicate with your MS Teams setup.</p>
<h2>Editing a Notification channel</h2>
<hr />
<p>To edit an existing MS Teams notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the MS Teams webhook URL and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>
<h2>Receive Alert in MS Teams</h2>
<hr />
<p>Once configured correctly, alerts from SigNoz will appear in Microsoft Teams whenever monitored metrics cross the thresholds specified in your alert rules. This setup ensures that you are promptly alerted to any issues in your applications or infrastructure components.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-in-ms-teams.webp&amp;w=3840&amp;q=75" alt="alert-in-ms-teams" /></p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter issues with the Microsoft Teams integration:</p>
<ul>
<li><strong>Check the Webhook URL</strong>: Ensure that the webhook URL entered in SigNoz matches the one provided by Microsoft Teams.</li>
<li><strong>Verify MS Teams Configuration</strong>: Confirm that the incoming webhook in Microsoft Teams is correctly set up and active.</li>
<li><strong>Test Connectivity</strong>: Use the Test button in SigNoz to check connectivity with Microsoft Teams. If the test fails, review your network settings and webhook URL.</li>
</ul>
<p>üí° Tip</p>
<p>If you encounter any unexpected challenges during the use of this integration, please contact SigNoz Support at <a href="mailto:support@signoz.io">support@signoz.io</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/value/
tag_set: dashboards, panel-types, value
image_urls: https://signoz.io/img/docs/dashboards/panel-types/value.png
tracking_id: docs-dashboards-panel-types-value
group_tracking_ids: docs-dashboards-panel-types-value
<h2>Value Panel Type</h2>
<h2>Value</h2>
<p>================</p>
<p>Value panels are used to display a single value of a metric, APM, or log.</p>
<h2>Data Formats</h2>
<hr />
<h3>## Supported signals</h3>
<ul>
<li>Logs</li>
<li>Traces</li>
<li>Metrics</li>
</ul>
<p>This panel type supports any time series data that can be aggregated to a single value. The time series data can be from logs, traces, or metrics.</p>
<h3>## Setup</h3>
<p>The following graph shows the combined average requests per second (req/s) for all services over a period of time.</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/value.png" alt="Value for req/s over period of time" /></p>
<p><em>Value for req/s over period of time.</em></p>
<h4>## Configuration</h4>
<ul>
<li>Choose the signal type (logs, traces, metrics)</li>
<li>Choose the metric, APM, or log to display
<ul>
<li>Metric: see the <a href="/docs/userguide/query-builder/#metrics-query-builder">Metrics query builder documentation</a> to configure metric query</li>
<li>Log: see the <a href="/docs/userguide/query-builder/#logs-and-traces-query-builder">Logs query builder documentation</a> to configure log query</li>
<li>Trace: see the <a href="/docs/userguide/query-builder/#logs-and-traces-query-builder">Traces query builder documentation</a> to configure trace query</li>
</ul>
</li>
<li>Reduce: choose the function to reduce the data (e.g. <code>avg</code>, <code>sum</code>, <code>max</code>, <code>min</code>, or <code>latest</code>). This step aggregates the data over the selected time period and returns a single value. For example, if you have a request rate over a period of time, the <code>avg</code> function will return the average of per-minute request rate over the period.</li>
</ul>
<h2>Options</h2>
<hr />
<h3>## Unit</h3>
<p>The unit of the value. The default unit is <code>None</code>.</p>
<h3>## Thresholds</h3>
<p>Thresholds are used to configure the color of the value. The thresholds are defined as a list of tuples. Each tuple contains the value and the color. The color is optional and if not provided, the default color will be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/operate/kubernetes/
tag_set: operate, kubernetes
image_urls: 
tracking_id: docs-operate-kubernetes
group_tracking_ids: docs-operate-kubernetes
<h2>Kubernetes</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>Once you have successfully installed SigNoz on Kubernetes, the following sections provide an overview of the activities that are required to successfully operate SigNoz.</p>
<h2>Stop/Start SigNoz Cluster</h2>
<hr />
<p>To stop SigNoz cluster:</p>
<pre><code>helm -n platform uninstall &quot;my-release&quot;
</code></pre>
<p>To start/resume SigNoz cluster:</p>
<pre><code>helm -n platform install &quot;my-release&quot;
</code></pre>
<p>üìù Note</p>
<p>The newly created release aka SigNoz cluster should mount to the existing persistent volume as long as the <strong>namespace</strong> and the <strong>release name</strong> matches the old one.</p>
<h2>Upgrade SigNoz Cluster</h2>
<hr />
<p>Use the steps below to upgrade to the latest version:</p>
<ol>
<li>
<p>Fetch the latest chart information from the Helm repositories</p>
<pre><code>helm repo update
</code></pre>
</li>
<li>
<p>Upgrade to the latest available version of the chart</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz
</code></pre>
</li>
</ol>
<p>‚úÖ Info</p>
<p>To override values in a Helm chart, you can also use the <code>values</code>/<code>-f</code> flag. See the <a href="https://helm.sh/docs/helm/helm_upgrade/">Helm Upgrade</a> page of the Helm documentation for more details.</p>
<p>In case you wish to upgrade the SigNoz cluster to a specific version, follow the steps below:</p>
<ol>
<li>
<p>List the available SigNoz Helm charts with their version and supported app version.</p>
<pre><code>helm search repo signoz --versions
</code></pre>
<p>The output should look similar to the following:</p>
<pre><code>NAME               	CHART VERSION	APP VERSION	DESCRIPTION
signoz/signoz      	0.2.5        	0.10.2     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.2.3        	0.10.1     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.2.2        	0.10.0     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.2.1        	0.10.0     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.2.0        	0.10.0     	SigNoz Observability Platform Helm Chart
signoz/signoz      	0.1.4        	0.9.2      	SigNoz Observability Platform Helm Chart
signoz/clickhouse  	23.4.0       	22.4.5     	A Helm chart for ClickHouse
signoz/clickhouse  	23.3.3       	22.4.5     	A Helm chart for ClickHouse
</code></pre>
</li>
<li>
<p>Run the following command to install the chart version <code>0.2.5</code> running SigNoz version <code>0.10.2</code> with the release name <code>my-release</code> and namespace <code>platform</code>:</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz --version 0.2.5
</code></pre>
</li>
</ol>
<p>‚ö†Ô∏è Warning</p>
<ul>
<li>Be careful with breaking changes across versions!</li>
<li>There might be misconfiguration caused by version mismatch.</li>
</ul>
<h2>Uninstall SigNoz Cluster</h2>
<hr />
<p>To uninstall/delete the <code>my-release</code> resources:</p>
<pre><code>helm -n platform uninstall &quot;my-release&quot;
</code></pre>
<p>See the <a href="https://helm.sh/docs/helm/helm_uninstall/">Helm docs</a> for documentation on the helm uninstall command.</p>
<p>The command above removes all the Kubernetes components associated with the chart and deletes the release except for ClickHouse CRD resources due to <code>finalizers</code>.</p>
<p>To delete resources accociated to <code>ClickHouseInstallations</code> instance:</p>
<pre><code>kubectl -n platform patch \
  clickhouseinstallations.clickhouse.altinity.com/my-release-clickhouse \
  -p '{&quot;metadata&quot;:{&quot;finalizers&quot;:[]}}' --type=merge
</code></pre>
<p>Deletion of the StatefulSet doesn't cascade to deleting associated PVCs. To delete them:</p>
<pre><code>kubectl -n platform delete pvc -l app.kubernetes.io/instance=my-release
</code></pre>
<p>At last, clean up the namespace:</p>
<pre><code>kubectl delete namespace platform
</code></pre>
<p>‚úÖ Info</p>
<p>Replace <code>my-release</code> and <code>platform</code> from above instructions with appropriate release name and SigNoz namespace respectively.</p>
<h2>Remove the Sample Application from Dashboard</h2>
<hr />
<p>Use the command below to remove the sample application:</p>
<pre><code>curl -sL https://github.com/SigNoz/signoz/raw/main/sample-apps/hotrod/hotrod-delete.sh \
  | HOTROD_NAMESPACE=sample-application bash
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/instrumentation/overview/
tag_set: instrumentation, overview
image_urls: 
tracking_id: docs-instrumentation-overview
group_tracking_ids: docs-instrumentation-overview
<h2>Overview</h2>
<h2>APM and Distributed Tracing - Overview</h2>
<hr />
<p>Instrumentation is the process of enabling your application code to generate telemetry data - anything that assists you in monitoring or measuring the performance and state of your application.</p>
<p>SigNoz supports <a href="https://opentelemetry.io/">OpenTelemetry</a> for instrumenting applications.</p>
<p><strong>OpenTelemetry</strong> is the leading open-source project that provides instrumentation libraries for major programming languages and popular frameworks. It is a project under Cloud Native Computing Foundation and is backed by a huge community. It provides a standardized data format for collected data, eliminating the need for specific vendor integrations.</p>
<p>This <a href="https://opentelemetry.io/docs/concepts/instrumenting">guide</a> introduces the basic concepts of instrumentation using OpenTelemetry. OpenTelemetry also has an ecosystem of libraries, plugins, integrations, and other useful tools which extend it. You can find these resources at Otel Registry <a href="https://opentelemetry.io/registry/">here</a>.</p>
<p><em>You can instrument using any open-standard library and use SigNoz as your observability backend to ingest, analyse and visualize data.</em></p>
<p>For instrumenting your code, you can use the instruction provided by OpenTelemetry for specific langauges.</p>
<p>SigNoz currently provides simple ways to instrument NodeJS, Java, Python and Golang applications using OpenTelemetry. Please follow the below guides.</p>
<ol>
<li>
<p><a href="/docs/instrumentation/python/">OpenTelemetry Python Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/javascript/">OpenTelemetry Javascript Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/java/">OpenTelemetry Java Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/golang/">OpenTelemetry Go Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/php/">OpenTelemetry PHP Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/dotnet/">OpenTelemetry .NET Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/ruby-on-rails/">OpenTelemetry Ruby on Rails Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/elixir/">OpenTelemetry Elixir Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/rust/">OpenTelemetry Rust Instrumentation</a></p>
</li>
<li>
<p><a href="/docs/instrumentation/swift/">OpenTelemetry Swift Instrumentation</a></p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP</h2>
<p>üí° Tip</p>
<p>The easiest way to run SigNoz is to use SigNoz Cloud - no installation, maintenance, or scaling needed.</p>
<p>New users get 30 days of unlimited access to all features. Click <a href="/teams/">here</a> to sign up.</p>
<p>First, we need to set up a Kubernetes cluster (see the <a href="https://cloud.google.com/kubernetes-engine/">official GCP documentation</a> for more info).</p>
<p>Follow the steps on this page to install SigNoz on Kubernetes with Helm.</p>
<p>The <a href="https://signoz.io/docs/install/kubernetes/">SigNoz Helm chart</a> will install the following components into your Kubernetes cluster:</p>
<ul>
<li>Query Service (backend service)</li>
<li>Web UI (frontend)</li>
<li>OpenTelemetry Collectors</li>
<li>Alertmanager</li>
<li>ClickHouse chart (datastore)</li>
<li>K8s-Infra chart (k8s infra metrics/logs collectors)</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#prerequisites
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-prerequisites
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Prerequisites</h2>
<ul>
<li>
<p>You must have a GKE cluster. Both Standard and Autopilot are supported.</p>
</li>
<li>
<p>Kubernetes version &gt;= <code>1.22</code></p>
</li>
<li>
<p><code>x86-64</code>/<code>amd64</code> workloads as currently <code>arm64</code> architecture is not supported</p>
</li>
<li>
<p>Helm version &gt;= <code>3.8</code></p>
</li>
<li>
<p>You must have <code>kubectl</code> access to your cluster</p>
</li>
<li>
<p>The following table describes the hardware requirements that are needed to install SigNoz on Kubernetes:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Minimal Requirements</th>
<th>Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory</td>
<td>8 GB</td>
<td>16 GB</td>
</tr>
<tr>
<td>CPU</td>
<td>4 cores</td>
<td>8 cores</td>
</tr>
<tr>
<td>Storage</td>
<td>30 GB</td>
<td>80 GB</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Suggestion: In case you want to use your own custom storage class for PVCs, you can set <code>global.storageClass</code> configuration to desired storage class.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#chart-configuration
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-chart-configuration
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Chart configuration</h2>
<p>Here's the minimal required <code>override-values.yaml</code> that we'll be using later. You can find an overview of the parameters that can be configured during installation under <a href="https://github.com/SigNoz/charts/tree/main/charts/signoz#configuration">chart configuration</a>.</p>
<h3>## GKE Standard</h3>
<p>In GKE Standard, you can either install with the default configuration or make use of the following <code>override-values.yaml</code>:</p>
<pre><code>global:
  storageClass: gce-resizable
  cloud: gcp

clickhouse:
  installCustomStorageClass: true
</code></pre>
<h3>## GKE Autopilot</h3>
<p>In GKE Autopilot, you must set <code>cloud</code> to <code>gcp/autogke</code> as well as update <code>kubeletMetrics</code> to use read-only Kubelet endpoint as shown below in the <code>override-values.yaml</code>:</p>
<pre><code>global:
  storageClass: gce-resizable
  cloud: gcp/autogke

clickhouse:
  installCustomStorageClass: true

k8s-infra:
  presets:
    kubeletMetrics:
      authType: none
      endpoint: ${K8S_NODE_NAME}:10255
</code></pre>
<p>GKE Autopilot automatically overriddes resource requests/limits. In our case, all <code>signoz</code> chart components as well as components from <code>clickhouse</code> and <code>k8s-infra</code> charts, if enabled. Therefore, make sure to have enough resource quota for the region where the cluster is deployed. Read more about it <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-resource-requests#defaults">here</a>.</p>
<p>‚úÖ Info</p>
<p>To list storage class in your Kubernetes cluster: <code>kubectl get storageclass</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#install-signoz-on-kubernetes-with-helm
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-install-signoz-on-kubernetes-with-helm
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Install SigNoz on Kubernetes with Helm</h2>
<ol>
<li>
<p>Add the SigNoz Helm repository to your client with name <code>signoz</code> by running the following command:</p>
<pre><code>helm repo add signoz https://charts.signoz.io
</code></pre>
</li>
<li>
<p>Verify that the repository is accessible to the Helm CLI by entering the following command:</p>
<pre><code>helm repo list
</code></pre>
</li>
<li>
<p>Use the <code>kubectl create ns</code> command to create a new namespace. SigNoz recommends you use <code>platform</code> for your new namespace:</p>
<pre><code>kubectl create ns platform
</code></pre>
</li>
<li>
<p>Run the following command to install the chart with the release name <code>my-release</code> and namespace <code>platform</code>:</p>
<p>helm --namespace platform install my-release signoz/signoz -f override-values.yaml</p>
</li>
</ol>
<p>Output:</p>
<pre><code>NAME: my-release
LAST DEPLOYED: Mon May 23 20:34:55 2022
NAMESPACE: platform
STATUS: deployed
REVISION: 1
NOTES:
1. You have just deployed SigNoz cluster:

- frontend version: '0.8.0'
- query-service version: '0.8.0'
- alertmanager version: '0.23.0-0.1'
- otel-collector version: '0.43.0-0.1'
- otel-collector-metrics version: '0.43.0-0.1'
</code></pre>
<p>*<em>Note that the above command installs the latest stable version of SigNoz.</em></p>
<p>(Optional) To install a different version, you can use the <code>--set</code> flag to specify the version you wish to install. The following example command installs SigNoz version <code>0.8.0</code>:</p>
<pre><code>helm --namespace platform install my-release signoz/signoz \
  --set frontend.image.tag=&quot;0.8.0&quot; \
  --set queryService.image.tag=&quot;0.8.0&quot;
</code></pre>
<p>‚úÖ Info</p>
<ul>
<li>If you use the <code>--set</code> flag, ensure that you specify the same versions for the <code>frontend</code> and <code>queryService</code> images. Specifying different versions could lead the SigNoz cluster to behave abnormally.</li>
<li>Do not use the <code>latest</code> or <code>develop</code> tags in a production environment. Specifying these tags could install different versions of SigNoz on your cluster and could lead to data loss.</li>
</ul>
<ol start="5">
<li>
<p>You can access SigNoz by setting up port forwarding and browsing to the specified port. The following <code>kubectl port-forward</code> example command forwards all connections made to <code>localhost:3301</code> to <code>&lt;signoz-frontend-service&gt;:3301</code>:</p>
<p>export SERVICE_NAME=$(kubectl get svc --namespace platform -l &quot;app.kubernetes.io/component=frontend&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)</p>
<p>kubectl --namespace platform port-forward svc/$SERVICE_NAME 3301:3301</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#verify-the-installation
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-verify-the-installation
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Verify the Installation</h2>
<p>Using the <code>kubectl -n platform get pods</code> command, monitor the SigNoz deployment process. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n platform get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                                                        READY   STATUS    RESTARTS   AGE
chi-signoz-cluster-0-0-0                                    1/1     Running   0          8m21s
clickhouse-operator-8cff468-n5s99                           2/2     Running   0          8m55s
my-release-signoz-alertmanager-0                            1/1     Running   0          8m54s
my-release-signoz-frontend-78774f44d7-wl87p                 1/1     Running   0          8m55s
my-release-signoz-otel-collector-66c8c7dc9d-d8v5c           1/1     Running   0          8m55s
my-release-signoz-otel-collector-metrics-68bcfd5556-9tkgh   1/1     Running   0          8m55s
my-release-signoz-query-service-0                           1/1     Running   0          8m54s
my-release-zookeeper-0                                      1/1     Running   0          8m54s
</code></pre>
<p>‚úÖ Info</p>
<p>By default, retention period is set to <strong>7 days</strong> for logs and traces, and <strong>30 days</strong> for metrics. To change this, navigate to the <strong>General</strong> tab on the <strong>Settings</strong> page of SigNoz UI.</p>
<p>For more details, refer to <a href="https://signoz.io/docs/userguide/retention-period">https://signoz.io/docs/userguide/retention-period</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#optional-install-a-sample-application-and-generate-tracing-data
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-optional-install-a-sample-application-and-generate-tracing-data
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: (Optional) Install a Sample Application and Generate Tracing Data</h2>
<p>Follow the steps in this section to install a sample application named <a href="https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod">HotR.O.D</a>
, and generate tracing data.</p>
<ol>
<li>
<p>Use the HotROD install script below to create a <code>sample-application</code> namespace and deploy HotROD application on it:</p>
<pre><code>curl -sL https://github.com/SigNoz/signoz/raw/develop/sample-apps/hotrod/hotrod-install.sh \
  | HELM_RELEASE=my-release SIGNOZ_NAMESPACE=platform bash
</code></pre>
</li>
<li>
<p>Using the <code>kubectl -n sample-application get pods</code> command, monitor the sample application pods. Wait for all the pods to be in running state:</p>
<pre><code>kubectl -n sample-application get pods
</code></pre>
<p>Output:</p>
<pre><code>NAME                            READY   STATUS    RESTARTS   AGE
hotrod-55bd58cc8d-mzxq8         1/1     Running   0          2m
locust-master-b65744bbf-l7v7n   1/1     Running   0          2m
locust-slave-688c86bcb7-ngx7w   1/1     Running   0          2m
</code></pre>
</li>
<li>
<p>Use the following command to generate load:</p>
<pre><code>kubectl --namespace sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl -X POST -F \
  'user_count=6' -F 'spawn_rate=2' http://locust-master:8089/swarm
</code></pre>
</li>
<li>
<p>Browse to <code>http://localhost:3301</code> and see the metrics and traces for your sample application.</p>
</li>
<li>
<p>Use the following command to stop load generation:</p>
<pre><code>kubectl -n sample-application run strzal --image=djbingham/curl \
  --restart='OnFailure' -i --tty --rm --command -- curl \
  http://locust-master:8089/stop
</code></pre>
</li>
</ol>
<p>Go to <a href="/docs/operate/kubernetes">Kubernetes Operate</a> section for detailed instructions.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/install/kubernetes/gcp/#next-steps
tag_set: install, kubernetes, gcp
image_urls: 
tracking_id: docs-install-kubernetes-gcp-next-steps
group_tracking_ids: docs-install-kubernetes-gcp
<h2>Deploying to GCP: Next Steps</h2>
<ul>
<li>
<p><a href="/docs/instrumentation/overview">Instrument Your Application</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/tutorial/opentelemetry-operator-usage/#opentelemetry-auto-instrumentation-injection">Use OpenTelemetry Operator for automatic instrumentation (if your applications are in k8s)</a></p>
</li>
<li>
<p><a href="/docs/tutorials/">Tutorials</a></p>
</li>
</ul>
<hr />

--------------------------------------------------------------------------------

link: https://signoz.io/docs/troubleshooting/signoz-cloud/logs-troubleshooting/
tag_set: troubleshooting, signoz-cloud, logs-troubleshooting
image_urls: 
tracking_id: docs-troubleshooting-signoz-cloud-logs-troubleshooting
group_tracking_ids: docs-troubleshooting-signoz-cloud-logs-troubleshooting
<h2>Logs SigNoz Cloud Troubleshooting</h2>
<p>These are instructions for logs troubleshooting for SigNoz Cloud.</p>
<h3>## Q. In java my mdc attributes are not visible in SigNoz</h3>
<p>Toggle for answer</p>
<p>You can get your mdc attributes by using the <a href="https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java#settings-for-appender-instrumentation-based-on-the-logging-library">appender settings</a> and passing it as <code>-D&lt;property&gt;=&lt;value&gt;</code> format.</p>
<p>For example, for <strong>logback</strong> you can use <code>otel.instrumentation.logback-appender.experimental.capture-mdc-attributes=*</code> and for <strong>log4j</strong> you can use <code>otel.instrumentation.log4j-appender.experimental.capture-mdc-attributes=*</code>. Checkout <a href="%60otel.instrumentation.log4j-appender.experimental.capture-mdc-attributes=*%60">this documentation</a> for more system properties.</p>
<h3>## Q. I can't see related logs for traces.</h3>
<p>Toggle for answer</p>
<ul>
<li>If you are collecting application logs from file then make sure that you are emitting the trace_id and span_id to those logs.</li>
<li>Once you have trace_id and span_id in your logs, you can parse them using pipelines trace parser - documentation <a href="https://signoz.io/docs/logs-pipelines/guides/trace/">here</a></li>
</ul>
<h3>## Q. I want to extract trace_id and span_id from my log line</h3>
<p>Toggle for answer</p>
<p>You can extract these using the logs pipelines trace parser - documentation <a href="https://signoz.io/docs/logs-pipelines/guides/trace/">here</a></p>
<h3>## Q.I want to extract a attribute from my log line</h3>
<p>Toggle for answer</p>
<p>The best way to do this is using Logs Pipeline. You can follow these two Docs:</p>
<ul>
<li>
<p><a href="https://signoz.io/docs/logs-pipelines/introduction/">Logs Pipeline Introductions</a></p>
</li>
<li>
<p><a href="https://signoz.io/docs/logs-pipelines/guides/json/">Parse JSON logs with Pipelines</a></p>
</li>
</ul>
<h3>## Q. I am sending logs but my logs are not visible on SigNoz</h3>
<p>Toggle for answer</p>
<p>Check by adding a console exporter on application level if you are using SDK else on a local collector level. Once the above is checked and if you are still facing issue then reach out to us on Intercom - the chatbox at the bottom right corner of your SigNoz Cloud interface.</p>
<h3>## Q. How to increase the retention period of logs from x days to y days?</h3>
<p>Toggle for answer</p>
<p>Reach out to us on Intercom - the chatbox at the bottom right corner of your SigNoz Cloud interface.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/webhook/
tag_set: alerts-management, notification-channel, webhook
image_urls: 
tracking_id: docs-alerts-management-notification-channel-webhook
group_tracking_ids: docs-alerts-management-notification-channel-webhook
<h2>Configure Webhook Channel</h2>
<h2>Prerequisites</h2>
<hr />
<p>Before configuring a Webhook channel in SigNoz, ensure that you have:</p>
<ul>
<li>
<p><strong>Webhook Application</strong>: Have an application ready to accept webhook messages.</p>
</li>
<li>
<p><strong>Webhook URL</strong>: Obtain a valid webhook URL reachable from SigNoz Alert Manager.</p>
</li>
<li>
<p><strong>SigNoz Version</strong>: Ensure you are using SigNoz version Webhook <a href="https://github.com/SigNoz/signoz/releases/tag/v0.7.4">v0.7.4</a> or later</p>
</li>
</ul>
<h2>Accessing Alert Channels</h2>
<hr />
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
<h2>Creating a new Webhook channel</h2>
<hr />
<p>To create a new Webhook notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Webhook as the channel type.</li>
<li><strong>Webhook URL</strong>: Enter the Webhook URL endpoint.</li>
<li><strong>Username and Password</strong> (Optional): Provide the necessary credentials for authentication.</li>
</ul>
<p><strong>Test Configuration</strong>: Click the Test button to test the connection with your application.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fwebhook-1.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<h2>Editing a Webhook channel</h2>
<hr />
<p>To edit an existing webhook notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the webhook URL and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fwebhook-2.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<h2>Receive Alert through Webhook</h2>
<hr />
<p>Once the configuration is set up correctly, you will receive alerts in your application through the configured Webhook channel whenever monitored metrics exceed the specified thresholds in alert rules.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fwebhook-3.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<h2>Sample Webhook message</h2>
<hr />
<p>A webhook message may contain multiple alerts. By default, the SigNoz alert manager groups alerts by the alert name and delivers grouped messages every 5 minutes.</p>
<p>For resolved alerts, the alert manager will send the time of resolution in <em>endsAt</em>. You can also use the fingerprint property to identify and process updates sent by the alert manager.</p>
<pre><code>{
   &quot;receiver&quot;:&quot;w1&quot;,
   &quot;status&quot;:&quot;firing&quot;,
   &quot;alerts&quot;:[\
      {\
         &quot;status&quot;:&quot;firing&quot;,\
         &quot;labels&quot;:{\
            &quot;alertname&quot;:&quot;DiskRunningFull&quot;,\
            &quot;dev&quot;:&quot;sda3&quot;,\
            &quot;instance&quot;:&quot;example3&quot;,\
            &quot;severity&quot;:&quot;critical&quot;\
         },\
         &quot;annotations&quot;:{\
            &quot;info&quot;:&quot;The disk sda3 is running full&quot;,\
            &quot;summary&quot;:&quot;please check the instance example1&quot;\
         },\
         &quot;startsAt&quot;:&quot;2022-04-25T14:35:19.490146+05:30&quot;,\
         &quot;endsAt&quot;:&quot;0001-01-01T00:00:00Z&quot;,\
         &quot;generatorURL&quot;:&quot;&quot;,\
         &quot;fingerprint&quot;:&quot;ad592b0afcbe2e79&quot;\
      }\
   ],
   &quot;groupLabels&quot;:{
      &quot;alertname&quot;:&quot;DiskRunningFull&quot;
   },
   &quot;commonLabels&quot;:{
      &quot;alertname&quot;:&quot;DiskRunningFull&quot;,
      &quot;dev&quot;:&quot;sda3&quot;,
      &quot;instance&quot;:&quot;example3&quot;,
      &quot;severity&quot;:&quot;critical&quot;
   },
   &quot;commonAnnotations&quot;:{
      &quot;info&quot;:&quot;The disk sda3 is running full&quot;,
      &quot;summary&quot;:&quot;please check the instance example1&quot;
   },
   &quot;externalURL&quot;:&quot;http://Apples-MacBook-Pro-3.local:9093&quot;,
   &quot;version&quot;:&quot;4&quot;,
   &quot;groupKey&quot;:&quot;{}/{}:{alertname=\&quot;DiskRunningFull\&quot;}&quot;,
   &quot;truncatedAlerts&quot;:0
}
</code></pre>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter issues:</p>
<ul>
<li>
<p><strong>Check the Webhook URL</strong>: Ensure the webhook URL is correctly entered in SigNoz.</p>
</li>
<li>
<p><strong>Verify Webhook Permissions</strong>: Confirm that the webhook has permissions to post alerts to the desired endpoint.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/opsgenie/
tag_set: alerts-management, notification-channel, opsgenie
image_urls: 
tracking_id: docs-alerts-management-notification-channel-opsgenie
group_tracking_ids: docs-alerts-management-notification-channel-opsgenie
<h2>Configure Opsgenie Channel</h2>
<h2>Prerequisites</h2>
<hr />
<p>Before setting up Opsgenie as a notification channel in SigNoz, ensure the following:</p>
<ul>
<li><strong>Create Integration and Obtain API Key</strong>: You need to create an integration in Opsgenie and obtain an API key. Follow the steps provided <a href="https://support.atlassian.com/opsgenie/docs/integrate-opsgenie-with-prometheus/">here</a> to create an integration and obtain the necessary API key.</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.28.0">v0.28.0</a> or later</li>
</ul>
<h2>Accessing Alert Channels</h2>
<hr />
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
<h2>Creating a new Notification channel</h2>
<hr />
<p>To create a new Opsgenie notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select Opsgenie as the channel type.</li>
<li><strong>API Key</strong>: Enter the API Key obtained from Opsgenie.</li>
<li>Customize the message, description, and priority using <a href="https://prometheus.io/docs/alerting/latest/notifications/">go templates</a>
.</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fopsgenie-new-channel.webp&amp;w=3840&amp;q=75" alt="new-notification-channel" /></p>
<p><strong>Test Configuration</strong>: Click the Test button to send a test alert to the configured Opsgenie channel. This verifies that SigNoz can communicate with your Opsgenie.</p>
<h2>Editing a Notification channel</h2>
<hr />
<p>To edit an existing Opsgenie notification channel:</p>
<ul>
<li>Navigate to the channel settings in SigNoz.</li>
<li>You can edit the opsgenie API Key and other parameters. However, note that the channel name and type are not editable after creation.</li>
</ul>
<h2>Receiving Alerts in Opsgenie</h2>
<hr />
<p>Once configured correctly, alerts from SigNoz will appear in Opsgenie Alerts whenever monitored metrics cross the thresholds specified in your alert rules.This ensures you are promptly notified of any issues in your applications or infrastructure components.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-in-opsgenie.webp&amp;w=3840&amp;q=75" alt="alert-in-opsgenie" /></p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter issues with the Opsgenie integration:</p>
<ul>
<li><strong>Check the API Key</strong>: Ensure that the API Key entered in SigNoz matches the one provided by Opsgenie.</li>
<li><strong>Verify Opsgenie Integration</strong>: Confirm that the integration in Opsgenie is correctly set up and active.</li>
<li><strong>Test Connectivity</strong>: Use the Test button in SigNoz to check connectivity with Opsgenie. If the test fails, review your network settings and API Key.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_troubleshooting/
tag_set: userguide, logs_troubleshooting
image_urls: 
tracking_id: docs-userguide-logs_troubleshooting
group_tracking_ids: docs-userguide-logs_troubleshooting
<h2>Troubleshooting</h2>
<p>This troubleshooting guide includes step-by-step instructions that should resolve most issues with logs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_troubleshooting/#schema-migrator-dirty-database-version
tag_set: userguide, logs_troubleshooting
image_urls: 
tracking_id: docs-userguide-logs_troubleshooting-schema-migrator-dirty-database-version
group_tracking_ids: docs-userguide-logs_troubleshooting
<h2>Troubleshooting: Schema Migrator: Dirty database version</h2>
<p>If you are migrating from older version of SigNoz to <code>V0.49.1</code> or <code>V0.50.0</code>, you might encounter issue with <code>signoz-schema-migrator</code> where it fails to run and results in an error message like this.</p>
<pre><code>Dirty database version 10. Fix and force version.
</code></pre>
<p>Similar error messages include</p>
<pre><code>Dirty database version 11. Fix and force version.


Dirty database version 12. Fix and force version.
</code></pre>
<p>To fix this issue, first disable the schema migrator.</p>
<ul>
<li>On k8s you can delete the job.</li>
<li>On Docker you can delete the schema migrator container.</li>
</ul>
<h4>## Run The Migrations Manually</h4>
<ul>
<li>
<p>Exec into the clickhouse container.</p>
</li>
<li>
<p>Start the clickhouse client by running <code>clickhouse client</code></p>
</li>
<li>
<p>Check the mutations table and clear it.</p>
<ul>
<li>
<p>Check the running mutations</p>
<pre><code>select * from system.mutations where is_done=0
</code></pre>
</li>
<li>
<p>If some mutation is stuck with error message you can kill it by running.</p>
<pre><code>KILL MUTATION where mutation_id='&lt;mutation_id&gt;'
</code></pre>
</li>
<li>
<p>If you have any doubt's please reach out to us on our community <a href="https://signoz.io/slack">slack</a></p>
</li>
</ul>
</li>
<li>
<p>Run the migrations manually by running each command from the below files on the clickhouse console. If the 10th migration failed then start with 10th, else start with the version where it failed.</p>
<ul>
<li>
<p>10:- <a href="https://github.com/SigNoz/signoz-otel-collector/blob/main/migrationmanager/migrators/logs/migrations/000010_body_ngram.up.sql">https://github.com/SigNoz/signoz-otel-collector/blob/main/migrationmanager/migrators/logs/migrations/000010_body_ngram.up.sql</a></p>
</li>
<li>
<p>11:- <a href="https://github.com/SigNoz/signoz-otel-collector/blob/main/migrationmanager/migrators/logs/migrations/000011_add_instrumentation_scope.up.sql">https://github.com/SigNoz/signoz-otel-collector/blob/main/migrationmanager/migrators/logs/migrations/000011_add_instrumentation_scope.up.sql</a></p>
</li>
<li>
<p>12:- <a href="https://github.com/SigNoz/signoz-otel-collector/blob/main/migrationmanager/migrators/logs/migrations/000012_rename_instrumentation_scope.up.sql">https://github.com/SigNoz/signoz-otel-collector/blob/main/migrationmanager/migrators/logs/migrations/000012_rename_instrumentation_scope.up.sql</a></p>
</li>
<li>
<p>13:- <a href="https://github.com/SigNoz/signoz-otel-collector/blob/main/migrationmanager/migrators/logs/migrations/000013_rename_instrumentation_scope.up.sql">https://github.com/SigNoz/signoz-otel-collector/blob/main/migrationmanager/migrators/logs/migrations/000013_rename_instrumentation_scope.up.sql</a></p>
</li>
</ul>
<p>While running them replace <code>{{.SIGNOZ_CLUSTER}}</code> with cluster.</p>
</li>
<li>
<p>Once the commands run successfully, update the schema migrations table.</p>
<ul>
<li>
<p>Truncate the schema_migrations table</p>
<pre><code>truncate table signoz_logs.schema_migrations
</code></pre>
</li>
<li>
<p>Insert data to the migrations table</p>
<pre><code>insert into signoz_logs.schema_migrations values (1 ,1 ,1720103647412982569), (1 ,0 ,1720103647810031709), (2 ,1 ,1720103647811955242), (2 ,0 ,1720103647869983948), (3 ,1 ,1720103647875299543), (3 ,0 ,1720103648053893053), (4 ,1 ,1720103648055714316), (4 ,0 ,1720103648113219798), (5 ,1 ,1720103648115626118), (5 ,0 ,1720103648678466995), (6 ,1 ,1720103648680332053), (6 ,0 ,1720103648795967894), (7 ,1 ,1720103648797859904), (7 ,0 ,1720103648967002426), (8 ,1 ,1720103648969563518), (8 ,0 ,1720103649251171410), (9 ,1 ,1720103649252821091), (9 ,0 ,1720103649365897523), (10 ,1 ,1720103649367680188), (10 ,0 ,1720103660451466954), (11 ,1 ,1720103660454458733), (11 ,0 ,1720103661073573745), (12 ,1 ,1720115099220469277), (12 ,0 ,1720115146203106526), (13 ,1 ,1720115146205945060), (13 ,0 ,1720115146319582687)`
</code></pre>
</li>
</ul>
</li>
</ul>
<h4>## Verify The Changes</h4>
<ul>
<li>
<p>Check the schema of logs table by running</p>
<pre><code>show create table signoz_logs.logs format Vertical;
</code></pre>
<p>The following should be present in the above response</p>
<pre><code>    `scope_name` String CODEC(ZSTD(1)),
    `scope_version` String CODEC(ZSTD(1)),
    `scope_string_key` Array(String) CODEC(ZSTD(1)),
    `scope_string_value` Array(String) CODEC(ZSTD(1)),
    ...
    INDEX body_idx lower(body) TYPE ngrambf_v1(4, 60000, 5, 0) GRANULARITY 1,
    ...
    INDEX scope_name_idx scope_name TYPE tokenbf_v1(10240, 3, 0) GRANULARITY 4,
    ...
</code></pre>
</li>
<li>
<p>Check the schema of distributed_logs table by running</p>
<pre><code>show create table signoz_logs.distributed_logs format Vertical;
</code></pre>
<p>The following should be present in the above response</p>
<pre><code>    `scope_name` String CODEC(ZSTD(1)),
    `scope_version` String CODEC(ZSTD(1)),
    `scope_string_key` Array(String) CODEC(ZSTD(1)),
    `scope_string_value` Array(String) CODEC(ZSTD(1)),
</code></pre>
</li>
<li>
<p>Check the schema of tag_attributes table</p>
<pre><code>SHOW CREATE TABLE signoz_logs.tag_attributes
</code></pre>
<p>The following should be present in the above response</p>
<pre><code>`tagType` Enum8('tag' = 1, 'resource' = 2, 'scope' = 3) CODEC(ZSTD(1)),
</code></pre>
</li>
</ul>
<p>Once verified run the schema migrator again</p>
<ul>
<li>On k8s run helm upgrade.</li>
<li>On Docker rerun the docker compose file.</li>
</ul>
<p>If you face any issue please reach out to us on <a href="https://signoz.io/slack">slack</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_troubleshooting/#filter-logs-of-same-applicationservice-from-different-host
tag_set: userguide, logs_troubleshooting
image_urls: 
tracking_id: docs-userguide-logs_troubleshooting-filter-logs-of-same-applicationservice-from-different-host
group_tracking_ids: docs-userguide-logs_troubleshooting
<h2>Troubleshooting: Filter logs of same application/service from different host</h2>
<p>If you have a application/service which is emitting similar kind of logs but there are multiple instances of it running on different hosts you can identify them by adding a resource attribute.</p>
<p>There are different ways to add a resource attribute.</p>
<ol>
<li>
<p>By passing it as an environment variable if you are using the opentelemtry SDK or auto-instrumentation</p>
<pre><code>OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;service_name&gt;,hostname=&lt;host_name&gt;
</code></pre>
<p>replace the value of <code>&lt;service_name&gt;</code> with the actual service name and <code>&lt;host_name&gt;</code> with the actual hostname</p>
</li>
<li>
<p>If you have a otel collector running on each host you can add a processor to add the hostname.</p>
<pre><code>processors:
    attributes/add_hostname:
        actions:
            - key: hostname
              value: &lt;hostname&gt;
              action: upsert
...
service:
    logs:
        processors: [attributes/add_hostname, batch]
</code></pre>
<p>replace the value of <code>&lt;host_name&gt;</code> with the actual hostname</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_troubleshooting/#missing-columns-issue
tag_set: userguide, logs_troubleshooting
image_urls: 
tracking_id: docs-userguide-logs_troubleshooting-missing-columns-issue
group_tracking_ids: docs-userguide-logs_troubleshooting
<h2>Troubleshooting: Missing Columns Issue</h2>
<p>In case you are using signoz version <code>0.27</code> or newer and in the past you ran the migration <code>0.27</code> You may face missing columns issue if you are using distributed clickhouse with multiple shards.</p>
<p>To solve this issue, exec into all the shards and follow the steps below:</p>
<pre><code>show create table signoz_logs.distributed_logs
</code></pre>
<p>If in one shard the name of a column is <code>host_name</code> and in other shard the name is <code>attribute_string_host_name</code> then run the following command</p>
<pre><code>alter table signoz_logs.distributed_logs on cluster cluster rename column if exists host_name to attribute_string_host_name
</code></pre>
<p>Run the above command for all the column names which were not migrated.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_troubleshooting/#k8s-attribute-filtering-issue-in-logs
tag_set: userguide, logs_troubleshooting
image_urls: 
tracking_id: docs-userguide-logs_troubleshooting-k8s-attribute-filtering-issue-in-logs
group_tracking_ids: docs-userguide-logs_troubleshooting
<h2>Troubleshooting: K8s Attribute Filtering Issue in Logs</h2>
<p>In the SigNoz charts releases <code>v0.9.1</code>, <code>v0.10.0</code> and <code>0.10.1</code>, some users who are facing issues querying the following selected fields.</p>
<ul>
<li><code>k8s_container_name</code></li>
<li><code>k8s_namespace_name</code></li>
<li><code>k8s_pod_name</code></li>
<li><code>k8s_container_restart_count</code></li>
<li><code>k8s_pod_uid</code></li>
</ul>
<p>If you have included any of the above to <code>selected</code> fields, and you get empty data when you filter using those fields then you will have to perform the following steps.</p>
<ul>
<li>
<p>Exec into your clickhouse container</p>
<pre><code>kubectl exec -n platform -it chi-my-release-clickhouse-cluster-0-0-0 -- sh

clickhouse client
</code></pre>
</li>
<li>
<p>Run the following queries</p>
<pre><code>use signoz_logs;

show create table logs;
</code></pre>
</li>
<li>
<p>For the corresponding field, you will find a materialized column and an index. For example: <code>k8s_namespace_name</code> you will have <code>k8s_namespace_name String MATERIALIZED attributes_string_value[indexOf(attributes_string_key, 'k8s_namespace_name')] CODEC(LZ4)</code> and index <code>INDEX k8s_namespace_name_idx k8s_namespace_name TYPE bloom_filter(0.01) GRANULARITY 64</code></p>
</li>
<li>
<p>You will have to delete the index and remove the materialized column</p>
<pre><code>alter table logs drop column k8s_namespace_name;
alter table logs drop index k8s_namespace_name_idx;
</code></pre>
</li>
<li>
<p>Perform the above steps for all the k8s fields listed.</p>
</li>
<li>
<p>Once done truncate the attribute keys table</p>
<pre><code>truncate table logs_atrribute_keys;
</code></pre>
</li>
<li>
<p>Now you can go back to the UI and convert them back to the selected field, and filters will work as expected.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/pie/
tag_set: dashboards, panel-types, pie
image_urls: https://signoz.io/img/docs/dashboards/panel-types/pie-chart.png
tracking_id: docs-dashboards-panel-types-pie
group_tracking_ids: docs-dashboards-panel-types-pie
<h2>Pie Chart Panel Type</h2>
<h2>Pie Chart</h2>
<p>========================</p>
<p>A Pie chart is a plot chart that shows the proportion of a single or a few different categories. This is useful when you want to show the composition of the data by categories.</p>
<h2>Data Formats</h2>
<hr />
<h3>## Supported signals</h3>
<ul>
<li>Logs</li>
<li>Traces</li>
<li>Metrics</li>
</ul>
<p>This panel type supports any time series data. The time series data can be from logs, traces, or metrics.</p>
<h3>## Examples</h3>
<p>The following graph shows the distribution of the service requests over the services.</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/pie-chart.png" alt="Pie chart for service requests" /></p>
<p><em>Pie chart for service requests.</em></p>
<h2>Configuration</h2>
<hr />
<p>This panel type doesn't support any configuration.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-logs-http/#overview
tag_set: userguide, send-logs-http
image_urls: 
tracking_id: docs-userguide-send-logs-http-overview
group_tracking_ids: docs-userguide-send-logs-http
<h2>Sending Logs to SigNoz over HTTP: Sending Logs to SigNoz over HTTP - Overview</h2>
<p>This guide provides detailed instructions on how to send logs to SigNoz using HTTP. Sending logs over HTTP offers flexibility, allowing users to create custom wrappers, directly transmit logs, or integrate existing loggers, making it a versatile choice for diverse use-cases.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-logs-http/#payload-structure
tag_set: userguide, send-logs-http
image_urls: 
tracking_id: docs-userguide-send-logs-http-payload-structure
group_tracking_ids: docs-userguide-send-logs-http
<h2>Sending Logs to SigNoz over HTTP: Payload Structure</h2>
<p>The payload is an array of logs in JSON format. It follows a structure similar to <a href="https://opentelemetry.io/docs/specs/otel/logs/data-model/">OTEL Logs Data Model</a>.</p>
<p>Below is how the payload would look like:</p>
<pre><code>[\
  {\
    &quot;timestamp&quot;: &lt;uint64&gt;,\
    &quot;trace_id&quot;: &lt;hex string&gt;,\
    &quot;span_id&quot;: &lt;hex string&gt;,\
    &quot;trace_flags&quot;: &lt;int&gt;\
    &quot;severity_text&quot;: &lt;string&gt;,\
    &quot;severity_number&quot;: &lt;int&gt;,\
    &quot;attributes&quot;: &lt;map&gt;,\
    &quot;resources&quot;: &lt;map&gt;,\
    &quot;body&quot;: &lt;string&gt;,\
  }\
]
</code></pre>
<p>Here's a brief description of each field in the log record:</p>
<table>
<thead>
<tr>
<th>Field Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>timestamp</td>
<td>Time when the event occurred</td>
</tr>
<tr>
<td>trace_id</td>
<td>Request trace id</td>
</tr>
<tr>
<td>span_id</td>
<td>Request span id</td>
</tr>
<tr>
<td>trace_flags</td>
<td><a href="https://www.w3.org/TR/trace-context/#trace-flags">W3C</a>&lt;br&gt; trace flag</td>
</tr>
<tr>
<td>severity_text</td>
<td>The severity text (also known as log level)</td>
</tr>
<tr>
<td>severity_number</td>
<td>Numerical value of the severity</td>
</tr>
<tr>
<td>attributes</td>
<td>Additional information about the event</td>
</tr>
<tr>
<td>resources</td>
<td>Describes the source of the log</td>
</tr>
<tr>
<td>body</td>
<td>The body of the log record</td>
</tr>
</tbody>
</table>
<p>To know more details about the different fields in a log record, you can check <a href="https://opentelemetry.io/docs/specs/otel/logs/data-model/#log-and-event-record-definition">this documentation</a>.</p>
<p>üìù Note</p>
<p><code>timestamp</code> is an uint64 showing time in <strong>nanoseconds</strong> since Unix epoch.</p>
<p>You can use <code>message</code> instead of <code>body</code> to represent the body of a log record.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-logs-http/#additional-keys
tag_set: userguide, send-logs-http
image_urls: 
tracking_id: docs-userguide-send-logs-http-additional-keys
group_tracking_ids: docs-userguide-send-logs-http
<h2>Sending Logs to SigNoz over HTTP: Additional Keys</h2>
<p>Any additional keys present in the log record, apart from the ones mentioned in the above <strong>payload structure</strong> will be moved to the <code>attributes</code> map.</p>
<p>For example, if the JSON payload has fields like <code>host</code>, <code>method</code> etc. which are not a part of the standard payload structure,</p>
<pre><code>[\
  {\
    &quot;host&quot;: &quot;myhost&quot;,\
    &quot;method&quot;: &quot;GET&quot;,\
    &quot;body&quot;: &quot;this is a log line&quot;\
  }\
]
</code></pre>
<p>Then they will be moved to <code>attributes</code> and the log record will be finally treated as:</p>
<pre><code>[\
  {\
    &quot;attributes&quot;: {\
      &quot;host&quot;: &quot;myhost&quot;,\
      &quot;method&quot;: &quot;GET&quot;\
    },\
    &quot;body&quot;: &quot;this is a log line&quot;\
  }\
]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-logs-http/#construct-the-curl-request
tag_set: userguide, send-logs-http
image_urls: 
tracking_id: docs-userguide-send-logs-http-construct-the-curl-request
group_tracking_ids: docs-userguide-send-logs-http
<p>Sending Logs to SigNoz over HTTP: Send logs to SigNoz Cloud: ## Send logs to SigNoz Cloud - Construct the cURL request: Construct the cURL request: Construct the cURL request</p>
<p>You can use cURL to send your logs. Below is a sample cURL request which is used to send a JSON-formatted log record to a Signoz cloud ingestion endpoint for logging:</p>
<pre><code>curl --location 'https://ingest.&lt;REGION&gt;.signoz.cloud:443/logs/json' \
--header 'Content-Type: application/json' \
--header 'signoz-access-token: &lt;SIGNOZ_INGESTION_KEY&gt;' \
--data '[\
    {\
        &quot;trace_id&quot;: &quot;000000000000000018c51935df0b93b9&quot;,\
        &quot;span_id&quot;: &quot;18c51935df0b93b9&quot;,\
        &quot;trace_flags&quot;: 0,\
        &quot;severity_text&quot;: &quot;info&quot;,\
        &quot;severity_number&quot;: 4,\
        &quot;attributes&quot;: {\
            &quot;method&quot;: &quot;GET&quot;,\
            &quot;path&quot;: &quot;/api/users&quot;\
        },\
        &quot;resources&quot;: {\
            &quot;host&quot;: &quot;myhost&quot;,\
            &quot;namespace&quot;: &quot;prod&quot;\
        },\
        &quot;message&quot;: &quot;This is a log line&quot;\
    }\
]'
</code></pre>
<p><code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> is the API token provided by SigNoz. You can find your ingestion key from SigNoz cloud account details sent on your email.</p>
<p><code>&lt;REGION&gt;</code> is the name of the region.</p>
<p>Depending on the choice of your region for SigNoz Cloud, the OTLP endpoint will vary according to the table below:</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>To include a specific timestamp in your log, be sure to incorporate the <code>timestamp</code> field in your cURL request. If timestamp field is not mentioned, then it will take the timestamp when the log record was sent. For instance:</p>
<pre><code>  curl --location 'https://ingest.&lt;REGION&gt;.signoz.cloud:443/logs/json' \
  --header 'Content-Type: application/json' \
  --header 'signoz-access-token: &lt;SIGNOZ_INGESTION_KEY&gt;' \
  --data '[\
      {\
      &quot;timestamp&quot;: 1698310066000000000, \
      &quot;trace_id&quot;: &quot;000000000000000018c51935df0b93b9&quot;, \
      ...\
\
</code></pre>
<p>\</p>
<h3><a href="#verfiy-your-request"></a>\</h3>
<p>Verfiy your request<br />
<br />
Once you run the above cURL request, you should be able to see it in SigNoz UI.<br />
<br />
<img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fhttp-log.webp&amp;w=3840&amp;q=75" alt="JSON Data in log body" /><br />
<br />
<a href="#send-logs-to-self-hosted-signoz"></a><br />
Send logs to Self-Hosted SigNoz<br />
--------------------------------------------------------------------<br />
\</p>
<h3><a href="#install-signoz"></a>\</h3>
<p>Install SigNoz<br />
<br />
Follow <a href="https://signoz.io/docs/install/">this link</a><br />
for instructions on how to install self-hosted signoz. Once you're done installing SigNoz, follow the steps below.<br />
\</p>
<h3><a href="#expose-port"></a>\</h3>
<p>Expose Port<br />
<br />
Inside the <code>deploy/docker/clickhouse-setup</code> directory of your SigNoz self-hosted installation, you will find <code>docker-compose.yaml</code> file which you should modify to expose a port, in this case <code>8082</code>. Below is a code snippet of the modified file.<br />
<br />
...<br />
otel-collector:<br />
image: signoz/signoz-otel-collector:0.88.11<br />
command: [&quot;--config=/etc/otel-collector-config.yaml&quot;]<br />
volumes:<br />
- ./otel-collector-config.yaml:/etc/otel-collector-config.yaml<br />
ports:<br />
- &quot;8082:8082&quot;<br />
...<br />
<br />
\</p>
<h3><a href="#add-and-include-receiver"></a>\</h3>
<p>Add and include receiver<br />
<br />
Inside the <code>deploy/docker/clickhouse-setup</code> directory of your SigNoz self-hosted installation, you will find <code>otel-collector-config.yaml</code> file which you should modify to add <code>httplogreceiver</code>.<br />
<br />
receivers:<br />
httplogreceiver/json:<br />
endpoint: 0.0.0.0:8082<br />
source: json<br />
...<br />
<br />
<br />
Next modify the pipeline section inside <code>otel-collector-config.yaml</code> to include the <code>httplogreceiver</code> we have created above.<br />
<br />
service:<br />
....<br />
logs:<br />
receivers: [otlp, httplogreceiver/json]<br />
processors: [batch]<br />
exporters: [clickhouselogsexporter]<br />
<br />
<br />
Now we can restart the <strong>otel collector container</strong> so that new changes are applied and we can send our logs to port <code>8082</code>.<br />
\</p>
<h3><a href="#construct-the-curl-request-1"></a>\</h3>
<p>Construct the cURL request<br />
<br />
You can use cURL to send your logs. Below is a sample cURL request which is used to send a JSON-formatted log record to SigNoz for logging:<br />
<br />
curl --location 'http://&lt;IP&gt;:8082' \
--header 'Content-Type: application/json' \
--data '[<br />
{<br />
&quot;trace_id&quot;: &quot;000000000000000018c51935df0b93b9&quot;,<br />
&quot;span_id&quot;: &quot;18c51935df0b93b9&quot;,<br />
&quot;trace_flags&quot;: 0,<br />
&quot;severity_text&quot;: &quot;info&quot;,<br />
&quot;severity_number&quot;: 4,<br />
&quot;attributes&quot;: {<br />
&quot;method&quot;: &quot;GET&quot;,<br />
&quot;path&quot;: &quot;/api/users&quot;<br />
},<br />
&quot;resources&quot;: {<br />
&quot;host&quot;: &quot;myhost&quot;,<br />
&quot;namespace&quot;: &quot;prod&quot;<br />
},<br />
&quot;message&quot;: &quot;This is a log line&quot;<br />
}<br />
]'<br />
<br />
<br />
<code>&lt;IP&gt;</code> is the IP of the system where your collector is running.<br />
<br />
To know more details about <code>&lt;IP&gt;</code>, checkout this <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">troubleshooting</a><br />
.<br />
<br />
üìù Note<br />
<br />
To include a specific timestamp in your log, be sure to incorporate the <code>timestamp</code> field in your cURL request. If timestamp field is not mentioned, then it will take the timestamp when the log record was sent. For instance:<br />
<br />
curl --location 'https://ingest.&lt;REGION&gt;.signoz.cloud:443/logs/json' \
--header 'Content-Type: application/json' \
--header 'signoz-access-token: &lt;SIGNOZ_INGESTION_KEY&gt;' \
--data '[<br />
{<br />
&quot;timestamp&quot;: 1698310066000000000, <br />
&quot;trace_id&quot;: &quot;000000000000000018c51935df0b93b9&quot;, <br />
...<br />
<br />
\</p>
<h3><a href="#verfiy-your-request-1"></a>\</h3>
<p>Verfiy your request<br />
<br />
Once you run the above cURL request, you should be able to see it in SigNoz UI.<br />
<br />
<img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fhttp-log.webp&amp;w=3840&amp;q=75" alt="test" /><br />
\</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/span-details/
tag_set: userguide, span-details
image_urls: 
tracking_id: docs-userguide-span-details
group_tracking_ids: docs-userguide-span-details
<h2>Span Details</h2>
<p>On the <strong>Span Details</strong> page, SigNoz displays all the spans associated with the current trace. Spans are displayed in two ways:</p>
<ul>
<li>As a flame graph. For an introduction to flame graphs, see <a href="https://www.brendangregg.com/flamegraphs.html">Brendan Gregg‚Äôs page</a>
.</li>
<li>As Gantt chart. This helps you visualize your spans as a parent-child tree.</li>
</ul>
<p>For each span, SigNoz shows the following details:</p>
<ul>
<li>The name of the operation</li>
<li>The name of the service</li>
<li>Start time</li>
<li>Duration</li>
<li>The list of tags associated with the currently selected span.</li>
<li>The list of events associated with the currently selected span. If your application emits events to complement traces, SigNoz will display them on this page. For details about raising events in your code, see the <a href="https://opentelemetry.io/docs/concepts/instrumenting-library/#events">Events</a> section of the OpenTelemetry website</li>
</ul>
<p>The following illustration shows the <strong>Span Details</strong> page:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fspan-details-page-v0.7.1.webp&amp;w=3840&amp;q=75" alt="Screenshot showing the span details page" /></p>
<p><strong>Legend</strong>:</p>
<ol>
<li><strong>Trace ID</strong>: At the top of the page, SigNoz displays the ID of the currently selected trace.</li>
<li><strong>Flame Graph</strong>: Shows the flame graph.</li>
<li><strong>Time</strong>: Displays the start time and duration of the currently selected trace.</li>
<li><strong>Focus</strong>: Allows you to focus on a specific span.</li>
<li><strong>Main content area</strong>: Displays all the spans as a tree structure. You can expand or collapse individual nodes in the tree to show or hide its children nodes. For each node, SigNoz displays the number of children nodes.</li>
<li><strong>Span Details</strong>: Displays the tags and events for the currently selected span.</li>
</ol>
<h2>View Details About a Span</h2>
<hr />
<p>To view details about a span, select it either in the flame graph or the main content area.</p>
<h2>Focus on a Specific Span</h2>
<hr />
<p>Select the <strong>Focus on selected span</strong> button to view only the currently selected span. In the following example, you can see how to:</p>
<ol>
<li>Select a span</li>
<li>Focus on that span</li>
<li>View all the spans again</li>
</ol>
<h2>Identify Spans with Errors</h2>
<hr />
<p>On the <strong>Span Details</strong> page SigNoz highlights in red all spans with errors. In the following example screenshot, you can see two spans with errors:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fidentify-span-with-errors-v0.7.1.webp&amp;w=3840&amp;q=75" alt="Screenshot showing a span with errors" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/ecs-fargate/
tag_set: aws-monitoring, ecs-fargate
image_urls: 
tracking_id: docs-aws-monitoring-ecs-fargate
group_tracking_ids: docs-aws-monitoring-ecs-fargate
<h2>Monitor your ECS Fargate launch type</h2>
<p>To monitor your ECS Fargate service, check out <a href="https://signoz.io/docs/userguide/collecting-ecs-sidecar-infra/">these detailed docs</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/table/
tag_set: dashboards, panel-types, table
image_urls: https://signoz.io/img/docs/dashboards/panel-types/table.png
tracking_id: docs-dashboards-panel-types-table
group_tracking_ids: docs-dashboards-panel-types-table
<h2>Table Panel Type</h2>
<h2>Table</h2>
<p>================</p>
<p>A Table is a plot chart that shows the data in a table.</p>
<h2>Data Formats</h2>
<hr />
<h3>## Supported signals</h3>
<ul>
<li>Logs</li>
<li>Traces</li>
<li>Metrics</li>
</ul>
<p>This panel type supports any time series data. The time series data can be from logs, traces, or metrics.</p>
<h3>## Examples</h3>
<p>The following graph shows the average requests per second (req/s) for a service over a period of time in a table.</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/table.png" alt="Table for service requests" /></p>
<p><em>Table for service requests.</em></p>
<h2>Configuration</h2>
<hr />
<h3>## Column Units</h3>
<p>When you want to format the column values in human readable format, you can use the column units. The column units are the units of the column values.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/application-monitoring/api-monitoring/
tag_set: application-monitoring, api-monitoring
image_urls: 
tracking_id: docs-application-monitoring-api-monitoring
group_tracking_ids: docs-application-monitoring-api-monitoring
<h2>Monitoring APIs in SigNoz</h2>
<h3>## Key Operations Section in Service Page</h3>
<p>Get details on API performance over time and filter based on different facets</p>
<h3>## Create Dashboards for Monitoring your Application on SigNoz</h3>
<p>Monitor key metrics like P95, p99 latecies and number of requests made for APIs of a service over time</p>
<p>Pre-built <a href="https://github.com/SigNoz/dashboards/tree/main/key-operations">dashboards</a> for monitoring key APIs</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#prerequisites
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-prerequisites
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Configure PagerDuty Channel - Prerequisites</h2>
<p>Before setting up PagerDuty as a notification channel in SigNoz, ensure the following:</p>
<ul>
<li><strong>Valid Integration Key</strong>: You have a valid Integration Key (also known as Routing Key) from PagerDuty.</li>
<li><strong>SigNoz Version</strong>: <a href="https://github.com/SigNoz/signoz/releases/tag/v0.8.0">v0.8.0</a> or later</li>
</ul>
<p>There are two main ways to integrate with PagerDuty:</p>
<ul>
<li>**Global <a href="https://support.pagerduty.com/docs/event-orchestration">Event Orchestration</a>
**: Ideal for automating incident creation and management.</li>
<li>**Direct Integration on <a href="https://support.pagerduty.com/docs/services-and-integrations">PagerDuty service</a>
**: Suitable for specific service-level integrations.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#accessing-alert-channels
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-accessing-alert-channels
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Accessing Alert Channels</h2>
<p>To manage your alert channels in SigNoz:</p>
<ul>
<li>
<p>Navigate to <code>Settings &gt; Alert Channels</code> tab within SigNoz. This tab displays a list of configured alert channels.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falert-channels.webp&amp;w=3840&amp;q=75" alt="alert-channels" /></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#obtaining-integration-or-routing-key
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-obtaining-integration-or-routing-key
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Obtaining Integration or Routing key</h2>
<h3>## For Global Event Orchestration</h3>
<ol>
<li>From the <strong>Automation</strong> menu, select <strong>Event Orchestration</strong></li>
<li>Create a new orchestration</li>
<li>Click on <strong>Global Orchestration Key</strong> and copy your <strong>integration key</strong> and keep it safe for later use.</li>
</ol>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-1.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<h3>## For PagerDuty Service Integration</h3>
<ol>
<li>Navigate to <strong>Services &gt; Service Directory</strong> and select the <strong>service</strong> where you‚Äôd like to add the integration.</li>
<li>Select <strong>Integration tab</strong> and click <strong>Add another integration</strong></li>
<li>Select <strong>Events API V2</strong> from the list</li>
<li>Click <strong>Add</strong></li>
<li>Locate your integration in the list and click down arrow to view and copy integration key.</li>
</ol>
<p>For more details on PagerDuty service setup, visit <a href="https://support.pagerduty.com/docs/services-and-integrations#add-integrations-to-an-existing-service">PagerDuty Service and Integrations</a>.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-2.webp&amp;w=1920&amp;q=75" alt="image" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#create-a-new-pagerduty-channel
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-create-a-new-pagerduty-channel
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Create a New PagerDuty Channel</h2>
<p>To create a new PagerDuty notification channel in SigNoz, follow these steps:</p>
<ul>
<li>Navigate to <code>Settings &gt; Alert Channels</code> and click on <code>New Channel</code>.</li>
<li>Enter a <strong>Name</strong> for the channel and select PagerDuty as the channel type.</li>
<li><strong>Routing Key</strong>: Enter Routing Key (Integration Key) obtained from PagerDuty.</li>
<li><strong>More information</strong>: Enter more information as necessary. Refer to the <a href="https://developer.pagerduty.com/docs/ZG9jOjExMDI5NTgw-events-api-v2-overview">Events API V2 Overview</a> for more details.</li>
</ul>
<p>üìù Note</p>
<p>You can use <a href="https://prometheus.io/docs/alerting/latest/notifications/">Go templates</a> for customizing the title and description.</p>
<p><strong>Test Configuration</strong>: Click the Test button to test the connection with your application.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-3.webp&amp;w=3840&amp;q=75" alt="image" /></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#testing-the-pagerduty-channel
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-testing-the-pagerduty-channel
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Testing the PagerDuty channel</h2>
<ol>
<li>
<p>Let's create a simple alert rule that monitors average CPU performance for each host. Go to <strong>Alerts</strong> page in <strong>your SigNoz app</strong> and click <code>New Alert</code> button. When the new alert page opens, edit metric query as shown below. Feel free to choose any other metric, the idea is to pick a metric with sufficient data to raise an alert.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-4.webp&amp;w=1920&amp;q=75" alt="image" /></p>
</li>
<li>
<p>We can now <strong>review the graph</strong> to identify a threshold that will definitely cause an alert. Here, anything below 0.2 looks like a good condition for threshold.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-5.webp&amp;w=1920&amp;q=75" alt="image" /></p>
</li>
<li>
<p>Let's <strong>set threshold to 0.12</strong> to be sure that alert will be raised in next few minutes.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-6.webp&amp;w=1920&amp;q=75" alt="image" /></p>
</li>
<li>
<p><strong>Save the alert</strong> rule. Feel free to edit severity and labels as necessary.</p>
</li>
<li>
<p>Go to your <strong>PagerDuty Alerts Dashboard</strong> (<code>PagerDuty Home &gt;&gt; Incident &gt;&gt; Alerts</code>) and wait for a few minutes. If all goes well, you will <strong>see an incident</strong>. You may have to refresh the page few times to see the alert.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-7.webp&amp;w=3840&amp;q=75" alt="image" /></p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Falerts%2Fpagerduty-8.webp&amp;w=1920&amp;q=75" alt="image" /></p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/alerts-management/notification-channel/pagerduty/#troubleshooting
tag_set: alerts-management, notification-channel, pagerduty
image_urls: 
tracking_id: docs-alerts-management-notification-channel-pagerduty-troubleshooting
group_tracking_ids: docs-alerts-management-notification-channel-pagerduty
<h2>Configure PagerDuty Channel: Troubleshooting</h2>
<p>If you encounter issues with the PagerDuty integration:</p>
<ul>
<li><strong>Check the Integration Key</strong>: Ensure that the Routing Key or Integration Key is correctly entered in SigNoz.</li>
<li><strong>Verify PagerDuty Configuration</strong>: Confirm that the PagerDuty service or global event orchestration is set up correctly.</li>
<li><strong>Test Connectivity</strong>: Use the Test button in SigNoz to verify connectivity with PagerDuty. If the test fails, review your network settings and integration key.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/dashboards/panel-types/timeseries/
tag_set: dashboards, panel-types, timeseries
image_urls: https://signoz.io/img/docs/dashboards/panel-types/line-chart.png
tracking_id: docs-dashboards-panel-types-timeseries
group_tracking_ids: docs-dashboards-panel-types-timeseries
<h2>Timeseries Panel Type</h2>
<h2>Timeseries Chart</h2>
<p>======================================</p>
<p>A Timeseries chart is a plot chart that shows trends over time. For example, you can show the memory usage over time, the number of requests over time, etc.</p>
<h2>Data Formats</h2>
<hr />
<h3>## Supported signals</h3>
<ul>
<li>Logs</li>
<li>Traces</li>
<li>Metrics</li>
</ul>
<p>This panel type supports any time series data. The time series data can be from logs, traces, or metrics.</p>
<h3>## Examples</h3>
<p>The following graph shows the requests per second (req/s) for a service over a period of time in line chart.</p>
<p><img src="https://signoz.io/img/docs/dashboards/panel-types/line-chart.png" alt="Line chart for req/s over period of time" /></p>
<p><em>Line chart for req/s over period of time.</em></p>
<h2>Configuration</h2>
<hr />
<h3>## Fill gaps</h3>
<p>The <code>Fill Gaps</code> option fills the gaps in the result data with zero. This is useful when you want to interpret the no data as zero. It is more appropriate for when the data is sparse. For example, if the result of a query for 10 minutes is <code>{t1: 12, t3: 21, t5: 42, t7:29}</code>, the <code>Fill Gaps</code> option will result in <code>{t1: 12, t2: 0, t3: 21, t4: 0, t5: 42, t6: 0, t7: 29, t8: 0, t9: 0, t10: 0}</code>.</p>
<h3>## Y-axis Unit</h3>
<p>The unit of the y-axis. The default unit is <code>None</code>.</p>
<h3>## Soft Min Max</h3>
<p>The soft min max is used to adjust the y-axis scale for better visualization. By default, the soft min max is disabled and the y-axis range will be auto-adjusted based on the data. If the soft min max is enabled, the y-axis range will be adjusted to use the soft min max values. It is particularly useful when you want to prevent small values in the result from being magnified too much.</p>
<h3>## Thresholds</h3>
<p>Thresholds are used to draw a line on the y-axis to highlight the value. The thresholds are defined as a list of tuples. Each tuple contains the value and the color. The color is optional and if not provided, the default color will be used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#table-schema-definitions--examples-for-metrics
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-table-schema-definitions-examples-for-metrics
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<h2>Writing a Metrics ClickHouse Query: Writing a Metrics ClickHouse Query - Table schema definitions &amp; examples for metrics</h2>
<p>There are four main tables in the database. One is for storing the samples/measurements and the rest are for storing the time series data.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#schema-for-samples-table
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-schema-for-samples-table
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<h2>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for samples table:</h2>
<p>üìù Note</p>
<p>The schemas are not final. We might change it in the future.</p>
<pre><code>CREATE TABLE signoz_metrics.distributed_samples_v4
(
    `env` LowCardinality(String) DEFAULT 'default',
    `temporality` LowCardinality(String) DEFAULT 'Unspecified',
    `metric_name` LowCardinality(String),
    `fingerprint` UInt64 CODEC(Delta(8), ZSTD(1)),
    `unix_milli` Int64 CODEC(DoubleDelta, ZSTD(1)),
    `value` Float64 CODEC(Gorilla, ZSTD(1))
)
ENGINE = Distributed('cluster', 'signoz_metrics', 'samples_v4', cityHash64(env, temporality, metric_name, fingerprint))
</code></pre>
<p>Explanation of the columns:</p>
<ul>
<li><strong>env</strong>: The <code>deployment.environment</code> label value. This is used to identify the environment in which the metric was observed. This is used to filter the metrics based on the environment.</li>
<li><strong>temporality</strong>: Temporality of the metric. This is used to identify the type of the metric. It can be one of the following values:
<ul>
<li>Unspecified: This is used for gauge metrics.</li>
<li>Cumulative: This is used for monotonic counters.</li>
<li>Delta: This is used for counters that send the delta values.</li>
</ul>
</li>
<li><strong>metric_name</strong>: Name of the metric</li>
<li><strong>fingerprint</strong>: Fingerprint of the metric. This is used to identify the metric uniquely. Currently, we are using the hash of the labels to generate the fingerprint.</li>
<li><strong>unix_milli</strong>: Timestamp in milliseconds when the metric was observed.</li>
<li><strong>value</strong>: Value of the metric</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#example-of-a-samples
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-example-of-a-samples
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Example of a samples</p>
<ol>
<li>
<p>Sample data for Counter metric <code>signoz_calls_total</code>.</p>
<pre><code>‚îå‚îÄenv‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄtemporality‚îÄ‚î¨‚îÄmetric_name‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄfingerprint‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄunix_milli‚îÄ‚î¨‚îÄvalue‚îÄ‚îê
</code></pre>
<ol>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354263778 ‚îÇ   560 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354323779 ‚îÇ  1140 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354383778 ‚îÇ  1669 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354443779 ‚îÇ  2180 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354503779 ‚îÇ  2745 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354563778 ‚îÇ  3300 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354623779 ‚îÇ  3870 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354683778 ‚îÇ  4460 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354743778 ‚îÇ  5030 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_calls_total ‚îÇ 826125025100650961 ‚îÇ 1715354803779 ‚îÇ  5590 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</li>
</ol>
</li>
<li>
<p>Sample data for Gauge metric <code>otelcol_exporter_queue_size</code></p>
<pre><code>‚îå‚îÄenv‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄtemporality‚îÄ‚î¨‚îÄmetric_name‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄfingerprint‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄunix_milli‚îÄ‚î¨‚îÄvalue‚îÄ‚îê
</code></pre>
<ol>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558433378 ‚îÇ     0 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558493378 ‚îÇ    20 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558553378 ‚îÇ     5 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558613378 ‚îÇ    13 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558673378 ‚îÇ     0 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558733378 ‚îÇ    12 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558793378 ‚îÇ     0 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558853378 ‚îÇ    14 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558913378 ‚îÇ     0 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Unspecified ‚îÇ otelcol_exporter_queue_size ‚îÇ 8219912421067840979 ‚îÇ 1715558973378 ‚îÇ     6 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</li>
</ol>
</li>
<li>
<p>Sample data for Histogram metrics <code>signoz_latency_bucket</code></p>
<pre><code>‚îå‚îÄenv‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄtemporality‚îÄ‚î¨‚îÄmetric_name‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄfingerprint‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄunix_milli‚îÄ‚î¨‚îÄvalue‚îÄ‚îê
</code></pre>
<ol>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764440740 ‚îÇ   145 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764500740 ‚îÇ   280 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764560740 ‚îÇ   412 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764620740 ‚îÇ   567 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764680740 ‚îÇ   715 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764740740 ‚îÇ   855 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764800740 ‚îÇ   992 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764860740 ‚îÇ  1135 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764920740 ‚îÇ  1265 ‚îÇ</li>
<li>‚îÇ default ‚îÇ Cumulative  ‚îÇ signoz_latency_bucket ‚îÇ 81477507584972766 ‚îÇ 1715764980742 ‚îÇ  1397 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</li>
</ol>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#schema-for-time-series-tables
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-schema-for-time-series-tables
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<h2>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Schema for time series tables:</h2>
<p>There are three time series tables, namely <code>time_series_v4</code>, <code>time_series_v4_6hrs</code>, <code>time_series_v4_1day</code>. The time series tables are used to store the label set that is used to identify the metric. See the description of the column <code>unix_milli</code> for the explanation of the granularity of the time series tables.</p>
<pre><code>CREATE TABLE signoz_metrics.distributed_time_series_v4
(
    `env` LowCardinality(String) DEFAULT 'default',
    `temporality` LowCardinality(String) DEFAULT 'Unspecified',
    `metric_name` LowCardinality(String),
    `description` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `unit` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `type` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `is_monotonic` Bool DEFAULT false CODEC(ZSTD(1)),
    `fingerprint` UInt64 CODEC(Delta(8), ZSTD(1)),
    `unix_milli` Int64 CODEC(Delta(8), ZSTD(1)),
    `labels` String CODEC(ZSTD(5))
)
ENGINE = Distributed('cluster', 'signoz_metrics', 'time_series_v4', cityHash64(env, temporality, metric_name, fingerprint))


CREATE TABLE signoz_metrics.distributed_time_series_v4_6hrs
(
    `env` LowCardinality(String) DEFAULT 'default',
    `temporality` LowCardinality(String) DEFAULT 'Unspecified',
    `metric_name` LowCardinality(String),
    `description` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `unit` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `type` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `is_monotonic` Bool DEFAULT false CODEC(ZSTD(1)),
    `fingerprint` UInt64 CODEC(Delta(8), ZSTD(1)),
    `unix_milli` Int64 CODEC(Delta(8), ZSTD(1)),
    `labels` String CODEC(ZSTD(5))
)
ENGINE = Distributed('cluster', 'signoz_metrics', 'time_series_v4_6hrs', cityHash64(env, temporality, metric_name, fingerprint))

CREATE TABLE signoz_metrics.distributed_time_series_v4_1day
(
    `env` LowCardinality(String) DEFAULT 'default',
    `temporality` LowCardinality(String) DEFAULT 'Unspecified',
    `metric_name` LowCardinality(String),
    `description` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `unit` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `type` LowCardinality(String) DEFAULT '' CODEC(ZSTD(1)),
    `is_monotonic` Bool DEFAULT false CODEC(ZSTD(1)),
    `fingerprint` UInt64 CODEC(Delta(8), ZSTD(1)),
    `unix_milli` Int64 CODEC(Delta(8), ZSTD(1)),
    `labels` String CODEC(ZSTD(5))
)
ENGINE = Distributed('cluster', 'signoz_metrics', 'time_series_v4_1day', cityHash64(env, temporality, metric_name, fingerprint))
</code></pre>
<p>Explanation of the columns:</p>
<ul>
<li><strong>env</strong>: The <code>deployment.environment</code> label value. This is used to identify the environment in which the metric was observed. This is used to filter the metrics based on the environment.</li>
<li><strong>temporality</strong>: Temporality of the metric. This is used to identify the type of the metric. It can be one of the following values:
<ul>
<li>Unspecified: This is the default value and used for gauge metrics.</li>
<li>Cumulative: This is used for monotonic counters.</li>
<li>Delta: This is used for counters that send the delta values.</li>
</ul>
</li>
<li><strong>metric_name</strong>: Name of the metric.</li>
<li><strong>description</strong>: Description of the metric.</li>
<li><strong>unit</strong>: Unit of the metric.</li>
<li><strong>type</strong>: Type of the metric. One of the following values:
<ul>
<li>Sum: This is used for monotonic counters.</li>
<li>Gauge: This is used for gauge metrics.</li>
<li>Histogram: This is used for histogram metrics.</li>
<li>ExponentialHistogram: This is used for exponential histogram metrics.</li>
</ul>
</li>
<li><strong>is_monotonic</strong>: This is used to identify if the metric is a monotonic counter. This is used to identify the type of the metric.</li>
<li><strong>fingerprint</strong>: Fingerprint of the metric. This is used to identify the metric uniquely. Currently, we are using the hash of the labels to generate the fingerprint.</li>
<li><strong>unix_milli</strong>: Dependent on the granularity of the time series table. The <code>time_series_v4</code> table has a granularity of 1 hour. The <code>time_series_v4_6hrs</code> table has a granularity of 6 hours. The <code>time_series_v4_1day</code> table has a granularity of 1 day. The value is the start of the time interval in milliseconds. Example: If the granularity is 1 hour, then the value will be the start of the hour in milliseconds. Why do we do this? If we were to store the timestamp of the metric, then we would have to store the timestamp for each sample. This would increase the size of the table and slow down the queries. By storing the start of the time interval, we try to balance the ability to know the most recent appearance of the metric and the speed of the queries. Even then, the scans on the time series table can be slow if we are looking for a week's worth of data. Hence, we have additional tables with different granularities to speed up the queries.</li>
<li><strong>labels</strong>: Labels of the metric; Stored as a JSON string.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#example-of-a-time-series
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-example-of-a-time-series
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Schema for time series tables:: Example of a time series</p>
<pre><code>env:          default
temporality:  Cumulative
metric_name:  signoz_calls_total
description:
unit:
type:         Sum
is_monotonic: true
fingerprint:  1918473123734562099
unix_milli:   1715385600000
labels:       {&quot;__name__&quot;:&quot;signoz_calls_total&quot;,&quot;__temporality__&quot;:&quot;Cumulative&quot;,&quot;deployment_environment&quot;:&quot;default&quot;,&quot;operation&quot;:&quot;FindDriverIDs&quot;,&quot;resource_signoz_collector_id&quot;:&quot;b2f22d66-0851-44f2-9104-2292189958d1&quot;,&quot;service_name&quot;:&quot;redis&quot;,&quot;service_namespace&quot;:&quot;default&quot;,&quot;signoz_collector_id&quot;:&quot;b2f22d66-0851-44f2-9104-2292189958d1&quot;,&quot;span_kind&quot;:&quot;SPAN_KIND_CLIENT&quot;,&quot;status_code&quot;:&quot;STATUS_CODE_UNSET&quot;}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#querying-the-metrics
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-querying-the-metrics
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<h2>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: Querying the metrics</h2>
<p>Querying the metrics is done in two steps. First, we query the time series table to get the fingerprints and labels of the metrics. Then, we use the fingerprints to query the samples table to get the actual samples. This is done to reduce the amount of data that needs to be scanned. Also, the time series table is much smaller than the samples table.</p>
<p>We use the JOIN operation to join the two tables. The JOIN operation is done on the fingerprint column. The JOIN operation is done on the cluster level. This means that the JOIN operation is done on the data nodes and not on the aggregator nodes. This is done to reduce the amount of data that needs to be transferred between the nodes. This works because the data is sharded based on the <code>env</code>, <code>temporality</code>, <code>metric_name</code>, <code>fingerprint</code> columns hence all the data for a particular metric and fingerprint will be present on the same node.</p>
<p>üìù Note</p>
<p>The queries should have a result column with the name <code>value</code> and a column with type <a href="https://clickhouse.com/docs/en/sql-reference/data-types/datetime">DateTime</a> for the graphs to work.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#retrieving-the-fingerprints-and-labels-of-the-metric
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-retrieving-the-fingerprints-and-labels-of-the-metric
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: ### ## Example queries - Retrieving the fingerprints and labels of the metric</p>
<pre><code>SELECT DISTINCT
    fingerprint,
    labels
FROM signoz_metrics.distributed_time_series_v4
WHERE (metric_name = 'signoz_calls_total') AND (temporality = 'Cumulative')
LIMIT 10
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#retrieving-the-samples-of-the-metric
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-retrieving-the-samples-of-the-metric
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: Retrieving the samples of the metric</p>
<pre><code>SELECT
    timestamp_ms,
    value
FROM signoz_metrics.distributed_samples_v4
WHERE metric_name = 'signoz_calls_total'
LIMIT 10
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#retrieving-the-label-values-of-a-metric
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-retrieving-the-label-values-of-a-metric
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: Retrieving the label values of a metric</p>
<pre><code>SELECT DISTINCT
    fingerprint,
    JSONExtractString(labels, 'service_name') AS service_name
FROM signoz_metrics.distributed_time_series_v4_1day
WHERE (metric_name = 'signoz_calls_total') AND (temporality = 'Cumulative')
LIMIT 10
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#example-queries-for-the-frontend-service-red-metrics
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-example-queries-for-the-frontend-service-red-metrics
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: Example queries for the frontend service RED metrics</p>
<p>üìù Note</p>
<p>The following queries can be modified to get the metrics for each service by adding grouping by the service name.</p>
<p>The following queries are used to generate the graphs for the frontend service RED metrics. The innermost join is used to get the raw samples joined with the time series table. Then, we use the runningDifference function to get the rate of change of the samples. Then, we use the outermost query to get the sum of the rate of change of the samples.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#request-rate-for-a-service
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-request-rate-for-a-service
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: Request rate for a service</p>
<pre><code>SELECT
    ts,
    sum(per_series_value) AS value
FROM
(
    SELECT
        ts,
        If((per_series_value - lagInFrame(per_series_value, 1, 0) OVER rate_window) &lt; 0, nan, If((ts - lagInFrame(ts, 1, toDate('1970-01-01')) OVER rate_window) &gt;= 86400, nan, (per_series_value - lagInFrame(per_series_value, 1, 0) OVER rate_window) / (ts - lagInFrame(ts, 1, toDate('1970-01-01')) OVER rate_window))) AS per_series_value
    FROM
    (
        SELECT
            fingerprint,
            toStartOfInterval(toDateTime(intDiv(unix_milli, 1000)), toIntervalSecond(60)) AS ts,
            max(value) AS per_series_value
        FROM signoz_metrics.distributed_samples_v4
        INNER JOIN
        (
            SELECT DISTINCT fingerprint
            FROM signoz_metrics.time_series_v4_1day
            WHERE (metric_name = 'signoz_calls_total') AND (temporality = 'Cumulative') AND (unix_milli &gt;= intDiv({{.start_timestamp_ms}}, 86400000) * 86400000) AND (unix_milli &lt; {{.end_timestamp_ms}}) AND JSONExtractString(labels, 'service_name') = 'frontend'
        ) AS filtered_time_series USING (fingerprint)
        WHERE (metric_name = 'signoz_calls_total') AND (unix_milli &gt;= {{.start_timestamp_ms}}) AND (unix_milli &lt; {{.end_timestamp_ms}})
        GROUP BY
            fingerprint,
            ts
        ORDER BY
            fingerprint ASC,
            ts ASC
    )
    WINDOW rate_window AS (PARTITION BY fingerprint ORDER BY fingerprint ASC, ts ASC)
)
WHERE isNaN(per_series_value) = 0
GROUP BY ts
ORDER BY ts ASC
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#error-rate-for-a-service
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-error-rate-for-a-service
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: Error rate for a service</p>
<p>This is the query for the error rate for a service. The query is similar to the request rate query. The only difference is that we are filtering the samples based on the status code. Then, we divide the error samples by the total samples to get the error rate.</p>
<pre><code>SELECT
    A.ts AS ts,
    (A.value * 100) / B.value AS value
FROM
(
    SELECT
        ts,
        sum(value) AS value
    FROM
    (
        SELECT
            ts,
            if(runningDifference(ts) &lt;= 0, nan, if(runningDifference(value) &lt; 0, value / runningDifference(ts), runningDifference(value) / runningDifference(ts))) AS value
        FROM
        (
            SELECT
                fingerprint,
                toStartOfInterval(toDateTime(intDiv(unix_milli, 1000)), toIntervalSecond(60)) AS ts,
                max(value) AS value
            FROM signoz_metrics.distributed_samples_v4
            INNER JOIN
            (
                SELECT DISTINCT fingerprint
                FROM signoz_metrics.time_series_v4_1day
                WHERE (metric_name = 'signoz_calls_total') AND (temporality = 'Cumulative') AND (unix_milli &gt;= intDiv({{.start_timestamp_ms}}, 86400000) * 86400000) AND (unix_milli &lt; {{.end_timestamp_ms}}) AND JSONExtractString(labels, 'service_name') = 'redis' AND (JSONExtractString(labels, 'status_code') IN ['STATUS_CODE_ERROR'])
            ) AS filtered_time_series USING (fingerprint)
            WHERE (metric_name = 'signoz_calls_total') AND (unix_milli &gt;= {{.start_timestamp_ms}}) AND (unix_milli &lt;= {{.end_timestamp_ms}})
            GROUP BY
                fingerprint,
                ts
            ORDER BY
                fingerprint ASC,
                ts ASC
        )
        WHERE isNaN(value) = 0
    )
    GROUP BY ts
    ORDER BY ts ASC
) AS A
INNER JOIN
(
    SELECT
        ts,
        sum(value) AS value
    FROM
    (
        SELECT
            ts,
            if(runningDifference(ts) &lt;= 0, nan, if(runningDifference(value) &lt; 0, value / runningDifference(ts), runningDifference(value) / runningDifference(ts))) AS value
        FROM
        (
            SELECT
                fingerprint,
                toStartOfInterval(toDateTime(intDiv(unix_milli, 1000)), toIntervalSecond(60)) AS ts,
                max(value) AS value
            FROM signoz_metrics.distributed_samples_v4
            INNER JOIN
            (
                SELECT DISTINCT fingerprint
                FROM signoz_metrics.time_series_v4_1day
                WHERE (metric_name = 'signoz_calls_total') AND (temporality = 'Cumulative') AND (unix_milli &gt;= intDiv({{.start_timestamp_ms}}, 86400000) * 86400000) AND (unix_milli &lt; {{.end_timestamp_ms}}) AND JSONExtractString(labels, 'service_name') = 'redis'
            ) AS filtered_time_series USING (fingerprint)
            WHERE (metric_name = 'signoz_calls_total') AND (unix_milli &gt;= {{.start_timestamp_ms}}) AND (unix_milli &lt;= {{.end_timestamp_ms}})
            GROUP BY
                fingerprint,
                ts
            ORDER BY
                fingerprint ASC,
                ts ASC
        )
        WHERE isNaN(value) = 0
    )
    GROUP BY ts
    ORDER BY ts ASC
) AS B ON A.ts = B.ts
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#99th-percentile-latency-for-a-service
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-99th-percentile-latency-for-a-service
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<p>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: 99th percentile latency for a service</p>
<p>This is the query for the 99th percentile latency for a service. The query is similar to the request rate query. We are using the histogramQuantile function to get the 99th percentile latency. The histogramQuantile function is used to get the quantile value from a histogram. The histogram is represented as two arrays. One array contains the buckets and the other array contains the values. The histogramQuantile function takes the two arrays and the quantile value as input and returns the quantile value. The bucket bounds and values are obtained by taking the rate of change for each bucket.</p>
<pre><code>SELECT
    ts,
    histogramQuantile(arrayMap(x -&gt; toFloat64(x), groupArray(le)), groupArray(value), 0.99) AS value
FROM
(
    SELECT
        le,
        ts,
        sum(value) AS value
    FROM
    (
        SELECT
            le,
            ts,
            if(runningDifference(ts) &lt;= 0, nan, if(runningDifference(value) &lt; 0, value / runningDifference(ts), runningDifference(value) / runningDifference(ts))) AS value
        FROM
        (
            SELECT
                fingerprint,
                le,
                toStartOfInterval(toDateTime(intDiv(unix_milli, 1000)), toIntervalSecond(60)) AS ts,
                max(value) AS value
            FROM signoz_metrics.distributed_samples_v4
            INNER JOIN
            (
                SELECT DISTINCT
                    JSONExtractString(labels, 'le') AS le,
                    fingerprint
                FROM signoz_metrics.time_series_v4_1day
                WHERE (metric_name = 'signoz_latency_bucket') AND (temporality = 'Cumulative') AND (unix_milli &gt;= intDiv({{.start_timestamp_ms}}, 86400000) * 86400000) AND (unix_milli &lt; {{.end_timestamp_ms}}) AND JSONExtractString(labels, 'service_name') = 'frontend'
            ) AS filtered_time_series USING (fingerprint)
            WHERE (metric_name = 'signoz_latency_bucket') AND (unix_milli &gt;= {{.start_timestamp_ms}}) AND (unix_milli &lt;= {{.end_timestamp_ms}})
            GROUP BY
                fingerprint,
                le,
                ts
            ORDER BY
                fingerprint ASC,
                le ASC,
                ts ASC
        )
        WHERE isNaN(value) = 0
    )
    GROUP BY le, ts
    HAVING isNaN(value) = 0
    ORDER BY
        le ASC,
        ts ASC
)
GROUP BY ts
ORDER BY ts ASC
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#using-variables-in-queries
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-using-variables-in-queries
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<h2>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: Using variables in queries</h2>
<p>SigNoz supports using variables in queries. This allows you to create a single dashboard for multiple services. For example, you can create a dashboard for the request rate for all the services. Then, you can use the service name as a variable in the query to get the request rate for a particular service. Please refer to the <a href="/docs/userguide/manage-variables/">Variables</a> section for more information on how to create variables.</p>
<h3>## Example queries using variables</h3>
<h4>## Request rate for a service</h4>
<p>The variable <code>{{.service_name}}</code> is used to get the service name from the variable. The variable is replaced with the service name when the query is executed.</p>
<pre><code>SELECT
    ts,
    sum(per_series_value) AS value
FROM
(
    SELECT
        ts,
        If((per_series_value - lagInFrame(per_series_value, 1, 0) OVER rate_window) &lt; 0, nan, If((ts - lagInFrame(ts, 1, toDate('1970-01-01')) OVER rate_window) &gt;= 86400, nan, (per_series_value - lagInFrame(per_series_value, 1, 0) OVER rate_window) / (ts - lagInFrame(ts, 1, toDate('1970-01-01')) OVER rate_window))) AS per_series_value
    FROM
    (
        SELECT
            fingerprint,
            toStartOfInterval(toDateTime(intDiv(unix_milli, 1000)), toIntervalSecond(60)) AS ts,
            max(value) AS per_series_value
        FROM signoz_metrics.distributed_samples_v4
        INNER JOIN
        (
            SELECT DISTINCT fingerprint
            FROM signoz_metrics.time_series_v4_1day
            WHERE (metric_name = 'signoz_calls_total') AND (temporality = 'Cumulative') AND (unix_milli &gt;= intDiv({{.start_timestamp_ms}}, 86400000) * 86400000) AND (unix_milli &lt; {{.end_timestamp_ms}}) AND JSONExtractString(labels, 'service_name') = {{.service_name}}
        ) AS filtered_time_series USING (fingerprint)
        WHERE (metric_name = 'signoz_calls_total') AND (unix_milli &gt;= {{.start_timestamp_ms}}) AND (unix_milli &lt; {{.end_timestamp_ms}})
        GROUP BY
            fingerprint,
            ts
        ORDER BY
            fingerprint ASC,
            ts ASC
    )
    WINDOW rate_window AS (PARTITION BY fingerprint ORDER BY fingerprint ASC, ts ASC)
)
WHERE isNaN(per_series_value) = 0
GROUP BY ts
ORDER BY ts ASC
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/write-a-metrics-clickhouse-query/#using-the-default-variables
tag_set: userguide, write-a-metrics-clickhouse-query
image_urls: 
tracking_id: docs-userguide-write-a-metrics-clickhouse-query-using-the-default-variables
group_tracking_ids: docs-userguide-write-a-metrics-clickhouse-query
<h2>Writing a Metrics ClickHouse Query: Schema for samples table:: Schema for time series tables:: Querying the metrics: Using the default variables</h2>
<p>The following variables are available by default:</p>
<ul>
<li><code>{{.start_timestamp_ms}}</code> - This is the start time of the query in milliseconds</li>
<li><code>{{.end_timestamp_ms}}</code> - This is the end time of the query in milliseconds</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/data-ingestion/#overview
tag_set: azure-monitoring, bootstrapping, data-ingestion
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-data-ingestion-overview
group_tracking_ids: docs-azure-monitoring-bootstrapping-data-ingestion
<h2>Centralized Collector Setup: Centralized Collector Setup - Overview</h2>
<p>Azure Event Hubs is a big data streaming platform and event ingestion service that can receive and process millions of events per second. It is an ideal solution for centralizing logging and enabling real-time log streaming for applications running on Azure or on-premises.</p>
<p>By creating an Event Hubs namespace and an event hub, you can stream logs from various sources into a central location. This allows you to collect, store, and analyze logs efficiently, gaining valuable insights into your application's behavior and performance.</p>
<p>SigNoz, an open-source application performance monitoring and observability tool, can be integrated with Azure Event Hubs to provide a powerful log management solution. By streaming logs to SigNoz via Event Hubs, you can leverage SigNoz's features for log aggregation, querying, visualization, and alerting.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/data-ingestion/#prerequisites
tag_set: azure-monitoring, bootstrapping, data-ingestion
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-data-ingestion-prerequisites
group_tracking_ids: docs-azure-monitoring-bootstrapping-data-ingestion
<h2>Centralized Collector Setup: Prerequisites</h2>
<p>Before proceeding with the setup, ensure you have the following:</p>
<ul>
<li>An Azure subscription</li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/data-ingestion/#setup
tag_set: azure-monitoring, bootstrapping, data-ingestion
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-data-ingestion-setup
group_tracking_ids: docs-azure-monitoring-bootstrapping-data-ingestion
<h2>Centralized Collector Setup: Setup</h2>
<p>To set up an Event Hubs namespace and an event hub for log streaming, follow these steps: You might want to repeat these steps for other regions.</p>
<ol>
<li><strong>Create an Event Hubs namespace:</strong>
<ul>
<li>Sign in to the Azure portal (<a href="https://portal.azure.com/">https://portal.azure.com</a>
).</li>
<li>Click on &quot;Create a resource&quot; and search for &quot;Event Hubs&quot;.</li>
<li>Click on &quot;Event Hubs&quot; and then click &quot;Create&quot;.</li>
<li>Fill in the required details:
<ul>
<li>Resource group: Choose an existing resource group in the same region as the resources you want to monitor or create a new one.</li>
<li>Namespace name: Enter a unique name for your namespace, such as¬†<code>&lt;orgName&gt;-obs-signoz</code>.</li>
<li>Pricing tier: Choose the appropriate pricing tier based on your logging requirements.</li>
<li>Region: Ideally should be the same region as the resources you want to monitor.</li>
<li>Throughput units: The throughput units determine the maximum number of events that can be processed per second. You need to choose a value that is appropriate for your logging requirements.</li>
</ul>
</li>
<li>Click &quot;Review + create&quot; and then &quot;Create&quot; to provision the Event Hubs namespace.</li>
</ul>
</li>
<li><strong>Create an event hub:</strong>
<ul>
<li>Once the Event Hubs namespace is created, navigate to it in the Azure portal.</li>
<li>Click on &quot;Event Hubs&quot; in the left menu.</li>
<li>Click on &quot;+ Event Hub&quot; to create a new event hub.</li>
<li>Enter a name for your event hub, such as <code>logs</code>.</li>
<li>Click &quot;Create&quot; to create the event hub.</li>
</ul>
</li>
<li><strong>Create a SAS policy for the event hub and copy the connection string:</strong>
<ul>
<li>Navigate to the Event Hubs namespace in the Azure portal.</li>
<li>Search for &quot;Event Hubs&quot; in the left menu and click on it.</li>
<li>Click on the event hub you created.</li>
<li>Click on &quot;Shared access policies&quot; in the left menu.</li>
<li>Click on &quot;Add&quot; to create a new policy, name the policy as <code>signozListen</code></li>
<li>Select the &quot;Listen&quot; permission and specify the expiration time for the policy.</li>
<li>Click &quot;Save&quot; to create the policy.</li>
<li>Now, click on the created policy to copy the <em>Connection string‚Äìprimary key</em>. Note this connection string as you will need it to configure the Central Collector.</li>
</ul>
</li>
<li><strong>Configure OpenTelemetry integration:</strong>
<ul>
<li>
<p>Add a new receiver to <a href="../collector-setup">Central Collector Setup</a></p>
</li>
<li>
<p>Configure the receiver to receive logs from Azure Event Hubs.</p>
</li>
<li>
<p>Provide the necessary connection details, such as the Event Hubs namespace connection string and the event hub name.</p>
</li>
</ul>
</li>
<li><strong>Stream logs to Event Hubs:</strong>
<ul>
<li>Configure the diagnostic settings of your Azure services to forward logs to the Event Hub you created.</li>
<li>Ensure that the logs are sent in the <a href="https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/resource-logs-schema">Azure Common Log Format</a> for compatibility with OpenTelemetry Receiver.</li>
<li>Verify that logs are being successfully streamed to Event Hubs and received by SigNoz.</li>
</ul>
</li>
</ol>
<p>For more detailed instructions on creating an Event Hubs namespace and event hub, refer to the Azure documentation: <a href="https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-create">Azure Event Hub</a>.</p>
<p>For more configuration options for the receiver see the OpenTelemetry Documentation, <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/azureeventhubreceiver">Azure Event Hub Receiver</a></p>
<p>üìù Note</p>
<p>This is a beginner-friendly guide. If you are a more advanced user, you might want to use IaC (Infrastructure as Code) to automate the deployment of the Azure resources and the OpenTelemetry Collector.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/data-ingestion/#troubleshooting
tag_set: azure-monitoring, bootstrapping, data-ingestion
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-data-ingestion-troubleshooting
group_tracking_ids: docs-azure-monitoring-bootstrapping-data-ingestion
<h2>Centralized Collector Setup: Setup: Troubleshooting</h2>
<p>If you encounter any issues during the setup process, consider the following troubleshooting steps:</p>
<ul>
<li>Verify that you have the necessary permissions to create resources in your Azure subscription.</li>
<li>Double-check the connection details provided to SigNoz for the Event Hubs integration.</li>
<li>Ensure that your applications or logging agents are correctly configured to send logs to the specified Event Hubs namespace and event hub.</li>
<li>Check the Event Hubs metrics and logs in the Azure portal for any error messages or abnormalities.</li>
<li>Consult the SigNoz documentation or reach out to their support channels for further assistance.</li>
</ul>
<p>By following this guide and integrating Azure Event Hubs with SigNoz, you can establish a robust and scalable logging infrastructure. Centralizing your logs and enabling real-time streaming allows you to gain valuable insights, troubleshoot issues efficiently, and maintain the health and performance of your applications.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/
tag_set: azure-monitoring, bootstrapping
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping
group_tracking_ids: docs-azure-monitoring-bootstrapping
<h2>Bootstrapping</h2>
<ul>
<li><a href="/docs/azure-monitoring/bootstrapping/collector-setup">üìÑÔ∏è Central Collector Setup: Overview</a></li>
<li><a href="/docs/azure-monitoring/bootstrapping/data-ingestion/">üìÑÔ∏è EventHub Streaming Ingestion: Overview</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#overview
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-overview
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Central Collector Setup - Overview</h2>
<p>Setting up a centralised OpenTelemetry Collector offers several benefits for managing observability data in Azure environments. It allows for a clear separation between platform and application teams, reduces configuration overhead for application teams, and enables the collection of system metrics from Azure Monitor and observability data from Azure Event Hub.</p>
<p>This guide will walk you through the process of creating a centralized OpenTelemetry Collector using Azure Kubernetes Service (AKS) or an Azure Virtual Machine (VM).</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#prerequisites
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-prerequisites
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Prerequisites</h2>
<p>Before proceeding with the setup, ensure you have the following prerequisites:</p>
<ol>
<li>
<p>An active Azure subscription</p>
</li>
<li>
<p>Azure CLI installed on your local machine</p>
</li>
<li>
<p><a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#setting-up-the-opentelemetry-collector
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-setting-up-the-opentelemetry-collector
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Setting Up the OpenTelemetry Collector</h2>
<p>Deploying the OpenTelemetry Collector can be approached in two distinct ways, depending on your infrastructure:</p>
<ol>
<li><strong>Utilizing OpenTelemetry Helm Charts:</strong>¬†Ideal for those with an existing Kubernetes infrastructure.</li>
<li><strong>Running the Collector on a Virtual Machine:</strong>¬†For users not using Kubernetes and relying on serverless environments for their workloads.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#installing-with-opentelemetry-helm-charts
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: https://signoz.io/img/docs/azure-monitoring/service-principal-app-overview.webp
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-installing-with-opentelemetry-helm-charts
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<p>Central Collector Setup: Setup: ## Setup - Installing with OpenTelemetry Helm Charts</p>
<p>Prior to installation, you must ensure your Kubernetes cluster is ready and that you have the necessary permissions to deploy applications. Follow these steps to use Helm for setting up the Collector:</p>
<ol>
<li>
<p><strong>Add the OpenTelemetry Helm repository:</strong></p>
<pre><code>helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
</code></pre>
</li>
<li>
<p><strong>Prepare the <code>otel-collector-values.yaml</code> Configuration</strong></p>
<h4>##     Azure Event Hub Receiver Configuration</h4>
<p>If you haven't created the logs Event Hub, you can create one by following the steps in the <a href="../../bootstrapping/data-ingestion">Azure Event Hubs documentation</a>
.</p>
<p>and replace the placeholders <code>&lt;Primary Connection String&gt;</code> with the primary connection string for your Event Hub, it should look something like this:</p>
<pre><code>connection: Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName
</code></pre>
<p>The Event Hub docs have a step to create a SAS policy for the event hub and copy the connection string.</p>
<h4>##     Azure Monitor Receiver Configuration</h4>
<p>You will need to set up a <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal">service principal</a> with Read permissions to receive data from Azure Monitor.</p>
<ol>
<li>
<p>Follow the steps in the <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#register-an-application-with-microsoft-entra-id-and-create-a-service-principal">Create a service principal Azure Doc</a> documentation to create a service principal. You can name it <code>signoz-central-collector-app</code> the redirect URI can be empty.</p>
</li>
<li>
<p>To add read permissions to Azure Monitor, Follow the <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#assign-a-role-to-the-application">Assign Role</a> documentation. The read acess can be given to the full subscription.</p>
</li>
<li>
<p>There are multiple ways to authenticate the service principal, we will use the client secret option, follow <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#option-3-create-a-new-client-secret">Creating a client secret</a> and don't forget to copy the client secret. The secret is used in the configuration file as <code>client_secret</code>.</p>
</li>
<li>
<p>To find <code>client_id</code> and <code>tenant_id</code>, go to the <a href="https://portal.azure.com/">Azure Portal</a> and search for the <code>Application</code> you created. You would see the <code>Application (client) ID</code> and <code>Directory (tenant) ID</code> in the Overview section.</p>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/service-principal-app-overview.webp" alt="Application Overview" /></p>
<p>_</p>
<p>Application Overview</p>
<p>_</p>
<ol start="5">
<li>
<p>To find <code>subscription_id</code>, follow steps in <a href="https://learn.microsoft.com/en-us/azure/azure-portal/get-subscription-tenant-id#find-your-azure-subscription">Find Your Subscription</a> and populate them in the configuration file.</p>
</li>
<li>
<p>Ensure you replace the placeholders <code>&lt;region&gt;</code> and <code>&lt;ingestion-key&gt;</code> with the appropriate values for your signoz cloud instance.</p>
</li>
</ol>
</li>
</ol>
<p>Below is an example targeting the SigNoz backend with Azure Monitor receivers configured:</p>
<pre><code>service:
  pipelines:
    metrics/am:
      receivers: [azuremonitor]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp, azureeventhub]
      processors: [batch]
      exporters: [otlp]
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  azureeventhub:
    connection: Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName
    format: &quot;azure&quot;
  azuremonitor:
    subscription_id: &quot;&lt;Subscription ID&gt;&quot;
    tenant_id: &quot;&lt;AD Tenant ID&gt;&quot;
    client_id: &quot;&lt;Client ID&gt;&quot;
    client_secret: &quot;&lt;Client Secret&gt;&quot;
    resource_groups: [&quot;&lt;rg-1&gt;&quot;]
    collection_interval: 60s
processors:
  batch: {}
exporters:
  otlp:
    endpoint: &quot;ingest.&lt;region&gt;.signoz.cloud:443&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;&lt;ingestion-key&gt;&quot;
</code></pre>
<ol start="3">
<li>
<p><strong>Deploy the OpenTelemetry Collector to your Kubernetes cluster:</strong></p>
<p>You'll need to prepare a custom configuration file, say <code>otel-collector-values.yaml</code>, that matches your environment's specific needs. Replace <code>&lt;namespace&gt;</code> with the Kubernetes namespace where you wish to install the Collector.</p>
<pre><code>helm install -n &lt;namespace&gt; --create-namespace otel-collector open-telemetry/opentelemetry-collector -f otel-collector-values.yaml
</code></pre>
<p>For more detail, refer to the <a href="https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector">official OpenTelemetry Helm Chart documentation</a>
, which offers comprehensive installation instructions and configuration options tailored to your environment's requirements.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#running-the-collector-on-a-virtual-machine
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: https://signoz.io/img/docs/azure-monitoring/service-principal-app-overview.webp
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-running-the-collector-on-a-virtual-machine
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<p>Central Collector Setup: Setup: Running the Collector on a Virtual Machine: Running the Collector on a Virtual Machine: Running the Collector on a Virtual Machine</p>
<p>If you're not using Kubernetes, setting up the OpenTelemetry Collector on a Virtual Machine (VM) is a straightforward alternative. This setup is compatible with cloud VM instances, your own data center, or even a local VM on your development machine. Here's how to do it:</p>
<ol>
<li>
<p><strong>Download and Install the OpenTelemetry Collector Binary:</strong></p>
<p>Please visit <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Documentation For VM</a> which provides further guidance on a VM installation.</p>
<p>It's prudent to check available resources to ensure you're following the latest practices and utilizing updated features offered by OpenTelemetry. Follow the documentation to setup your collector and test the setup.</p>
</li>
<li>
<p><strong>Configure the OpenTelemetry Collector:</strong></p>
<p>The Collector requires configuration before it can start receiving Azure Monitor data. Create a file named <code>config.yaml</code> and populate it with the necessary configuration details. This file should be configured to fit your specific environment's requirements. For instance, you may want to setup receivers for the telemetry data you expect to collect and processors to handle that data appropriately, as well as exporters to send the data to your desired backend, like SigNoz, Jaeger, or Prometheus.</p>
<p>Below is a basic configuration example:</p>
<pre><code>service:
  pipelines:
    metrics/am:
      receivers: [azuremonitor]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp, azureeventhub]
      processors: [batch]
      exporters: [otlp]
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
    azureeventhub:
      connection: &lt;Primary Connection String&gt;
      format: &quot;azure&quot;
    azuremonitor:
      subscription_id: &quot;&lt;Subscription ID&gt;&quot;
      tenant_id: &quot;&lt;AD Tenant ID&gt;&quot;
      client_id: &quot;&lt;Client ID&gt;&quot;
      client_secret: &quot;&lt;Client Secret&gt;&quot;
      resource_groups: [&quot;&lt;rg-1&gt;&quot;]
      collection_interval: 60s
  processors:
    batch: {}
  exporters:
    otlp:
      endpoint: &quot;ingest.&lt;region&gt;.signoz.cloud:443&quot;
      tls:
        insecure: false
      headers:
        &quot;signoz-access-token&quot;: &quot;&lt;ingestion-key&gt;&quot;
</code></pre>
<h4>##     Azure Event Hub Receiver Configuration</h4>
<p>If you haven't created the logs Event Hub, you can create one by following the steps in the <a href="../../bootstrapping/data-ingestion">Azure Event Hubs documentation</a>
.</p>
<p>and replace the placeholders <code>&lt;Primary Connection String&gt;</code> with the primary connection string for your Event Hub, it should look something like this:</p>
<pre><code>connection: Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName
</code></pre>
<p>The Event Hub docs have a step to create a SAS policy for the event hub and copy the connection string.</p>
<h4>##     Azure Monitor Receiver Configuration</h4>
<p>You will need to set up a <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal">service principal</a> with Read permissions to receive data from Azure Monitor.</p>
<ol>
<li>
<p>Follow the steps in the <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#register-an-application-with-microsoft-entra-id-and-create-a-service-principal">Create a service principal Azure Doc</a> documentation to create a service principal. You can name it <code>signoz-central-collector-app</code> the redirect URI can be empty.</p>
</li>
<li>
<p>To add read permissions to Azure Monitor, Follow the <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#assign-a-role-to-the-application">Assign Role</a> documentation. The read acess can be given to the full subscription.</p>
</li>
<li>
<p>There are multiple ways to authenticate the service principal, we will use the client secret option, follow <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#option-3-create-a-new-client-secret">Creating a client secret</a> and don't forget to copy the client secret. The secret is used in the configuration file as <code>client_secret</code>.</p>
</li>
<li>
<p>To find <code>client_id</code> and <code>tenant_id</code>, go to the <a href="https://portal.azure.com/">Azure Portal</a> and search for the <code>Application</code> you created. You would see the <code>Application (client) ID</code> and <code>Directory (tenant) ID</code> in the Overview section.</p>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/service-principal-app-overview.webp" alt="Application Overview" /></p>
<p>_</p>
<p>Application Overview</p>
<p>_</p>
<ol start="5">
<li>
<p>To find <code>subscription_id</code>, follow steps in <a href="https://learn.microsoft.com/en-us/azure/azure-portal/get-subscription-tenant-id#find-your-azure-subscription">Find Your Subscription</a> and populate them in the configuration file.</p>
</li>
<li>
<p>Ensure you replace the placeholders <code>&lt;region&gt;</code> and <code>&lt;ingestion-key&gt;</code> with the appropriate values for your signoz cloud instance.</p>
</li>
</ol>
</li>
<li>
<p><strong>Run the Collector:</strong></p>
<p>With your configuration file ready, you can now start the Collector using the following command:</p>
<pre><code># Runs in background with the configuration we just created
./otelcol-contrib --config ./config.yaml &amp;&gt; otelcol-output.log &amp; echo &quot;$!&quot; &gt; otel-pid 
</code></pre>
</li>
<li>
<p><strong>Open Ports:</strong></p>
<p>You will need to open the following ports on your Azure VM:</p>
<ul>
<li>4317 for gRPC</li>
<li>4318 for HTTP</li>
</ul>
<p>You can do this by navigating to the Azure VM's Networking section and adding a new inbound rule for the ports.</p>
</li>
<li>
<p><strong>Validating the Deployment:</strong></p>
<p>Once the Collector is running, ensure that telemetry data is being successfully sent and received. Use the logging exporter as defined in your configuration file, or check the logs for any startup errors.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#configure-dns-label-for-collector
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: https://signoz.io/img/docs/azure-monitoring/ip-address-dns-label.webp
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-configure-dns-label-for-collector
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Setup: Configure DNS label For Collector</h2>
<p>To the IP address of the collector, you can add a DNS label to the Public IP address. This will make it easier to refer to the centralized collector from other services. You can do this by following these steps:</p>
<ol>
<li>Go to the Public IP address of the collector. This would be the IP address of the VM or Load Balancer in case of Kubernetes or Load Balanced collector.</li>
<li>Click on the &quot;Configuration&quot; tab.</li>
<li>Enter the DNS label you want to use for the collector.</li>
<li>Click on &quot;Save&quot;.</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/ip-address-dns-label.webp" alt="DNS label for collector" /></p>
<p>_</p>
<p>DNS label for collector</p>
<p>_</p>
<p>üìù Note</p>
<p>Please take note of the DNS label you have entered. You will need to configure this for your other services as well. In this example, the <em>Central Collector DNS Name</em> is <code>signoz-demo-central-collector.eastus.cloudapp.azure.com</code>.</p>
<p>If you're using kubernetes, you probably have ExternalDNS configured and you can use that to set up DNS name for your collector as well.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#troubleshooting
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-troubleshooting
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Setup: Troubleshooting</h2>
<p>If you encounter any issues during the setup or data collection process, consider the following troubleshooting steps:</p>
<ol>
<li>Verify that the OpenTelemetry Collector is running and accessible from the Azure compute services and applications.</li>
<li>Check the OpenTelemetry Collector logs for any error messages or warnings.</li>
<li>Ensure that the necessary ports and firewall rules are configured correctly to allow incoming connections to the Collector.</li>
<li>Verify that the Azure compute services and applications are properly instrumented and configured to send observability data to the Collector endpoint.</li>
<li>Confirm that the OpenTelemetry Collector is configured to export data to your SigNoz account and that the SigNoz account is accessible.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-blob-storage/metrics/
tag_set: azure-monitoring, az-blob-storage, metrics
image_urls: https://signoz.io/img/docs/azure-monitoring/blob-store-metrics-signoz.webp
tracking_id: docs-azure-monitoring-az-blob-storage-metrics
group_tracking_ids: docs-azure-monitoring-az-blob-storage-metrics
<h2>Azure Blob Storage Metrics</h2>
<h2>QuickStart</h2>
<hr />
<p>To monitor Azure Blob Storage's system metrics like Total Requests, Total Ingress / Egress, and Total Errors with SigNoz, you just need to set up the OpenTelemetry Collector with the Azure Monitor exporter.</p>
<h2>Overview</h2>
<hr />
<p>Azure Blob Storage is a cloud storage service that provides scalable, durable, and highly available storage for your data. It is designed to store large amounts of unstructured data, such as files, blobs, and objects, and is optimized for data access and retrieval.</p>
<p>In this guide, you will learn how to monitor Azure Blob Storage's system metrics like Total Requests, Total Ingress / Egress, and Total Errors with SigNoz. By monitoring these metrics, you can keep track of your application's resource utilization and performance.</p>
<h2>Prerequisites</h2>
<hr />
<p>Before you can monitor your Azure Blob Storage with SigNoz, you need to ensure the following prerequisites are met:</p>
<ol>
<li>You have an Azure subscription and an Azure Blob Storage instance running.</li>
<li>You have set up the Central Collector with the Azure Monitor exporter. If you haven't set it up yet, follow the instructions in the¬†<a href="../../bootstrapping/collector-setup">Central Collector Setup</a></li>
</ol>
<h2>Dashboard Example</h2>
<hr />
<p>Once you have completed the prerequisites, you can start monitoring your Azure Blob Storage's system metrics with SigNoz.</p>
<ol>
<li>Log in to your SigNoz account.</li>
<li>Navigate to the Dashboards, and add an dashboard</li>
<li>Add a Timeseries Panel</li>
<li>In <em>Metrics</em>, select <code>azure_ingress_total</code> and <em>Avg By</em> select tag <code>location</code></li>
<li>In Filter say <code>name = &lt;storage-account-name&gt;</code></li>
<li>Hit ‚ÄúSave Changes‚Äù You now have Total Ingress of your Azure Blob Storage in a Dashboard for reporting and alerting</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/blob-store-metrics-signoz.webp" alt="Time Series Panel for Blob Storage Total Ingress" /></p>
<p>_</p>
<p>Time Series Panel for Blob Storage Total Ingress</p>
<p>_</p>
<p>That's it! You have successfully set up monitoring for your Azure Blob Storage's system metrics with SigNoz. You can now start creating other panels and dashboards to monitor Azure Blob Storage's other metrics.</p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter any issues while setting up monitoring for your Azure Blob Storage's system metrics with SigNoz, here are a few troubleshooting steps you can try:</p>
<ol>
<li>Check if the OpenTelemetry Collector is running and properly configured with the Azure Monitor exporter.</li>
<li>Verify that your Azure Blob Storage instance is running and accessible.</li>
<li>Ensure that you have the necessary permissions to access the metrics in your Azure subscription.</li>
<li>Double-check the configuration of the OpenTelemetry Collector with the Azure Monitor exporter to ensure that a resource group filter is not preventing the metrics from being collected.</li>
</ol>
<p>By following this guide, you should be able to easily monitor your Azure Blob Storage's system metrics with SigNoz and gain valuable insights into your application's performance and resource utilization.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/aws-monitoring/ecs-ec2-external/
tag_set: aws-monitoring, ecs-ec2-external
image_urls: 
tracking_id: docs-aws-monitoring-ecs-ec2-external
group_tracking_ids: docs-aws-monitoring-ecs-ec2-external
<h2>Monitor your ECS EC2 and External launch type</h2>
<p>To monitor your ECS EC2 or external service, check out <a href="https://signoz.io/docs/userguide/collecting-ecs-logs-and-metrics/">these detailed docs</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/introduction/
tag_set: logs-pipelines, introduction
image_urls: https://signoz.io/img/logs/pipelines/raw-nginx-log.webp, https://signoz.io/img/logs/pipelines/parsed-nginx-log.webp, https://signoz.io/img/logs/pipelines/nginx-requests-by-user-agent.webp
tracking_id: docs-logs-pipelines-introduction
group_tracking_ids: docs-logs-pipelines-introduction
<h2>Unleash the Potential of Your Logs with Logs Pipelines</h2>
<p>With Logs Pipelines, you can transform logs to suit your querying and aggregation needs before they get stored in the database.</p>
<p>Once you start sending logs to SigNoz, you can start searching their text and create basic reports based on standard fields. For example, you can find logs with text containing a particular user's id or plot the count of error logs by service name.<br />
However, the text in your logs will typically contain a lot of other valuable information, and it can be inefficient or outright impossible to query and aggregate on that information.</p>
<p>Logs pipelines enable you to unleash the full potential of your logs by pre-processing them to suit your needs before they get stored. This unlocks valuable logs based queries and dashboards that wouldn't be possible otherwise.<br />
You can also use logs preprocessing to achieve other goals like cleaning sensitive information in your logs or normalizing names of fields across services.</p>
<p><img src="https://signoz.io/img/logs/pipelines/raw-nginx-log.webp" alt="Raw Nginx Log" /></p>
<p>_</p>
<p>A raw Nginx log</p>
<p>_</p>
<p><img src="https://signoz.io/img/logs/pipelines/parsed-nginx-log.webp" alt="Parsed Nginx Log" /></p>
<p>_</p>
<p>A parsed Nginx log</p>
<p>_</p>
<p><img src="https://signoz.io/img/logs/pipelines/nginx-requests-by-user-agent.webp" alt="Nginx Requests Count by User Agent" /></p>
<p>_</p>
<p>A Report for Requests by User Agent, made possible by a pipeline that extracts User Agent from Nginx text logs</p>
<p>_</p>
<p>While you can achieve these goals by changing your application code or by changing config for your otel collectors, logs pipelines allow you to do it in SigNoz UI without having to ship changes or redeploy your applications and collectors.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/long-term-storage/
tag_set: logs-management, long-term-storage
image_urls: 
tracking_id: docs-logs-management-long-term-storage
group_tracking_ids: docs-logs-management-long-term-storage
<h2>Long Term Storage of Logs in SigNoz Cloud</h2>
<h2>Overview</h2>
<hr />
<p>SigNoz provides two ways for long term log storage for SigNoz Cloud customers.</p>
<ol>
<li>Forwarding to your own S3/Google Cloud Storage</li>
<li>Long term storage with object storage also in SigNoz Cloud</li>
</ol>
<h3>## Forwarding to your own S3/Google Cloud Storage</h3>
<p>You can configure forwarding logs from SigNoz to S3/Google Cloud Storage in your own infra. The data is stored in ClickHouse native format and you would need an instance of SigNoz to query this data.</p>
<p>The pricing for this plan is as follows:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Log forwarding cost</td>
<td>$0.25/GB</td>
</tr>
<tr>
<td>Rehydration cost</td>
<td>$1.2/mn log lines</td>
</tr>
</tbody>
</table>
<h4>## Notes</h4>
<ul>
<li>Logs data is stored in ClickHouse Native format in the object storage</li>
<li>Even if you stop subscribing SigNoz Cloud, you can query it using SigNoz open source version</li>
</ul>
<h3>## Long term storage with object storage also in SigNoz Cloud</h3>
<p>In this option, customers don't need to send data to a separate object storage instance. They can configure in SigNoz how many days they want to keep their data in Hot storage (faster query response) and how many days do they want to keep their data in Cold storage (slower query response). This option allows users to manage their costs even when storing logs data for long period of times for compliance and other needs.</p>
<p>The pricing for this plan is as follows:</p>
<table>
<thead>
<tr>
<th>Hot Storage Period</th>
<th>Period in Cold Storage after Hot Storage</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>0 day</td>
<td>90 days</td>
<td>180 days</td>
<td>1 yr</td>
<td>2 yrs</td>
<td></td>
</tr>
<tr>
<td>15 days (in $/GB)</td>
<td>0.3</td>
<td>0.45</td>
<td>0.6</td>
<td>0.9</td>
<td>1.3</td>
</tr>
<tr>
<td>30 days (in $/GB)</td>
<td>0.4</td>
<td>0.55</td>
<td>0.7</td>
<td>1</td>
<td>1.4</td>
</tr>
<tr>
<td>90 days (in $/GB)</td>
<td>0.6</td>
<td>0.75</td>
<td>0.9</td>
<td>1.2</td>
<td>1.6</td>
</tr>
<tr>
<td>180 days (in $/GB)</td>
<td>0.8</td>
<td>0.95</td>
<td>1.1</td>
<td>1.4</td>
<td>1.8</td>
</tr>
</tbody>
</table>
<h4>## Notes</h4>
<ul>
<li>If you need a retention period longer than 2 years, please reach out to us on our in-product chat support (located at the bottom right corner of your SigNoz account) or contact us at <a href="mailto:cloud-support@signoz.io">cloud-support@signoz.io</a></li>
</ul>
<h2>FAQ</h2>
<hr />
<h4>## How does querying in Hot storage compare to querying in Cold storage?</h4>
<p>For a typical query, cold storage is typically 3 times slower. The exact performance difference depends a lot on the exact query.</p>
<h2>Interested in this feature?</h2>
<hr />
<p>If you are a SigNoz Cloud user, please use in-product chat support located at the bottom right corner of your SigNoz instance or contact us at <a href="mailto:cloud-support@signoz.io">cloud-support@signoz.io</a> to get instructions on how to enable this feature for your account.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_python/
tag_set: userguide, collecting_application_logs_otel_sdk_python
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_python
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_python
<h2>Collecting Application Logs Using OTEL Python SDK</h2>
<p>You can directly send logs of your application to SigNoz using the Python SDKs provided by opentlemetry. Please find an example <a href="https://github.com/open-telemetry/opentelemetry-python/tree/main/docs/examples/logs">here</a>.</p>
<p>üìù Note</p>
<p>The default logging level in Python is WARNING.</p>
<p>To send all the logs to SigNoz please change the default log level to DEBUG.</p>
<pre><code>import logging
logging.basicConfig(level=logging.DEBUG)
</code></pre>
<h2>For SigNoz Cloud</h2>
<hr />
<p>For sending logs to SigNoz cloud, while running the above example set the below environment variables</p>
<ul>
<li>
<p>The value of <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> environment variable will be <code>https://ingest.{region}.signoz.cloud:443</code> where depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>The value of <code>OTEL_EXPORTER_OTLP_HEADERS</code> environment variable will be <code>signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code> where <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> is your ingestion key</p>
</li>
<li>
<p>Your run command will look like</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; python3 example.py`
</code></pre>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#collecting-application-logs-using-otel-java-agent
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-collecting-application-logs-using-otel-java-agent
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h1>Collecting Application Logs Using OTEL Java Agent: Collecting Application Logs Using OTEL Java Agent - Collecting Application Logs Using OTEL Java Agent</h1>
<p>You can directly send your application logs to SigNoz using <a href="https://signoz.io/docs/instrumentation/java/">Java Agent provided by OpenTelemetry</a>. In this doc we will run a sample java application with the OpenTelemetry Java agent to send logs to SigNoz.</p>
<p>For collecting logs we will have to download the java agent from <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar">here</a>.</p>
<p>To sends logs from a Java application you will have to add the agent and add the environment variables for the agent.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#for-sending-logs-to-signoz-cloud
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-for-sending-logs-to-signoz-cloud
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: For Sending Logs To SigNoz Cloud</h2>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; java -javaagent:/path/opentelemetry-javaagent.jar -jar  &lt;myapp&gt;.jar
</code></pre>
<p>You will have to add <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> and depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#for-sending-logs-to-signoz-hosted-locally
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-for-sending-logs-to-signoz-hosted-locally
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: For Sending Logs To SigNoz Hosted Locally</h2>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; java -javaagent:/path/opentelemetry-javaagent.jar -jar  &lt;myapp&gt;.jar
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#settings-for-appender-instrumentation-based-on-the-logging-library
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-settings-for-appender-instrumentation-based-on-the-logging-library
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: Settings for Appender instrumentation based on the logging library</h2>
<p>You can use appender settings by passing it as an argument in the <code>-D&lt;property&gt;=&lt;value&gt;</code> format.</p>
<p>ex:- <code>-Dotel.instrumentation.logback-appender.experimental-log-attributes=true</code></p>
<h3>## Logback</h3>
<p>LINK - <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/tree/main/instrumentation/logback/logback-appender-1.0/javaagent">Logback</a></p>
<table>
<thead>
<tr>
<th>System property</th>
<th>Type</th>
<th>Default Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental-log-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of experimental log attributes <code>thread.name</code> and <code>thread.id</code>.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-code-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of [source code attributes]. Note that capturing source code attributes at logging sites might add a performance overhead.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-marker-attribute</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of Logback markers as attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-key-value-pair-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of Logback key value pairs as attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-logger-context-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of Logback logger context properties as attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-mdc-attributes</code></td>
<td>String</td>
<td>NA</td>
<td>Comma separated list of MDC attributes to capture. Use the wildcard character <code>*</code> to capture all attributes.</td>
</tr>
</tbody>
</table>
<h3>## Log4j</h3>
<p>LINK - <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/tree/main/instrumentation/log4j/log4j-appender-2.17/javaagent">Log4j</a></p>
<table>
<thead>
<tr>
<th>System property</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>otel.instrumentation.log4j-appender.experimental-log-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of experimental log attributes <code>thread.name</code> and <code>thread.id</code>.</td>
</tr>
<tr>
<td><code>otel.instrumentation.log4j-appender.experimental.capture-map-message-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of <code>MapMessage</code> attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.log4j-appender.experimental.capture-marker-attribute</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of Log4j markers as attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.log4j-appender.experimental.capture-mdc-attributes</code></td>
<td>String</td>
<td></td>
<td>Comma separated list of context data attributes to capture. Use the wildcard character <code>*</code> to capture all attributes.</td>
</tr>
</tbody>
</table>
<p>In the below example we will configure a java application to send logs to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#example-how-to-collect-application-logs-using-otel-java-agent
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-example-how-to-collect-application-logs-using-otel-java-agent
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: [Example] How to Collect Application Logs Using OTEL Java Agent?</h2>
<ul>
<li>
<p>Clone this <a href="https://github.com/SigNoz/spring-petclinic">repository</a></p>
</li>
<li>
<p>Build the application using <code>./mvnw package</code></p>
</li>
<li>
<p>Now run the application</p>
</li>
</ul>
<h3>## For SigNoz Cloud</h3>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; OTEL_RESOURCE_ATTRIBUTES=service.name=myapp java -javaagent:/path/opentelemetry-javaagent.jar -jar target/*.jar
</code></pre>
<p>You will have to replace the value of <code>{region}</code> according to region of your cloud account and also add <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code></p>
<h3>## For SigNoz Hosted Locally</h3>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;host&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=myapp java -javaagent:/path/opentelemetry-javaagent.jar -jar target/*.jar
</code></pre>
<p>You will have to replace your the value of <code>host</code> as <code>0.0.0.0</code> if SigNoz is running in the same host, for other configurations please check the <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">troubleshooting</a> guide.</p>
<ul>
<li>Visit <code>http://localhost:8090</code> to access the application.</li>
<li>Once you use the application logs will be visible on SigNoz UI.</li>
<li>If you want to enable settings here is how you do it.</li>
</ul>
<p>Let's say we want to enable <code>-Dotel.instrumentation.logback-appender.experimental-log-attributes=true</code></p>
<h3>## For SigNoz Cloud</h3>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; OTEL_RESOURCE_ATTRIBUTES=service.name=myapp java -javaagent:/path/opentelemetry-javaagent.jar -Dotel.instrumentation.logback-appender.experimental-log-attributes=true -jar target/*.jar
</code></pre>
<p>You will have to replace the value of <code>{region}</code> according to the region of your cloud account and also replace <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> with your SigNoz Cloud Ingestion key.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#for-signoz-hosted-locally-1
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-for-signoz-hosted-locally-1
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: For SigNoz Hosted Locally</h2>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;host&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=myapp java -javaagent:/path/opentelemetry-javaagent.jar -Dotel.instrumentation.logback-appender.experimental-log-attributes=true -jar target/*.jar
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_kubernetes_pod_logs/
tag_set: userguide, collect_kubernetes_pod_logs
image_urls: 
tracking_id: docs-userguide-collect_kubernetes_pod_logs
group_tracking_ids: docs-userguide-collect_kubernetes_pod_logs
<h2>Collecting Kubernetes pod logs</h2>
<p>SigNoz can automatically collect all your pod logs and you can perform various action on top of that data.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_kubernetes_pod_logs/#collect-kubernetes-pod-logs-in-signoz-cloud
tag_set: userguide, collect_kubernetes_pod_logs
image_urls: 
tracking_id: docs-userguide-collect_kubernetes_pod_logs-collect-kubernetes-pod-logs-in-signoz-cloud
group_tracking_ids: docs-userguide-collect_kubernetes_pod_logs
<h2>Collecting Kubernetes pod logs: Collect Kubernetes Pod Logs in SigNoz Cloud</h2>
<p>To collect logs from your kubernetes cluster, you will need to deploy k8s-infra chart. Please follow the guide <a href="/docs/tutorial/kubernetes-infra-metrics/">here</a>. Log collection of pods from all namespaces is enabled by default except for pods in <code>kube-system</code> and <code>hotrod</code>. To modify the log collection mechanism, please follow the guides below.</p>
<ul>
<li>
<p><a href="#steps-to-disable-automatic-pod-logs-collection">Disable automatic pod logs collection</a></p>
</li>
<li>
<p><a href="#steps-to-filterexclude-logs-collection">Filter/Exclude logs collection</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_kubernetes_pod_logs/#collect-kubernetes-pod-logs-in-self-hosted-signoz
tag_set: userguide, collect_kubernetes_pod_logs
image_urls: 
tracking_id: docs-userguide-collect_kubernetes_pod_logs-collect-kubernetes-pod-logs-in-self-hosted-signoz
group_tracking_ids: docs-userguide-collect_kubernetes_pod_logs
<h2>Collecting Kubernetes pod logs: Collect Kubernetes Pod Logs in Self-Hosted SigNoz: Collect Kubernetes Pod Logs in Self-Hosted SigNoz</h2>
<p>When you deploy SigNoz to your kubernetes cluster it will automatically start collecting all the pod logs. It will automatically parse out different attributes from the logs like name, namespace, container name, uid etc. But if you want to parse specific attributes from certain kind of logs you can use different kinds of operators provided by OpenTelemetry <a href="/docs/userguide/logs/#operators-for-parsing-and-manipulating-logs">here</a>.</p>
<p>If your signoz cluster is hosted in a different cluster then you will have to install k8s-infra chart on your application kubernetes cluster. Please follow the guide <a href="/docs/tutorial/kubernetes-infra-metrics/">here</a>. Log collection of pods from all namespaces is enabled by default except for pods in <code>kube-system</code> and <code>hotrod</code>. To modify the log collection mechanism, please follow the guides below.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_kubernetes_pod_logs/#steps-to-disable-automatic-pod-logs-collection
tag_set: userguide, collect_kubernetes_pod_logs
image_urls: 
tracking_id: docs-userguide-collect_kubernetes_pod_logs-steps-to-disable-automatic-pod-logs-collection
group_tracking_ids: docs-userguide-collect_kubernetes_pod_logs
<p>Collecting Kubernetes pod logs: Collect Kubernetes Pod Logs in Self-Hosted SigNoz: Steps to disable automatic pod logs collection</p>
<ul>
<li>
<p>Modify/Create the <code>override-values.yaml</code> file</p>
<pre><code>k8s-infra:
  presets:
    logsCollection:
      enabled: false
</code></pre>
<p>You can apply this yaml file by running the following command:</p>
<pre><code>helm -n platform upgrade my-release signoz/signoz -f override-values.yaml
</code></pre>
<p>In case of external K8s cluster where only k8s-infra chart is installed, users can disable log collections by including the following in <code>override-values.yaml</code> :</p>
<pre><code>presets:
  logsCollection:
    enabled: false
</code></pre>
<p>You can apply this yaml file by running the following command:</p>
<pre><code>helm -n platform upgrade my-release signoz/k8s-infra -f override-values.yaml
</code></pre>
<p>Once the above is applied to your k8s cluster, logs collection will be disabled.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_kubernetes_pod_logs/#steps-to-filterexcludeinclude-logs-collection
tag_set: userguide, collect_kubernetes_pod_logs
image_urls: 
tracking_id: docs-userguide-collect_kubernetes_pod_logs-steps-to-filterexcludeinclude-logs-collection
group_tracking_ids: docs-userguide-collect_kubernetes_pod_logs
<p>Collecting Kubernetes pod logs: Collect Kubernetes Pod Logs in Self-Hosted SigNoz: Steps to Filter/Exclude/Include Logs Collection</p>
<ul>
<li>
<p><strong>Exclude certain log files</strong> : If you want to exclude logs of certain namespaces, pods or containers, you can append the following config in your Helm override values file.</p>
<p><em>override-values.yaml</em></p>
<p>SigNoz chartK8s-Infra chart</p>
<pre><code>k8s-infra:
  presets:
    logsCollection:
      # whether to enable log collection
      enabled: true
      blacklist:
        # whether to enable blacklisting
        enabled: true
        # whether to exclude signoz logs
        signozLogs: false
        # which namespaces to exclude
        namespaces:
          - kube-system
        # which pods to exclude
        pods:
          - hotrod
          - locust
        # which containers to exclude
        containers: []
        # additional exclude rules
        additionalExclude: []
</code></pre>
</li>
<li>
<p><strong>Include certain log files only</strong> : If you want to only include logs of certain namespaces, pods or containers, you can append the following config in your Helm override values file.</p>
<p><em>override-values.yaml</em></p>
<p>SigNoz chartK8s-Infra chart</p>
<pre><code>k8s-infra:
  presets:
    logsCollection:
      # whether to enable log collection
      enabled: true
      whitelist:
        # whether to enable whitelisting
        enabled: true
        # whether to include signoz logs
        signozLogs: false
        # which namespaces to include
        namespaces:
          - platform
          - my-application-namespace
        # which pods to include
        pods:
          - otel  # all pods with otel prefix
          - my-application-pod
        # which containers to include
        containers: []
        # additional include rules
        additionalInclude: []
</code></pre>
</li>
<li>
<p><strong>Using filter operator in filelog receiver</strong> : You can also use the filter operator to filter out logs by changing the operators here <a href="https://github.com/SigNoz/charts/blob/main/charts/k8s-infra/values.yaml">charts</a>
.</p>
<pre><code>....
  operators:
    - type: filter
      expr: 'body matches &quot;^LOG: .* END$&quot;'
      drop_ratio: 1.0
....
</code></pre>
<p>Here we are matching logs using an expression and dropping the entire log by setting <code>drop_ratio: 1.0</code> . You can read more about the filter operator <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/filter.md">here</a></p>
</li>
<li>
<p>Now you can restart the otel collector pod so that new changes are applied.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/trace/#overview
tag_set: logs-pipelines, guides, trace
image_urls: https://signoz.io/img/logs/pipelines/log-before-trace-parsing.png, https://signoz.io/img/logs/pipelines/log-after-trace-parsing.png
tracking_id: docs-logs-pipelines-guides-trace-overview
group_tracking_ids: docs-logs-pipelines-guides-trace
<h2>Parse Trace Information for your Logs: Parse Trace Information for your Logs - Overview</h2>
<p>If your logs contain trace information outside the <a href="https://opentelemetry.io/docs/specs/otel/logs/data-model/#log-and-event-record-definition">standard OpenTelemetry fields</a>
, you can use log pipelines to parse that information into the right fields and enable correlation of your logs to and from corresponding traces.</p>
<p><img src="https://signoz.io/img/logs/pipelines/log-before-trace-parsing.png" alt="A log with trace information in log attributes named traceId and spanId, while span_id and trace_id fields are empty" /></p>
<p>_</p>
<p>A log with trace information in log attributes named traceId and spanId, while span_id and trace_id fields are empty</p>
<p>_</p>
<p><img src="https://signoz.io/img/logs/pipelines/log-after-trace-parsing.png" alt="A log with span_id and trace_id fields populated based on log attributes, with trace_id linking to corresponding trace" /></p>
<p>_</p>
<p>A log with span_id and trace_id fields populated based on log attributes, with trace_id linking to corresponding trace</p>
<p>_</p>
<p>In this guide, you will see how to parse values from log attributes into the trace_id, span_id and trace_flags fields.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/trace/#prerequisites
tag_set: logs-pipelines, guides, trace
image_urls: 
tracking_id: docs-logs-pipelines-guides-trace-prerequisites
group_tracking_ids: docs-logs-pipelines-guides-trace
<h2>Parse Trace Information for your Logs: Prerequisites</h2>
<ul>
<li>You are <a href="/docs/userguide/logs/">sending logs to SigNoz</a>
.</li>
<li>Your logs contain trace information in log attributes.</li>
</ul>
<p>üìù Note</p>
<p>If your logs contain trace information in the body, you can <a href="/docs/logs-pipelines/guides/json/">parse them</a> out into their own attributes before populating trace information based on them.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/trace/#create-a-pipeline-to-parse-trace-information-out-of-log-attributes
tag_set: logs-pipelines, guides, trace
image_urls: 
tracking_id: docs-logs-pipelines-guides-trace-create-a-pipeline-to-parse-trace-information-out-of-log-attributes
group_tracking_ids: docs-logs-pipelines-guides-trace
<h2>Parse Trace Information for your Logs: Create a Pipeline to Parse Trace Information out of Log Attributes: Create a Pipeline to Parse Trace Information out of Log Attributes</h2>
<p>You can follow the steps below to create a log pipeline for populating trace information based on data in log attributes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/trace/#step-1-navigate-to-logs-pipelines-page
tag_set: logs-pipelines, guides, trace
image_urls: https://signoz.io/img/logs/pipelines/navigate-to-logs-pipelines.png
tracking_id: docs-logs-pipelines-guides-trace-step-1-navigate-to-logs-pipelines-page
group_tracking_ids: docs-logs-pipelines-guides-trace
<p>Parse Trace Information for your Logs: Create a Pipeline to Parse Trace Information out of Log Attributes: Step 1: Navigate to Logs Pipelines Page</p>
<p>Hover over the <strong>Logs</strong> menu in the sidebar and click on the <strong>Logs Pipeline</strong> submenu item.</p>
<p><img src="https://signoz.io/img/logs/pipelines/navigate-to-logs-pipelines.png" alt="Sidebar navigation for getting to Logs Pipelines page" /></p>
<p>_</p>
<p>Sidebar navigation for getting to Logs Pipelines page</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/trace/#step-2-create-a-new-pipeline
tag_set: logs-pipelines, guides, trace
image_urls: https://signoz.io/img/logs/pipelines/empty-state-new-pipeline-button.png, https://signoz.io/img/logs/pipelines/enter-edit-mode.png, https://signoz.io/img/logs/pipelines/add-a-new-pipeline.png, https://signoz.io/img/logs/pipelines/create-new-trace-parsing-pipeline.png
tracking_id: docs-logs-pipelines-guides-trace-step-2-create-a-new-pipeline
group_tracking_ids: docs-logs-pipelines-guides-trace
<p>Parse Trace Information for your Logs: Create a Pipeline to Parse Trace Information out of Log Attributes: Step 2: Create a New Pipeline</p>
<ul>
<li>
<p>Open the &quot;Create New Pipeline&quot; dialog.</p>
<ul>
<li>If you do not have existing pipelines, press the &quot;<strong>New Pipeline</strong>&quot; button.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/empty-state-new-pipeline-button.png" alt="New Pipeline Button" /></p>
<p>_</p>
<p>New Pipeline Button</p>
<p>_</p>
<ul>
<li>If you already have some pipelines, press the &quot;<strong>Enter Edit Mode</strong>&quot; button and then click the &quot;<strong>Add a New Pipeline</strong>&quot; button at the bottom of the list of pipelines.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/enter-edit-mode.png" alt="Enter edit mode button" /></p>
<p>_</p>
<p>Enter Edit Mode button</p>
<p>_</p>
<p><img src="https://signoz.io/img/logs/pipelines/add-a-new-pipeline.png" alt="Add a New Pipeline button" /></p>
<p>_</p>
<p>Add a New Pipeline button</p>
<p>_</p>
</li>
<li>
<p>Provide details about the pipeline in the Create Pipeline Dialog.</p>
<ul>
<li>Use the <strong>Name</strong> field to give your pipeline a descriptive short name.</li>
<li>Use the <strong>Description</strong> field to add a detailed description for your pipeline.</li>
<li>Use the <strong>Filter</strong> field to select the logs you want to process with this pipeline. Typically, these are filters identifying the source of the logs you want to process. <code>service = checkout</code> for example.</li>
<li>Use the <strong>Filtered Logs Preview</strong> to verify that the logs you want to process will be selected by the pipeline.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/create-new-trace-parsing-pipeline.png" alt="Create New Pipeline dialog" /></p>
<p>_</p>
<p>Create New Pipeline dialog</p>
<p>_</p>
</li>
<li>
<p>Press the &quot;<strong>Create</strong>&quot; button if everything looks right.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/trace/#step-3-add-processors-for-parsing-trace-information
tag_set: logs-pipelines, guides, trace
image_urls: https://signoz.io/img/logs/pipelines/post-create-trace-parsing-pipeline.png, https://signoz.io/img/logs/pipelines/expanded-trace-parsing-processor.png, https://signoz.io/img/logs/pipelines/create-new-trace-parsing-processor.png, https://signoz.io/img/logs/pipelines/remove-trace-id-processor.png
tracking_id: docs-logs-pipelines-guides-trace-step-3-add-processors-for-parsing-trace-information
group_tracking_ids: docs-logs-pipelines-guides-trace
<p>Parse Trace Information for your Logs: Create a Pipeline to Parse Trace Information out of Log Attributes: Step 3: Add Processors for Parsing Trace Information</p>
<ul>
<li>Expand the new Pipeline to <a href="https://signoz.io/docs/logs-pipelines/processors/">add processors</a> to it.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/post-create-trace-parsing-pipeline.png" alt="Creating a Pipeline adds it to the end of the Pipelines List. It can be expanded by clicking the highlighted icon." /></p>
<p>_</p>
<p>Creating a Pipeline adds it to the end of the Pipelines List. It can be expanded by clicking the highlighted icon.</p>
<p>_</p>
<p><img src="https://signoz.io/img/logs/pipelines/expanded-trace-parsing-processor.png" alt="Expanding a pipeline shows the Add Processor button" /></p>
<p>_</p>
<p>Expanding a pipeline shows the Add Processor button</p>
<p>_</p>
<ul>
<li>
<p>Add a processor to parse trace information out of log attributes.</p>
<ul>
<li>Click the <strong>Add Processor</strong> Button to bring up the Dialog for adding a new processor.</li>
<li>Select <code>Trace Parser</code> in the <strong>Select Processor Type</strong> field.</li>
<li>Use the <strong>Name of Trace Parser Processor</strong> field to set a short descriptive name for the processor.</li>
<li>Use the <strong>Parse Trace Id From</strong> field to specify the log attribute containing trace id.</li>
<li>Use the <strong>Parse Span Id From</strong> field to specify the log attribute containing span id.</li>
<li>Use the <strong>Parse Trace Flags From</strong> field to specify the log attribute containing trace flags.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/create-new-trace-parsing-processor.png" alt="Add New Processor Dialog" /></p>
<p>_</p>
<p>Add New Processor Dialog</p>
<p>_</p>
<p>üìù Note</p>
<p>At least one of the Parse From fields (<code>Parse Trace Id From</code>, <code>Parse Span Id From</code> and <code>Parse Trace Flags From</code>) must be specified.</p>
<ul>
<li>Press the <strong>Create</strong> button to finish adding the processor.</li>
</ul>
</li>
<li>
<p>Optional: Add processors for removing log attributes containing trace information</p>
<ul>
<li>Click the <strong>Add Processor</strong> Button to bring up the Dialog for adding a new processor.</li>
<li>Select <code>Remove</code> in the <strong>Select Processor Type</strong> field.</li>
<li>Use the <strong>Name of Remove Processor</strong> field to set a short descriptive name for the processor.</li>
<li>Set <strong>Field</strong> input to path of the attribute containing trace data. <code>attributes.traceId</code> for example.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/remove-trace-id-processor.png" alt="Remove Processor Dialog" /></p>
<p>_</p>
<p>Remove Processor Dialog</p>
<p>_</p>
<ul>
<li>Press the <strong>Create</strong> button to finish adding the processor.</li>
<li>Repeat these steps to create a <strong>Remove</strong> processor for removing each log attribute whose data has been parsed into trace fields.</li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/trace/#step-4-preview-and-validate-pipeline-processing
tag_set: logs-pipelines, guides, trace
image_urls: https://signoz.io/img/logs/pipelines/trace-parsing-pipeline-expanded.png, https://signoz.io/img/logs/pipelines/pipeline-preview-init.png, https://signoz.io/img/logs/pipelines/pipeline-preview-processed.png
tracking_id: docs-logs-pipelines-guides-trace-step-4-preview-and-validate-pipeline-processing
group_tracking_ids: docs-logs-pipelines-guides-trace
<p>Parse Trace Information for your Logs: Create a Pipeline to Parse Trace Information out of Log Attributes: Step 4: Preview and Validate Pipeline Processing</p>
<p>At this point you should have the pipeline ready with all necessary processors.</p>
<p><img src="https://signoz.io/img/logs/pipelines/trace-parsing-pipeline-expanded.png" alt="Expanded Pipeline with Processors for parsing desired fields out of JSON body into their own log attributes" /></p>
<p>_</p>
<p>Expanded Pipeline with Processors for parsing trace information out of log attributes.</p>
<p>_</p>
<p>Before we save and deploy the pipeline, it is best to simulate processing on some sample logs to validate that the pipeline will work as expected.<br />
Click the &quot;eye&quot; icon in the actions column for the pipeline to bring up the Pipeline Preview Dialog</p>
<p><img src="https://signoz.io/img/logs/pipelines/pipeline-preview-init.png" alt="Pipeline Preview with Sample Logs" /></p>
<p>_</p>
<p>Pipeline Preview with Sample Logs</p>
<p>_</p>
<p>The preview Dialog will start out with sample logs queried from the database. You can adjust the sample logs search duration if there are no recent samples available.<br />
To simulate pipeline processing, press the <strong>Simulate Processing</strong> button in the bottom section of the Pipeline Preview Dialog.<br />
This will simulate pipeline processing on the sample logs and show the output.</p>
<p><img src="https://signoz.io/img/logs/pipelines/pipeline-preview-processed.png" alt="Pipeline Preview with Processed Output" /></p>
<p>_</p>
<p>Pipeline Preview with Processed Output</p>
<p>_</p>
<p>You can click on the <em>expand icon</em> on the right end of each processed log to open the detailed view for that log. Expand some of the processed logs to verify that trace information was populated as expected.<br />
If you see any issues, you can close the preview, edit your processors as needed and preview again to verify. Iterate on your pipeline and processor config until it all works just the way you want it.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/trace/#step-5-save-pipelines-and-verify
tag_set: logs-pipelines, guides, trace
image_urls: https://signoz.io/img/logs/pipelines/save-trace-parsing-pipeline.png, https://signoz.io/img/logs/pipelines/change-history.png
tracking_id: docs-logs-pipelines-guides-trace-step-5-save-pipelines-and-verify
group_tracking_ids: docs-logs-pipelines-guides-trace
<p>Parse Trace Information for your Logs: Create a Pipeline to Parse Trace Information out of Log Attributes: Step 5: Save Pipelines and Verify</p>
<p>Once you have previewed your pipeline and verified that it will work as expected, press the <strong>Save Configuration</strong> button at the bottom of the pipelines list to save pipelines. This will store the latest state of your pipelines and will deploy them for pre-processing.</p>
<p><img src="https://signoz.io/img/logs/pipelines/save-trace-parsing-pipeline.png" alt="Save Configuration Button" /></p>
<p>_</p>
<p>Save Configuration Button</p>
<p>_</p>
<p>You can track the deployment status of your pipelines using the <strong>Change History</strong> tab at the top of the pipelines page.</p>
<p><img src="https://signoz.io/img/logs/pipelines/change-history.png" alt="Pipelines Change History" /></p>
<p>_</p>
<p>Pipelines Change History</p>
<p>_</p>
<p>Wait for a few minutes to let the pipelines deploy and for the latest batches of logs to get pre-processed and stored in the database. Then you can head over to the logs explorer to verify that trace fields (trace_id, span_id and trace_flags) are being populated in your logs as expected.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/json/#overview
tag_set: logs-pipelines, guides, json
image_urls: https://signoz.io/img/logs/pipelines/json-log-body-filter.png, https://signoz.io/img/logs/pipelines/attributes-parsed-from-json-body.png, https://signoz.io/img/logs/pipelines/avg-order-value-by-country.png
tracking_id: docs-logs-pipelines-guides-json-overview
group_tracking_ids: docs-logs-pipelines-guides-json
<h2>Parse JSON logs with Pipelines: Parse JSON logs with Pipelines - Overview</h2>
<p>If your logs contain serialized JSON in their bodies, the log detail view in Signoz UI will display the body in a parsed, easy to use structure. You can also <a href="/docs/userguide/logs_query_builder/#writing-json-filters-in-the-new-logs-explorer">filter your logs based on JSON data in the body</a>.</p>
<p><img src="https://signoz.io/img/logs/pipelines/json-log-body-filter.png" alt="Structured display of a log body containing serialized JSON for an e-commerce Order" /></p>
<p>_</p>
<p>Structured display of a log body containing serialized JSON for an e-commerce Order</p>
<p>_</p>
<p>While these powerful features work out of the box, you can take things up a notch by pre-processing your log records to parse interesting fields out of JSON bodies into their own log attributes.</p>
<p><img src="https://signoz.io/img/logs/pipelines/attributes-parsed-from-json-body.png" alt="Log attributes for Country Code and Order Total parsed out of JSON body containing Order details" /></p>
<p>_</p>
<p>Log attributes for Country Code and Order Total parsed out of JSON body containing Order details</p>
<p>_</p>
<p>Filtering by log attributes is more efficient and this also unlocks aggregations based on fields extracted from your JSON data.</p>
<p><img src="https://signoz.io/img/logs/pipelines/avg-order-value-by-country.png" alt="Average Order Value by Country based on log attributes parsed from JSON bodies containing Order details" /></p>
<p>_</p>
<p>Average Order Value by Country based on log attributes parsed from JSON bodies containing Order details</p>
<p>_</p>
<p>The parsed attributes can also be used to further enrich your log records. For example, if the serialized JSON contained trace information, you could <a href="/docs/logs-pipelines/guides/trace/">populate trace details</a> in your log records from the parsed attributes, enabling correlation of your logs to and from corresponding traces.</p>
<p>In this guide, you will see how to parse interesting fields out of serialized JSON bodies into their own log attributes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/json/#prerequisites
tag_set: logs-pipelines, guides, json
image_urls: 
tracking_id: docs-logs-pipelines-guides-json-prerequisites
group_tracking_ids: docs-logs-pipelines-guides-json
<h2>Parse JSON logs with Pipelines: Prerequisites</h2>
<ul>
<li>You are <a href="/docs/userguide/logs/">sending logs to SigNoz</a>
.</li>
<li>Your logs contain <strong>serialized JSON</strong> data in the body.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/json/#create-a-pipeline-to-parse-log-attributes-out-of-json-body
tag_set: logs-pipelines, guides, json
image_urls: 
tracking_id: docs-logs-pipelines-guides-json-create-a-pipeline-to-parse-log-attributes-out-of-json-body
group_tracking_ids: docs-logs-pipelines-guides-json
<h2>Parse JSON logs with Pipelines: Create a Pipeline to Parse Log Attributes out of JSON Body: Create a Pipeline to Parse Log Attributes out of JSON Body</h2>
<p>After you have started sending logs with serialized JSON bodies to SigNoz, you can follow the steps below to create a pipeline for parsing log attributes out of the JSON data.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/json/#step-1-navigate-to-logs-pipelines-page
tag_set: logs-pipelines, guides, json
image_urls: https://signoz.io/img/logs/pipelines/navigate-to-logs-pipelines.png
tracking_id: docs-logs-pipelines-guides-json-step-1-navigate-to-logs-pipelines-page
group_tracking_ids: docs-logs-pipelines-guides-json
<p>Parse JSON logs with Pipelines: Create a Pipeline to Parse Log Attributes out of JSON Body: Step 1: Navigate to Logs Pipelines Page</p>
<p>Hover over the <strong>Logs</strong> menu in the sidebar and click on the <strong>Logs Pipeline</strong> submenu item.</p>
<p><img src="https://signoz.io/img/logs/pipelines/navigate-to-logs-pipelines.png" alt="Sidebar navigation for getting to Logs Pipelines page" /></p>
<p>_</p>
<p>Sidebar navigation for getting to Logs Pipelines page</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/json/#step-2-create-a-new-pipeline
tag_set: logs-pipelines, guides, json
image_urls: https://signoz.io/img/logs/pipelines/empty-state-new-pipeline-button.png, https://signoz.io/img/logs/pipelines/enter-edit-mode.png, https://signoz.io/img/logs/pipelines/add-a-new-pipeline.png, https://signoz.io/img/logs/pipelines/add-new-pipeline-modal.png
tracking_id: docs-logs-pipelines-guides-json-step-2-create-a-new-pipeline
group_tracking_ids: docs-logs-pipelines-guides-json
<p>Parse JSON logs with Pipelines: Create a Pipeline to Parse Log Attributes out of JSON Body: Step 2: Create a New Pipeline</p>
<ul>
<li>
<p>Open the &quot;Create New Pipeline&quot; dialog.</p>
<ul>
<li>If you do not have existing pipelines, press the &quot;<strong>New Pipeline</strong>&quot; button.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/empty-state-new-pipeline-button.png" alt="New Pipeline Button" /></p>
<p>_</p>
<p>New Pipeline Button</p>
<p>_</p>
<ul>
<li>If you already have some pipelines, press the &quot;<strong>Enter Edit Mode</strong>&quot; button and then click the &quot;<strong>Add a New Pipeline</strong>&quot; button at the bottom of the list of pipelines.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/enter-edit-mode.png" alt="Enter edit mode button" /></p>
<p>_</p>
<p>Enter Edit Mode button</p>
<p>_</p>
<p><img src="https://signoz.io/img/logs/pipelines/add-a-new-pipeline.png" alt="Add a New Pipeline button" /></p>
<p>_</p>
<p>Add a New Pipeline button</p>
<p>_</p>
</li>
<li>
<p>Provide details about the pipeline in the Create Pipeline Dialog.</p>
<ul>
<li>Use the <strong>Name</strong> field to give your pipeline a descriptive short name.</li>
<li>Use the <strong>Description</strong> field to add a detailed description for your pipeline.</li>
<li>Use the <strong>Filter</strong> field to select the logs you want to process with this pipeline. Typically, these are filters identifying the source of the logs you want to process. <code>service = checkout</code> for example.</li>
<li>Use the <strong>Filtered Logs Preview</strong> to verify that the logs you want to process will be selected by the pipeline. Note that while it is not ideal, it is ok if your filter selects other non JSON logs too.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/add-new-pipeline-modal.png" alt="Create New Pipeline dialog" /></p>
<p>_</p>
<p>Create New Pipeline dialog</p>
<p>_</p>
</li>
<li>
<p>Press the &quot;<strong>Create</strong>&quot; button if everything looks right.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/json/#step-3-add-processors-for-parsing-desired-fields-into-log-attributes
tag_set: logs-pipelines, guides, json
image_urls: https://signoz.io/img/logs/pipelines/newly-created-json-parsing-pipeline.png, https://signoz.io/img/logs/pipelines/json-pipeline-add-processor-button.png, https://signoz.io/img/logs/pipelines/add-json-parsing-processor-dialog.png, https://signoz.io/img/logs/pipelines/add-move-processor-for-country.png, https://signoz.io/img/logs/pipelines/json-parser-remove-temp-attribute.png
tracking_id: docs-logs-pipelines-guides-json-step-3-add-processors-for-parsing-desired-fields-into-log-attributes
group_tracking_ids: docs-logs-pipelines-guides-json
<p>Parse JSON logs with Pipelines: Create a Pipeline to Parse Log Attributes out of JSON Body: Step 3: Add Processors for Parsing Desired Fields into Log Attributes: Step 3: Add Processors for Parsing Desired Fields into Log Attributes: Step 3: Add Processors for Parsing Desired Fields into Log Attributes</p>
<p>Each added attribute increases the size of your log records in the database. So it is often desirable to parse only a few fields of interest out of the JSON body into their own log attributes.<br />
To achieve this, we will first use a JSON parsing processor to parse the log body into a temporary attribute, then we will move the desired fields from the temporary attribute into their own log attributes, and finally remove the temporary log attribute.</p>
<ul>
<li>Expand the new Pipeline to add processors to it.</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/newly-created-json-parsing-pipeline.png" alt="Creating a Pipeline adds it to the end of the Pipelines List. It can be expanded by clicking the highlighted icon." /></p>
<p>_</p>
<p>Creating a Pipeline adds it to the end of the Pipelines List. It can be expanded by clicking the highlighted icon.</p>
<p>_</p>
<p><img src="https://signoz.io/img/logs/pipelines/json-pipeline-add-processor-button.png" alt="Expanding a pipeline shows the Add Processor button" /></p>
<p>_</p>
<p>Expanding a pipeline shows the Add Processor button</p>
<p>_</p>
<ul>
<li>
<p>Add a processor to parse the JSON log body into a temporary attribute.</p>
<ul>
<li>
<p>Click the <strong>Add Processor</strong> Button to bring up the Dialog for adding a new processor.</p>
</li>
<li>
<p>Select <code>Json Parser</code> in the <strong>Select Processor Type</strong> field.</p>
</li>
<li>
<p>Use the <strong>Name of Json Parser Processor</strong> field to set a short descriptive name for the processor.</p>
</li>
<li>
<p>Set the <strong>Parse From</strong> field to <code>body</code></p>
</li>
<li>
<p>Use <strong>Parse To</strong> field to define the attribute where the parsed JSON body should be stored temporarily. For example <code>attributes.temp_parsed_body</code>.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/add-json-parsing-processor-dialog.png" alt="Add New Processor Dialog" /></p>
<p>_</p>
<p>Add New Processor Dialog</p>
<p>_</p>
<ul>
<li>Press the <strong>Create</strong> button to finish adding the processor.</li>
</ul>
</li>
<li>
<p>Add <strong>Move</strong> processors to get desired fields out of the temporary attribute containing parsed JSON into their own log attributes.</p>
<ul>
<li>
<p>Click the <strong>Add Processor</strong> Button to bring up the Dialog for adding a new processor.</p>
</li>
<li>
<p>Select <code>Move</code> in the <strong>Select Processor Type</strong> field.</p>
</li>
<li>
<p>Use the <strong>Name of Move Processor</strong> field to set a short descriptive name for the processor.</p>
</li>
<li>
<p>Set the <strong>From</strong> field to the path of the parsed JSON field to be extracted. For example <code>attributes.temp_parsed_body.country</code></p>
</li>
<li>
<p>Use <strong>To</strong> field to define the attribute where the JSON field should be stored.</p>
</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/add-move-processor-for-country.png" alt="Add Move Processor Dialog" /></p>
<p>_</p>
<p>Add Move Processor Dialog</p>
<p>_</p>
<ul>
<li>Press the <strong>Create</strong> button to finish adding the processor.</li>
<li>Repeat these steps to create a <strong>Move</strong> processor for moving each desired JSON field into its own log attribute.</li>
</ul>
</li>
<li>
<p>Add processor for removing attribute used for temporarily storing the parsed JSON log body.</p>
<ul>
<li>
<p>Click the <strong>Add Processor</strong> Button to bring up the Dialog for adding a new processor.</p>
</li>
<li>
<p>Select <code>Remove</code> in the <strong>Select Processor Type</strong> field.</p>
</li>
<li>
<p>Use the <strong>Name of Remove Processor</strong> field to set a short descriptive name for the processor.</p>
</li>
<li>
<p>Set <strong>Field</strong> input to path of the attribute we used for storing parsed JSON body temporarily. For example <code>attributes.temp_parsed_body</code></p>
</li>
</ul>
<p><img src="https://signoz.io/img/logs/pipelines/json-parser-remove-temp-attribute.png" alt="Remove Processor Dialog" /></p>
<p>_</p>
<p>Remove Processor Dialog</p>
<p>_</p>
<ul>
<li>Press the <strong>Create</strong> button to finish adding the processor.</li>
</ul>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/json/#step-4-preview-and-validate-pipeline-processing
tag_set: logs-pipelines, guides, json
image_urls: https://signoz.io/img/logs/pipelines/json-parsing-pipeline-expanded.png, https://signoz.io/img/logs/pipelines/json-parsing-pipeline-preview-init.png, https://signoz.io/img/logs/pipelines/json-parsing-pipeline-preview.png
tracking_id: docs-logs-pipelines-guides-json-step-4-preview-and-validate-pipeline-processing
group_tracking_ids: docs-logs-pipelines-guides-json
<p>Parse JSON logs with Pipelines: Create a Pipeline to Parse Log Attributes out of JSON Body: Step 3: Add Processors for Parsing Desired Fields into Log Attributes: Step 4: Preview and Validate Pipeline Processing</p>
<p>At this point you should have the pipeline ready with all necessary processors.</p>
<p><img src="https://signoz.io/img/logs/pipelines/json-parsing-pipeline-expanded.png" alt="Expanded Pipeline with Processors for parsing desired fields out of JSON body into their own log attributes" /></p>
<p>_</p>
<p>Expanded Pipeline with Processors for parsing desired fields out of JSON body into their own log attributes</p>
<p>_</p>
<p>Before we save and deploy the pipeline, it is best to simulate processing on some sample logs to validate that the pipeline will work as expected.<br />
Click the &quot;eye&quot; icon in the actions column for the pipeline to bring up the Pipeline Preview Dialog</p>
<p><img src="https://signoz.io/img/logs/pipelines/json-parsing-pipeline-preview-init.png" alt="Pipeline Preview with Sample Logs" /></p>
<p>_</p>
<p>Pipeline Preview with Sample Logs</p>
<p>_</p>
<p>The preview Dialog will start out with sample logs queried from the database. You can adjust the sample logs search duration if there are no recent samples available.<br />
To simulate pipeline processing, press the <strong>Simulate Processing</strong> button in the bottom section of the Pipeline Preview Dialog.<br />
This will simulate pipeline processing on the sample logs and show the output.</p>
<p><img src="https://signoz.io/img/logs/pipelines/json-parsing-pipeline-preview.png" alt="Pipeline Preview with Processed Output" /></p>
<p>_</p>
<p>Pipeline Preview with Processed Output</p>
<p>_</p>
<p>You can click on the <em>expand icon</em> on the right end of each processed log to open the detailed view for that log. Expand some of the processed logs to verify that your desired log attributes were extracted as expected.<br />
If you see any issues, you can close the preview, edit your processors as needed and preview again to verify. Iterate on your pipeline and processor config until it all works just the way you want it.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/guides/json/#step-5-save-pipelines-and-verify
tag_set: logs-pipelines, guides, json
image_urls: https://signoz.io/img/logs/pipelines/save-json-parsing-pipeline.png, https://signoz.io/img/logs/pipelines/change-history.png
tracking_id: docs-logs-pipelines-guides-json-step-5-save-pipelines-and-verify
group_tracking_ids: docs-logs-pipelines-guides-json
<p>Parse JSON logs with Pipelines: Create a Pipeline to Parse Log Attributes out of JSON Body: Step 3: Add Processors for Parsing Desired Fields into Log Attributes: Step 5: Save Pipelines and Verify</p>
<p>Once you have previewed your pipeline and verified that it will work as expected, press the <strong>Save Configuration</strong> button at the bottom of the pipelines list to save pipelines. This will store the latest state of your pipelines and will deploy them for pre-processing.</p>
<p><img src="https://signoz.io/img/logs/pipelines/save-json-parsing-pipeline.png" alt="Save Configuration Button" /></p>
<p>_</p>
<p>Save Configuration Button</p>
<p>_</p>
<p>You can track the deployment status of your pipelines using the <strong>Change History</strong> tab at the top of the pipelines page.</p>
<p><img src="https://signoz.io/img/logs/pipelines/change-history.png" alt="Pipelines Change History" /></p>
<p>_</p>
<p>Pipelines Change History</p>
<p>_</p>
<p>Wait for a few minutes to let the pipelines deploy and for the latest batches of logs to get pre-processed and stored in the database. Then you can head over to the logs explorer to verify that log attributes are getting parsed out of serialized JSON in log bodies as expected.<br />
You can now start using the new log attributes you have extracted for more efficient filtering and aggregations.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/troubleshooting/signoz-cloud/metrics-troubleshooting/
tag_set: troubleshooting, signoz-cloud, metrics-troubleshooting
image_urls: https://signoz.io/img/docs/troubleshooting/signoz-cloud/cloud-troubleshooting-metrics-1.webp
tracking_id: docs-troubleshooting-signoz-cloud-metrics-troubleshooting
group_tracking_ids: docs-troubleshooting-signoz-cloud-metrics-troubleshooting
<h2>Metrics SigNoz Cloud Troubleshooting</h2>
<h3>## Q. I see an empty Dashboard, what should I do ?</h3>
<p>Toggle for answer</p>
<p>Try changing the variable in Dashboard. For example, if you're using the <strong><a href="https://github.com/SigNoz/dashboards/blob/main/k8s-infra-metrics/kubernetes-pod-metrics-overall.json">Kubernetes Pod Metrics - Overall</a></strong> dashboard, you can select $node_name variable as <strong>ALL</strong> to see the data as shown in below image.</p>
<p><img src="https://signoz.io/img/docs/troubleshooting/signoz-cloud/cloud-troubleshooting-metrics-1.webp" alt="Select a variable to see Dashboard Values" /></p>
<p>_</p>
<p>Selecting ALL as $node_name variable in Kubernetes Pod Metrics - Overall Dashboard</p>
<p>_</p>
<h3>## Q. How to see metrics of a service ?</h3>
<p>Toggle for answer</p>
<p><a href="https://signoz.io/docs/userguide/manage-panels/">Add a panel</a> in a Dashboard and you can see the metrics in dropdown.</p>
<p>We are working on Metrics explorer which will allow viewing all metrics without using the Dashboards. This will be live soon!</p>
<h3>## Q. I want to import Grafana Dashboards but the option is disabled for me.</h3>
<p>Toggle for answer</p>
<p>This functionality doesn't work at the moment so it's disabled.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/overview/
tag_set: traces-management, trace-api, overview
image_urls: 
tracking_id: docs-traces-management-trace-api-overview
group_tracking_ids: docs-traces-management-trace-api-overview
<h2>Trace API</h2>
<h2>Overview</h2>
<hr />
<p>The SigNoz Trace API is a robust interface which enables developers to manage and analyze trace data efficiently. This API facilitates various operations:</p>
<ul>
<li>
<p><strong>Searching Traces:</strong> Allows users to search for traces based on specific filters. For example, searching for traces based on the error code.</p>
</li>
<li>
<p><strong>Aggregating Traces:</strong> Allows users to aggregate traces based on specific filters. For example, aggregating traces based on the error code.</p>
</li>
</ul>
<h2>API Endpoint</h2>
<hr />
<p>Endpoint for Trace API:</p>
<p><code>POST</code> <code>https://{URL}/api/v3/query_range</code></p>
<p>Replace <code>{URL}</code> with your instance URL, e.g., example.signoz.io.</p>
<h2>Prerequisites</h2>
<hr />
<p>Access Token: To access this API, you need an Access Token. Navigate to the <code>Settings</code> page in the SigNoz UI and create a new Access Token.</p>
<p><img src="https://github.com/SigNoz/signoz/assets/9512100/9ca6c788-b60d-4051-8c14-c260a31bdb7d" alt="image" /></p>
<p>Access Tokens can only be created/managed by users with the <code>Admin</code> role. If you don't have the <code>Admin</code> role, contact your organization's admin to create an Access Token for you.</p>
<h2>Authentication</h2>
<hr />
<p>Using the Access Token: Add the Access token to your request header as follows:</p>
<pre><code>SIGNOZ-API-KEY:{YOUR_ACCESS_TOKEN}
</code></pre>
<p>üí° Tip</p>
<p>Secure storage and handling of your Access Token is crucial to prevent unauthorized access.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/vercel_logs_to_signoz/
tag_set: userguide, vercel_logs_to_signoz
image_urls: 
tracking_id: docs-userguide-vercel_logs_to_signoz
group_tracking_ids: docs-userguide-vercel_logs_to_signoz
<h2>Stream Logs from Vercel to SigNoz</h2>
<p>If you are running your applications on <a href="https://vercel.com/">Vercel</a>
, you can stream logs from Vercel to SigNoz using <a href="https://vercel.com/docs/observability/log-drains-overview/log-drains#configure-a-log-drain">log drains</a>.</p>
<p>Log Drains are only supported in Vercel Pro and Enterprise accounts.</p>
<h2>Stream Vercel logs to SigNoz in SigNoz cloud</h2>
<hr />
<ul>
<li>
<p>From the Vercel dashboard, go to Team Settings &gt; Log Drains.</p>
</li>
<li>
<p>Select sources to collect logs</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fvercel%2Fsources.webp&amp;w=1920&amp;q=75" alt="Vercel sources" /></p>
</li>
<li>
<p>Choose delivery format as <code>JSON</code></p>
</li>
<li>
<p>Choose your target projects</p>
</li>
<li>
<p>Enter the endpoint. It will be</p>
<pre><code>https://ingest.&lt;REGION&gt;.signoz.cloud:443/logs/json
</code></pre>
<p>Depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Enable custom headers and add the headers <code>signoz-access-token</code> and <code>x-vercel-verify</code> The value of <code>x-vercel-verify</code> will be visible on the screen and <code>signoz-access-token</code> will be the token you use for sending data to SigNoz</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fvercel%2Fheaders.webp&amp;w=2048&amp;q=75" alt="Vercel Custom Headers" /></p>
</li>
<li>
<p>Click on <code>Test Log Drain</code> , if successful you will see sample logs on SigNoz UI (Might take some time).</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fvercel%2Fsample-logs.webp&amp;w=3840&amp;q=75" alt="Sample Logs" /></p>
</li>
<li>
<p>Click on <code>Verify</code> button and then <code>Add Log Drain</code> in Vercel.</p>
</li>
</ul>
<p>Now your logs will be sent to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_syslogs/#collecting-syslogs
tag_set: userguide, collecting_syslogs
image_urls: 
tracking_id: docs-userguide-collecting_syslogs-collecting-syslogs
group_tracking_ids: docs-userguide-collecting_syslogs
<h1>Collecting syslogs: Collecting syslogs - Collecting Syslogs</h1>
<p>With SigNoz you can collect your syslogs logs and perform different queries on top of it. We will demonstrate how to configure <code>rsyslog</code> to forward system logs to tcp endpoint of otel-collector and use <code>syslog</code> receiver in OpenTelemetry Collector to receive and parse the logs. Below are the steps to collect syslogs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_syslogs/#collect-syslogs-in-signoz-cloud
tag_set: userguide, collecting_syslogs
image_urls: 
tracking_id: docs-userguide-collecting_syslogs-collect-syslogs-in-signoz-cloud
group_tracking_ids: docs-userguide-collecting_syslogs
<h2>Collecting syslogs: Collect Syslogs in SigNoz cloud</h2>
<p>If you don‚Äôt already have a SigNoz cloud account, you can sign up <a href="https://signoz.io/teams/">here</a>.</p>
<ul>
<li>
<p>Add otel collector binary to your VM by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">guide</a>
.</p>
</li>
<li>
<p>Add the syslog reciever to <code>config.yaml</code> to otel-collector.</p>
<pre><code>receivers:
  syslog:
    tcp:
      listen_address: &quot;0.0.0.0:54527&quot;
    protocol: rfc3164
    location: UTC
    operators:
      - type: move
        from: attributes.message
        to: body
...
</code></pre>
<p>Here we are collecting the logs and moving message from attributes to body using operators that are available. You can read more about operators <a href="/docs/userguide/logs/#operators-for-parsing-and-manipulating-logs">here</a>
.</p>
<p>For more configurations that are available for syslog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/syslogreceiver">here</a>
.</p>
</li>
<li>
<p>Next we will modify our pipeline inside <code>config.yaml</code> of otel-collector to include the receiver we have created above.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, syslog]
        processors: [batch]
        exporters: [otlp]
</code></pre>
</li>
<li>
<p>Now we can restart the otel collector so that new changes are applied and we can forward our logs to port <code>54527</code>.</p>
</li>
<li>
<p>Modify your <code>rsyslog.conf</code> file present inside <code>/etc/</code> by running the following command:</p>
<pre><code>sudo vim /etc/rsyslog.conf
</code></pre>
<p>and adding the this line at the end</p>
<pre><code>template(
  name=&quot;UTCTraditionalForwardFormat&quot;
  type=&quot;string&quot;
  string=&quot;&lt;%PRI%&gt;%TIMESTAMP:::date-utc% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%&quot;
)

*.* action(type=&quot;omfwd&quot; target=&quot;0.0.0.0&quot; port=&quot;54527&quot; protocol=&quot;tcp&quot; template=&quot;UTCTraditionalForwardFormat&quot;)
</code></pre>
<p>For production use cases it is recommended to use something like below:</p>
<pre><code>template(
  name=&quot;UTCTraditionalForwardFormat&quot;
  type=&quot;string&quot;
  string=&quot;&lt;%PRI%&gt;%TIMESTAMP:::date-utc% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%&quot;
)

*.*  action(type=&quot;omfwd&quot; target=&quot;0.0.0.0&quot; port=&quot;54527&quot; protocol=&quot;tcp&quot;
        action.resumeRetryCount=&quot;10&quot;
        queue.type=&quot;linkedList&quot; queue.size=&quot;10000&quot; template=&quot;UTCTraditionalForwardFormat&quot;)
</code></pre>
<p>So that you have retries and queue in place to de-couple the sending from the other logging action. Also we are assuming that you are running the otel binary on the same host. If not, the value of <code>target</code> might change depending on your environment.</p>
</li>
<li>
<p>Now restart your rsyslog service by running <code>sudo systemctl restart rsyslog.service</code></p>
</li>
<li>
<p>You can check the status of service by running <code>sudo systemctl status rsyslog.service</code></p>
</li>
<li>
<p>If there are no errors your logs will be visible on SigNoz UI.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_syslogs/#collect-syslogs-in-self-hosted-signoz
tag_set: userguide, collecting_syslogs
image_urls: 
tracking_id: docs-userguide-collecting_syslogs-collect-syslogs-in-self-hosted-signoz
group_tracking_ids: docs-userguide-collecting_syslogs
<h2>Collecting syslogs: Collect Syslogs in Self-Hosted SigNoz</h2>
<ul>
<li>
<p>Modify the <code>docker-compose.yaml</code> file present inside <code>deploy/docker/clickhouse-setup</code> to expose a port, in this case <code>54527</code> so that we can forward syslogs to this port.</p>
<pre><code>...
otel-collector:
    image: signoz/signoz-otel-collector:0.88.11
    command: [&quot;--config=/etc/otel-collector-config.yaml&quot;]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - &quot;54527:54527&quot;
...
</code></pre>
</li>
<li>
<p>Add the syslog reciever to <code>otel-collector-config.yaml</code> which is present inside <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>receivers:
  syslog:
    tcp:
      listen_address: &quot;0.0.0.0:54527&quot;
    protocol: rfc3164
    location: UTC
    operators:
      - type: move
        from: attributes.message
        to: body
...
</code></pre>
<p>Here we are collecting the logs and moving message from attributes to body using operators that are available. You can read more about operators <a href="/docs/userguide/logs/#operators-for-parsing-and-manipulating-logs">here</a></p>
<p>For more configurations that are available for syslog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/syslogreceiver">here</a>
.</p>
</li>
<li>
<p>Next we will modify our pipeline inside <code>otel-collector-config.yaml</code> to include the receiver we have created above.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, syslog]
        processors: [batch]
        exporters: [clickhouselogsexporter]
</code></pre>
</li>
<li>
<p>Now we can restart the otel collector container so that new changes are applied and we can forward our logs to port <code>54527</code>.</p>
</li>
<li>
<p>Modify your <code>rsyslog.conf</code> file present inside <code>/etc/</code> by running <code>sudo vim /etc/rsyslog.conf</code> and adding the this line at the end</p>
<pre><code>template(
  name=&quot;UTCTraditionalForwardFormat&quot;
  type=&quot;string&quot;
  string=&quot;&lt;%PRI%&gt;%TIMESTAMP:::date-utc% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%&quot;
)

*.* action(type=&quot;omfwd&quot; target=&quot;0.0.0.0&quot; port=&quot;54527&quot; protocol=&quot;tcp&quot; template=&quot;UTCTraditionalForwardFormat&quot;)
</code></pre>
<p>For production use cases it is recommended to using something like</p>
<pre><code>template(
  name=&quot;UTCTraditionalForwardFormat&quot;
  type=&quot;string&quot;
  string=&quot;&lt;%PRI%&gt;%TIMESTAMP:::date-utc% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%&quot;
)

*.*  action(type=&quot;omfwd&quot; target=&quot;0.0.0.0&quot; port=&quot;54527&quot; protocol=&quot;tcp&quot;
        action.resumeRetryCount=&quot;10&quot;
        queue.type=&quot;linkedList&quot; queue.size=&quot;10000&quot; template=&quot;UTCTraditionalForwardFormat&quot;)
</code></pre>
<p>So that you have retires and queue in place to de-couple the sending from the other logging action.</p>
<p>The value of <code>target</code> might vary depending on where SigNoz is deployed, since it is deployed on the same host I am using <code>0.0.0.0</code> for more help you can visit <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">here</a></p>
</li>
<li>
<p>Now restart your rsyslog service by running <code>sudo systemctl restart rsyslog.service</code></p>
</li>
<li>
<p>You can check the status of service by running <code>sudo systemctl status rsyslog.service</code></p>
</li>
<li>
<p>If there are no errors your logs will be visible on SigNoz UI.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/data-ingestion/#overview
tag_set: azure-monitoring, bootstrapping, data-ingestion
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-data-ingestion-overview
group_tracking_ids: docs-azure-monitoring-bootstrapping-data-ingestion
<h2>Centralized Collector Setup: Centralized Collector Setup - Overview</h2>
<p>Azure Event Hubs is a big data streaming platform and event ingestion service that can receive and process millions of events per second. It is an ideal solution for centralizing logging and enabling real-time log streaming for applications running on Azure or on-premises.</p>
<p>By creating an Event Hubs namespace and an event hub, you can stream logs from various sources into a central location. This allows you to collect, store, and analyze logs efficiently, gaining valuable insights into your application's behavior and performance.</p>
<p>SigNoz, an open-source application performance monitoring and observability tool, can be integrated with Azure Event Hubs to provide a powerful log management solution. By streaming logs to SigNoz via Event Hubs, you can leverage SigNoz's features for log aggregation, querying, visualization, and alerting.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/data-ingestion/#prerequisites
tag_set: azure-monitoring, bootstrapping, data-ingestion
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-data-ingestion-prerequisites
group_tracking_ids: docs-azure-monitoring-bootstrapping-data-ingestion
<h2>Centralized Collector Setup: Prerequisites</h2>
<p>Before proceeding with the setup, ensure you have the following:</p>
<ul>
<li>An Azure subscription</li>
<li><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/data-ingestion/#setup
tag_set: azure-monitoring, bootstrapping, data-ingestion
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-data-ingestion-setup
group_tracking_ids: docs-azure-monitoring-bootstrapping-data-ingestion
<h2>Centralized Collector Setup: Setup</h2>
<p>To set up an Event Hubs namespace and an event hub for log streaming, follow these steps: You might want to repeat these steps for other regions.</p>
<ol>
<li><strong>Create an Event Hubs namespace:</strong>
<ul>
<li>Sign in to the Azure portal (<a href="https://portal.azure.com/">https://portal.azure.com</a>
).</li>
<li>Click on &quot;Create a resource&quot; and search for &quot;Event Hubs&quot;.</li>
<li>Click on &quot;Event Hubs&quot; and then click &quot;Create&quot;.</li>
<li>Fill in the required details:
<ul>
<li>Resource group: Choose an existing resource group in the same region as the resources you want to monitor or create a new one.</li>
<li>Namespace name: Enter a unique name for your namespace, such as¬†<code>&lt;orgName&gt;-obs-signoz</code>.</li>
<li>Pricing tier: Choose the appropriate pricing tier based on your logging requirements.</li>
<li>Region: Ideally should be the same region as the resources you want to monitor.</li>
<li>Throughput units: The throughput units determine the maximum number of events that can be processed per second. You need to choose a value that is appropriate for your logging requirements.</li>
</ul>
</li>
<li>Click &quot;Review + create&quot; and then &quot;Create&quot; to provision the Event Hubs namespace.</li>
</ul>
</li>
<li><strong>Create an event hub:</strong>
<ul>
<li>Once the Event Hubs namespace is created, navigate to it in the Azure portal.</li>
<li>Click on &quot;Event Hubs&quot; in the left menu.</li>
<li>Click on &quot;+ Event Hub&quot; to create a new event hub.</li>
<li>Enter a name for your event hub, such as <code>logs</code>.</li>
<li>Click &quot;Create&quot; to create the event hub.</li>
</ul>
</li>
<li><strong>Create a SAS policy for the event hub and copy the connection string:</strong>
<ul>
<li>Navigate to the Event Hubs namespace in the Azure portal.</li>
<li>Search for &quot;Event Hubs&quot; in the left menu and click on it.</li>
<li>Click on the event hub you created.</li>
<li>Click on &quot;Shared access policies&quot; in the left menu.</li>
<li>Click on &quot;Add&quot; to create a new policy, name the policy as <code>signozListen</code></li>
<li>Select the &quot;Listen&quot; permission and specify the expiration time for the policy.</li>
<li>Click &quot;Save&quot; to create the policy.</li>
<li>Now, click on the created policy to copy the <em>Connection string‚Äìprimary key</em>. Note this connection string as you will need it to configure the Central Collector.</li>
</ul>
</li>
<li><strong>Configure OpenTelemetry integration:</strong>
<ul>
<li>
<p>Add a new receiver to <a href="../collector-setup">Central Collector Setup</a></p>
</li>
<li>
<p>Configure the receiver to receive logs from Azure Event Hubs.</p>
</li>
<li>
<p>Provide the necessary connection details, such as the Event Hubs namespace connection string and the event hub name.</p>
</li>
</ul>
</li>
<li><strong>Stream logs to Event Hubs:</strong>
<ul>
<li>Configure the diagnostic settings of your Azure services to forward logs to the Event Hub you created.</li>
<li>Ensure that the logs are sent in the <a href="https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/resource-logs-schema">Azure Common Log Format</a> for compatibility with OpenTelemetry Receiver.</li>
<li>Verify that logs are being successfully streamed to Event Hubs and received by SigNoz.</li>
</ul>
</li>
</ol>
<p>For more detailed instructions on creating an Event Hubs namespace and event hub, refer to the Azure documentation: <a href="https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-create">Azure Event Hub</a>.</p>
<p>For more configuration options for the receiver see the OpenTelemetry Documentation, <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/azureeventhubreceiver">Azure Event Hub Receiver</a></p>
<p>üìù Note</p>
<p>This is a beginner-friendly guide. If you are a more advanced user, you might want to use IaC (Infrastructure as Code) to automate the deployment of the Azure resources and the OpenTelemetry Collector.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/data-ingestion/#troubleshooting
tag_set: azure-monitoring, bootstrapping, data-ingestion
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-data-ingestion-troubleshooting
group_tracking_ids: docs-azure-monitoring-bootstrapping-data-ingestion
<h2>Centralized Collector Setup: Setup: Troubleshooting</h2>
<p>If you encounter any issues during the setup process, consider the following troubleshooting steps:</p>
<ul>
<li>Verify that you have the necessary permissions to create resources in your Azure subscription.</li>
<li>Double-check the connection details provided to SigNoz for the Event Hubs integration.</li>
<li>Ensure that your applications or logging agents are correctly configured to send logs to the specified Event Hubs namespace and event hub.</li>
<li>Check the Event Hubs metrics and logs in the Azure portal for any error messages or abnormalities.</li>
<li>Consult the SigNoz documentation or reach out to their support channels for further assistance.</li>
</ul>
<p>By following this guide and integrating Azure Event Hubs with SigNoz, you can establish a robust and scalable logging infrastructure. Centralizing your logs and enabling real-time streaming allows you to gain valuable insights, troubleshoot issues efficiently, and maintain the health and performance of your applications.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#overview
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-overview
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Central Collector Setup - Overview</h2>
<p>Setting up a centralised OpenTelemetry Collector offers several benefits for managing observability data in Azure environments. It allows for a clear separation between platform and application teams, reduces configuration overhead for application teams, and enables the collection of system metrics from Azure Monitor and observability data from Azure Event Hub.</p>
<p>This guide will walk you through the process of creating a centralized OpenTelemetry Collector using Azure Kubernetes Service (AKS) or an Azure Virtual Machine (VM).</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#prerequisites
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-prerequisites
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Prerequisites</h2>
<p>Before proceeding with the setup, ensure you have the following prerequisites:</p>
<ol>
<li>
<p>An active Azure subscription</p>
</li>
<li>
<p>Azure CLI installed on your local machine</p>
</li>
<li>
<p><a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#setting-up-the-opentelemetry-collector
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-setting-up-the-opentelemetry-collector
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Setting Up the OpenTelemetry Collector</h2>
<p>Deploying the OpenTelemetry Collector can be approached in two distinct ways, depending on your infrastructure:</p>
<ol>
<li><strong>Utilizing OpenTelemetry Helm Charts:</strong>¬†Ideal for those with an existing Kubernetes infrastructure.</li>
<li><strong>Running the Collector on a Virtual Machine:</strong>¬†For users not using Kubernetes and relying on serverless environments for their workloads.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#installing-with-opentelemetry-helm-charts
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: https://signoz.io/img/docs/azure-monitoring/service-principal-app-overview.webp
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-installing-with-opentelemetry-helm-charts
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<p>Central Collector Setup: Setup: ## Setup - Installing with OpenTelemetry Helm Charts</p>
<p>Prior to installation, you must ensure your Kubernetes cluster is ready and that you have the necessary permissions to deploy applications. Follow these steps to use Helm for setting up the Collector:</p>
<ol>
<li>
<p><strong>Add the OpenTelemetry Helm repository:</strong></p>
<pre><code>helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
</code></pre>
</li>
<li>
<p><strong>Prepare the <code>otel-collector-values.yaml</code> Configuration</strong></p>
<h4>##     Azure Event Hub Receiver Configuration</h4>
<p>If you haven't created the logs Event Hub, you can create one by following the steps in the <a href="../../bootstrapping/data-ingestion">Azure Event Hubs documentation</a>
.</p>
<p>and replace the placeholders <code>&lt;Primary Connection String&gt;</code> with the primary connection string for your Event Hub, it should look something like this:</p>
<pre><code>connection: Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName
</code></pre>
<p>The Event Hub docs have a step to create a SAS policy for the event hub and copy the connection string.</p>
<h4>##     Azure Monitor Receiver Configuration</h4>
<p>You will need to set up a <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal">service principal</a> with Read permissions to receive data from Azure Monitor.</p>
<ol>
<li>
<p>Follow the steps in the <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#register-an-application-with-microsoft-entra-id-and-create-a-service-principal">Create a service principal Azure Doc</a> documentation to create a service principal. You can name it <code>signoz-central-collector-app</code> the redirect URI can be empty.</p>
</li>
<li>
<p>To add read permissions to Azure Monitor, Follow the <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#assign-a-role-to-the-application">Assign Role</a> documentation. The read acess can be given to the full subscription.</p>
</li>
<li>
<p>There are multiple ways to authenticate the service principal, we will use the client secret option, follow <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#option-3-create-a-new-client-secret">Creating a client secret</a> and don't forget to copy the client secret. The secret is used in the configuration file as <code>client_secret</code>.</p>
</li>
<li>
<p>To find <code>client_id</code> and <code>tenant_id</code>, go to the <a href="https://portal.azure.com/">Azure Portal</a> and search for the <code>Application</code> you created. You would see the <code>Application (client) ID</code> and <code>Directory (tenant) ID</code> in the Overview section.</p>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/service-principal-app-overview.webp" alt="Application Overview" /></p>
<p>_</p>
<p>Application Overview</p>
<p>_</p>
<ol start="5">
<li>
<p>To find <code>subscription_id</code>, follow steps in <a href="https://learn.microsoft.com/en-us/azure/azure-portal/get-subscription-tenant-id#find-your-azure-subscription">Find Your Subscription</a> and populate them in the configuration file.</p>
</li>
<li>
<p>Ensure you replace the placeholders <code>&lt;region&gt;</code> and <code>&lt;ingestion-key&gt;</code> with the appropriate values for your signoz cloud instance.</p>
</li>
</ol>
</li>
</ol>
<p>Below is an example targeting the SigNoz backend with Azure Monitor receivers configured:</p>
<pre><code>service:
  pipelines:
    metrics/am:
      receivers: [azuremonitor]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp, azureeventhub]
      processors: [batch]
      exporters: [otlp]
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  azureeventhub:
    connection: Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName
    format: &quot;azure&quot;
  azuremonitor:
    subscription_id: &quot;&lt;Subscription ID&gt;&quot;
    tenant_id: &quot;&lt;AD Tenant ID&gt;&quot;
    client_id: &quot;&lt;Client ID&gt;&quot;
    client_secret: &quot;&lt;Client Secret&gt;&quot;
    resource_groups: [&quot;&lt;rg-1&gt;&quot;]
    collection_interval: 60s
processors:
  batch: {}
exporters:
  otlp:
    endpoint: &quot;ingest.&lt;region&gt;.signoz.cloud:443&quot;
    tls:
      insecure: false
    headers:
      &quot;signoz-access-token&quot;: &quot;&lt;ingestion-key&gt;&quot;
</code></pre>
<ol start="3">
<li>
<p><strong>Deploy the OpenTelemetry Collector to your Kubernetes cluster:</strong></p>
<p>You'll need to prepare a custom configuration file, say <code>otel-collector-values.yaml</code>, that matches your environment's specific needs. Replace <code>&lt;namespace&gt;</code> with the Kubernetes namespace where you wish to install the Collector.</p>
<pre><code>helm install -n &lt;namespace&gt; --create-namespace otel-collector open-telemetry/opentelemetry-collector -f otel-collector-values.yaml
</code></pre>
<p>For more detail, refer to the <a href="https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector">official OpenTelemetry Helm Chart documentation</a>
, which offers comprehensive installation instructions and configuration options tailored to your environment's requirements.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#running-the-collector-on-a-virtual-machine
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: https://signoz.io/img/docs/azure-monitoring/service-principal-app-overview.webp
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-running-the-collector-on-a-virtual-machine
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<p>Central Collector Setup: Setup: Running the Collector on a Virtual Machine: Running the Collector on a Virtual Machine: Running the Collector on a Virtual Machine</p>
<p>If you're not using Kubernetes, setting up the OpenTelemetry Collector on a Virtual Machine (VM) is a straightforward alternative. This setup is compatible with cloud VM instances, your own data center, or even a local VM on your development machine. Here's how to do it:</p>
<ol>
<li>
<p><strong>Download and Install the OpenTelemetry Collector Binary:</strong></p>
<p>Please visit <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">Documentation For VM</a> which provides further guidance on a VM installation.</p>
<p>It's prudent to check available resources to ensure you're following the latest practices and utilizing updated features offered by OpenTelemetry. Follow the documentation to setup your collector and test the setup.</p>
</li>
<li>
<p><strong>Configure the OpenTelemetry Collector:</strong></p>
<p>The Collector requires configuration before it can start receiving Azure Monitor data. Create a file named <code>config.yaml</code> and populate it with the necessary configuration details. This file should be configured to fit your specific environment's requirements. For instance, you may want to setup receivers for the telemetry data you expect to collect and processors to handle that data appropriately, as well as exporters to send the data to your desired backend, like SigNoz, Jaeger, or Prometheus.</p>
<p>Below is a basic configuration example:</p>
<pre><code>service:
  pipelines:
    metrics/am:
      receivers: [azuremonitor]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp, azureeventhub]
      processors: [batch]
      exporters: [otlp]
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
    azureeventhub:
      connection: &lt;Primary Connection String&gt;
      format: &quot;azure&quot;
    azuremonitor:
      subscription_id: &quot;&lt;Subscription ID&gt;&quot;
      tenant_id: &quot;&lt;AD Tenant ID&gt;&quot;
      client_id: &quot;&lt;Client ID&gt;&quot;
      client_secret: &quot;&lt;Client Secret&gt;&quot;
      resource_groups: [&quot;&lt;rg-1&gt;&quot;]
      collection_interval: 60s
  processors:
    batch: {}
  exporters:
    otlp:
      endpoint: &quot;ingest.&lt;region&gt;.signoz.cloud:443&quot;
      tls:
        insecure: false
      headers:
        &quot;signoz-access-token&quot;: &quot;&lt;ingestion-key&gt;&quot;
</code></pre>
<h4>##     Azure Event Hub Receiver Configuration</h4>
<p>If you haven't created the logs Event Hub, you can create one by following the steps in the <a href="../../bootstrapping/data-ingestion">Azure Event Hubs documentation</a>
.</p>
<p>and replace the placeholders <code>&lt;Primary Connection String&gt;</code> with the primary connection string for your Event Hub, it should look something like this:</p>
<pre><code>connection: Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName
</code></pre>
<p>The Event Hub docs have a step to create a SAS policy for the event hub and copy the connection string.</p>
<h4>##     Azure Monitor Receiver Configuration</h4>
<p>You will need to set up a <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal">service principal</a> with Read permissions to receive data from Azure Monitor.</p>
<ol>
<li>
<p>Follow the steps in the <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#register-an-application-with-microsoft-entra-id-and-create-a-service-principal">Create a service principal Azure Doc</a> documentation to create a service principal. You can name it <code>signoz-central-collector-app</code> the redirect URI can be empty.</p>
</li>
<li>
<p>To add read permissions to Azure Monitor, Follow the <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#assign-a-role-to-the-application">Assign Role</a> documentation. The read acess can be given to the full subscription.</p>
</li>
<li>
<p>There are multiple ways to authenticate the service principal, we will use the client secret option, follow <a href="https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#option-3-create-a-new-client-secret">Creating a client secret</a> and don't forget to copy the client secret. The secret is used in the configuration file as <code>client_secret</code>.</p>
</li>
<li>
<p>To find <code>client_id</code> and <code>tenant_id</code>, go to the <a href="https://portal.azure.com/">Azure Portal</a> and search for the <code>Application</code> you created. You would see the <code>Application (client) ID</code> and <code>Directory (tenant) ID</code> in the Overview section.</p>
</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/service-principal-app-overview.webp" alt="Application Overview" /></p>
<p>_</p>
<p>Application Overview</p>
<p>_</p>
<ol start="5">
<li>
<p>To find <code>subscription_id</code>, follow steps in <a href="https://learn.microsoft.com/en-us/azure/azure-portal/get-subscription-tenant-id#find-your-azure-subscription">Find Your Subscription</a> and populate them in the configuration file.</p>
</li>
<li>
<p>Ensure you replace the placeholders <code>&lt;region&gt;</code> and <code>&lt;ingestion-key&gt;</code> with the appropriate values for your signoz cloud instance.</p>
</li>
</ol>
</li>
<li>
<p><strong>Run the Collector:</strong></p>
<p>With your configuration file ready, you can now start the Collector using the following command:</p>
<pre><code># Runs in background with the configuration we just created
./otelcol-contrib --config ./config.yaml &amp;&gt; otelcol-output.log &amp; echo &quot;$!&quot; &gt; otel-pid 
</code></pre>
</li>
<li>
<p><strong>Open Ports:</strong></p>
<p>You will need to open the following ports on your Azure VM:</p>
<ul>
<li>4317 for gRPC</li>
<li>4318 for HTTP</li>
</ul>
<p>You can do this by navigating to the Azure VM's Networking section and adding a new inbound rule for the ports.</p>
</li>
<li>
<p><strong>Validating the Deployment:</strong></p>
<p>Once the Collector is running, ensure that telemetry data is being successfully sent and received. Use the logging exporter as defined in your configuration file, or check the logs for any startup errors.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#configure-dns-label-for-collector
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: https://signoz.io/img/docs/azure-monitoring/ip-address-dns-label.webp
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-configure-dns-label-for-collector
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Setup: Configure DNS label For Collector</h2>
<p>To the IP address of the collector, you can add a DNS label to the Public IP address. This will make it easier to refer to the centralized collector from other services. You can do this by following these steps:</p>
<ol>
<li>Go to the Public IP address of the collector. This would be the IP address of the VM or Load Balancer in case of Kubernetes or Load Balanced collector.</li>
<li>Click on the &quot;Configuration&quot; tab.</li>
<li>Enter the DNS label you want to use for the collector.</li>
<li>Click on &quot;Save&quot;.</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/ip-address-dns-label.webp" alt="DNS label for collector" /></p>
<p>_</p>
<p>DNS label for collector</p>
<p>_</p>
<p>üìù Note</p>
<p>Please take note of the DNS label you have entered. You will need to configure this for your other services as well. In this example, the <em>Central Collector DNS Name</em> is <code>signoz-demo-central-collector.eastus.cloudapp.azure.com</code>.</p>
<p>If you're using kubernetes, you probably have ExternalDNS configured and you can use that to set up DNS name for your collector as well.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/collector-setup/#troubleshooting
tag_set: azure-monitoring, bootstrapping, collector-setup
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-collector-setup-troubleshooting
group_tracking_ids: docs-azure-monitoring-bootstrapping-collector-setup
<h2>Central Collector Setup: Setup: Troubleshooting</h2>
<p>If you encounter any issues during the setup or data collection process, consider the following troubleshooting steps:</p>
<ol>
<li>Verify that the OpenTelemetry Collector is running and accessible from the Azure compute services and applications.</li>
<li>Check the OpenTelemetry Collector logs for any error messages or warnings.</li>
<li>Ensure that the necessary ports and firewall rules are configured correctly to allow incoming connections to the Collector.</li>
<li>Verify that the Azure compute services and applications are properly instrumented and configured to send observability data to the Collector endpoint.</li>
<li>Confirm that the OpenTelemetry Collector is configured to export data to your SigNoz account and that the SigNoz account is accessible.</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/virtual-machines/
tag_set: azure-monitoring, virtual-machines
image_urls: 
tracking_id: docs-azure-monitoring-virtual-machines
group_tracking_ids: docs-azure-monitoring-virtual-machines
<h2>Virtual Machines</h2>
<ul>
<li><a href="/docs/azure-monitoring/virtual-machines/vm-metrics/">üìÑÔ∏è VM Host Metrics and Logging: Overview</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#introduction
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-strategy-introduction
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Azure Monitoring Strategy - Introduction</h2>
<p>This documentation guides you through an effective full-stack unified monitoring strategy for Azure using SigNoz and Azure Monitoring. By integrating SigNoz with Azure, you can achieve comprehensive observability of your applications and infrastructure, ensuring you have the insights needed to maintain optimal performance and reliability.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#overview-of-monitoring-modalities
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: https://signoz.io/img/docs/azure-monitoring/unified-collection.webp
tracking_id: docs-azure-monitoring-bootstrapping-strategy-overview-of-monitoring-modalities
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Overview of Monitoring Modalities</h2>
<p><img src="https://signoz.io/img/docs/azure-monitoring/unified-collection.webp" alt="Azure Monitoring Strategy" /></p>
<p>_</p>
<p>OpenTelemetry Unified Monitoring</p>
<p>_</p>
<h3>## Platform-Level Instrumentation</h3>
<ol>
<li>
<p><strong>System Logs</strong>: Logs generated by the Azure platform itself, providing insights into the underlying infrastructure's state and behavior. These logs can be exported and enriched with additional attributes for better context.</p>
<h5>##     Example:</h5>
<p>App Service's HTTP access logs, these are not generated by your application but by the Azure platform.</p>
<h5>##     Collected by:</h5>
<p>Platform Service streams to Event Hub and SigNoz's Central Collector pushes to SigNoz Cloud</p>
</li>
<li>
<p><strong>Infra Metrics</strong>: Metrics related to the infrastructure, such as CPU usage, memory consumption, and network traffic. These metrics are crucial for understanding the performance and health of your Azure resources.</p>
<h5>##     Example:</h5>
<p>App Service's CPU and memory usage metrics.</p>
<h5>##     Collected by:</h5>
<p>Platform Service streams to Azure Monitor and SigNoz's Central Collector scrapes and pushes to SigNoz Cloud</p>
</li>
<li>
<p><strong>Infra Attributes</strong>: Additional metadata about your infrastructure, such as resource names, locations, and configurations. These attributes help in correlating and contextualizing logs and metrics.</p>
<h5>##     Example:</h5>
<p>App Service's resource name and location attributes.</p>
<h5>##     Collected by:</h5>
<p>Central Collector collects and export these attributes..</p>
</li>
</ol>
<h3>## Application-Level Instrumentation</h3>
<ol>
<li>
<p><strong>App Logs</strong>: These logs are generated by your application and provide insights into application behavior, errors, and events. In an Azure context, these logs can be streamed to stdout and collected by Azure App Service, Function App, and Container App.</p>
<h5>##     Example:</h5>
<p><code>System.Diagnostics.TraceSource</code> in .NET, <code>logger.info()</code> in Java, <code>console.log()</code> in JavaScript.</p>
<h5>##     Collected by:</h5>
<p>Platform Service streams stdout to Event Hub (specific to the service) and SigNoz's Central Collector pushes to SigNoz Cloud.</p>
<p>Alternatively, for <strong>unmanaged services</strong> or <strong>non-stdout logging</strong>, you can use Otel SDK to send logs to Central Collector directly.</p>
</li>
<li>
<p><strong>App Metrics</strong>: These are metrics that your application emits, such as request queue length, request counts, error rates, and response times. These metrics can be collected using OpenTelemetry instrumentation SDKs.</p>
<h5>##     Example:</h5>
<p>Spring Actuator Metrics like <code>jvm.gc.pause.seconds_count</code>, <code>http.server.requests.total</code>, <code>process.cpu.usage</code></p>
<h5>##     Collected by:</h5>
<p>OpenTelemetry SDKs integrated with your application send metrics to Central Collector</p>
</li>
<li>
<p><strong>Traces</strong>: Traces provide a detailed view of the execution flow within your application. They help you understand the path and timing of requests as they traverse through various services and components.</p>
<h5>##     Collected by:</h5>
<p>OpenTelemetry SDKs integrated with your application send traces to Central Collector</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#implementation-guide
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: https://signoz.io/img/docs/azure-monitoring/unified-implementation.webp
tracking_id: docs-azure-monitoring-bootstrapping-strategy-implementation-guide
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Implementation Guide</h2>
<p><img src="https://signoz.io/img/docs/azure-monitoring/unified-implementation.webp" alt="Azure Monitoring Strategy" /></p>
<p>_</p>
<p>Azure Unified Monitoring Strategy</p>
<p>_</p>
<h3>## Platform-Level Instrumentation</h3>
<p>Azure provides the capability to export system logs, infrastructure metrics, and attributes. These can be streamed and collected by Azure services such as App Service, Function App, and Container App.</p>
<p>Please refer to the <a href="../../">Azure Monitoring documentation</a> for detailed information on how to export logs and metrics to SigNoz for your particular service.</p>
<h3>## Application-Level Instrumentation</h3>
<p>For application metrics and traces, use the OpenTelemetry instrumentation SDK. For example, in a Node.js application, you can install the necessary packages using npm:</p>
<pre><code>npm install @opentelemetry/api
npm install @opentelemetry/auto-instrumentations-node
</code></pre>
<p>Please refer to our <a href="https://signoz.io/blog/">Blog</a> or <a href="../../../instrumentation/">SigNoz Tutorials</a> for detailed information on how to instrument your application like Spring, FastAPI, NextJS, Langchain, Node.js, Flask, Django, etc. with OpenTelemetry.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#next-steps
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-strategy-next-steps
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Next Steps</h2>
<p>Please complete the bootstrapping process to get started with monitoring your application running on Azure.</p>
<ul>
<li>
<p><a href="../data-ingestion">Event Hub Setup</a></p>
</li>
<li>
<p><a href="../collector-setup">Central Collector Setup</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#frequently-asked-questions-faq
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-strategy-frequently-asked-questions-faq
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Frequently Asked Questions (FAQ)</h2>
<h3>## How do I monitor traces in Azure?</h3>
<p>To monitor traces in Azure, you need to instrument your application using the OpenTelemetry SDK. Once instrumented, the traces can be collected and exported to a SigNoz Cloud for analysis.</p>
<ol>
<li><strong>Instrument Your Application</strong>: Use the OpenTelemetry SDK to add tracing to your application code.</li>
<li><strong>Configure Exporter</strong>: Set up the OpenTelemetry Collector to export traces to your <a href="../collector-setup">Central Collector</a> in SigNoz Cloud.</li>
</ol>
<p>By following these steps, you can achieve comprehensive trace monitoring in Azure, helping you to identify and resolve performance issues effectively.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-blob-storage/
tag_set: azure-monitoring, az-blob-storage
image_urls: 
tracking_id: docs-azure-monitoring-az-blob-storage
group_tracking_ids: docs-azure-monitoring-az-blob-storage
<h2>Azure Blob Storage</h2>
<ul>
<li><a href="/docs/azure-monitoring/az-blob-storage/metrics/">üìÑÔ∏è Metrics: monitor Azure Blob Storage's system metrics like Total Requests...</a></li>
<li><a href="/docs/azure-monitoring/az-blob-storage/logging/">üìÑÔ∏è Logging: Blob Storage Audit Logging is a feature of Azure Blob Storage...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/app-service/
tag_set: azure-monitoring, app-service
image_urls: 
tracking_id: docs-azure-monitoring-app-service
group_tracking_ids: docs-azure-monitoring-app-service
<h2>App Service</h2>
<ul>
<li><a href="/docs/azure-monitoring/app-service/metrics/">üìÑÔ∏è Metrics: To monitor Azure App Service's system metrics like CPU Percentage...</a></li>
<li><a href="/docs/azure-monitoring/app-service/logging/">üìÑÔ∏è Logging: The following categories of App Service Logs are available to export...</a></li>
<li><a href="/docs/azure-monitoring/app-service/logging/">üìÑÔ∏è APM &amp; Tracing: To get started with monitoring your Azure App Service...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/aks/
tag_set: azure-monitoring, aks
image_urls: 
tracking_id: docs-azure-monitoring-aks
group_tracking_ids: docs-azure-monitoring-aks
<h2>AKS Metrics &amp; Logging</h2>
<h2>Overview</h2>
<hr />
<p><a href="https://learn.microsoft.com/en-us/azure/aks/what-is-aks">AKS (Azure Kubernetes Service)</a> is a managed Kubernetes service provided by Microsoft Azure that simplifies the deployment, management, and operations of Kubernetes clusters.</p>
<h2>Prerequisites</h2>
<hr />
<ul>
<li>AKS cluster</li>
<li><code>kubectl</code> installed and logged in to the AKS cluster</li>
<li>Helm</li>
</ul>
<h2>Quick Start</h2>
<hr />
<p>This setup is similar to the central collector but with a different function.</p>
<pre><code>helm repo add signoz &lt;https://charts.signoz.io&gt;
helm install -n signoz  --create-namespace kubelet-otel signoz/k8s-infra \\
--set signozApiKey=&lt;ingestionKey&gt; --set otelCollectorEndpoint=&quot;ingest.&lt;region&gt;.signoz.cloud:443&quot; --set otelInsecure=false
</code></pre>
<p>This should start sending logs and metrics to SigNoz.</p>
<h2>Tracing</h2>
<hr />
<h3>## eBPF Tracing</h3>
<p>There are solution to collect metrics and traces without modifying the application code. These solutions come under the category of eBPF Tracing. These solutions are relatively new and are still in the early stages of development.</p>
<p>However, there are some open source projects that export metrics and traces to OpenTelemetry.</p>
<ul>
<li>
<p><a href="https://github.com/pixie-io/pixie">Pixie</a></p>
</li>
<li>
<p><a href="https://github.com/cilium/hubble">Hubble</a></p>
</li>
<li>
<p><a href="https://github.com/open-telemetry/opentelemetry-network">OpenTelemetry eBPF</a></p>
</li>
</ul>
<p>For example, Pixie can be configured by following the instructions in the respective repositories. <a href="https://docs.px.dev/tutorials/integrations/otel/">Pixie OpenTelemetry Tutorial</a> url='otel-collector.kubelet-otel.svc.cluster.local:4317',
insecure=True
),</p>
<p>:::note These solutions may not be suitable for all use cases, and are still may not be production-ready. It is recommended to evaluate solutions and choose the one that best fits your needs. :::</p>
<h3>## Application-Level Tracing</h3>
<p>For application-level tracing, you can use the OpenTelemetry SDKs integrated with your application. These SDKs will automatically collect and forward traces to the Central Collector.</p>
<p>Please refer to our <a href="../../instrumentation/">SigNoz Tutorials</a> or <a href="https://signoz.io/blog/">Blog</a> to find information on how to instrument your application like Spring, FastAPI, NextJS, Langchain, Node.js, Flask, Django, etc.</p>
<pre><code># Node.js example
npm install @opentelemetry/api
npm install @opentelemetry/auto-instrumentations-node
npm install @opentelemetry/exporter-trace-otlp-http
</code></pre>
<h3>## Configure the OpenTelemetry SDK</h3>
<pre><code># Set env vars or config file
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://otel-collector.kubelet-otel.svc.cluster.local:4318/&quot;
</code></pre>
<p>For application-level traces and metrics, configure your application to use the kube-dns name of the <a href="../bootstrapping/collector-setup">Central Collector</a> you set up earlier.</p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter any issues while setting up logging and metrics for your AKS cluster, follow these troubleshooting steps:</p>
<ol>
<li>
<p>Check the logs of the OpenTelemetry Collector:</p>
<pre><code>kubectl logs -f -n signoz -l app.kubernetes.io/component=otel-agent
</code></pre>
<p>Review the logs for any error messages or indications of misconfiguration.</p>
</li>
<li>
<p>Verify the rendered configuration:</p>
<pre><code>kubectl get cm/kubelet-otel-k8s-infra-otel-agent -n signoz -o yaml
</code></pre>
<p>Ensure that the configuration matches your expected settings, including the SigNoz API key and the OpenTelemetry Collector endpoint.</p>
</li>
<li>
<p>Confirm that the necessary Kubernetes resources are created:</p>
<pre><code>kubectl get pods,services,configmaps -n signoz
</code></pre>
<p>Check if the required pods, services, and config maps are running and in a healthy state.</p>
</li>
<li>
<p>Verify network connectivity:</p>
<ul>
<li>Ensure that the AKS cluster has network access to the SigNoz ingestion endpoint (<code>ingest.&lt;region&gt;.signoz.cloud:443</code>).</li>
<li>Check if there are any network security groups or firewalls blocking the required ports.</li>
</ul>
</li>
<li>
<p>Double-check the SigNoz API key:</p>
<ul>
<li>Confirm that the provided <code>signozApiKey</code> is correct and has the necessary permissions to ingest data.</li>
</ul>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/python-logs-auto-instrumentation/
tag_set: userguide, python-logs-auto-instrumentation
image_urls: 
tracking_id: docs-userguide-python-logs-auto-instrumentation
group_tracking_ids: docs-userguide-python-logs-auto-instrumentation
<h2>Python Logs Auto-Instrumentation</h2>
<h2>Collecting Python Application Logs Using Auto-Instrumentation</h2>
<hr />
<p>If you are using python auto-instrumentation for instrumenting your python application you can send logs to SigNoz easily with auto-instrumentation.</p>
<p>To enable logs auto-instrumentation just add this environment variable</p>
<pre><code>OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true
</code></pre>
<h2>Example application</h2>
<hr />
<p>Here is a sample python application</p>
<ol>
<li>
<p>Create a file named main.py and paste the following code</p>
<pre><code>from flask import Flask
import logging

app = Flask(__name__)

@app.route('/')
def hello_world():
    logging.warning(&quot;hello world log message&quot;)
    return 'Hello World'

if __name__ == '__main__':
    app.run()
</code></pre>
</li>
<li>
<p>Create a virual environment</p>
<pre><code>python -m venv venv
source ./venv/bin/activate
</code></pre>
</li>
<li>
<p>Install dependencies</p>
<pre><code>pip install opentelemetry-distro
pip install flask requests
pip install opentelemetry-exporter-otlp
</code></pre>
</li>
<li>
<p>Run the opentelemetry-bootstrap command:</p>
<pre><code>opentelemetry-bootstrap -a install
</code></pre>
</li>
<li>
<p>Run the application</p>
<pre><code>OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true opentelemetry-instrument --traces_exporter none --metrics_exporter none --logs_exporter console python main.py
</code></pre>
</li>
</ol>
<p>You will be able to see the otel logs on the console once you visit <code>http://localhost:5000</code></p>
<p>If you want to send data to SigNoz cloud or self host SigNoz the run command will change and will be described in the next steps</p>
<p>SigNoz CloudSelf-Host</p>
<p>For SigNoz Cloud the run command will be</p>
<pre><code>OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true \
OTEL_EXPORTER_OTLP_ENDPOINT=&lt;SIGNOZ_ENDPOINT&gt; \
OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;INGESTION_KEY&gt; \
opentelemetry-instrument --traces_exporter otlp --metrics_exporter otlp --logs_exporter otlp python main.py
</code></pre>
<ul>
<li>The value of <code>SIGNOZ_ENDPOINT</code> will be <code>https://ingest.{region}.signoz.cloud:443</code> where depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</li>
</ul>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<ul>
<li>The value of <code>INGESTION_KEY</code> is your ingestion key.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-container-app/
tag_set: azure-monitoring, az-container-app
image_urls: 
tracking_id: docs-azure-monitoring-az-container-app
group_tracking_ids: docs-azure-monitoring-az-container-app
<h2>Azure Container Apps</h2>
<ul>
<li><a href="/docs/azure-monitoring/az-fns/metrics/">üìÑÔ∏è Metrics: To monitor Azure Container App's system metrics like CPU Percentage...</a></li>
<li><a href="/docs/azure-monitoring/az-fns/logging/">üìÑÔ∏è Logging: The following categories of Container Apps Logs are available...</a></li>
<li><a href="/docs/azure-monitoring/az-fns/tracing/">üìÑÔ∏è APM &amp; Tracing: To get started with monitoring your Azure Container App...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-fns/
tag_set: azure-monitoring, az-fns
image_urls: 
tracking_id: docs-azure-monitoring-az-fns
group_tracking_ids: docs-azure-monitoring-az-fns
<h2>Azure Functions</h2>
<ul>
<li><a href="/docs/azure-monitoring/az-fns/metrics/">üìÑÔ∏è Metrics: To monitor Azure Function's system metrics like CPU Percentage...</a></li>
<li><a href="/docs/azure-monitoring/az-fns/logging/">üìÑÔ∏è Logging: The following categories of Logs are available to export to Storage Account or EventHub...</a></li>
<li><a href="/docs/azure-monitoring/az-fns/tracing/">üìÑÔ∏è APM &amp; Tracing: To get started with monitoring your Azure Function App...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/aggregate-traces/
tag_set: traces-management, trace-api, aggregate-traces
image_urls: 
tracking_id: docs-traces-management-trace-api-aggregate-traces
group_tracking_ids: docs-traces-management-trace-api-aggregate-traces
<h2>Aggregate Traces</h2>
<p>This section demonstrates how to perform aggregation operations on trace data using the SigNoz Trace API.</p>
<h2>Aggregation Example</h2>
<hr />
<p>This example is useful for scenarios where you need to count occurrences of specific values (e.g., customer) and group them by another attribute (e.g., serviceName).</p>
<h3>## Query Description</h3>
<ul>
<li><strong>Objective</strong>: Count customer values and group them by serviceName</li>
<li><strong>Attributes</strong>:
<ul>
<li><strong>aggregateAttribute</strong>: <code>customer</code></li>
<li><strong>groupBy</strong>: <code>serviceName</code></li>
</ul>
</li>
</ul>
<h3>## Sample Payload</h3>
<p>This is the JSON payload for the example query.</p>
<pre><code>{
    &quot;start&quot;: 1702019380000,
    &quot;end&quot;: 1702021180000,
    &quot;step&quot;: 60,
    &quot;variables&quot;: {},
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;table&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;dataSource&quot;: &quot;traces&quot;,
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;aggregateOperator&quot;: &quot;count&quot;,
                &quot;aggregateAttribute&quot;: {
                    &quot;key&quot;: &quot;customer&quot;,
                    &quot;dataType&quot;: &quot;string&quot;,
                    &quot;type&quot;: &quot;tag&quot;,
                    &quot;isColumn&quot;: false
                },
                &quot;filters&quot;: {
                    &quot;items&quot;: [],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;having&quot;: [],
                &quot;stepInterval&quot;: 60,
                &quot;limit&quot;: null,
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ],
                &quot;groupBy&quot;: [\
                    {\
                        &quot;key&quot;: &quot;serviceName&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    }\
                ],
                &quot;legend&quot;: &quot;&quot;,
                &quot;reduceTo&quot;: &quot;sum&quot;
            }
        }
    },
    &quot;dataSource&quot;: &quot;traces&quot;
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/db-metrics/
tag_set: azure-monitoring, db-metrics
image_urls: https://signoz.io/img/docs/azure-monitoring/db-metrics-signoz.webp
tracking_id: docs-azure-monitoring-db-metrics
group_tracking_ids: docs-azure-monitoring-db-metrics
<h2>SQL Database Metrics</h2>
<h2>QuickStart</h2>
<hr />
<p>To monitor Database's system metrics like CPU Percentage, Memory Percentage, Storage Usage with SigNoz, you just need to set up the OpenTelemetry Collector with the Azure Monitor exporter and enable Monitoring for the databases.</p>
<h2>Overview</h2>
<hr />
<p>In this guide, you will learn how to monitor Database's system metrics like CPU Percentage, Memory Percentage, Data In, and Data Out with SigNoz. By monitoring these metrics, you can keep track of your application's resource utilization and performance.</p>
<p>For application-level traces and metrics, you can use the DNS name of the OpenTelemetry Collector you set up earlier. Simply configure your application to send traces and metrics to the Central Collector, and they will be forwarded to SigNoz automatically.</p>
<h2>Prerequisites</h2>
<hr />
<p>Before you can monitor your Database with SigNoz, you need to ensure the following prerequisites are met:</p>
<ol>
<li>
<p>You have an Azure subscription and an Database instance running.</p>
</li>
<li>
<p>You have set up the Central Collector with the Azure Monitor exporter. If you haven't set it up yet, follow the instructions in the¬†<a href="../bootstrapping/collector-setup">Central Collector Setup</a></p>
</li>
<li>
<p>You should have sql monitoring profile created to monitor the databases in Azure Monitor if not, Follow this guide to <a href="https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-insights-enable?view=azuresql#create-sql-monitoring-profile">Create SQL Monitoring Profile</a></p>
</li>
</ol>
<h2>Setup</h2>
<hr />
<p>Once you have completed the prerequisites, you can start monitoring your Database's system metrics with SigNoz. Here's how you can do it:</p>
<ol>
<li>Log in to your SigNoz account.</li>
<li>Navigate to the Dashboards, and add an dashboard</li>
<li>Add a Timeseries Panel</li>
<li>In <em>Metrics</em>, select <code>azure_storage_maximum</code> and <em>Avg By</em> select tag <code>location</code></li>
<li>In Filter say <code>name = &lt;database-name&gt;</code></li>
<li>Hit ‚ÄúSave Changes‚Äù You now have Memory Usage of your Database in a Dashboard for reporting and alerting</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/db-metrics-signoz.webp" alt="Signoz Dashboard" /></p>
<p>_</p>
<p>Signoz Dashboard</p>
<p>_</p>
<p>That's it! You have successfully set up monitoring for your Database's system metrics with SigNoz.</p>
<p>üìù Note</p>
<p>Make sure you have created a sql monitoring profile in Azure Monitor if not, Follow this guide to <a href="https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-insights-enable?view=azuresql#create-sql-monitoring-profile">Create SQL Monitoring Profile</a>. You can monitor multiple databases in a single profile.</p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter any issues while setting up monitoring for your Database's system metrics with SigNoz, here are a few troubleshooting steps you can try:</p>
<ol>
<li>Check if the OpenTelemetry Collector is running and properly configured with the Azure Monitor exporter.</li>
<li>Verify that your Database instance is running and accessible.</li>
<li>Ensure that you have the necessary permissions to access the metrics in your Azure subscription.</li>
<li>Double-check the configuration of the OpenTelemetry Collector with the Azure Monitor exporter to ensure that a resource group filter is not preventing the metrics from being collected.</li>
</ol>
<p>By following this guide, you should be able to easily monitor your Database's system metrics with SigNoz and gain valuable insights into your application's performance and resource utilization.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_nodejs_winston_logs/
tag_set: userguide, collecting_nodejs_winston_logs
image_urls: 
tracking_id: docs-userguide-collecting_nodejs_winston_logs
group_tracking_ids: docs-userguide-collecting_nodejs_winston_logs
<h2>Collecting NodeJS winston logs</h2>
<p>If you are using <code>winston</code> as your logging library in your Nodejs application, you can export these logs to SigNoz very easily using various transports provided by <code>winston</code>.</p>
<h2>Collecting Nodejs logs when application is deployed on Docker or Kubernetes</h2>
<hr />
<p>When your application is deployed in Docker or a Kubernetes cluster the logs from the console are automatically collected and stored in the node. The SigNoz collector will automatically collect the logs and it will be visible on the SigNoz UI.</p>
<p>You can add a console transport very easily as stated <a href="https://github.com/winstonjs/winston/blob/master/docs/transports.md#console-transport">here</a>.</p>
<pre><code>logger.add(new winston.transports.Console(options));
</code></pre>
<h2>Collecting Nodejs logs when application is deployed on a Host</h2>
<hr />
<p>When you run your application directly on the host, you will be required to add a intermediary medium ex:- a file, where you can export your logs and the otel-collector can read them and push to signoz.</p>
<p>You can add a file transport very easily as stated <a href="https://github.com/winstonjs/winston/blob/master/docs/transports.md#file-transport">here</a>.</p>
<pre><code>logger.add(new winston.transports.File(options));
</code></pre>
<p>Once you run your application and the logs are added to a file, you can configure otel collector to read from that file.</p>
<p>For configuring it you can follow the guide <a href="/docs/userguide/collect_logs_from_file/">here</a>.</p>
<p>Once you configure the otel collector the logs will be visible on the UI.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_docker_logs/
tag_set: userguide, collect_docker_logs
image_urls: 
tracking_id: docs-userguide-collect_docker_logs
group_tracking_ids: docs-userguide-collect_docker_logs
<h2>Collecting Docker container logs</h2>
<p>With SigNoz you can collect all your docker container logs and perform different queries on top of it. Below are the steps to collect docker container logs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_docker_logs/#collect-docker-container-logs-in-signoz-cloud
tag_set: userguide, collect_docker_logs
image_urls: 
tracking_id: docs-userguide-collect_docker_logs-collect-docker-container-logs-in-signoz-cloud
group_tracking_ids: docs-userguide-collect_docker_logs
<h2>Collecting Docker container logs: Collect Docker container logs in SigNoz cloud</h2>
<ul>
<li>
<p>Clone this <a href="https://github.com/SigNoz/docker-container-logs">repository</a></p>
</li>
<li>
<p>Update <code>otel-collector-config.yaml</code> and set the values of <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> and <code>{region}</code>.</p>
</li>
</ul>
<p>Depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>Start the containers <code>docker compose up -d</code></p>
</li>
<li>
<p>If there are no errors your logs will be exported and will be visible on the SigNoz UI.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_docker_logs/#collect-docker-container-logs-in-self-hosted-signoz
tag_set: userguide, collect_docker_logs
image_urls: 
tracking_id: docs-userguide-collect_docker_logs-collect-docker-container-logs-in-self-hosted-signoz
group_tracking_ids: docs-userguide-collect_docker_logs
<h2>Collecting Docker container logs: Collect Docker container logs in Self-Hosted SigNoz</h2>
<h3>## Steps for collecting logs if SigNoz is running on the same host.</h3>
<p>Once you deploy SigNoz in docker, it will automatically start collecting logs of all the docker containers, except for the container logs of SigNoz.</p>
<h4>## Disable automatic container log collection.</h4>
<p>You can disable automatic container logs collection by modifying the <code>otel-collector-config.yaml</code> file which is present inside <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>...
service:
  pipelines:
    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [clickhouselogsexporter]
...
</code></pre>
<p>Here we have modified the value of receivers from <code>[otlp, tcplog/docker]</code> to <code>[otlp]</code>. Now you can restart SigNoz and the changes will be applied.</p>
<h4>## Filter/Exclude logs</h4>
<p>If you want to exclude certain logs you can exclude them based the container name or based on pattern.</p>
<ul>
<li>
<p><strong>Using container name</strong> : We will modify the <code>tcplog/docker</code> reciever in <code>otel-collector-config.yaml</code> file which is present inside <code>deploy/docker/clickhouse-setup</code> and add a new operator after <code>signoz_logs_filter</code></p>
<pre><code>...
- type: filter
  expr: 'attributes.container_name matches &quot;^(&lt;container_name&gt;|&lt;container_name&gt;)'
...
</code></pre>
<p>Replace <code>&lt;container_name&gt;</code> with the name of the containers that you want to exclude.</p>
<p>If you want to collect logs of signoz containers you can remove the names of signoz containers from the filter operator with id <code>signoz_logs_filter</code> operator.</p>
</li>
<li>
<p><strong>Based on pattern</strong> : You can also use the filter operator to filter out logs based on a pattern</p>
<pre><code>....
  operators:
    - type: filter
      expr: 'body matches &quot;^LOG: .* END$&quot;'
      drop_ratio: 1.0
....
</code></pre>
<p>Here we are matching logs using an expression and dropping the entire log by setting <code>drop_ratio: 1.0</code> . You can read more about the filter operator <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/filter.md">here</a></p>
</li>
<li>
<p>Now we can restart the otel collector container so that new changes are applied and the docker container logs will be dropped for the specified containers.</p>
</li>
</ul>
<h3>## Steps for collecting logs if SigNoz is running on a different host.</h3>
<p>If you have a signoz running on a different host then you can run logspout on the host and send logs to SigNoz cluster.</p>
<ul>
<li>
<p>Expose port <code>2255</code> of otel-collector by modifying the <code>docker-compose.yaml</code> file present inside <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>...
otel-collector:
    image: signoz/signoz-otel-collector:latest
    command: [&quot;--config=/etc/otel-collector-config.yaml&quot;]
    ports:
      - &quot;2255:2255&quot;
</code></pre>
</li>
<li>
<p>Run logspout</p>
<pre><code>docker run --net=host --rm --name=&quot;logspout&quot; \
        --volume=/var/run/docker.sock:/var/run/docker.sock \
        gliderlabs/logspout \
        syslog+tcp://&lt;host&gt;:2255
</code></pre>
<p>For finding the right host for your SigNoz cluster please follow the guide <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">here</a>
.</p>
</li>
<li>
<p>If there are no errors your logs will be exported and visible on the SigNoz UI.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/parse-multiline-logs/#overview
tag_set: userguide, parse-multiline-logs
image_urls: 
tracking_id: docs-userguide-parse-multiline-logs-overview
group_tracking_ids: docs-userguide-parse-multiline-logs
<h2>Parsing Multiline Logs: Parsing Multiline Logs - Overview</h2>
<p>By default when you use the filelog receiver each newline in the log line creates a new log line.</p>
<p>This doc goes through different ways to solve this problem where you can parse/recombine multiline logs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/parse-multiline-logs/#sample-multiline-logs
tag_set: userguide, parse-multiline-logs
image_urls: https://signoz.io/img/logs/multiline/multiline_broken.png
tracking_id: docs-userguide-parse-multiline-logs-sample-multiline-logs
group_tracking_ids: docs-userguide-parse-multiline-logs
<h2>Parsing Multiline Logs: Sample Multiline Logs</h2>
<p>Here is an example of multiline logs</p>
<pre><code>2024-20-06 18:58:05,898 ERROR:Exception on main handler
Traceback (most recent call last):
File &quot;python-logger.py&quot;, line 9, in make_log
  return area[10]
IndexError: string index out of range
2024-20-06 18:58:05,898 DEBUG:Query Started
</code></pre>
<p>In the above example there there are two log lines spread over multiple line, but since by default each newline is treated as end of log line, multiple log lines will be created as seen in the image below.</p>
<p><img src="https://signoz.io/img/logs/multiline/multiline_broken.png" alt="Multiline logs broken" /></p>
<p>_</p>
<p>Multiline logs as multiple individual log lines</p>
<p>_</p>
<p>There are two ways you can combine these logs</p>
<ol>
<li>Parse the logs lines as multiline at the receiver itself.</li>
<li>Recombine the multiline logs later at processor level</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/parse-multiline-logs/#parse-multiline-logs-at-receiver
tag_set: userguide, parse-multiline-logs
image_urls: https://signoz.io/img/logs/multiline/multiline_python.png
tracking_id: docs-userguide-parse-multiline-logs-parse-multiline-logs-at-receiver
group_tracking_ids: docs-userguide-parse-multiline-logs
<h2>Parsing Multiline Logs: Parse Multiline Logs at Receiver</h2>
<p>Since we are using the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md">filelog</a> receiver. It has a multiline configuration option.</p>
<p>In order to parse these type of logs we will have to identify a start or end pattern.</p>
<p>Let's understand with example</p>
<pre><code>2024-20-06 18:58:05,898 ERROR:Exception on main handler
Traceback (most recent call last):
File &quot;python-logger.py&quot;, line 9, in make_log
  return area[10]
IndexError: string index out of range
2024-20-06 18:58:05,898 DEBUG:Query Started
</code></pre>
<p>For the above log lines we can see that the new log line starts with <code>Date</code>, so our <code>line_start_pattern</code> will be</p>
<pre><code>line_start_pattern: ^\d{4}-\d{2}-\d{2}
</code></pre>
<p>Once you have your <code>line_start_pattern</code> or <code>line_end_pattern</code> this is how you configuration of filelog receiver will look like</p>
<pre><code>receivers:
  filelog:
    include:
    - /var/log/example/multiline.log
    multiline:
      line_start_pattern: ^\d{4}-\d{2}-\d{2}
</code></pre>
<p>Once it is deployed correctly this is how your log lines will look</p>
<p><img src="https://signoz.io/img/logs/multiline/multiline_python.png" alt="Multiline logs fixed" /></p>
<p>_</p>
<p>Multiline logs as a single individual log line</p>
<p>_</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/parse-multiline-logs/#use-recombine-operator-to-combine-multiline-logs
tag_set: userguide, parse-multiline-logs
image_urls: https://signoz.io/img/logs/multiline/multiline_broken.png, https://signoz.io/img/logs/multiline/multiline_python.png
tracking_id: docs-userguide-parse-multiline-logs-use-recombine-operator-to-combine-multiline-logs
group_tracking_ids: docs-userguide-parse-multiline-logs
<h2>Parsing Multiline Logs: Use Recombine Operator to Combine Multiline Logs</h2>
<p>In case the above configuration is not feasible for you, you can use the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md">recombine</a> operator to join back the multiline logs.</p>
<p>Let's take the same example again</p>
<pre><code>2024-20-06 18:58:05,898 ERROR:Exception on main handler
Traceback (most recent call last):
File &quot;python-logger.py&quot;, line 9, in make_log
  return area[10]
IndexError: string index out of range
2024-20-06 18:58:05,898 DEBUG:Query Started
</code></pre>
<p>Here we know that log lines is splitted into multiple individual log lines by default</p>
<p><img src="https://signoz.io/img/logs/multiline/multiline_broken.png" alt="Multiline logs broken" /></p>
<p>_</p>
<p>Multiline logs as multiple individual log lines</p>
<p>_</p>
<p>Since we know the start pattern of the new log line, here is how we will recombine it</p>
<pre><code>processors:
  logstransform/multiline:
    operators:
      - type: recombine
        combine_field: body
        is_first_entry: body matches &quot;^\\d{4}-\\d{2}-\\d{2}&quot;
        source_identifier: attributes[&quot;log.file.path&quot;]
</code></pre>
<p>Here we are matching the first line of a multiline log using <code>is_first_entry</code> and then combining the <code>body</code> field for each unique <code>log.file.path</code> value</p>
<p>once the above is deployed this is what it will look like on the UI</p>
<p><img src="https://signoz.io/img/logs/multiline/multiline_python.png" alt="Multiline logs fixed" /></p>
<p>_</p>
<p>Multiline logs as a single individual log line</p>
<p>_</p>
<p>There are more options to play around with the recombine operator which can be checked <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/windows-events-log/
tag_set: logs-management, send-logs, windows-events-log
image_urls: https://signoz.io/img/docs/logs-management/send-logs/windows-events-logs.webp
tracking_id: docs-logs-management-send-logs-windows-events-log
group_tracking_ids: docs-logs-management-send-logs-windows-events-log
<h2>Windows Events log to SigNoz</h2>
<h2>Overview</h2>
<hr />
<p>If you are using a Windows environment, you can stream Windows Event Log to SigNoz using OpenTelemetry Collector.</p>
<p>Monitoring specific Event Log sources, known as Channels, can be done using the Windows Event Log receiver which is configured in the OpenTelemetry Collector configuration file.</p>
<p>Key channels typically monitored include:</p>
<ul>
<li><strong>Application:</strong> Logs events related to applications running on the system.</li>
<li><strong>Security:</strong> Records security-related events such as login attempts and resource access.</li>
<li><strong>System:</strong> Captures events related to system components, drivers, and services.</li>
</ul>
<h2>Prerequisites</h2>
<hr />
<ul>
<li>
<p>SigNoz <a href="https://signoz.io/teams/">cloud account</a></p>
</li>
<li>
<p>Microsoft User account with permissions to access EventLog and Services</p>
</li>
</ul>
<h2>Setup</h2>
<hr />
<h3>## Step 1: Install OpenTelemetry Collector</h3>
<p>The OpenTelemetry collector provides a vendor-neutral way to collect, process, and export your telemetry data such as logs, metrics, and traces.</p>
<p>You can install OpenTelemetry collector as an agent on your Virtual Machine by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">documentation</a>.</p>
<h3>## Step 2: Add windowseventlog receiver</h3>
<h4>## Configure windowseventlog receiver</h4>
<p>Modify the <code>config.yaml</code> file created in the previous step to include the <code>windowseventlog</code> receiver in the receiver section. The below codeblock shows how you can add the receiver to get the windows application and system logs.</p>
<pre><code>receivers:
    windowseventlog/application:
        channel: application
    windowseventlog/system:
        channel: system
</code></pre>
<p>There are more configuration options available <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowseventlogreceiver/README.md">here</a></p>
<h4>## Update pipelines configuration</h4>
<p>In the same <code>config.yaml</code> file, update the pipelines section to include the <code>windowseventlog/application</code> and <code>windowseventlog/system</code> receivers under <code>logs</code>.</p>
<pre><code>service:
  pipelines:
  ....
    logs:
      receivers: [windowseventlog/application, windowseventlog/system]
      processors: [batch]
      exporters: [otlp]
</code></pre>
<p>If there are no errors, your Event logs will be visible in SigNoz under the Logs Tab.</p>
<h2>Output</h2>
<hr />
<p><img src="https://signoz.io/img/docs/logs-management/send-logs/windows-events-logs.webp" alt="Windows System Events Logs visible in SigNoz" /></p>
<p><em>Windows System Events Logs in SigNoz</em></p>
<p>This is what the typical output will look like with the configurations we made above:</p>
<p><strong>Application Log</strong></p>
<pre><code>{
  &quot;body&quot;: &quot;{\&quot;channel\&quot;:\&quot;Application\&quot;,\&quot;computer\&quot;:\&quot;logs-windows\&quot;,\&quot;event_data\&quot;:{},
  \&quot;event_id\&quot;:{\&quot;id\&quot;:16384,\&quot;qualifiers\&quot;:16384},\&quot;keywords\&quot;:[\&quot;Classic\&quot;],\&quot;level\&quot;:\&quot;Information\&quot;,
  \&quot;message\&quot;:\&quot;Successfully scheduled Software Protection service for re-start at 2024-08-10T18:52:44Z.
   Reason: RulesEngine.\&quot;,\&quot;opcode\&quot;:\&quot;0\&quot;,\&quot;provider\&quot;:{\&quot;event_source\&quot;:\&quot;Software Protection Platform 
   Service\&quot;,\&quot;guid\&quot;:\&quot;{E23B33B0-C8C9-472C-A5F9-F2BDFEA0F156}\&quot;,\&quot;name\&quot;:\&quot;Microsoft-Windows-Security-SPP\&quot;},
   \&quot;record_id\&quot;:750,\&quot;system_time\&quot;:\&quot;2024-08-03T19:29:44.9757970Z\&quot;,\&quot;task\&quot;:\&quot;0\&quot;}&quot;,
  &quot;id&quot;: &quot;2k2Ud5JPPt8hVRQpgF6gXTxl1Yd&quot;,
  &quot;timestamp&quot;: &quot;2024-08-03T19:29:44.975797Z&quot;,
  &quot;attributes&quot;: {},
  &quot;resources&quot;: {},
  &quot;severity_text&quot;: &quot;INFO&quot;,
  &quot;severity_number&quot;: 9,
  &quot;span_id&quot;: &quot;&quot;,
  &quot;trace_flags&quot;: 0,
  &quot;trace_id&quot;: &quot;&quot;
}
</code></pre>
<p><strong>System Log</strong></p>
<pre><code>{
  &quot;body&quot;: &quot;{\&quot;channel\&quot;:\&quot;System\&quot;,\&quot;computer\&quot;:\&quot;logs-windows\&quot;,
  \&quot;event_data\&quot;:{\&quot;param1\&quot;:\&quot;Background Intelligent Transfer Service\&quot;,
  \&quot;param2\&quot;:\&quot;auto start\&quot;,\&quot;param3\&quot;:\&quot;demand start\&quot;,\&quot;param4\&quot;:\&quot;BITS\&quot;},\&quot;event_id\&quot;:{\&quot;id\&quot;:7040,
  \&quot;qualifiers\&quot;:16384},\&quot;keywords\&quot;:[\&quot;Classic\&quot;],\&quot;level\&quot;:\&quot;Information\&quot;,
  \&quot;message\&quot;:\&quot;The start type of the Background Intelligent Transfer Service service was changed from 
  auto start to demand start.\&quot;,\&quot;opcode\&quot;:\&quot;0\&quot;,\&quot;provider\&quot;:{\&quot;event_source\&quot;:\&quot;Service Control Manager\&quot;,
  \&quot;guid\&quot;:\&quot;{555908d1-a6d7-4695-8e1e-26931d2012f4}\&quot;,\&quot;name\&quot;:\&quot;Service Control Manager\&quot;},
  \&quot;record_id\&quot;:893,\&quot;system_time\&quot;:\&quot;2024-08-03T19:32:41.9476831Z\&quot;,\&quot;task\&quot;:\&quot;0\&quot;}&quot;,
  &quot;id&quot;: &quot;2k2Ud5JPPt8hVRQpgF6gXTxl1Yf&quot;,
  &quot;timestamp&quot;: &quot;2024-08-03T19:32:41.9476831Z&quot;,
  &quot;attributes&quot;: {},
  &quot;resources&quot;: {},
  &quot;severity_text&quot;: &quot;INFO&quot;,
  &quot;severity_number&quot;: 9,
  &quot;span_id&quot;: &quot;&quot;,
  &quot;trace_flags&quot;: 0,
  &quot;trace_id&quot;: &quot;&quot;
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/concepts/
tag_set: logs-pipelines, concepts
image_urls: https://signoz.io/img/logs/pipelines/pipelines-list.webp, https://signoz.io/img/logs/pipelines/nginx-pipeline-processors.webp
tracking_id: docs-logs-pipelines-concepts
group_tracking_ids: docs-logs-pipelines-concepts
<h2>Concepts</h2>
<h2>Pipelines</h2>
<hr />
<p>Rules for transforming logs are configured by creating Logs Processing Pipelines in SigNoz UI.<br />
A pipeline is typically dedicated to a single preprocessing responsibility. For example, extraction of attributes from nginx text logs would happen in its own pipeline, and there would be another pipeline for parsing application logs and yet another for dropping PII fields from log attributes.</p>
<p><img src="https://signoz.io/img/logs/pipelines/pipelines-list.webp" alt="A list of pipelines, each addressing a single responsibility" /></p>
<p>_</p>
<p>A list of pipelines, each addressing a single responsibility</p>
<p>_</p>
<p>Logs get preprocessed by passing them through the chain of logs pipelines one by one. If a log matches a pipeline‚Äôs filter, it gets processed (transformed) by that pipeline, before moving on to test the log against the next pipeline‚Äôs filter and so on.</p>
<p>In the example above, each incoming log would first get tested for &quot;nginx logs parser&quot; pipeline's filter, and if it is a match, it will be transformed by that pipeline. The transformed log will then be tested for a match with the &quot;Application Logs Parser&quot; pipeline, followed by other pipelines in the chain one by one.</p>
<h2>Processors</h2>
<hr />
<p>Apart from specifying a filter to identify the logs it can process, a pipeline is composed of a chain of log processors. Each processor takes care of a particular type of log transformation.<br />
When a log matches a pipeline‚Äôs filter, it is processed through its chain of processors one by one.</p>
<p><img src="https://signoz.io/img/logs/pipelines/nginx-pipeline-processors.webp" alt="Processors for an Nginx pipeline" /></p>
<p>_</p>
<p>Processors for an Nginx pipeline</p>
<p>_</p>
<p>SigNoz provides <a href="/docs/logs-pipelines/processors/">a variety of processors</a> for achieving desired log transformations.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors</h2>
<p>Every pipeline includes a chain of processors that define the transformations it will apply to logs. When a log matches a pipeline's filter, it is transformed by each processor in the pipeline one by one.</p>
<p>The following log transformation processors are available for defining pipelines.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#regex
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-regex
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Regex</h2>
<p>The Regex processor can be used to extract information out of text using <a href="https://www3.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html">regular expressions</a>.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>Pattern</td>
<td>The regex pattern to be used. Must include atleast one named capture group</td>
</tr>
<tr>
<td>Parse¬†From</td>
<td>The log text field to parse from. Eg: <code>body</code> or <code>attributes.sessionInfo</code></td>
</tr>
<tr>
<td>Parse¬†To</td>
<td>The path to parse to. Eg: If set to <code>attributes</code>, a capture group like <code>(?P&lt;userId&gt;.+)</code> in the regex pattern would get stored in <code>attributes.userId</code></td>
</tr>
<tr>
<td>On¬†Error</td>
<td>What to do if the processor fails. Options are to <code>drop</code> the log or <code>send</code> it to the next processor</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#grok
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-grok
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Grok</h2>
<p>The Grok processor can be used to extract information out of text using grok patterns.<br />
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/grok.html">Grok</a> is a regular expression dialect with convenient <a href="https://github.com/vjeantet/grok/blob/master/patterns.go">aliases</a> for commonly used expressions.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>Pattern</td>
<td>The <a href="https://grokdebugger.com/">grok pattern</a>&lt;br&gt; to be used. Must include atleast one named capture group</td>
</tr>
<tr>
<td>Parse¬†From</td>
<td>The log text field to parse from. Eg: <code>body</code> or <code>attributes.sessionInfo</code></td>
</tr>
<tr>
<td>Parse¬†To</td>
<td>The path to parse to. Eg: If set to <code>attributes</code>, a capture group like <code>%{WORD:userId}</code> in the grok pattern would get stored in <code>attributes.userId</code></td>
</tr>
<tr>
<td>On¬†Error</td>
<td>What to do if the processor fails. Options are to <code>drop</code> the log or <code>send</code> it to the next processor</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>By default, values extracted using grok patterns are strings.<br />
For example, parsing <code>status: 202</code> with the pattern <code>status: %{INT:status_code}</code> will extract <code>status_code</code> as a string with value <code>&quot;202&quot;</code>.</p>
<p>However, it is possible to extract <code>float</code> or <code>int</code> typed values by adding a 3rd part to grok capture groups.<br />
For example, parsing <code>status: 202</code> with the pattern <code>status: %{INT:status_code:int}</code> will extract <code>status_code</code> as an integer with value <code>202</code>.</p>
<p>This can enable the use of numeric operators (<code>&gt;</code>, <code>&lt;</code> etc) on the extracted values and unlock features like using the values as metrics in dashboards.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#json-parser
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-json-parser
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: JSON Parser</h2>
<p>The JSON parsing processor can be used to parse serialized JSON text into log attributes.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>Parse¬†From</td>
<td>The log field containing serialized JSON text. Eg: <code>body</code> or <code>attributes.sessionInfo</code></td>
</tr>
<tr>
<td>Parse¬†To</td>
<td>The path to parse to. Eg: If set to <code>attributes</code>, parsing the JSON text <code>'{ &quot;userId&quot;: 8888 }'</code> would set <code>attributes.userId</code> to <code>8888</code></td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#trace-parser
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-trace-parser
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Trace Parser</h2>
<p>The trace processor can be used to populate trace id, span id and trace flags for a log.<br />
Populating trace identifiers in logs allows navigation to and from corresponding traces for correlation.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>Parse¬†Trace¬†Id¬†From</td>
<td>The log field containing otel Trace Id. Eg: <code>attributes.myTraceId</code>  &lt;br&gt;Value at the specified path must be an even length string of hex characters</td>
</tr>
<tr>
<td>Parse¬†Span¬†Id¬†From</td>
<td>The log field containing otel Span Id. Eg: <code>attributes.mySpanId</code>  &lt;br&gt;Value at the specified path must be an even length string of hex characters</td>
</tr>
<tr>
<td>Parse¬†Trace¬†Flags¬†From</td>
<td>The log field containing otel Trace Flags. Eg: <code>attributes.myTraceFlags</code>  &lt;br&gt;Value at the specified path must be an unsigned int</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>At least one field among <code>Parse Trace Id From</code>, <code>Parse Span Id From</code> and <code>Parse Trace Flags From</code> must be specified.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#timestamp-parser
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-timestamp-parser
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Timestamp Parser</h2>
<p>The timestamp parsing processor can be used to parse log timestamp out of a log field.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name¬†of¬†Timestamp¬†Parsing¬†Processor</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>Parse¬†Timestamp¬†Value¬†From</td>
<td>The log field containing timestamp value. Eg: <code>attributes.timestamp</code></td>
</tr>
<tr>
<td>Timestamp¬†Format¬†Type</td>
<td>Type of timestamp value to be parsed.  &lt;br&gt;<code>epoch</code> can be used for parsing <a href="https://en.wikipedia.org/wiki/Unix_time">unix time</a>&lt;br&gt; and <code>strptime</code> can be used for parsing human readable values using <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/coreinternal/timeutils/internal/ctimefmt/ctimefmt.go#L68">ctime-like directives</a>&lt;br&gt; such as %Y (4-digit year) and %H (2-digit hour).</td>
</tr>
<tr>
<td>Timestamp¬†Format</td>
<td>Format for parsing timestamp value.  &lt;br&gt;For example <code>%Y-%m-%d</code> can be used for parsing values like <code>2023-12-06</code> when Timestamp Format Type is <code>strptime</code>, or <code>seconds.milliseconds</code> can be used for parsing unix time values like <code>1701869406.245</code> when Timestamp Format Type is <code>epoch</code></td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#severity-parser
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-severity-parser
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Severity Parser</h2>
<p>The severity parsing processor can be used to parse log severity out of a log field.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name¬†of¬†Severity¬†Parsing¬†Processor</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>Parse¬†Severity¬†Value¬†From</td>
<td>The log field to parse severity from. For example <code>attributes.log_level</code></td>
</tr>
<tr>
<td>Values¬†for¬†level¬†TRACE</td>
<td>Comma separated list of values that should be mapped to level TRACE. For example <code>0, trace</code></td>
</tr>
<tr>
<td>Values¬†for¬†level¬†DEBUG</td>
<td>Comma separated list of values that should be mapped to level DEBUG. For example <code>debug, 2xx</code></td>
</tr>
<tr>
<td>Values¬†for¬†level¬†INFO</td>
<td>Comma separated list of values that should be mapped to level INFO. For example <code>info, 3xx</code></td>
</tr>
<tr>
<td>Values¬†for¬†level¬†WARN</td>
<td>Comma separated list of values that should be mapped to level WARN. For example <code>warning, 4xx</code></td>
</tr>
<tr>
<td>Values¬†for¬†level¬†ERROR</td>
<td>Comma separated list of values that should be mapped to level ERROR. For example <code>error, 5xx</code></td>
</tr>
<tr>
<td>Values¬†for¬†level¬†FATAL</td>
<td>Comma separated list of values that should be mapped to level FATAL. For example <code>panic, -1</code></td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>Severity level values are case insensitive.<br />
As a special case, 2xx, 3xx, 4xx and 5xx can be used to map number ranges to severity levels. This can be useful for mapping HTTP status codes. For example <code>5xx</code> can be used to parse numbers between 500-599 into level <code>ERROR</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#add
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-add
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Add</h2>
<p>The add processor can be used to add a field to the log.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>Field</td>
<td>Path of the field to be added. Must be of the form <code>attributes.*</code> or <code>resource.*</code></td>
</tr>
<tr>
<td>Value</td>
<td>The value to be set in the specified field</td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>The value field can be set to an <a href="https://github.com/open-telemetry/opentelemetry-log-collection/blob/main/docs/types/expression.md">expression</a> which will get evaluated for each entry to set the value.<br />
For example the value can be set to <code>EXPR(attributes.subtotal + attributes.taxes)</code> to add a new field for total.<br />
Value expressions are also useful for accessing array items that can't be referenced with field paths in operators like <code>COPY</code> and <code>MOVE</code>, for example <code>EXPR(attributes.locale[0])</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#remove
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-remove
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Remove</h2>
<p>The remove processor can be used for removing unwanted log fields such as PII.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>Field</td>
<td>Path of the field to be removed. Must be of the form <code>attributes.*</code> or <code>resource.*</code></td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#move
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-move
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Move</h2>
<p>The move processor can be used to move or rename a log field.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>From</td>
<td>Path of the field to be moved. Must be of the form <code>attributes.*</code> or <code>resource.*</code></td>
</tr>
<tr>
<td>To</td>
<td>Path to move the field to. Must be of the form <code>attributes.*</code> or <code>resource.*</code></td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-pipelines/processors/#copy
tag_set: logs-pipelines, processors
image_urls: 
tracking_id: docs-logs-pipelines-processors-copy
group_tracking_ids: docs-logs-pipelines-processors
<h2>Log Processors: Copy</h2>
<p>The copy processor can be used to copy log fields.</p>
<h4>## Processor Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>A descriptive name for the processor.</td>
</tr>
<tr>
<td>From</td>
<td>Path of the field to be copied. Must be of the form <code>attributes.*</code> or <code>resource.*</code></td>
</tr>
<tr>
<td>To</td>
<td>Path to copy the field to. Must be of the form <code>attributes.*</code> or <code>resource.*</code></td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_fields/
tag_set: userguide, logs_fields
image_urls: 
tracking_id: docs-userguide-logs_fields
group_tracking_ids: docs-userguide-logs_fields
<h2>Fields in Logs</h2>
<p>A log line contains different attributes attached to it. These attributes helps you to filter your logs so that you can write effiecient queries and get your results faster. These attributes are reffered to as fields in SigNoz.</p>
<p>There are two kind of fields <strong>interesting</strong> and <strong>selected</strong> .</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_fields/#interesting-log-fields
tag_set: userguide, logs_fields
image_urls: 
tracking_id: docs-userguide-logs_fields-interesting-log-fields
group_tracking_ids: docs-userguide-logs_fields
<h2>Fields in Logs: Interesting Log Fields</h2>
<p>These kind of fields are the resource and log attributes which are parsed by the otel collector but is not indexed. These fields are also not auto suggested by the query builder. But you can still use these fields for querying by writing the query manually.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_fields/#selected-log-fields
tag_set: userguide, logs_fields
image_urls: 
tracking_id: docs-userguide-logs_fields-selected-log-fields
group_tracking_ids: docs-userguide-logs_fields
<h2>Fields in Logs: Selected Log Fields</h2>
<p>These are created by converting an interesting field. When a interesting field is converted to selected field, an index is added to the field so that queries for this fields are faster. In addition to that when you write a query this fields will be autosuggested. Selected fields are also displyed explicitly with each log line.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_fields/#configuring-the-signoz-collector
tag_set: userguide, logs_fields
image_urls: 
tracking_id: docs-userguide-logs_fields-configuring-the-signoz-collector
group_tracking_ids: docs-userguide-logs_fields
<h2>Fields in Logs: Configuring the SigNoz Collector</h2>
<p>If you take a look in the SigNoz repository, you‚Äôll find the configuration for the collector in <a href="https://github.com/SigNoz/signoz/blob/main/deploy/docker/clickhouse-setup/otel-collector-config.yaml">/deploy/docker/clickhouse-setup/otel-collector-config.yaml</a> You can edit this file to filter what logs are being stored after being received by the collector.</p>
<p>After editing this file, you‚Äôll need to restart the collector. If using Docker, the command would be <code>docker restart signoz-otel-collector</code></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_fields/#adding-atttributes
tag_set: userguide, logs_fields
image_urls: 
tracking_id: docs-userguide-logs_fields-adding-atttributes
group_tracking_ids: docs-userguide-logs_fields
<h2>Fields in Logs: Adding Atttributes</h2>
<p>To add attributes to all lines logged by this collector, add a section to the <code>processors</code> to add an attribute. This example is a bit contrived, but let‚Äôs say that this collector is only gathering data for a single client organization:</p>
<pre><code>attributes/clientid:
    actions:
      - key: client_id
        value: 1123
        action: insert
</code></pre>
<p>Adding this mapping, however, isn‚Äôt enough to add the attribute, we haven‚Äôt yet added this processor to our pipeline, check the <code>pipelines</code> mapping for <code>logs</code>:</p>
<pre><code>logs:
      receivers: [otlp, tcplog/docker]
      processors: [logstransform/internal, batch]
      exporters: [clickhouselogsexporter]'
</code></pre>
<p>To actually affect our data, we must add it to the pipeline. The revised version looks like this:</p>
<pre><code>logs:
      receivers: [otlp, tcplog/docker]
      processors: [attributes/clientid, logstransform/internal, batch]
      exporters: [clickhouselogsexporter]
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_fields/#creating-log-fields
tag_set: userguide, logs_fields
image_urls: 
tracking_id: docs-userguide-logs_fields-creating-log-fields
group_tracking_ids: docs-userguide-logs_fields
<h2>Fields in Logs: Creating Log Fields</h2>
<p>By default whenever you receive a log from a non OTLP receivers it will be stored directly in the body and you won't be able to filter logs based on fields/attributes. Opentelemetry provides different ways to parse attributes from your logs using different <a href="/docs/userguide/logs/#operators-for-parsing-and-manipulating-logs">operators</a> that the available. These parsed attributes are referred to as fields in signoz.</p>
<p>Ex :- Lets say we have our logs formatted as</p>
<pre><code>{&quot;time&quot;: &quot;2022-09-20,15:27:17 +0530&quot;, &quot;message&quot;: &quot;Logging test...&quot;, &quot;service&quot;: &quot;python&quot;}
</code></pre>
<p>Here we have a timestamp, a message and an attribute named service. Now we will have to parse these in our otel collector config.</p>
<pre><code>receivers:
    ...
    filelog:
    include: [ /tmp/app.log ]
    start_at: beginning
    operators:
        - type: json_parser
        timestamp:
            parse_from: attributes.time
            layout: '%Y-%m-%d,%H:%M:%S %z'
        - type: move
        from: attributes.message
        to: body
        - type: remove
        field: attributes.time
...
</code></pre>
<ul>
<li>In the yaml file above we are using the json parser. This will parse the json log line and add it in the attributes key.</li>
<li>Since we want to populate timestamp we are using the timestamp parser and pointing it to attributes.time which was parsed by the json parser.</li>
<li>Now we want the value inside <code>message</code> key to be in the log body, so we are moving it to body using move operator.</li>
<li>And finally we are removing time from attributes as we have already populated the value of timestamp from it.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_fields/#transforming-attributes
tag_set: userguide, logs_fields
image_urls: 
tracking_id: docs-userguide-logs_fields-transforming-attributes
group_tracking_ids: docs-userguide-logs_fields
<h2>Fields in Logs: Transforming Attributes</h2>
<p>Logs data can be transformed arbitrarily using the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/README.md">OpenTelemetry Transformation Language</a> (OTTL). It‚Äôs not necessary or practical to learn all the ins and outs of this language, but let‚Äôs start with a simple bit of processing that adds some useful attributes.</p>
<p>Within the <code>processors</code> section. we‚Äôll add a <code>transform</code></p>
<pre><code>  transform:
    log_statements:
    - context: log
      statements:
        - set(severity_text, &quot;FAIL&quot;) where body == &quot;request failed&quot;
</code></pre>
<p>When faced with excessively <a href="https://signoz.io/blog/high-cardinality-data/">high cardinality data</a>
, it may be useful to 'crush' attribute values by replacing with generics. Here‚Äôs an example where some metric crushing will make our lives easier later</p>
<pre><code>logs:
  replace_match(attributes[&quot;http.target&quot;], &quot;/user/*/list/*&quot;, &quot;/user/{userId}/list/{listId}&quot;)
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_fields/#removing-sensitive-data-from-logs
tag_set: userguide, logs_fields
image_urls: https://signoz.io/img/docs/user_social.webp
tracking_id: docs-userguide-logs_fields-removing-sensitive-data-from-logs
group_tracking_ids: docs-userguide-logs_fields
<h2>Fields in Logs: Removing Sensitive Data from Logs</h2>
<p>The collector is one more place where you can control potentially sensitive data from being collected or transmitted. You can remove attributes with simple regex-style matching, like in this example where all values should be removed:</p>
<pre><code>transform:
    log_statements:
    - context: log
      statements:
        - set(severity_text, &quot;FAIL&quot;) where body == &quot;request failed&quot;
        - replace_match(attributes[&quot;social_security_number&quot;], &quot;*&quot;, &quot;{userSocial}&quot;)
</code></pre>
<p>Or you can use pattern matching to find any strings that look similar:</p>
<pre><code>  transform:
    log_statements:
    - context: log
      statements:
        - set(severity_text, &quot;FAIL&quot;) where body == &quot;request failed&quot;
        - replace_all_patterns(attributes, &quot;value&quot;, &quot;^\\D*\\d{3}-\\d{2}-\\d{4}&quot;, &quot;{ss_number}&quot;)
</code></pre>
<p>Remember that in these examples you'll need to add <code>transform</code> to the <code>pipeline</code> section of your config and restart the collector for these changes to take effect. The result is a value replaced with a placeholder</p>
<p><img src="https://signoz.io/img/docs/user_social.webp" alt="a detail view of a log with the attribute for a social security number replaced with a placeholder" /></p>
<p><em>Either of the above configurations results in this much safer placeholder.</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/category/guides/
tag_set: category, guides
image_urls: 
tracking_id: docs-category-guides
group_tracking_ids: docs-category-guides
<h2>Logs Pipeline Guides</h2>
<p>See these guides for detailed walkthroughs on creating Log Pipelines for specific purposes.</p>
<ul>
<li><a href="/docs/logs-pipelines/guides/json/">üìÑÔ∏è Parse JSON Logs: Parse JSON logs with Pipelines...</a></li>
<li><a href="/docs/logs-pipelines/guides/trace/">üìÑÔ∏è Parse Trace Information: Parse Trace Information for your Logs...</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model
group_tracking_ids: docs-traces-management-trace-api-payload-model
<h2>Trace API Payload Model</h2>
<p>The SigNoz Trace API uses a JSON payload for queries, which includes various fields and nested fields. This document provides a detailed explanation of each field to help users construct effective queries.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#top-level
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-top-level
group_tracking_ids: docs-traces-management-trace-api-payload-model
<p>Trace API Payload Model: : Top-level</p>
<p>The top-level of the payload model has the following fields:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>start</td>
<td>Epoch timestamp marking the start of the query range (in milliseconds)</td>
</tr>
<tr>
<td>end</td>
<td>Epoch timestamp marking the end of the query range (in milliseconds)</td>
</tr>
<tr>
<td>step</td>
<td>Aggregation interval for the query, specified in seconds</td>
</tr>
<tr>
<td>compositeQuery</td>
<td>This contains the <a href="#composite-query"><strong>compositeQuery</strong></a>&lt;br&gt; which is explained below</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#composite-query
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-composite-query
group_tracking_ids: docs-traces-management-trace-api-payload-model
<p>Trace API Payload Model: : Composite Query</p>
<p>The <code>compositeQuery</code> field consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>queryType</td>
<td>Type of query (e.g., builder, clickhouse, prometheus).Scope of this documentation is limited to <strong>builder</strong> type</td>
</tr>
<tr>
<td>panelType</td>
<td>Type of panel (e.g., list, graph, table, trace, value)</td>
</tr>
<tr>
<td>offset</td>
<td>Offset used in pagination</td>
</tr>
<tr>
<td>limit</td>
<td>Limit number of results</td>
</tr>
<tr>
<td>builderQueries</td>
<td>Map of <a href="#builder-query"><strong>builderQuery</strong></a></td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#builder-query
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-builder-query
group_tracking_ids: docs-traces-management-trace-api-payload-model
<p>Trace API Payload Model: : Builder Query</p>
<p>A <code>builderQuery</code> consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>stepInterval</td>
<td>Aggregation interval for query in seconds</td>
</tr>
<tr>
<td>queryName</td>
<td>Name of the query, should match the key to this map value</td>
</tr>
<tr>
<td>dataSource</td>
<td>Source of data, e.g., traces</td>
</tr>
<tr>
<td>aggregateOperator</td>
<td>Type of aggregation - noop, count, count_distinct, sum, avg, min, max, p05, p10, p20, p25, p50, p75, p90, p95, p99, rate, rate_sum, rate_avg, rate_min, rate_max</td>
</tr>
<tr>
<td>aggregateAttribute</td>
<td>The <a href="#attribute"><strong>attribute</strong></a>&lt;br&gt; against which the aggregateOperator is applied</td>
</tr>
<tr>
<td>filters</td>
<td>Array of <a href="#filter"><strong>filter</strong></a>&lt;br&gt; used for filtering data</td>
</tr>
<tr>
<td>selectColumns</td>
<td>Array of <a href="#attribute"><strong>attribute</strong></a>&lt;br&gt; which is used in list panelType to fetch columns/attributes of list</td>
</tr>
<tr>
<td>groupBy</td>
<td>Array of <a href="#attribute"><strong>attribute</strong></a>&lt;br&gt; used for groupBy</td>
</tr>
<tr>
<td>orderBy</td>
<td>Array of <a href="#order-by"><strong>orderBy</strong></a>&lt;br&gt; used for ordering data</td>
</tr>
<tr>
<td>having</td>
<td>Array of <a href="#having"><strong>having</strong></a>&lt;br&gt; used for filtering data after aggregation</td>
</tr>
<tr>
<td>expression</td>
<td>Will be same as query name but different in case of formulas</td>
</tr>
<tr>
<td>disabled</td>
<td>Specifies if the query is disabled</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#filter
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-filter
group_tracking_ids: docs-traces-management-trace-api-payload-model
<p>Trace API Payload Model: : Filter</p>
<p>A <code>filter</code> consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>items</td>
<td>Array of <a href="#filter-item"><strong>filterItem</strong></a></td>
</tr>
<tr>
<td>op</td>
<td>Operator defining how filter items are joined (e.g., AND).</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#filter-item
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-filter-item
group_tracking_ids: docs-traces-management-trace-api-payload-model
<p>Trace API Payload Model: : Filter Item</p>
<p>The <code>filterItem</code> includes:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>key</td>
<td>Corresponding <a href="#attribute"><strong>attribute</strong></a></td>
</tr>
<tr>
<td>op</td>
<td>Operators - =, !=, &gt;, &gt;=, &lt;, &lt;=, in, nin, contains, ncontains, regex, nregex, like, nlike, exists, nexists</td>
</tr>
<tr>
<td>value</td>
<td>Value for the filter, can be empty for some <strong>op</strong> like <code>exists</code> and <code>nexists</code></td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>The <code>value</code> parameter will be empty for <code>exists</code> and <code>nexists</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#attribute
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-attribute
group_tracking_ids: docs-traces-management-trace-api-payload-model
<p>Trace API Payload Model: : Attribute</p>
<p>An <code>attribute</code> consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>key</td>
<td>Name of the key</td>
</tr>
<tr>
<td>type</td>
<td>Type of the key, i.e., tag/resource. It is empty for top level fields. (e.g., tag = method, resource = k8s_deployment_name, (empty) = trace_id)</td>
</tr>
<tr>
<td>dataType</td>
<td>Data type of the key (e.g., string, int64, float64, bool)</td>
</tr>
<tr>
<td>isColumn</td>
<td>Indicates if it's column, i.e., it would be faster to query using this attribute</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#order-by
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-order-by
group_tracking_ids: docs-traces-management-trace-api-payload-model
<p>Trace API Payload Model: : Order By</p>
<p>An <code>orderBy</code> consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>columnName</td>
<td>Name of the attribute/key</td>
</tr>
<tr>
<td>order</td>
<td>Order of the column (e.g., asc, desc)</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#having
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-having
group_tracking_ids: docs-traces-management-trace-api-payload-model
<p>Trace API Payload Model: : Having</p>
<p>A <code>having</code> consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>columnName</td>
<td>Name of the attribute/key along with aggregate operator (e.g., COUNT(httpMethod))</td>
</tr>
<tr>
<td>op</td>
<td>Operators - =, !=, &gt;, &gt;=, &lt;, &lt;=</td>
</tr>
<tr>
<td>value</td>
<td>Value for the filter</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/payload-model/#sample-payload
tag_set: traces-management, trace-api, payload-model
image_urls: 
tracking_id: docs-traces-management-trace-api-payload-model-sample-payload
group_tracking_ids: docs-traces-management-trace-api-payload-model
<h2>Trace API Payload Model: : Sample Payload</h2>
<p>This sample payload contains the different fields that we looked at above. It queries the SigNoz Trace API and illustrates how to count distinct <code>hasError</code> values and group them by <code>serviceName</code> where <code>hasError</code> is <code>true</code>.</p>
<pre><code>{
    &quot;start&quot;: 1702007766000,
    &quot;end&quot;: 1702009566000,
    &quot;step&quot;: 60,
    &quot;variables&quot;: {},
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;table&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;dataSource&quot;: &quot;traces&quot;,
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;aggregateOperator&quot;: &quot;count&quot;,
                &quot;aggregateAttribute&quot;: {
                    &quot;key&quot;: &quot;hasError&quot;,
                    &quot;dataType&quot;: &quot;bool&quot;,
                    &quot;type&quot;: &quot;tag&quot;,
                    &quot;isColumn&quot;: true
                },
                &quot;filters&quot;: {
                    &quot;items&quot;: [\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;hasError&quot;,\
                                &quot;dataType&quot;: &quot;bool&quot;,\
                                &quot;type&quot;: &quot;tag&quot;,\
                                &quot;isColumn&quot;: true\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;true&quot;\
                        }\
                    ],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;having&quot;: [],
                &quot;stepInterval&quot;: 60,
                &quot;limit&quot;: null,
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ],
                &quot;groupBy&quot;: [\
                    {\
                        &quot;key&quot;: &quot;serviceName&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    }\
                ],
                &quot;legend&quot;: &quot;&quot;,
                &quot;reduceTo&quot;: &quot;sum&quot;
            }
        }
    },
    &quot;dataSource&quot;: &quot;traces&quot;
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/application-logs/
tag_set: logs-management, send-logs, application-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-application-logs
group_tracking_ids: docs-logs-management-send-logs-application-logs
<h2>Send Application logs to SigNoz</h2>
<p>There are multiple ways in which you can send application logs to SigNoz</p>
<ul>
<li><a href="/docs/userguide/collect_logs_from_file">From log file: Read logs from log file and push them to SigNoz</a></li>
<li><a href="/docs/userguide/python-logs-auto-instrumentation">Python logs Auto-Intrumentation: Collect python logs using auto-instrumentation</a></li>
<li><a href="/docs/userguide/collecting_application_logs_otel_sdk_python">Using OpenTelemetry Python SDK: Send application logs directly using OpenTelemetry Python SDK</a></li>
<li><a href="/docs/userguide/collecting_application_logs_otel_sdk_java">Using OpenTelemetry Java SDK: Send application logs directly using OpenTelemetry Java SDK</a></li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/guides/pii-scrubbing/
tag_set: logs-management, guides, pii-scrubbing
image_urls: https://signoz.io/img/docs/logs-management/guides/sample-log-scrubbed-output.webp
tracking_id: docs-logs-management-guides-pii-scrubbing
group_tracking_ids: docs-logs-management-guides-pii-scrubbing
<h2>Guide to perform PII Scrubbing using SigNoz</h2>
<h2>Overview</h2>
<hr />
<p>PII scrubbing removes sensitive personal information from data to protect privacy and comply with regulations like GDPR and CCPA. SigNoz simplifies this process for your applications using OpenTelemetry. This guide explains how to implement PII scrubbing in your data pipeline before sending information to SigNoz.</p>
<h2>Prerequisite</h2>
<hr />
<ul>
<li><a href="https://signoz.io/teams/">SigNoz Cloud</a> account</li>
<li>Logs in log file</li>
</ul>
<h2>Send application logs to SigNoz</h2>
<hr />
<p>You need to configure OpenTelemetry Collector to send your logs in log files to SigNoz, checkout <a href="https://signoz.io/docs/userguide/collect_logs_from_file/">this documentation</a> for detailed instructions on how to do this.</p>
<h2>Process of Scrubbing PII Data</h2>
<hr />
<p>OpenTelemetry Collector offers a powerful <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md">transform processor</a> to filter and modify sensitive data before it reaches SigNoz. We'll use this processor to perform PII scrubbing on our logs.</p>
<h3>## Sample log</h3>
<p>Let's use the following sample log line to demonstrate the PII scrubbing process:</p>
<pre><code>{&quot;timestamp&quot;: &quot;2024-07-05T12:34:56Z&quot;, &quot;severity&quot;: &quot;INFO&quot;, &quot;message&quot;: &quot;User john.doe@example.com (SSN: 123-45-6789) made a purchase with credit card 4111-1111-1111-1111&quot;, &quot;user_id&quot;: &quot;12345&quot;, &quot;purchase_amount&quot;: 99.99}
</code></pre>
<p>This log contains several pieces of PII that we want to scrub: an email address, a social security number (SSN), and a credit card number inside the log body.</p>
<h3>## Configuring the Transform Processor</h3>
<p>To scrub this PII, we'll use the transform processor in our OpenTelemetry Collector configuration. Trasform processor makes use of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions">Regular Expressions</a> for scrubbing your sensitive data. Here's how we can set it up:</p>
<pre><code>processors:
  ...
  transform:
    log_statements:
      - context: log
        statements:
          # Scrub email address
          - replace_pattern(body, &quot;(\\w+)@(\\w+)\\.(\\w+)&quot;, &quot;****@$2.$3&quot;)
          # Scrub SSN
          - replace_pattern(body, &quot;\\d{3}-\\d{2}-\\d{4}&quot;, &quot;***-**-****&quot;)
          # Scrub credit card number
          - replace_pattern(body, &quot;\\d{4}-\\d{4}-\\d{4}-\\d{4}&quot;, &quot;****-****-****-****&quot;)
</code></pre>
<p>Let's break down what each statement does:</p>
<ul>
<li>The first replace_pattern statement looks for email addresses and replaces the username part with asterisks.</li>
<li>The second statement finds patterns matching SSNs (###-##-####) and replaces them entirely with asterisks.</li>
<li>The third statement identifies credit card number patterns and replaces all digits with asterisks.</li>
</ul>
<h3>## Applying the Processor</h3>
<p>To apply this processor to your logs pipeline, add it to the processors list in your pipeline configuration:</p>
<pre><code>service:
  pipelines:
    ...
    logs:
      receivers: [otlp, filelog/app]
      processors: [batch, transform]
      exporters: [otlp]
</code></pre>
<h3>## Result</h3>
<p>After applying this configuration and restarting your OpenTelemetry Collector, the sample log line would be transformed to:</p>
<pre><code>{
  &quot;timestamp&quot;: &quot;2024-07-05T12:34:56Z&quot;,
  &quot;severity&quot;: &quot;INFO&quot;,
  &quot;message&quot;: &quot;User ****@. (SSN: ***-**-****) made a purchase with credit card ****-****-****-****&quot;,
  &quot;user_id&quot;: &quot;12345&quot;,
  &quot;purchase_amount&quot;: 99.99
}
</code></pre>
<p><img src="https://signoz.io/img/docs/logs-management/guides/sample-log-scrubbed-output.webp" alt="Sample log output with PII data scrubbed out" /></p>
<p><em>Sample log output with PII data scrubbed out</em></p>
<p>As you can see, the email address, SSN, and credit card number have been scrubbed, protecting this sensitive information before it reaches SigNoz. Remember to adjust the regular expressions in the <code>replace_pattern</code> statements as needed to match the specific formats of PII in your logs. You can add more statements to handle additional types of sensitive data that may appear in your logs.</p>
<p>To know more about the capabilities of trasform processsor, checkout <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md">this documentation</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/app-service/metrics/
tag_set: azure-monitoring, app-service, metrics
image_urls: https://signoz.io/img/docs/azure-monitoring/app-svc-metrics-signoz.webp
tracking_id: docs-azure-monitoring-app-service-metrics
group_tracking_ids: docs-azure-monitoring-app-service-metrics
<h2>App Service Metrics</h2>
<h2>QuickStart</h2>
<hr />
<p>To monitor Azure App Service's system metrics like CPU Percentage, Memory Percentage, Data In, and Data Out with SigNoz, you just need to set up the OpenTelemetry Collector with the Azure Monitor exporter. No changes are needed to your application code.</p>
<h2>Overview</h2>
<hr />
<p>In this guide, you will learn how to monitor Azure App Service's system metrics like CPU Percentage, Memory Percentage, Data In, and Data Out with SigNoz. By monitoring these metrics, you can keep track of your application's resource utilization and performance.</p>
<p>For application-level traces and metrics, you can use the DNS name of the OpenTelemetry Collector you set up earlier. Simply configure your application to send traces and metrics to the Central Collector, and they will be forwarded to SigNoz automatically.</p>
<h2>Prerequisites</h2>
<hr />
<p>Before you can monitor your Azure App Service with SigNoz, you need to ensure the following prerequisites are met:</p>
<ol>
<li>You have an Azure subscription and an Azure App Service instance running.</li>
<li>You have set up the Central Collector with the Azure Monitor exporter. If you haven't set it up yet, follow the instructions in the¬†<a href="../../bootstrapping/collector-setup">Central Collector Setup</a></li>
</ol>
<h2>Dashboard Example</h2>
<hr />
<p>Once you have completed the prerequisites, you can start monitoring your Azure App Service's system metrics with SigNoz. Here's how you can do it:</p>
<ol>
<li>Log in to your SigNoz account.</li>
<li>Navigate to the Dashboards, and add an dashboard</li>
<li>Add a Timeseries Panel</li>
<li>In <em>Metrics</em>, select <code>azure_memorypercentage_total</code> and <em>Avg By</em> select tag <code>location</code></li>
<li>In Filter say <code>name = &lt;app-svc-plan-name&gt;</code></li>
<li>Hit ‚ÄúSave Changes‚Äù You now have Memory Usage of your App Service in a Dashboard for reporting and alerting</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/app-svc-metrics-signoz.webp" alt="Time Series Panel for App Service Memory Usage" /></p>
<p>_</p>
<p>Time Series Panel for App Service Memory Usage</p>
<p>_</p>
<p>That's it! You have successfully set up monitoring for your Azure App Service's system metrics with SigNoz.</p>
<p>üìù Note</p>
<p>You don't need to make any changes to your application code to monitor the system metrics. The OpenTelemetry Collector with the Azure Monitor exporter takes care of collecting and sending the metrics to SigNoz.</p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter any issues while setting up monitoring for your Azure App Service's system metrics with SigNoz, here are a few troubleshooting steps you can try:</p>
<ol>
<li>Check if the OpenTelemetry Collector is running and properly configured with the Azure Monitor exporter.</li>
<li>Verify that your Azure App Service instance is running and accessible.</li>
<li>Ensure that you have the necessary permissions to access the metrics in your Azure subscription.</li>
</ol>
<p>By following this guide, you should be able to easily monitor your Azure App Service's system metrics with SigNoz and gain valuable insights into your application's performance and resource utilization.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/app-service/logging/
tag_set: azure-monitoring, app-service, logging
image_urls: https://signoz.io/img/docs/azure-monitoring/app-svc-diag-settings.webp, https://signoz.io/img/docs/azure-monitoring/app-svc-diag-configuration.webp
tracking_id: docs-azure-monitoring-app-service-logging
group_tracking_ids: docs-azure-monitoring-app-service-logging
<h2>App Service Logging</h2>
<h2>Overview</h2>
<hr />
<p>The following categories of App Service Logs are available to export to Storage Account or EventHub.</p>
<ul>
<li>HTTP logs</li>
<li>App Service Console Logs</li>
<li>App Service Application Logs</li>
<li>Access Audit Logs</li>
<li>IPSecurity Audit logs</li>
<li>App Service Platform logs</li>
</ul>
<p>Although, the application logs could be sent directly in the Application Level using a Opentelemetry Log Appender, this might not be an ideal solution for legacy software or micro-services model. It‚Äôs easier to do centralised logging for both application logs, system logs and SIEM Audit logs.</p>
<h3>## Prerequisites</h3>
<ul>
<li>
<p><a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p><a href="../../bootstrapping/collector-setup">Central Collector Setup</a></p>
</li>
</ul>
<h2>Setup</h2>
<hr />
<ol>
<li>
<p>Navigate to your App Service in the Azure portal</p>
</li>
<li>
<p>Search for &quot;Diagnostic settings&quot; in the left navigation menu</p>
</li>
<li>
<p>Click on &quot;Add Diagnostic Setting&quot;</p>
<p><img src="https://signoz.io/img/docs/azure-monitoring/app-svc-diag-settings.webp" alt="App Service Diagnostic Settings" /></p>
<p>_</p>
<p>App Service Diagnostic Settings</p>
<p>_</p>
</li>
<li>
<p>Select the desired log categories to export:</p>
<ul>
<li>HTTP logs</li>
<li>App Service Console Logs</li>
<li>App Service Application Logs</li>
<li>Access Audit Logs</li>
<li>IPSecurity Audit logs</li>
<li>App Service Platform logs</li>
</ul>
<p><img src="https://signoz.io/img/docs/azure-monitoring/app-svc-diag-configuration.webp" alt="App Service Diagnostic Configuration" /></p>
<p>_</p>
<p>App Service Diagnostic Configuration</p>
<p>_</p>
</li>
<li>
<p>Configure the destination details as &quot;<strong>Stream to an Event Hub</strong>&quot; and select the Event Hub namespace and Event Hub name created during the <a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p>Save the diagnostic settings</p>
</li>
</ol>
<p>That's it! You have successfully set up logging for your Azure App Service.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/virtual-machines/vm-metrics/#overview
tag_set: azure-monitoring, virtual-machines, vm-metrics
image_urls: 
tracking_id: docs-azure-monitoring-virtual-machines-vm-metrics-overview
group_tracking_ids: docs-azure-monitoring-virtual-machines-vm-metrics
<h2>VM Host Metrics &amp; Logging: VM Host Metrics &amp; Logging - Overview</h2>
<p>In this guide, we'll walk you through the process of setting up an Azure Virtual Machine to send logs, traces and metrics to SigNoz, an open-source observability platform. By following these steps, you'll be able to monitor your Azure VM's performance and troubleshoot issues using SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/virtual-machines/vm-metrics/#prerequisites
tag_set: azure-monitoring, virtual-machines, vm-metrics
image_urls: 
tracking_id: docs-azure-monitoring-virtual-machines-vm-metrics-prerequisites
group_tracking_ids: docs-azure-monitoring-virtual-machines-vm-metrics
<h2>VM Host Metrics &amp; Logging: Prerequisites</h2>
<p>Before you begin, ensure that you have the following:</p>
<ol>
<li>
<p><a href="https://signoz.io/teams/">SigNoz Cloud Account</a></p>
</li>
<li>
<p>An Azure subscription with permissions to create and manage Virtual Machines.</p>
</li>
<li>
<p><a href="../../bootstrapping/collector-setup">Central Collector Setup</a></p>
</li>
<li>
<p>Azure Linux VM with SSH access enabled. Follow <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/ssh-keys-portal">SSH Keys Guide</a> to enable SSH access.</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/virtual-machines/vm-metrics/#setup
tag_set: azure-monitoring, virtual-machines, vm-metrics
image_urls: 
tracking_id: docs-azure-monitoring-virtual-machines-vm-metrics-setup
group_tracking_ids: docs-azure-monitoring-virtual-machines-vm-metrics
<h2>VM Host Metrics &amp; Logging: Setup</h2>
<p>This document assumes that you have already set up your Azure VM and have SSH access to it. If not, follow the steps outlined in the <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-portal?tabs=ubuntu#create-virtual-machine">Azure VM Guide</a> and <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/ssh-keys-portal">SSH Keys Guide</a> to create VM and enable SSH access.</p>
<h3>## Connect to the VM</h3>
<p>The <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/ssh-keys-portal#connect-to-the-vm">SSH Keys Guide</a> has steps on how to connect to your VM via SSH.</p>
<h3>## Install OpenTelemetry Collector</h3>
<p>Follow the <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">OpenTelemetry SigNoz Guide</a> to install the OpenTelemetry Collector.</p>
<h3>## Configure Collector</h3>
<p>The configuration file for the OpenTelemetry Collector is located at <code>/etc/otelcol-contrib/config.yaml</code>. We send the logs, traces and metrics to the central collector instead of SigNoz directly, in order to adopt a scalable architecture pattern. We recommend to our users to use the same pattern in your Azure subscription.</p>
<pre><code>cat &gt; /etc/otelcol-contrib/config.yaml &lt;&lt; EOF
receivers:
  filelog:
    include: [ &lt;file paths&gt; ] # /var/log/myservice/*.json 
    operators:
      - type: json_parser
        timestamp:
          parse_from: attributes.time
          layout: '%Y-%m-%d %H:%M:%S'
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  hostmetrics:
    collection_interval: 60s
    scrapers:
      cpu: {}
      disk: {}
      load: {}
      filesystem: {}
      memory: {}
      network: {}
      paging: {}
      process:
        mute_process_name_error: true
        mute_process_exe_error: true
        mute_process_io_error: true
      processes: {}
  prometheus:
    config:
      global:
        scrape_interval: 60s
      scrape_configs:
        - job_name: otel-collector-binary
          static_configs:
            - targets:
              # - localhost:8888
processors:
  batch:
    send_batch_size: 1000
    timeout: 10s
  # Ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md
  resourcedetection:
    detectors: [env, azure, system] 
    # Using OTEL_RESOURCE_ATTRIBUTES envvar, env detector adds custom labels.
    timeout: 2s
    system:
      hostname_sources: [dns, os] 
extensions:
  health_check: {}
  zpages: {}
exporters:
  otlp:
    endpoint: &quot;&lt;Central Collector DNS Name&gt;:4318&quot;
  logging:
    verbosity: normal
service:
  telemetry:
    metrics:
      address: 0.0.0.0:8888
  extensions: [health_check, zpages]
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics/internal:
      receivers: [prometheus, hostmetrics]
      processors: [resourcedetection, batch]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp, filelog]
      processors: [batch]
      exporters: [otlp]
EOF
</code></pre>
<h4>## OLTP Exporter Configuration</h4>
<p>Make sure to replace <code>&lt;Central Collector DNS Name&gt;</code> with the DNS name of your central collector. If you don't have a central collector yet, follow the <a href="../../bootstrapping/collector-setup">Central Collector Setup</a> guide to set one up.</p>
<h4>## File Logs Receiver Configuration</h4>
<p>The file logs receiver needs to be configured with the paths to the log files that you want to stream to SigNoz. You can specify multiple paths by separating them as a array.</p>
<p>You can also specify globed path patterns to match multiple log files. For example, <code>/var/log/myservice/*.json</code> will match all log files in the <code>/var/log/myservice</code> directory with a <code>.json</code> extension.</p>
<h3>## Start the OpenTelemetry Collector</h3>
<p>Once we are done with the above configurations, we can now run the collector service with the following command:</p>
<pre><code>./otelcol-contrib --config ./config.yaml &amp;&gt; otelcol-output.log &amp; echo &quot;$!&quot; &gt; otel-pid
</code></pre>
<h3>## SigNoz Dashboard</h3>
<p>Once the collector is running, you can access the SigNoz dashboard to view the logs and metrics from your Azure VM.</p>
<p>Please refer to the <a href="https://signoz.io/docs/userguide/hostmetrics/">Hostmetrics Dashboard</a> for information on how to import and use the dashboard.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/virtual-machines/vm-metrics/#troubleshooting
tag_set: azure-monitoring, virtual-machines, vm-metrics
image_urls: 
tracking_id: docs-azure-monitoring-virtual-machines-vm-metrics-troubleshooting
group_tracking_ids: docs-azure-monitoring-virtual-machines-vm-metrics
<h2>VM Host Metrics &amp; Logging: Troubleshooting</h2>
<p>If you encounter any issues during the setup process, here are a few troubleshooting steps:</p>
<ul>
<li>
<p>Check the OpenTelemetry Collector logs for any errors:</p>
<pre><code>tail -f -n 50 otelcol-output.log 
</code></pre>
</li>
<li>
<p>Verify that the necessary outbound ports (4317 for gRPC, 4318 for HTTP) are open in the Azure VM's out network security group.</p>
</li>
<li>
<p>Verify that the central collector is running and configured correctly.</p>
</li>
</ul>
<p>That's it! You have now successfully set up your Azure Virtual Machine to send logs and metrics to SigNoz. You can start monitoring your VM's performance and troubleshooting any issues using the SigNoz dashboard.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-fns/metrics/
tag_set: azure-monitoring, az-fns, metrics
image_urls: https://signoz.io/img/docs/azure-monitoring/azure-fns-metrics-signoz.webp
tracking_id: docs-azure-monitoring-az-fns-metrics
group_tracking_ids: docs-azure-monitoring-az-fns-metrics
<h2>Azure Function Metrics</h2>
<h2>QuickStart</h2>
<hr />
<p>To monitor Azure Function's system metrics like CPU Percentage, Memory Percentage, Data In, and Data Out with SigNoz, you just need to set up the OpenTelemetry Collector with the Azure Monitor exporter. No changes are needed to your application code.</p>
<h2>Overview</h2>
<hr />
<p>In this guide, you will learn how to monitor Azure Function's system metrics like CPU Percentage, Memory Percentage, Data In, and Data Out with SigNoz. By monitoring these metrics, you can keep track of your application's resource utilization and performance.</p>
<p>For application-level traces and metrics, you can use the DNS name of the OpenTelemetry Collector you set up earlier. Simply configure your application to send traces and metrics to the Central Collector, and they will be forwarded to SigNoz automatically.</p>
<h2>Prerequisites</h2>
<hr />
<p>Before you can monitor your Azure Function with SigNoz, you need to ensure the following prerequisites are met:</p>
<ol>
<li>You have an Azure subscription and an Azure Function instance running.</li>
<li>You have set up the Central Collector with the Azure Monitor exporter. If you haven't set it up yet, follow the instructions in the¬†<a href="../../bootstrapping/collector-setup">Central Collector Setup</a></li>
</ol>
<h2>Dashboard Example</h2>
<hr />
<p>Once you have completed the prerequisites, you can start monitoring your Azure Function's system metrics with SigNoz. Here's how you can do it:</p>
<ol>
<li>Log in to your SigNoz account.</li>
<li>Navigate to the Dashboards, and add an dashboard</li>
<li>Add a Timeseries Panel</li>
<li>In <em>Metrics</em>, select <code>azure_requests_total</code> and <em>Avg By</em> select tag <code>location</code></li>
<li>In Filter say <code>name = &lt;function-name&gt;</code></li>
<li>Hit ‚ÄúSave Changes‚Äù You now have Total Requests of your Azure Function in a Dashboard for reporting and alerting</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/azure-fns-metrics-signoz.webp" alt="Time Series Panel for Function's Total Requests" /></p>
<p>_</p>
<p>Time Series Panel for Function's Total Requests</p>
<p>_</p>
<p>That's it! You have successfully set up monitoring for your Azure Function's system metrics with SigNoz.</p>
<p>üìù Note</p>
<p>You don't need to make any changes to your application code to monitor the system metrics. The OpenTelemetry Collector with the Azure Monitor exporter takes care of collecting and sending the metrics to SigNoz.</p>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter any issues while setting up monitoring for your Azure Function's system metrics with SigNoz, here are a few troubleshooting steps you can try:</p>
<ol>
<li>Check if the OpenTelemetry Collector is running and properly configured with the Azure Monitor exporter.</li>
<li>Verify that your Azure Function instance is running and accessible.</li>
<li>Ensure that you have the necessary permissions to access the metrics in your Azure subscription.</li>
<li>Double-check the configuration of the OpenTelemetry Collector with the Azure Monitor exporter to ensure that a resource group filter is not preventing the metrics from being collected.</li>
</ol>
<p>By following this guide, you should be able to easily monitor your Azure Function's system metrics with SigNoz and gain valuable insights into your application's performance and resource utilization.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#overview
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-overview
group_tracking_ids: docs-userguide-collect_logs_from_file
<h2>Collecting Application Logs from Log file: Collecting Application Logs from Log file - Overview</h2>
<p>This guide provides detailed instructions on configuring the OpenTelemetry Collector to read logs from a file and push them to SigNoz, enabling you to analyze your application logs effectively.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#sample-log-file
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-sample-log-file
group_tracking_ids: docs-userguide-collect_logs_from_file
<h2>Collecting Application Logs from Log file: Sample Log File</h2>
<p>As an example, we can create a sample log file called <code>app.log</code> with the following dummy data:</p>
<pre><code>This is log line 1
This is log line 2
This is log line 3
</code></pre>
<p>This file represents a log file of your application. You can choose any file which contains your application's log entries.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#collect-logs-in-signoz-cloud
tag_set: userguide, collect_logs_from_file
image_urls: https://signoz.io/img/docs/application-logs-output.webp
tracking_id: docs-userguide-collect_logs_from_file-collect-logs-in-signoz-cloud
group_tracking_ids: docs-userguide-collect_logs_from_file
<h2>Collecting Application Logs from Log file: Collect Logs in SigNoz Cloud</h2>
<h3>## Prerequisite</h3>
<ul>
<li>SigNoz <a href="https://signoz.io/teams/">cloud</a> account</li>
</ul>
<p>Sending logs to SigNoz cloud can be achieved by following these simple steps:</p>
<ul>
<li>Installing OpenTelemetry Collector</li>
<li>Configuring filelog receiver</li>
</ul>
<h3>## Install OpenTelemetry Collector</h3>
<p>The OpenTelemetry collector provides a vendor-neutral way to collect, process, and export your telemetry data such as logs, metrics, and traces.</p>
<p>You can install OpenTelemetry collector as an agent on your Virtual Machine by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">guide</a>.</p>
<h3>## Configure filelog receiver</h3>
<p>Modify the <code>config.yaml</code> file that you created while installing OTel collector in the previous step to include the filelog receiver. This involves specifying the path to your <code>app.log</code> file and setting the <code>start_at</code> parameter, which specifies where to start reading logs from the log file. For more fields that are available for filelog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">this link</a>.</p>
<pre><code>receivers:
  ...
  filelog/app:
    include: [ /tmp/app.log ] #include the full path to your log file
    start_at: end
...
</code></pre>
<p>üìù Note</p>
<p>The <code>start_at: end</code> configuration ensures that only newly added logs are transmitted. If you wish to include historical logs from the file, remember to modify <code>start_at</code> to <code>beginning</code>.</p>
<p>Log lines from the file will be visible on the SigNoz UI and you will able to filter them once new logs are added to the file while using <code>start_at: end</code></p>
<h3>## Update Pipelines Configuration</h3>
<p>In the same <code>config.yaml</code> file, update the pipeline settings to include the new filelog receiver. This step is crucial for ensuring that the logs are correctly processed and sent to SigNoz.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, filelog/app]
        processors: [batch]
        exporters: [otlp]
</code></pre>
<p>Now restart the OTel collector so that new changes are applied. The steps to run the OTel collector can be found <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a></p>
<h3>## Verify Export</h3>
<p>The logs will be exported to SigNoz UI. If you add more entries to your <code>app.log</code> file they will also be visible in SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/application-logs-output.webp" alt="Logs of the dummy app.log file visible in SigNoz" /></p>
<p><em>Sample log file data shown in SigNoz Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#collecting-logs-in-self-hosted-signoz
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-collecting-logs-in-self-hosted-signoz
group_tracking_ids: docs-userguide-collect_logs_from_file
<h2>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Collecting Logs in self-hosted SigNoz</h2>
<p>Collecting logs in Self-Hosted SigNoz can have two scenarios:</p>
<ul>
<li>SigNoz running on the same host</li>
<li>SigNoz running on different host</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#running-on-the-same-host
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-running-on-the-same-host
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Running on the same host</p>
<p>If your self-hosted SigNoz is running on the same host, then you can follow these steps to collect your application logs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#install-signoz
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-install-signoz
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Install SigNoz</p>
<p>You can install Self-Hosted SigNoz using the instructions <a href="https://signoz.io/docs/install/docker/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#modify-docker-compose-file
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-modify-docker-compose-file
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Modify Docker Compose file</p>
<p>In your self-hosted SigNoz setup, locate and edit the <code>docker-compose.yaml</code> file found in the <code>deploy/docker/clickhouse-setup</code> directory. You'll need to mount the log file of your application to the <code>tmp</code> directory of SigNoz OTel collector.</p>
<pre><code>  ...
  otel-collector:
  image: signoz/signoz-otel-collector:0.88.11
  command: [&quot;--config=/etc/otel-collector-config.yaml&quot;]
  volumes:
    - ~/&lt;path&gt;/app.log:/tmp/app.log
  ....
</code></pre>
<p>Replace <code>&lt;path&gt;</code> with the path where your log file is present. Please ensure that the file path is correctly specified.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#add-filelog-receiver
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-add-filelog-receiver
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Add filelog receiver</p>
<p>Add the filelog reciever to <code>otel-collector-config.yaml</code> which is present inside <code>deploy/docker/clickhouse-setup</code> directory in your self-hosted SigNoz setup. The configuratoin below tells the collector where to find your log file and how to start processing it.</p>
<pre><code>receivers:
  ...
  filelog:
    include: [ /tmp/app.log ]
    start_at: end
...
</code></pre>
<p>üìù Note</p>
<p>The <code>start_at: end</code> configuration ensures that only newly added logs are transmitted. If you wish to include historical logs from the file, remember to modify <code>start_at</code> to <code>beginning</code>.</p>
<p>Log lines from the file will be visible on the SigNoz UI and you will able to filter them once new logs are added to the file while using <code>start_at: end</code></p>
<p>For more fields that are available for filelog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">this link</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#update-pipeline-configuration
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-update-pipeline-configuration
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Update Pipeline configuration</p>
<p>Modify the pipeline inside <code>otel-collector-config.yaml</code> to include the filelog receiver. This step is crucial for ensuring that the logs are correctly processed and sent to SigNoz.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, filelog]
        processors: [batch]
        exporters: [clickhouselogsexporter]
</code></pre>
<p>Now, restart the OTel collector so that new changes are applied. You can find instructions to run OTel collector <a href="https://signoz.io/docs/install/docker/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#verify-export-1
tag_set: userguide, collect_logs_from_file
image_urls: https://signoz.io/img/docs/application-logs-output.webp
tracking_id: docs-userguide-collect_logs_from_file-verify-export-1
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Verify Export</p>
<p>The logs will be exported to SigNoz UI if there are no errors. If you add more entries to your <code>app.log</code> file they will also be visible in SigNoz.</p>
<p><img src="https://signoz.io/img/docs/application-logs-output.webp" alt="Logs of the dummy app.log file visible in SigNoz" /></p>
<p><em>Sample log file data shown in SigNoz Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#running-on-a-different-host
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-running-on-a-different-host
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Running on a different host</p>
<p>If you have a SigNoz running on a different host then you will have to run a OTel collector to export logs from your host to the host where SigNoz is running.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#create-otel-collector-configuration
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-create-otel-collector-configuration
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Create OTel collector configuration</p>
<p>You need to create an <code>otel-collector-config.yaml</code> file, this file defines how the OTel collector will process and forward logs to your SigNoz instance.</p>
<pre><code>receivers:
  filelog:
    include: [ /tmp/app.log ]
    start_at: end
processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s
exporters:
  otlp/log:
    endpoint: http://&lt;host&gt;:&lt;port&gt;
    tls:
      insecure: true
service:
  pipelines:
    logs:
      receivers: [filelog]
      processors: [batch]
      exporters: [ otlp/log ]
</code></pre>
<p>The parsed logs are batched up using the batch processor and then exported to the host where SigNoz is deployed. For finding the right host and port for your SigNoz cluster please follow the guide <a href="../install/troubleshooting.md#signoz-otel-collector-address-grid">here</a>.</p>
<p>üìù Note</p>
<p>The <code>otlp/log</code> exporter in the above configuration file uses a <code>http</code> endpoint but if you want to use <code>https</code> you will have to provide the certificate and the key. You can read more about it <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/otlpexporter/README.md">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#mount-the-log-file
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-mount-the-log-file
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Mount the log file</p>
<p>Run this docker command</p>
<pre><code>docker run -d --name signoz-host-otel-collector --user root -v $(pwd)/app.log:/tmp/app.log:ro -v $(pwd)/otel-collector-config.yaml:/etc/otel/config.yaml signoz/signoz-otel-collector:0.88.11
</code></pre>
<p>The above command runs an OpenTelemetry collector provided by SigNoz in a Docker container. It runs in the background with root privileges, mounts a log file and a configuration file from the host to the container</p>
<p>After running the collector, if there are no errors your logs will be exported and will be visible in SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/heroku_logs_to_signoz/
tag_set: userguide, heroku_logs_to_signoz
image_urls: 
tracking_id: docs-userguide-heroku_logs_to_signoz
group_tracking_ids: docs-userguide-heroku_logs_to_signoz
<h2>Stream Logs from Heroku to SigNoz</h2>
<p>If you are running your applications on heroku, you can stream logs from Heroku to SigNoz using <a href="https://devcenter.heroku.com/articles/log-drains#https-drains">httpsdrain</a>.</p>
<h2>Stream Heroku logs to SigNoz in SigNoz cloud</h2>
<hr />
<ul>
<li>
<p>Use the heroku cli to add a https drain</p>
<pre><code>heroku drains:add https://&lt;TENANT_NAME&gt;:&lt;SIGNOZ_INGESTION_KEY&gt;@ingest.&lt;REGION&gt;.signoz.cloud:443/logs/heroku -a &lt;YOUR_APP_NAME&gt;
</code></pre>
<p>Set the values of <code>&lt;TENANT_NAME&gt;</code>, <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code>, <code>&lt;REGION&gt;</code> and <code>&lt;YOUR APP NAME&gt;</code>.</p>
<p><code>&lt;TENANT_NAME&gt;</code> is name of your instance. Ex:- If the url is <code>https://cpvo-test.us.signoz.cloud</code> the <code>TENANT_NAME</code> is <code>cpvo-test</code>.</p>
<p><code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> is the ingestion key.</p>
<p><code>&lt;YOUR_APP_NAME&gt;</code> is the name of the application where you want to add the drain.</p>
<p>Depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Once added you can verify by going to the SigNoz UI.</p>
</li>
</ul>
<h2>Stream Heroku logs to SigNoz in Self-Hosted SigNoz</h2>
<hr />
<ul>
<li>
<p>Modify the <code>docker-compose.yaml</code> file present inside <code>deploy/docker/clickhouse-setup</code> to expose a port, in this case <code>8081</code>.</p>
<pre><code>...
otel-collector:
    image: signoz/signoz-otel-collector:0.88.11
    command: [&quot;--config=/etc/otel-collector-config.yaml&quot;]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - &quot;8081:8081&quot;
...
</code></pre>
</li>
<li>
<p>Add the httplogreceiver reciever to <code>otel-collector-config.yaml</code> which is present inside <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>receivers:
  httplogreceiver/heroku:
    endpoint: 0.0.0.0:8081
    source: heroku
...
</code></pre>
</li>
<li>
<p>Next we will modify our pipeline inside <code>otel-collector-config.yaml</code> to include the receiver we have created above.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, httplogreceiver/heroku]
        processors: [batch]
        exporters: [clickhouselogsexporter]
</code></pre>
</li>
<li>
<p>Now we can restart the otel collector container so that new changes are applied and we can forward our logs to port <code>8081</code>.</p>
</li>
<li>
<p>Use the heroku cli to add a https drain</p>
<pre><code>heroku drains:add http://&lt;IP&gt;:8081 -a &lt;YOUR_APP_NAME&gt;
</code></pre>
<p>Replace IP with IP of the system where your collector is running. For more info check <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">troubleshooting</a>
.</p>
</li>
<li>
<p>Once added you can verify by going to the SigNoz UI.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-fns/logging/
tag_set: azure-monitoring, az-fns, logging
image_urls: 
tracking_id: docs-azure-monitoring-az-fns-logging
group_tracking_ids: docs-azure-monitoring-az-fns-logging
<h2>Azure Functions Logging</h2>
<h2>Overview</h2>
<hr />
<p>The following categories of Logs are available to export to Storage Account or EventHub.</p>
<ul>
<li>Function App logs</li>
<li>Function Authentication logs (beta)</li>
</ul>
<p>Although, the application logs could be sent directly in the Application Level using a OpenTelemetry Log Appender, this might not an option for managed services.</p>
<h3>## Prerequisites</h3>
<ul>
<li>
<p><a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p><a href="../../bootstrapping/collector-setup">Central Collector Setup</a></p>
</li>
</ul>
<h2>Setup</h2>
<hr />
<ol>
<li>
<p>Navigate to your Azure Function in the Azure portal</p>
</li>
<li>
<p>Search for &quot;Diagnostic settings&quot; in the left navigation menu</p>
</li>
<li>
<p>Click on &quot;Add Diagnostic Setting&quot;</p>
</li>
<li>
<p>Select the desired log categories to export:</p>
<ul>
<li>Function App logs</li>
</ul>
</li>
<li>
<p>Configure the destination details as &quot;<strong>Stream to an Event Hub</strong>&quot; and select the Event Hub namespace and Event Hub name created during the <a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p>Save the diagnostic settings</p>
</li>
</ol>
<p>That's it! You have successfully set up logging for your Azure Function.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-fns/tracing/
tag_set: azure-monitoring, az-fns, tracing
image_urls: 
tracking_id: docs-azure-monitoring-az-fns-tracing
group_tracking_ids: docs-azure-monitoring-az-fns-tracing
<h2>Azure Function Tracing</h2>
<h2>QuickStart</h2>
<hr />
<p>To get started with monitoring your Azure Function App, we recommend using OpenTelemetry (Otel) SDKs to instrument your application. These SDKs will allow you to collect and forward metrics and traces to a Central Collector.</p>
<h3>## Installing the OpenTelemetry SDK</h3>
<p>Please refer to our <a href="../../../instrumentation/">SigNoz Tutorials</a> or <a href="https://signoz.io/blog/">Blog</a> to find information on how to instrument your application like Spring, FastAPI, NextJS, Langchain, Node.js, Flask, Django, etc. with OpenTelemetry.</p>
<pre><code># Node.js example
npm install @opentelemetry/api
npm install @opentelemetry/auto-instrumentations-node
npm install @opentelemetry/exporter-trace-otlp-http
</code></pre>
<h3>## Configure the OpenTelemetry SDK</h3>
<pre><code># Set env vars or config file
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;Your-Central-Collector-DNS&gt;:4318/&quot;
</code></pre>
<p>For application-level traces and metrics, configure your application to use the DNS name of the <a href="../../bootstrapping/collector-setup">Central Collector</a> you set up earlier. This Central Collector will automatically forward the collected data to SigNoz.</p>
<h2>Overview</h2>
<hr />
<p>Unified monitoring of your Azure Function App involves capturing application-level metrics and traces to provide comprehensive insights into your application's performance and resource utilization. For more detailed information on the unified monitoring in Azure, please refer to the <a href="../../bootstrapping/strategy">Azure Monitoring Strategy</a>.</p>
<h2>Prerequisites</h2>
<hr />
<p>Before you proceed, ensure the following prerequisites are met:</p>
<ol>
<li><strong>Azure Subscription &amp; Container App</strong>: You need an active Azure subscription with a running Azure Function App instance.</li>
<li><strong>Central Collector Setup</strong>: Make sure you have set up the Central Collector with the Azure Monitor exporter. If you haven't completed this setup, follow the instructions in the <a href="../../bootstrapping/collector-setup">Central Collector Setup</a>
.</li>
</ol>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter any issues while setting up monitoring for your Azure Function App, here are a few troubleshooting steps to help you resolve common problems:</p>
<ol>
<li>
<p><strong>OpenTelemetry Collector Configuration</strong>:</p>
<ul>
<li>Verify that the OpenTelemetry Collector is running.</li>
<li>Ensure it is properly configured with the OLTP HTTP receiver.</li>
</ul>
</li>
<li>
<p><strong>Azure Function App Accessibility</strong>:</p>
<ul>
<li>Confirm that your Azure Function App instance is up and accessible.</li>
</ul>
</li>
</ol>
<p>By following this guide, you should be able to monitor your Azure Function App's traces with SigNoz effectively, gaining valuable insights into your application's performance and resource utilization.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/search-traces/
tag_set: traces-management, trace-api, search-traces
image_urls: 
tracking_id: docs-traces-management-trace-api-search-traces
group_tracking_ids: docs-traces-management-trace-api-search-traces
<h2>Search Traces</h2>
<p>This section provides detailed examples of searching traces using the SigNoz Trace API. The example queries demonstrate querying traces with specific attributes and filters.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/search-traces/#searching-all-spans
tag_set: traces-management, trace-api, search-traces
image_urls: 
tracking_id: docs-traces-management-trace-api-search-traces-searching-all-spans
group_tracking_ids: docs-traces-management-trace-api-search-traces
<h2>Search Traces: Searching All Spans</h2>
<p>This query is useful when identifying all spans with specific characteristics, such as method type or error status.</p>
<h3>## Query Description</h3>
<p>This query searches for all spans where:</p>
<ul>
<li><code>deployment_name</code>= hotrod</li>
<li><code>httpMethod</code>= GET</li>
<li><code>hasError</code> = true</li>
</ul>
<h3>## Attributes and Columns</h3>
<ul>
<li>Resource Attribute: <code>deployment_name</code></li>
<li>Tag Attributes + Columns: <code>httpMethod</code>, <code>hasError</code></li>
<li>Selected Columns: <code>serviceName</code>, <code>httpMethod</code>, <code>responseStatusCode</code>, <code>httpUrl</code></li>
</ul>
<p>You can specify the start and end timestamps in Unix format (milliseconds).</p>
<h3>## Sample Payload</h3>
<p>This is the JSON payload for the example query.</p>
<pre><code>{
    &quot;start&quot;: 1702009280000,
    &quot;end&quot;: 1702011080000,
    &quot;step&quot;: 60,
    &quot;variables&quot;: {},
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;list&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;dataSource&quot;: &quot;traces&quot;,
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;aggregateOperator&quot;: &quot;noop&quot;,
                &quot;aggregateAttribute&quot;: {},
                &quot;filters&quot;: {
                    &quot;items&quot;: [\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;deployment_name&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;type&quot;: &quot;resource&quot;,\
                                &quot;isColumn&quot;: false\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;hotrod&quot;\
                        },\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;httpMethod&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;type&quot;: &quot;tag&quot;,\
                                &quot;isColumn&quot;: true\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;GET&quot;\
                        },\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;hasError&quot;,\
                                &quot;dataType&quot;: &quot;bool&quot;,\
                                &quot;type&quot;: &quot;tag&quot;,\
                                &quot;isColumn&quot;: true\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;true&quot;\
                        }\
                    ],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;having&quot;: [],
                &quot;stepInterval&quot;: 60,
                &quot;limit&quot;: 10,
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ],
                &quot;groupBy&quot;: [],
                &quot;legend&quot;: &quot;&quot;,
                &quot;reduceTo&quot;: &quot;sum&quot;,
                &quot;offset&quot;: 0,
                &quot;selectColumns&quot;: [\
                    {\
                        &quot;key&quot;: &quot;serviceName&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    },\
                    {\
                        &quot;key&quot;: &quot;httpMethod&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    },\
                    {\
                        &quot;key&quot;: &quot;responseStatusCode&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    },\
                    {\
                        &quot;key&quot;: &quot;httpUrl&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    }\
                ]
            }
        }
    },
    &quot;dataSource&quot;: &quot;traces&quot;
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/traces-management/trace-api/search-traces/#searching-root-spans
tag_set: traces-management, trace-api, search-traces
image_urls: 
tracking_id: docs-traces-management-trace-api-search-traces-searching-root-spans
group_tracking_ids: docs-traces-management-trace-api-search-traces
<h2>Search Traces: Searching Root Spans</h2>
<p>This type of query can be useful to find the initial operation or root spans in a set of traces, which can be crucial for tracing the origin of an issue.</p>
<h3>## Query Description</h3>
<p>This query searches for root spans with:</p>
<ul>
<li><code>deployment_name</code>= hotrod</li>
<li><code>httpMethod</code>= GET</li>
</ul>
<h3>## Attributes and Columns</h3>
<ul>
<li>Resource Attribute: <code>deployment_name</code></li>
<li>Tag Attributes + Columns: <code>httpMethod</code></li>
<li>Selected Columns: <code>serviceName</code>, <code>httpMethod</code>, <code>responseStatusCode</code>, <code>some_custom_tag</code></li>
</ul>
<p>You can specify the start and end timestamps in Unix format (milliseconds).</p>
<p>üìù Note</p>
<p>To search for root spans, you need to add <code>parentSpanID</code> value to <code>&quot;&quot;</code> in filters</p>
<h3>## Sample Payload</h3>
<p>This is the JSON payload for the example query.</p>
<pre><code>{
    &quot;start&quot;: 1702009280000,
    &quot;end&quot;: 1702011080000,
    &quot;step&quot;: 60,
    &quot;variables&quot;: {},
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;list&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;dataSource&quot;: &quot;traces&quot;,
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;aggregateOperator&quot;: &quot;noop&quot;,
                &quot;aggregateAttribute&quot;: {},
                &quot;filters&quot;: {
                    &quot;items&quot;: [\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;deployment_name&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;type&quot;: &quot;resource&quot;,\
                                &quot;isColumn&quot;: false\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;hotrod&quot;\
                        },\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;httpMethod&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;type&quot;: &quot;tag&quot;,\
                                &quot;isColumn&quot;: true\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;GET&quot;\
                        },\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;parentSpanID&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;type&quot;: &quot;tag&quot;,\
                                &quot;isColumn&quot;: true\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;&quot;\
                        }\
                    ],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;having&quot;: [],
                &quot;stepInterval&quot;: 60,
                &quot;limit&quot;: 10,
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ],
                &quot;groupBy&quot;: [],
                &quot;legend&quot;: &quot;&quot;,
                &quot;reduceTo&quot;: &quot;sum&quot;,
                &quot;offset&quot;: 0,
                &quot;selectColumns&quot;: [\
                    {\
                        &quot;key&quot;: &quot;serviceName&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    },\
                    {\
                        &quot;key&quot;: &quot;httpMethod&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    },\
                    {\
                        &quot;key&quot;: &quot;responseStatusCode&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true\
                    },\
                    {\
                        &quot;key&quot;: &quot;some_custom_tag&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: false\
                    }\
                ]
            }
        }
    },
    &quot;dataSource&quot;: &quot;traces&quot;
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-container-apps/tracing/
tag_set: azure-monitoring, az-container-apps, tracing
image_urls: 
tracking_id: docs-azure-monitoring-az-container-apps-tracing
group_tracking_ids: docs-azure-monitoring-az-container-apps-tracing
<h2>Container Apps Tracing</h2>
<h2>QuickStart</h2>
<hr />
<p>To get started with monitoring your Azure Container App, we recommend using OpenTelemetry (Otel) SDKs to instrument your application. These SDKs will allow you to collect and forward metrics and traces to a Central Collector.</p>
<h3>## Installing the OpenTelemetry SDK</h3>
<p>Please refer to our <a href="../../../instrumentation/">SigNoz Tutorials</a> or <a href="https://signoz.io/blog/">Blog</a> to find information on how to instrument your application like Spring, FastAPI, NextJS, Langchain, Node.js, Flask, Django, etc. with OpenTelemetry.</p>
<pre><code># Node.js example
npm install @opentelemetry/api
npm install @opentelemetry/auto-instrumentations-node
npm install @opentelemetry/exporter-trace-otlp-http
</code></pre>
<h3>## Configure the OpenTelemetry SDK</h3>
<pre><code># Set env vars or config file
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;Your-Central-Collector-DNS&gt;:4318/&quot;
</code></pre>
<p>For application-level traces and metrics, configure your application to use the DNS name of the <a href="../../bootstrapping/collector-setup">Central Collector</a> you set up earlier. This Central Collector will automatically forward the collected data to SigNoz.</p>
<h2>Overview</h2>
<hr />
<p>Unified monitoring of your Azure Container App involves capturing application-level metrics and traces to provide comprehensive insights into your application's performance and resource utilization. For more detailed information on the unified monitoring in Azure, please refer to the <a href="../../bootstrapping/strategy">Azure Monitoring Strategy</a>.</p>
<h2>Prerequisites</h2>
<hr />
<p>Before you proceed, ensure the following prerequisites are met:</p>
<ol>
<li><strong>Azure Subscription &amp; Container App</strong>: You need an active Azure subscription with a running Azure Container App instance.</li>
<li><strong>Central Collector Setup</strong>: Make sure you have set up the Central Collector with the Azure Monitor exporter. If you haven't completed this setup, follow the instructions in the <a href="../../bootstrapping/collector-setup">Central Collector Setup</a>
.</li>
</ol>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter any issues while setting up monitoring for your Azure Container App, here are a few troubleshooting steps to help you resolve common problems:</p>
<ol>
<li>
<p><strong>OpenTelemetry Collector Configuration</strong>:</p>
<ul>
<li>Verify that the OpenTelemetry Collector is running.</li>
<li>Ensure it is properly configured with the OLTP HTTP receiver.</li>
</ul>
</li>
<li>
<p><strong>Azure Container App Accessibility</strong>:</p>
<ul>
<li>Confirm that your Azure Container App instance is up and accessible.</li>
</ul>
</li>
</ol>
<p>By following this guide, you should be able to monitor your Azure Container App's traces with SigNoz effectively, gaining valuable insights into your application's performance and resource utilization.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/app-service/tracing/
tag_set: azure-monitoring, app-service, tracing
image_urls: 
tracking_id: docs-azure-monitoring-app-service-tracing
group_tracking_ids: docs-azure-monitoring-app-service-tracing
<h2>App Service Tracing</h2>
<h2>QuickStart</h2>
<hr />
<p>To get started with monitoring your Azure App Service, we recommend using OpenTelemetry (Otel) SDKs to instrument your application. These SDKs will allow you to collect and forward metrics and traces to a Central Collector.</p>
<h3>## Installing the OpenTelemetry SDK</h3>
<p>Please refer to our <a href="../../../instrumentation/">SigNoz Tutorials</a> or <a href="https://signoz.io/blog/">Blog</a> to find information on how to instrument your application like Spring, FastAPI, NextJS, Langchain, Node.js, Flask, Django, etc. with OpenTelemetry.</p>
<pre><code># Node.js example
npm install @opentelemetry/api
npm install @opentelemetry/auto-instrumentations-node
npm install @opentelemetry/exporter-trace-otlp-http
</code></pre>
<h3>## Configure the OpenTelemetry SDK</h3>
<pre><code># Set env vars or config file
export OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;Your-Central-Collector-DNS&gt;:4318/&quot;
</code></pre>
<p>For application-level traces and metrics, configure your application to use the DNS name of the <a href="../../bootstrapping/collector-setup">Central Collector</a> you set up earlier. This Central Collector will automatically forward the collected data to SigNoz.</p>
<h2>Overview</h2>
<hr />
<p>Unified monitoring of your Azure App Service involves capturing application-level metrics and traces to provide comprehensive insights into your application's performance and resource utilization. For more detailed information on the unified monitoring in Azure, please refer to the <a href="../../bootstrapping/strategy">Azure Monitoring Strategy</a>.</p>
<h2>Prerequisites</h2>
<hr />
<p>Before you proceed, ensure the following prerequisites are met:</p>
<ol>
<li><strong>Azure Subscription &amp; App Service</strong>: You need an active Azure subscription with a running Azure App Service instance.</li>
<li><strong>Central Collector Setup</strong>: Make sure you have set up the Central Collector with the Azure Monitor exporter. If you haven't completed this setup, follow the instructions in the <a href="../../bootstrapping/collector-setup">Central Collector Setup</a>
.</li>
</ol>
<h2>Troubleshooting</h2>
<hr />
<p>If you encounter any issues while setting up monitoring for your Azure App Service, here are a few troubleshooting steps to help you resolve common problems:</p>
<ol>
<li>
<p><strong>OpenTelemetry Collector Configuration</strong>:</p>
<ul>
<li>Verify that the OpenTelemetry Collector is running.</li>
<li>Ensure it is properly configured with the OLTP HTTP receiver.</li>
</ul>
</li>
<li>
<p><strong>Azure App Service Accessibility</strong>:</p>
<ul>
<li>Confirm that your Azure App Service instance is up and accessible.</li>
</ul>
</li>
</ol>
<p>By following this guide, you should be able to monitor your Azure App Service's traces with SigNoz effectively, gaining valuable insights into your application's performance and resource utilization.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-container-apps/logging/
tag_set: azure-monitoring, az-container-apps, logging
image_urls: https://signoz.io/img/docs/azure-monitoring/azure-container-app-diag-settings.webp
tracking_id: docs-azure-monitoring-az-container-apps-logging
group_tracking_ids: docs-azure-monitoring-az-container-apps-logging
<h2>Container App Logging</h2>
<h2>Overview</h2>
<hr />
<p>The following categories of Container Apps Logs are available to export to Storage Account or EventHub.</p>
<ul>
<li>Container App console logs</li>
<li>Container App system logs</li>
<li>Spring App console logs</li>
</ul>
<p>Although, the application logs could be sent directly in the Application Level using a OpenTelemetry Log Appender, this might not be an ideal solution for legacy software or micro-services model. It‚Äôs easier to do centralised logging for both application logs, system logs and SIEM Audit logs.</p>
<h3>## Prerequisites</h3>
<ul>
<li>
<p><a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p><a href="../../bootstrapping/collector-setup">Central Collector Setup</a></p>
</li>
</ul>
<h2>Setup</h2>
<hr />
<ol>
<li>
<p>Navigate to your Container Apps in the Azure portal</p>
</li>
<li>
<p>Click on &quot;Container Apps Environment&quot; to open the Container Apps Environment</p>
</li>
<li>
<p>Search for &quot;Diagnostic settings&quot; in the left navigation menu</p>
</li>
<li>
<p>Click on &quot;Add Diagnostic Setting&quot;</p>
</li>
<li>
<p>Select the desired log categories to export:</p>
<ul>
<li>Container App console logs</li>
<li>Container App system logs</li>
<li>Spring App console logs</li>
</ul>
<p><img src="https://signoz.io/img/docs/azure-monitoring/azure-container-app-diag-settings.webp" alt="Container Apps Environment" /></p>
<p>_</p>
<p>Container Apps Environment Diagnostic Configuration</p>
<p>_</p>
</li>
<li>
<p>Configure the destination details as &quot;<strong>Stream to an Event Hub</strong>&quot; and select the Event Hub namespace and Event Hub name created during the <a href="../../bootstrapping/data-ingestion">EventHub Setup</a></p>
</li>
<li>
<p>Save the diagnostic settings</p>
</li>
</ol>
<p>That's it! You have successfully set up logging for your Azure Container App.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/send-cloudwatch-logs-to-signoz/
tag_set: userguide, send-cloudwatch-logs-to-signoz
image_urls: 
tracking_id: docs-userguide-send-cloudwatch-logs-to-signoz
group_tracking_ids: docs-userguide-send-cloudwatch-logs-to-signoz
<h2>Send Cloudwatch Logs to SigNoz</h2>
<h2>Overview</h2>
<hr />
<p>AWS CloudWatch is a monitoring service that helps users keep tabs on their AWS resources. There are some challenges that users encounter on Cloudwatch, such as the absence of a unified observability experience, slightly higher costs, a focus on AWS-centric environments, and user experience limitations.</p>
<p>SigNoz effectively addresses these challenges, and in the following steps, we'll outline how to forward logs from AWS CloudWatch to SigNoz seamlessly.</p>
<h2>Setup</h2>
<hr />
<p>You can choose from the two options below.</p>
<p>SigNoz CloudSelf-Host</p>
<p><strong>Step 1</strong> : Setup the OTel Collector</p>
<p>Follow the instructions in the SigNoz Cloud section of <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">this tutorial</a> to setup the OpenTelemetry Collector.</p>
<p><strong>Step 2</strong> : Configure AWS</p>
<p>Create a <code>~/.aws/credentials</code> file in the machine which should have <code>aws_access_key_id</code> and the <code>aws_secret_access_key</code> in the default section of credentials file.</p>
<p>The below snippet shows an example of the credentials file.</p>
<pre><code>[default]
aws_access_key_id=AKIAIOSFODNN7EXAMPLE
aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

[user1]
aws_access_key_id=AKIAI44QH8DHBEXAMPLE
aws_secret_access_key=je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY
</code></pre>
<p>The account corresponding to these credentials should have the below-mentioned AWS Identity and Access Management (IAM) policy. This policy allows the account associated with these permissions to describe and filter log events within all log groups in the specified AWS account, which is crucial for setting up the necessary permissions to forward CloudWatch logs to SigNoz.</p>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [\
        {\
            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\
            &quot;Effect&quot;: &quot;Allow&quot;,\
            &quot;Action&quot;: [\
                &quot;logs:DescribeLogGroups&quot;,\
                &quot;logs:FilterLogEvents&quot;\
            ],\
            &quot;Resource&quot;: &quot;arn:aws:logs:*:090340947446:log-group:*&quot;\
        }\
    ]
}
</code></pre>
<p>üìù Note</p>
<p>Make sure you have AWS configured on the machine where otel-collector is running</p>
<p><strong>Step 3</strong> : Configure the awscloudwatch receiver</p>
<p>We‚Äôll add an awscloudwatch receiver inside the receivers section of the <code>config.yaml</code> that we created in Step 1 for the OTel collector.</p>
<p>You can configure your receiver to collect logs with different conditions. To see the different parameters, some sample configurations and more details about the awscloudwatch receiver, check out this <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/awscloudwatchreceiver">GitHub link</a>.</p>
<p>Here are two sample configurations:</p>
<ol>
<li>
<p>This configuration below will do autodiscovery and collect 100 log groups starting with prefix application.</p>
<p>receivers:
...
awscloudwatch:
region: us-east-1
logs:
poll_interval: 1m
groups:
autodiscover:
limit: 100
prefix: application
...</p>
</li>
<li>
<p>This configuration below will not do autodiscovery and specifies the names of the log groups to collect.</p>
<p>receivers:
...
awscloudwatch:
profile: 'my-profile'
region: us-west-1
logs:
poll_interval: 5m
groups:
named:
/aws/eks/dev-0/cluster:</p>
<p>...</p>
</li>
</ol>
<p><strong>Step 4</strong> : Send logs to SigNoz<br />
To test out the receiver, create a pipeline in the pipeline section of the <code>config.yaml</code> created in Step 1 for Otel Collector.</p>
<pre><code>...
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics/internal:
      receivers: [prometheus, hostmetrics]
      processors: [resourcedetection, batch]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp, awscloudwatch]
      processors: [batch]
      exporters: [otlp]
</code></pre>
<p>This will log out everything from the receiver and you should be able to see your Cloudwatch logs in the logs tabs of SigNoz Cloud UI.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/vector-logs-to-signoz/
tag_set: logs-management, send-logs, vector-logs-to-signoz
image_urls: 
tracking_id: docs-logs-management-send-logs-vector-logs-to-signoz
group_tracking_ids: docs-logs-management-send-logs-vector-logs-to-signoz
<h2>Send Logs from Vector to SigNoz</h2>
<p>If you have <a href="https://vector.dev/">Vector</a> running on your system, you can forward logs from Vector to SigNoz by following this guide.</p>
<h2>Send Logs from Vector to SigNoz in SigNoz Cloud</h2>
<hr />
<ul>
<li>
<p>Update the vector <code>config.yaml</code> and add the following changes.</p>
<pre><code>signoz_sink:
  type: http
  inputs:
    - &lt;SOURCE_NAME&gt;
  uri: https://ingest.&lt;REGION&gt;.signoz.cloud/logs/vector
  encoding:
    codec: json
  request:
    headers:
      signoz-access-token: &quot;&lt;SIGNOZ_INGESTION_KEY&gt;&quot;
</code></pre>
<p>Set the values of <code>&lt;SOURCE_NAME&gt;</code>, <code>&lt;REGION&gt;</code> and <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code>.</p>
<p><code>&lt;SOURCE_NAME&gt;</code> is the name of the source eg:- <code>docker</code> . Read about sources <a href="https://vector.dev/docs/reference/configuration/sources/">here</a>
.</p>
<p><code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> is the ingestion key for SigNoz.</p>
<p>Depending on the choice of your region for SigNoz cloud, the URI will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Once the changes are applied, data will be sent to SigNoz from Vector.</p>
</li>
</ul>
<h2>Mapping for SigNoz</h2>
<p>==========================================</p>
<ul>
<li>
<p>Any logs that you send, the <code>timestamp</code> will be parsed and the <code>source_type</code> will be moved to resource attributes.</p>
</li>
<li>
<p>The <code>message</code> key will be mapped to <code>body</code>.</p>
</li>
<li>
<p>You can transform your logs at Vector level to convert it to OTEL schema which will look like this.</p>
<pre><code>{
  &quot;trace_id&quot;: &lt;hex string&gt;,
  &quot;span_id&quot;: &lt;hex string&gt;,
  &quot;trace_flags&quot;: &lt;int&gt;
  &quot;severity_text&quot;: &lt;string&gt;,
  &quot;severity_number&quot;: &lt;int&gt;,
  &quot;attributes&quot;: &lt;map&gt;,
  &quot;resources&quot;: &lt;map&gt;,
  &quot;source_type&quot;: &lt;string&gt;,
  &quot;message&quot;: &lt;string&gt;
}
</code></pre>
<p>Eg:-</p>
<pre><code>{
  &quot;trace_id&quot;: &quot;000000000000000018c51935df0b93b9&quot;,
  &quot;span_id&quot;: &quot;18c51935df0b93b9&quot;,
  &quot;trace_flags&quot;: 0,
  &quot;severity_text&quot;: &quot;info&quot;,
  &quot;severity_number&quot;: 4,
  &quot;attributes&quot;: {
      &quot;method&quot;: &quot;GET&quot;,
      &quot;path&quot;: &quot;/api/users&quot;
  },
  &quot;resources&quot;: {
      &quot;host&quot;: &quot;myhost&quot;,
      &quot;namespace&quot;: &quot;prod&quot;
  },
  &quot;source_type&quot;: &quot;docker&quot;,
  &quot;message&quot;: &quot;This is a log line&quot;
}
</code></pre>
</li>
<li>
<p>Any other keys which are not present in the above schema will be moved to log attributes.</p>
</li>
<li>
<p>If you don't want to transform using Vector, you can transform using SigNoz <a href="https://signoz.io/docs/logs-pipelines/introduction/">pipelines</a>
.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_query_builder/
tag_set: userguide, logs_query_builder
image_urls: 
tracking_id: docs-userguide-logs_query_builder
group_tracking_ids: docs-userguide-logs_query_builder
<h2>Logs Query Builder</h2>
<p>We recently released an updated logs explorer page and query builder in SigNoz to make experience of our logs product much more intuitive and seamless.</p>
<p>Some of the key features:</p>
<ol>
<li>
<p>Advanced filtering based on attributes and auto suggestions for filters</p>
<p>You can now create more complex queries for how you match attributes, and the query field will automatically suggest both attributes and values for your query.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fquery-builder%2F02.webp&amp;w=3840&amp;q=75" alt="Logs Query Builder Interface" /></p>
<p>After entering your query hit 'run query' to see a default bar chart and results down below:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fquery-builder%2F03.webp&amp;w=3840&amp;q=75" alt="Logs Query Builder Interface" /></p>
<p>To explore additional ways to filter your query, click 'view details' on any log line to get a list of parameters on the log, and click any value to automatically add a <code>{attribute} IN {value}</code> to your query.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fquery-builder%2F04.webp&amp;w=3840&amp;q=75" alt="Logs Query Builder Interface" /></p>
</li>
<li>
<p>Aggregation options like Group By, ability to specificy aggregation intervals, etc.</p>
<p>The Group By drop down is automatically populated with common attributes, and selecting one, like log level in this example, gets us a few things right away</p>
<ul>
<li>
<p>A comparitive bar chart, with a legend (you can enter a format for the legend including explanatory text eg in this case you might enter <code>The log level is {{level}}</code> to make the chart more readable by others)</p>
</li>
<li>
<p>A timeseries comparing the relative volume of results by time</p>
</li>
</ul>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fquery-builder%2F05.webp&amp;w=3840&amp;q=75" alt="Logs Query Builder Interface" /></p>
</li>
<li>
<p>Plot multiple queries and formulae based on them in the same charts</p>
<p>For comparing two timeseries or values, you can now add multiple queries or formulae on the same chart, which is especially useful if you are comparing similar data across two different services.</p>
</li>
<li>
<p>Modify your query with a click</p>
<p>By clicking on the details of any log line, you can see the attributes for that item. Click any value, and your query will be modified to require that attribute and value.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fquery-builder%2F06.webp&amp;w=3840&amp;q=75" alt="Logs Query Builder Interface" /></p>
</li>
<li>
<p>Create Dashboards and Alerts in a single click from logs query builder</p>
<p>Directly from results you can add the query to a dashboard and set up an alert. This makes the timeseries view especially useful, as you can now create an alert when a certain event is logged beyond a certain rate.</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Fdocs%2Fquery-builder%2F07.webp&amp;w=3840&amp;q=75" alt="Logs Query Builder Interface" /></p>
</li>
</ol>
<p>With the alert query you can perform sophisticated comparisons or other math on your measurement from your logs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_query_builder/#writing-json-filters-in-the-new-logs-explorer
tag_set: userguide, logs_query_builder
image_urls: 
tracking_id: docs-userguide-logs_query_builder-writing-json-filters-in-the-new-logs-explorer
group_tracking_ids: docs-userguide-logs_query_builder
<h2>Logs Query Builder: Writing JSON Filters In The New Logs Explorer</h2>
<p>In the new logs explorer, you can query your JSON data present inside the body. JSON data in the body will be rendered like this:</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fjson_log.webp&amp;w=3840&amp;q=75" alt="JSON Data in log body" /></p>
<p>You can click on one of the keys and then filter them out</p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Fjson_log_filter.webp&amp;w=3840&amp;q=75" alt="JSON Data filter" /></p>
<p>If you want to write the query on your own in the filter bar then you can follow the following rules.</p>
<ul>
<li>For json query inside body it starts with a prefix <code>body.</code></li>
<li>To access a value of a key you can use the notation <code>body.key_name</code></li>
<li>If it is nested key then use <code>.</code> to signify nested keys like <code>body.level1.level2.key</code></li>
<li>If the type of value is array use <code>[*]</code></li>
<li>operators supported for arrays are <code>HAS</code> and <code>NHAS</code></li>
</ul>
<h3>## Example for JSON filter</h3>
<p>Lets say we have this example data in your json log body</p>
<pre><code>{
  &quot;message&quot;: &quot;hello&quot;,
  &quot;request&quot;: {
    &quot;services&quot;: [\
      &quot;service_1&quot;,\
      &quot;service_2&quot;\
    ],
    &quot;service_meta&quot;: [\
      {\
        &quot;name&quot;: &quot;service_1&quot;,\
        &quot;latency&quot;: 101,\
        &quot;tags&quot;: [\
          &quot;tag1&quot;,\
          &quot;tag2&quot;\
        ]\
      },\
      {\
        &quot;name&quot;: &quot;service_2&quot;,\
        &quot;latency&quot;: 200,\
        &quot;tags&quot;: [\
          &quot;tag1&quot;,\
          &quot;tag2&quot;\
        ]\
      }\
    ]
  }
}
</code></pre>
<ul>
<li>
<p>logs where value of message is <code>hello</code></p>
<pre><code>body.message = hello
</code></pre>
</li>
<li>
<p>logs where value of message is contains <code>he</code></p>
<pre><code>body.message CONTAINS he
</code></pre>
</li>
<li>
<p>logs where value of latency of service_1 is &gt;100</p>
<pre><code>body.request.service_meta[*].latency &gt; 100
</code></pre>
</li>
<li>
<p>logs where <code>tag1</code> is present in service tags</p>
<pre><code>body.request.service_meta[*].tags[*] HAS tag1
</code></pre>
</li>
<li>
<p>logs where <code>tag2</code> is not present in service tags</p>
<pre><code>body.request.service_meta[*].tags[*] NHAS tag2
</code></pre>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_query_builder/#logs-query-builder-in-old-logs-explorer
tag_set: userguide, logs_query_builder
image_urls: 
tracking_id: docs-userguide-logs_query_builder-logs-query-builder-in-old-logs-explorer
group_tracking_ids: docs-userguide-logs_query_builder
<h1>Logs Query Builder: Logs Query Builder in old Logs Explorer</h1>
<p>This section will walk you through the query language that is used by SigNoz for filtering logs in the old logs explorer.</p>
<p>This query language for logs is a simplified version of SQL. The current state of the query language is good enough for daily uses. As of now we don't support nesting and parenthesis for explicit ordering due to added complexity.</p>
<p>If you have a use case which you are not able to fullfill with the current features please reach out to us on our slack community or Github issues. We plan to improve the query as we go forward while keeping it simple</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_query_builder/#types-of-queries-supported-by-signoz
tag_set: userguide, logs_query_builder
image_urls: 
tracking_id: docs-userguide-logs_query_builder-types-of-queries-supported-by-signoz
group_tracking_ids: docs-userguide-logs_query_builder
<h2>Logs Query Builder: Types of queries supported by SigNoz:</h2>
<ul>
<li>
<p><strong>Freehand query</strong><br />
When a user writes a plan text query without using any kind of operators, the query is directly searched against the log body. ( inefficient over huge log data)</p>
<p>eg:-<code>Dispatch Successful</code></p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Flogs_query_freehand.webp&amp;w=3840&amp;q=75" alt="Freehand" /></p>
</li>
<li>
<p><strong>Filtering queries</strong><br />
When a user writes queries using a <code>key</code>, <code>operator</code> and a <code>value</code> separated by <code>and</code> , <code>or</code> operators it is a filtering query. This kind of queries are faster as they reduce the search space by using indexes.</p>
<p>eg:- <code>id IN ('2DCVZOsKHioCeuvbObzVzzL1eZ5') AND fulltext contains 'Dispatch Successful'</code></p>
<p><img src="https://signoz.io/_next/image/?url=%2Fimg%2Flogs%2Flogs_query_filtering.webp&amp;w=3840&amp;q=75" alt="Filtering" /></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_query_builder/#list-of-operators-supported-by-signoz
tag_set: userguide, logs_query_builder
image_urls: 
tracking_id: docs-userguide-logs_query_builder-list-of-operators-supported-by-signoz
group_tracking_ids: docs-userguide-logs_query_builder
<h2>Logs Query Builder: List of Operators supported by SigNoz</h2>
<ul>
<li>
<p>Here is a list of all the operators that are supported:</p>
<table>
<thead>
<tr>
<th>Operator</th>
<th>Multivalued</th>
<th>Examples</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>IN</td>
<td>yes</td>
<td>num in (1,2,3)  &lt;br&gt;strdata in ('a', 'b', 'c')</td>
<td>In</td>
</tr>
<tr>
<td>NIN</td>
<td>yes</td>
<td>num nin (1,2,3)  &lt;br&gt;strdata nin ('a', 'b', 'c')</td>
<td>Not In</td>
</tr>
<tr>
<td>GTE</td>
<td>no</td>
<td>num gte 10  &lt;br&gt;dict_word gte 'cat'</td>
<td>Greater than or Equal to</td>
</tr>
<tr>
<td>GT</td>
<td>no</td>
<td>num gt 10  &lt;br&gt;dict_word gt 'cat'</td>
<td>Greater than</td>
</tr>
<tr>
<td>LTE</td>
<td>no</td>
<td>num lte 10  &lt;br&gt;dict_word lte 'cat'</td>
<td>Less than or Equal to</td>
</tr>
<tr>
<td>LT</td>
<td>no</td>
<td>num lt 10  &lt;br&gt;dict_word lt 'cat'</td>
<td>Less than</td>
</tr>
<tr>
<td>CONTAINS</td>
<td>no</td>
<td>stream contains 'err'</td>
<td>Contains</td>
</tr>
<tr>
<td>NCONTAINS</td>
<td>no</td>
<td>stream ncontains 'err'</td>
<td>Doesn't Contain</td>
</tr>
</tbody>
</table>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_query_builder/#fulltext-key
tag_set: userguide, logs_query_builder
image_urls: 
tracking_id: docs-userguide-logs_query_builder-fulltext-key
group_tracking_ids: docs-userguide-logs_query_builder
<h2>Logs Query Builder: Fulltext Key</h2>
<p>The fulltext key is used when we want to write freehand queries and combine them with filters.</p>
<p>eg:-</p>
<pre><code>id IN ('2DCVZOsKHioCeuvbObzVzzL1eZ5') AND fulltext contains 'Dispatch Successful'
</code></pre>
<p>In this cases we are searching <code>Dispatch Successful</code> as fulltext along with the id filter.</p>
<p>üìù Note</p>
<p>The <code>fulltext</code> keyword can be only used with <code>contains</code> and the <code>ncontains</code> operator.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_query_builder/#pointers-to-note-while-writing-queries
tag_set: userguide, logs_query_builder
image_urls: 
tracking_id: docs-userguide-logs_query_builder-pointers-to-note-while-writing-queries
group_tracking_ids: docs-userguide-logs_query_builder
<h2>Logs Query Builder: Pointers to note while writing queries</h2>
<ul>
<li>
<p>Text always needs to be <strong>enclosed in single quotes</strong> in <strong>filtering queries</strong></p>
<p>eg:- If you want to search for logs with stream error and which contains Mozilla in body, the corresponding query on the ui will be</p>
<pre><code>stream IN ('stderr') AND fulltext contains 'Mozilla'
</code></pre>
<p>as we can see <code>Mozilla</code> is enclosed in single quotes as well as <code>stderr</code>.</p>
</li>
<li>
<p>Order of execution is similar to sql i.e left to right and <code>and</code> has higher precedence over <code>or</code> , but currently SigNoz doesn‚Äôt support combining explicitly using parenthesis.</p>
<p><strong>correct</strong> ‚úÖ :-</p>
<pre><code>stream IN ('stdout') and fulltext contains 'Mozilla' or stream IN ('stderr')
</code></pre>
<p><strong>incorrect</strong> ‚ùå :-</p>
<pre><code>(stream IN ('stdout') and fulltext contains 'Mozilla') or stream IN ('stderr')
</code></pre>
<p>While the above to evaluates to the same expression, it‚Äôs not necessarily same for the one below</p>
<p><strong>correct</strong> ‚úÖ :-</p>
<pre><code>stream IN ('stdout') and fulltext contains 'Mozilla' or stream IN ('stderr'
</code></pre>
<p><strong>incorrect</strong> ‚ùå :-</p>
<pre><code>stream IN ('stdout') and (fulltext contains 'Mozilla' or stream IN ('stderr'))
</code></pre>
</li>
</ul>
<p>Here both the statements are not equivalent of each other <strong>i.e it is currently not supported</strong></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logs_query_builder/#query-examples
tag_set: userguide, logs_query_builder
image_urls: 
tracking_id: docs-userguide-logs_query_builder-query-examples
group_tracking_ids: docs-userguide-logs_query_builder
<h2>Logs Query Builder: Query Examples</h2>
<p>Here are a few examples of valid and invalid queries:</p>
<ul>
<li>
<p><strong>Valid Queries</strong></p>
<table>
<thead>
<tr>
<th>Query</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPERATION in ('add') AND FULLTEXT contains 'search string'</td>
<td>fulltext with filtering query</td>
</tr>
<tr>
<td>my fulltext search</td>
<td>fulltext search query</td>
</tr>
<tr>
<td>status gte 200 AND FULLTEXT contains 'search string'</td>
<td>fulltext with filtering query</td>
</tr>
<tr>
<td>service IN ('name&gt;100') AND length GT 100</td>
<td>filtering query</td>
</tr>
<tr>
<td>service IN ('name &gt; 100') AND name GT 'myname'</td>
<td>filtering query</td>
</tr>
<tr>
<td>hello in 2</td>
<td>fulltext search query</td>
</tr>
<tr>
<td>hello in (2,3)</td>
<td>filtering query</td>
</tr>
<tr>
<td>hello lt 2</td>
<td>filtering query</td>
</tr>
</tbody>
</table>
</li>
<li>
<p><strong>Invalid Queries</strong></p>
<table>
<thead>
<tr>
<th>Query</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPERATION in ('bcd') AND 'abcd' FULLTEXT contains 'search string'</td>
<td>AND missing between 'abcd' and FULLTEXT</td>
</tr>
<tr>
<td>OPERATION in ('bcd') AND 'search string'</td>
<td>Operator missing before 'search string'</td>
</tr>
</tbody>
</table>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#overview
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-overview
group_tracking_ids: docs-userguide-collect_logs_from_file
<h2>Collecting Application Logs from Log file: Collecting Application Logs from Log file - Overview</h2>
<p>This guide provides detailed instructions on configuring the OpenTelemetry Collector to read logs from a file and push them to SigNoz, enabling you to analyze your application logs effectively.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#sample-log-file
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-sample-log-file
group_tracking_ids: docs-userguide-collect_logs_from_file
<h2>Collecting Application Logs from Log file: Sample Log File</h2>
<p>As an example, we can create a sample log file called <code>app.log</code> with the following dummy data:</p>
<pre><code>This is log line 1
This is log line 2
This is log line 3
</code></pre>
<p>This file represents a log file of your application. You can choose any file which contains your application's log entries.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#collect-logs-in-signoz-cloud
tag_set: userguide, collect_logs_from_file
image_urls: https://signoz.io/img/docs/application-logs-output.webp
tracking_id: docs-userguide-collect_logs_from_file-collect-logs-in-signoz-cloud
group_tracking_ids: docs-userguide-collect_logs_from_file
<h2>Collecting Application Logs from Log file: Collect Logs in SigNoz Cloud</h2>
<h3>## Prerequisite</h3>
<ul>
<li>SigNoz <a href="https://signoz.io/teams/">cloud</a> account</li>
</ul>
<p>Sending logs to SigNoz cloud can be achieved by following these simple steps:</p>
<ul>
<li>Installing OpenTelemetry Collector</li>
<li>Configuring filelog receiver</li>
</ul>
<h3>## Install OpenTelemetry Collector</h3>
<p>The OpenTelemetry collector provides a vendor-neutral way to collect, process, and export your telemetry data such as logs, metrics, and traces.</p>
<p>You can install OpenTelemetry collector as an agent on your Virtual Machine by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">guide</a>.</p>
<h3>## Configure filelog receiver</h3>
<p>Modify the <code>config.yaml</code> file that you created while installing OTel collector in the previous step to include the filelog receiver. This involves specifying the path to your <code>app.log</code> file and setting the <code>start_at</code> parameter, which specifies where to start reading logs from the log file. For more fields that are available for filelog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">this link</a>.</p>
<pre><code>receivers:
  ...
  filelog/app:
    include: [ /tmp/app.log ] #include the full path to your log file
    start_at: end
...
</code></pre>
<p>üìù Note</p>
<p>The <code>start_at: end</code> configuration ensures that only newly added logs are transmitted. If you wish to include historical logs from the file, remember to modify <code>start_at</code> to <code>beginning</code>.</p>
<p>Log lines from the file will be visible on the SigNoz UI and you will able to filter them once new logs are added to the file while using <code>start_at: end</code></p>
<h3>## Update Pipelines Configuration</h3>
<p>In the same <code>config.yaml</code> file, update the pipeline settings to include the new filelog receiver. This step is crucial for ensuring that the logs are correctly processed and sent to SigNoz.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, filelog/app]
        processors: [batch]
        exporters: [otlp]
</code></pre>
<p>Now restart the OTel collector so that new changes are applied. The steps to run the OTel collector can be found <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a></p>
<h3>## Verify Export</h3>
<p>The logs will be exported to SigNoz UI. If you add more entries to your <code>app.log</code> file they will also be visible in SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/application-logs-output.webp" alt="Logs of the dummy app.log file visible in SigNoz" /></p>
<p><em>Sample log file data shown in SigNoz Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#collecting-logs-in-self-hosted-signoz
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-collecting-logs-in-self-hosted-signoz
group_tracking_ids: docs-userguide-collect_logs_from_file
<h2>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Collecting Logs in self-hosted SigNoz</h2>
<p>Collecting logs in Self-Hosted SigNoz can have two scenarios:</p>
<ul>
<li>SigNoz running on the same host</li>
<li>SigNoz running on different host</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#running-on-the-same-host
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-running-on-the-same-host
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Running on the same host</p>
<p>If your self-hosted SigNoz is running on the same host, then you can follow these steps to collect your application logs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#install-signoz
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-install-signoz
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Install SigNoz</p>
<p>You can install Self-Hosted SigNoz using the instructions <a href="https://signoz.io/docs/install/docker/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#modify-docker-compose-file
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-modify-docker-compose-file
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Modify Docker Compose file</p>
<p>In your self-hosted SigNoz setup, locate and edit the <code>docker-compose.yaml</code> file found in the <code>deploy/docker/clickhouse-setup</code> directory. You'll need to mount the log file of your application to the <code>tmp</code> directory of SigNoz OTel collector.</p>
<pre><code>  ...
  otel-collector:
  image: signoz/signoz-otel-collector:0.88.11
  command: [&quot;--config=/etc/otel-collector-config.yaml&quot;]
  volumes:
    - ~/&lt;path&gt;/app.log:/tmp/app.log
  ....
</code></pre>
<p>Replace <code>&lt;path&gt;</code> with the path where your log file is present. Please ensure that the file path is correctly specified.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#add-filelog-receiver
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-add-filelog-receiver
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Add filelog receiver</p>
<p>Add the filelog reciever to <code>otel-collector-config.yaml</code> which is present inside <code>deploy/docker/clickhouse-setup</code> directory in your self-hosted SigNoz setup. The configuratoin below tells the collector where to find your log file and how to start processing it.</p>
<pre><code>receivers:
  ...
  filelog:
    include: [ /tmp/app.log ]
    start_at: end
...
</code></pre>
<p>üìù Note</p>
<p>The <code>start_at: end</code> configuration ensures that only newly added logs are transmitted. If you wish to include historical logs from the file, remember to modify <code>start_at</code> to <code>beginning</code>.</p>
<p>Log lines from the file will be visible on the SigNoz UI and you will able to filter them once new logs are added to the file while using <code>start_at: end</code></p>
<p>For more fields that are available for filelog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">this link</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#update-pipeline-configuration
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-update-pipeline-configuration
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Update Pipeline configuration</p>
<p>Modify the pipeline inside <code>otel-collector-config.yaml</code> to include the filelog receiver. This step is crucial for ensuring that the logs are correctly processed and sent to SigNoz.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, filelog]
        processors: [batch]
        exporters: [clickhouselogsexporter]
</code></pre>
<p>Now, restart the OTel collector so that new changes are applied. You can find instructions to run OTel collector <a href="https://signoz.io/docs/install/docker/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#verify-export-1
tag_set: userguide, collect_logs_from_file
image_urls: https://signoz.io/img/docs/application-logs-output.webp
tracking_id: docs-userguide-collect_logs_from_file-verify-export-1
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Verify Export</p>
<p>The logs will be exported to SigNoz UI if there are no errors. If you add more entries to your <code>app.log</code> file they will also be visible in SigNoz.</p>
<p><img src="https://signoz.io/img/docs/application-logs-output.webp" alt="Logs of the dummy app.log file visible in SigNoz" /></p>
<p><em>Sample log file data shown in SigNoz Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#running-on-a-different-host
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-running-on-a-different-host
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Running on a different host</p>
<p>If you have a SigNoz running on a different host then you will have to run a OTel collector to export logs from your host to the host where SigNoz is running.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#create-otel-collector-configuration
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-create-otel-collector-configuration
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Create OTel collector configuration</p>
<p>You need to create an <code>otel-collector-config.yaml</code> file, this file defines how the OTel collector will process and forward logs to your SigNoz instance.</p>
<pre><code>receivers:
  filelog:
    include: [ /tmp/app.log ]
    start_at: end
processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s
exporters:
  otlp/log:
    endpoint: http://&lt;host&gt;:&lt;port&gt;
    tls:
      insecure: true
service:
  pipelines:
    logs:
      receivers: [filelog]
      processors: [batch]
      exporters: [ otlp/log ]
</code></pre>
<p>The parsed logs are batched up using the batch processor and then exported to the host where SigNoz is deployed. For finding the right host and port for your SigNoz cluster please follow the guide <a href="../install/troubleshooting.md#signoz-otel-collector-address-grid">here</a>.</p>
<p>üìù Note</p>
<p>The <code>otlp/log</code> exporter in the above configuration file uses a <code>http</code> endpoint but if you want to use <code>https</code> you will have to provide the certificate and the key. You can read more about it <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/otlpexporter/README.md">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collect_logs_from_file/#mount-the-log-file
tag_set: userguide, collect_logs_from_file
image_urls: 
tracking_id: docs-userguide-collect_logs_from_file-mount-the-log-file
group_tracking_ids: docs-userguide-collect_logs_from_file
<p>Collecting Application Logs from Log file: Collecting Logs in self-hosted SigNoz: Mount the log file</p>
<p>Run this docker command</p>
<pre><code>docker run -d --name signoz-host-otel-collector --user root -v $(pwd)/app.log:/tmp/app.log:ro -v $(pwd)/otel-collector-config.yaml:/etc/otel/config.yaml signoz/signoz-otel-collector:0.88.11
</code></pre>
<p>The above command runs an OpenTelemetry collector provided by SigNoz in a Docker container. It runs in the background with root privileges, mounts a log file and a configuration file from the host to the container</p>
<p>After running the collector, if there are no errors your logs will be exported and will be visible in SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/python-logs-auto-instrumentation/
tag_set: userguide, python-logs-auto-instrumentation
image_urls: 
tracking_id: docs-userguide-python-logs-auto-instrumentation
group_tracking_ids: docs-userguide-python-logs-auto-instrumentation
<h2>Python Logs Auto-Instrumentation</h2>
<h2>Collecting Python Application Logs Using Auto-Instrumentation</h2>
<hr />
<p>If you are using python auto-instrumentation for instrumenting your python application you can send logs to SigNoz easily with auto-instrumentation.</p>
<p>To enable logs auto-instrumentation just add this environment variable</p>
<pre><code>OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true
</code></pre>
<h2>Example application</h2>
<hr />
<p>Here is a sample python application</p>
<ol>
<li>
<p>Create a file named main.py and paste the following code</p>
<pre><code>from flask import Flask
import logging

app = Flask(__name__)

@app.route('/')
def hello_world():
    logging.warning(&quot;hello world log message&quot;)
    return 'Hello World'

if __name__ == '__main__':
    app.run()
</code></pre>
</li>
<li>
<p>Create a virual environment</p>
<pre><code>python -m venv venv
source ./venv/bin/activate
</code></pre>
</li>
<li>
<p>Install dependencies</p>
<pre><code>pip install opentelemetry-distro
pip install flask requests
pip install opentelemetry-exporter-otlp
</code></pre>
</li>
<li>
<p>Run the opentelemetry-bootstrap command:</p>
<pre><code>opentelemetry-bootstrap -a install
</code></pre>
</li>
<li>
<p>Run the application</p>
<pre><code>OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true opentelemetry-instrument --traces_exporter none --metrics_exporter none --logs_exporter console python main.py
</code></pre>
</li>
</ol>
<p>You will be able to see the otel logs on the console once you visit <code>http://localhost:5000</code></p>
<p>If you want to send data to SigNoz cloud or self host SigNoz the run command will change and will be described in the next steps</p>
<p>SigNoz CloudSelf-Host</p>
<p>For SigNoz Cloud the run command will be</p>
<pre><code>OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true \
OTEL_EXPORTER_OTLP_ENDPOINT=&lt;SIGNOZ_ENDPOINT&gt; \
OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;INGESTION_KEY&gt; \
opentelemetry-instrument --traces_exporter otlp --metrics_exporter otlp --logs_exporter otlp python main.py
</code></pre>
<ul>
<li>The value of <code>SIGNOZ_ENDPOINT</code> will be <code>https://ingest.{region}.signoz.cloud:443</code> where depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</li>
</ul>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
<ul>
<li>The value of <code>INGESTION_KEY</code> is your ingestion key.</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_python/
tag_set: userguide, collecting_application_logs_otel_sdk_python
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_python
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_python
<h2>Collecting Application Logs Using OTEL Python SDK</h2>
<p>You can directly send logs of your application to SigNoz using the Python SDKs provided by opentlemetry. Please find an example <a href="https://github.com/open-telemetry/opentelemetry-python/tree/main/docs/examples/logs">here</a>.</p>
<p>üìù Note</p>
<p>The default logging level in Python is WARNING.</p>
<p>To send all the logs to SigNoz please change the default log level to DEBUG.</p>
<pre><code>import logging
logging.basicConfig(level=logging.DEBUG)
</code></pre>
<h2>For SigNoz Cloud</h2>
<hr />
<p>For sending logs to SigNoz cloud, while running the above example set the below environment variables</p>
<ul>
<li>
<p>The value of <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> environment variable will be <code>https://ingest.{region}.signoz.cloud:443</code> where depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>The value of <code>OTEL_EXPORTER_OTLP_HEADERS</code> environment variable will be <code>signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt;</code> where <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> is your ingestion key</p>
</li>
<li>
<p>Your run command will look like</p>
<pre><code>OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; python3 example.py`
</code></pre>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#collecting-application-logs-using-otel-java-agent
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-collecting-application-logs-using-otel-java-agent
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h1>Collecting Application Logs Using OTEL Java Agent: Collecting Application Logs Using OTEL Java Agent - Collecting Application Logs Using OTEL Java Agent</h1>
<p>You can directly send your application logs to SigNoz using <a href="https://signoz.io/docs/instrumentation/java/">Java Agent provided by OpenTelemetry</a>. In this doc we will run a sample java application with the OpenTelemetry Java agent to send logs to SigNoz.</p>
<p>For collecting logs we will have to download the java agent from <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar">here</a>.</p>
<p>To sends logs from a Java application you will have to add the agent and add the environment variables for the agent.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#for-sending-logs-to-signoz-cloud
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-for-sending-logs-to-signoz-cloud
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: For Sending Logs To SigNoz Cloud</h2>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; java -javaagent:/path/opentelemetry-javaagent.jar -jar  &lt;myapp&gt;.jar
</code></pre>
<p>You will have to add <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> and depending on the choice of your region for SigNoz cloud, the otlp endpoint will vary according to this table.</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Endpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>US</td>
<td>ingest.us.signoz.cloud:443</td>
</tr>
<tr>
<td>IN</td>
<td>ingest.in.signoz.cloud:443</td>
</tr>
<tr>
<td>EU</td>
<td>ingest.eu.signoz.cloud:443</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#for-sending-logs-to-signoz-hosted-locally
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-for-sending-logs-to-signoz-hosted-locally
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: For Sending Logs To SigNoz Hosted Locally</h2>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;IP of SigNoz Backend&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;app_name&gt; java -javaagent:/path/opentelemetry-javaagent.jar -jar  &lt;myapp&gt;.jar
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#settings-for-appender-instrumentation-based-on-the-logging-library
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-settings-for-appender-instrumentation-based-on-the-logging-library
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: Settings for Appender instrumentation based on the logging library</h2>
<p>You can use appender settings by passing it as an argument in the <code>-D&lt;property&gt;=&lt;value&gt;</code> format.</p>
<p>ex:- <code>-Dotel.instrumentation.logback-appender.experimental-log-attributes=true</code></p>
<h3>## Logback</h3>
<p>LINK - <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/tree/main/instrumentation/logback/logback-appender-1.0/javaagent">Logback</a></p>
<table>
<thead>
<tr>
<th>System property</th>
<th>Type</th>
<th>Default Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental-log-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of experimental log attributes <code>thread.name</code> and <code>thread.id</code>.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-code-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of [source code attributes]. Note that capturing source code attributes at logging sites might add a performance overhead.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-marker-attribute</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of Logback markers as attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-key-value-pair-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of Logback key value pairs as attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-logger-context-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of Logback logger context properties as attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.logback-appender.experimental.capture-mdc-attributes</code></td>
<td>String</td>
<td>NA</td>
<td>Comma separated list of MDC attributes to capture. Use the wildcard character <code>*</code> to capture all attributes.</td>
</tr>
</tbody>
</table>
<h3>## Log4j</h3>
<p>LINK - <a href="https://github.com/open-telemetry/opentelemetry-java-instrumentation/tree/main/instrumentation/log4j/log4j-appender-2.17/javaagent">Log4j</a></p>
<table>
<thead>
<tr>
<th>System property</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>otel.instrumentation.log4j-appender.experimental-log-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of experimental log attributes <code>thread.name</code> and <code>thread.id</code>.</td>
</tr>
<tr>
<td><code>otel.instrumentation.log4j-appender.experimental.capture-map-message-attributes</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of <code>MapMessage</code> attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.log4j-appender.experimental.capture-marker-attribute</code></td>
<td>Boolean</td>
<td><code>false</code></td>
<td>Enable the capture of Log4j markers as attributes.</td>
</tr>
<tr>
<td><code>otel.instrumentation.log4j-appender.experimental.capture-mdc-attributes</code></td>
<td>String</td>
<td></td>
<td>Comma separated list of context data attributes to capture. Use the wildcard character <code>*</code> to capture all attributes.</td>
</tr>
</tbody>
</table>
<p>In the below example we will configure a java application to send logs to SigNoz.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#example-how-to-collect-application-logs-using-otel-java-agent
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-example-how-to-collect-application-logs-using-otel-java-agent
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: [Example] How to Collect Application Logs Using OTEL Java Agent?</h2>
<ul>
<li>
<p>Clone this <a href="https://github.com/SigNoz/spring-petclinic">repository</a></p>
</li>
<li>
<p>Build the application using <code>./mvnw package</code></p>
</li>
<li>
<p>Now run the application</p>
</li>
</ul>
<h3>## For SigNoz Cloud</h3>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; OTEL_RESOURCE_ATTRIBUTES=service.name=myapp java -javaagent:/path/opentelemetry-javaagent.jar -jar target/*.jar
</code></pre>
<p>You will have to replace the value of <code>{region}</code> according to region of your cloud account and also add <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code></p>
<h3>## For SigNoz Hosted Locally</h3>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;host&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=myapp java -javaagent:/path/opentelemetry-javaagent.jar -jar target/*.jar
</code></pre>
<p>You will have to replace your the value of <code>host</code> as <code>0.0.0.0</code> if SigNoz is running in the same host, for other configurations please check the <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">troubleshooting</a> guide.</p>
<ul>
<li>Visit <code>http://localhost:8090</code> to access the application.</li>
<li>Once you use the application logs will be visible on SigNoz UI.</li>
<li>If you want to enable settings here is how you do it.</li>
</ul>
<p>Let's say we want to enable <code>-Dotel.instrumentation.logback-appender.experimental-log-attributes=true</code></p>
<h3>## For SigNoz Cloud</h3>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;https://ingest.{region}.signoz.cloud:443&quot; OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=&lt;SIGNOZ_INGESTION_KEY&gt; OTEL_RESOURCE_ATTRIBUTES=service.name=myapp java -javaagent:/path/opentelemetry-javaagent.jar -Dotel.instrumentation.logback-appender.experimental-log-attributes=true -jar target/*.jar
</code></pre>
<p>You will have to replace the value of <code>{region}</code> according to the region of your cloud account and also replace <code>&lt;SIGNOZ_INGESTION_KEY&gt;</code> with your SigNoz Cloud Ingestion key.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/collecting_application_logs_otel_sdk_java/#for-signoz-hosted-locally-1
tag_set: userguide, collecting_application_logs_otel_sdk_java
image_urls: 
tracking_id: docs-userguide-collecting_application_logs_otel_sdk_java-for-signoz-hosted-locally-1
group_tracking_ids: docs-userguide-collecting_application_logs_otel_sdk_java
<h2>Collecting Application Logs Using OTEL Java Agent: For SigNoz Hosted Locally</h2>
<pre><code>OTEL_LOGS_EXPORTER=otlp OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://&lt;host&gt;:4317&quot; OTEL_RESOURCE_ATTRIBUTES=service.name=myapp java -javaagent:/path/opentelemetry-javaagent.jar -Dotel.instrumentation.logback-appender.experimental-log-attributes=true -jar target/*.jar
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/guides/drop-logs/
tag_set: logs-management, guides, drop-logs
image_urls: 
tracking_id: docs-logs-management-guides-drop-logs
group_tracking_ids: docs-logs-management-guides-drop-logs
<h2>Guide to drop logs</h2>
<h2>Overview</h2>
<hr />
<p>The filter processor in OpenTelemetry allows you to drop logs based on body, log level (aka severity text), or other attributes. This is useful if you want to exclude certain logs from being sent to SigNoz.</p>
<p>The filter processor is configured in the <code>processors::filter</code> section of the <code>otel-collector-config.yaml</code> file.</p>
<p>üìù Note</p>
<p>The processor needs to be added to the logs pipeline to take effect.</p>
<pre><code>logs:
  receivers: [otlp]
  processors: [filter/drop_logs_by_severity_text, batch]
  exporters: [otlp]
</code></pre>
<h2>Drop Logs</h2>
<hr />
<ol>
<li>
<p>Drop logs by log level / severity text</p>
<p>processors:
filter/drop_logs_by_level:
logs:
log_record:
- 'IsMatch(severity_text, &quot;(?i)\b(DEBUG)\b&quot;)'</p>
</li>
<li>
<p>Drop logs by body regex</p>
<p>processors:
filter/drop_logs_by_body_regex:
logs:
log_record:
- 'IsMatch(body, &quot;.<em>password.</em>&quot;)'</p>
</li>
<li>
<p>Drop logs by resource attributes (like service.name, host.name, k8s.pod.name, etc.)</p>
<p>processors:
filter/drop_logs_by_label_values:
logs:
log_record:
- resource.attributes[&quot;k8s.pod.name&quot;] == &quot;test-pod&quot;</p>
</li>
<li>
<p>Drop logs by resource attributes regex</p>
<p>processors:
filter/drop_logs_by_label_values_regex:
logs:
log_record:
- IsMatch(resource.attributes[&quot;k8s.pod.name&quot;], &quot;test-pod-.*&quot;)</p>
</li>
<li>
<p>Drop logs by logs attributes (like remote_addr, user_agent.name, etc.)</p>
<p>processors:
filter/drop_logs_by_label_values:
logs:
log_record:
- attributes[&quot;user_agent.name&quot;] == &quot;Safari&quot;</p>
</li>
<li>
<p>Drop logs by logs attributes regex</p>
<p>processors:
filter/drop_logs_by_label_values_regex:
logs:
log_record:
- IsMatch(attributes[&quot;http.method&quot;], &quot;GET|POST&quot;)</p>
</li>
</ol>
<p>Refer to the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor">OpenTelemetry documentation</a> for more details on how to configure the filter processor.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#introduction
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-strategy-introduction
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Azure Monitoring Strategy - Introduction</h2>
<p>This documentation guides you through an effective full-stack unified monitoring strategy for Azure using SigNoz and Azure Monitoring. By integrating SigNoz with Azure, you can achieve comprehensive observability of your applications and infrastructure, ensuring you have the insights needed to maintain optimal performance and reliability.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#overview-of-monitoring-modalities
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: https://signoz.io/img/docs/azure-monitoring/unified-collection.webp
tracking_id: docs-azure-monitoring-bootstrapping-strategy-overview-of-monitoring-modalities
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Overview of Monitoring Modalities</h2>
<p><img src="https://signoz.io/img/docs/azure-monitoring/unified-collection.webp" alt="Azure Monitoring Strategy" /></p>
<p>_</p>
<p>OpenTelemetry Unified Monitoring</p>
<p>_</p>
<h3>## Platform-Level Instrumentation</h3>
<ol>
<li>
<p><strong>System Logs</strong>: Logs generated by the Azure platform itself, providing insights into the underlying infrastructure's state and behavior. These logs can be exported and enriched with additional attributes for better context.</p>
<h5>##     Example:</h5>
<p>App Service's HTTP access logs, these are not generated by your application but by the Azure platform.</p>
<h5>##     Collected by:</h5>
<p>Platform Service streams to Event Hub and SigNoz's Central Collector pushes to SigNoz Cloud</p>
</li>
<li>
<p><strong>Infra Metrics</strong>: Metrics related to the infrastructure, such as CPU usage, memory consumption, and network traffic. These metrics are crucial for understanding the performance and health of your Azure resources.</p>
<h5>##     Example:</h5>
<p>App Service's CPU and memory usage metrics.</p>
<h5>##     Collected by:</h5>
<p>Platform Service streams to Azure Monitor and SigNoz's Central Collector scrapes and pushes to SigNoz Cloud</p>
</li>
<li>
<p><strong>Infra Attributes</strong>: Additional metadata about your infrastructure, such as resource names, locations, and configurations. These attributes help in correlating and contextualizing logs and metrics.</p>
<h5>##     Example:</h5>
<p>App Service's resource name and location attributes.</p>
<h5>##     Collected by:</h5>
<p>Central Collector collects and export these attributes..</p>
</li>
</ol>
<h3>## Application-Level Instrumentation</h3>
<ol>
<li>
<p><strong>App Logs</strong>: These logs are generated by your application and provide insights into application behavior, errors, and events. In an Azure context, these logs can be streamed to stdout and collected by Azure App Service, Function App, and Container App.</p>
<h5>##     Example:</h5>
<p><code>System.Diagnostics.TraceSource</code> in .NET, <code>logger.info()</code> in Java, <code>console.log()</code> in JavaScript.</p>
<h5>##     Collected by:</h5>
<p>Platform Service streams stdout to Event Hub (specific to the service) and SigNoz's Central Collector pushes to SigNoz Cloud.</p>
<p>Alternatively, for <strong>unmanaged services</strong> or <strong>non-stdout logging</strong>, you can use Otel SDK to send logs to Central Collector directly.</p>
</li>
<li>
<p><strong>App Metrics</strong>: These are metrics that your application emits, such as request queue length, request counts, error rates, and response times. These metrics can be collected using OpenTelemetry instrumentation SDKs.</p>
<h5>##     Example:</h5>
<p>Spring Actuator Metrics like <code>jvm.gc.pause.seconds_count</code>, <code>http.server.requests.total</code>, <code>process.cpu.usage</code></p>
<h5>##     Collected by:</h5>
<p>OpenTelemetry SDKs integrated with your application send metrics to Central Collector</p>
</li>
<li>
<p><strong>Traces</strong>: Traces provide a detailed view of the execution flow within your application. They help you understand the path and timing of requests as they traverse through various services and components.</p>
<h5>##     Collected by:</h5>
<p>OpenTelemetry SDKs integrated with your application send traces to Central Collector</p>
</li>
</ol>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#implementation-guide
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: https://signoz.io/img/docs/azure-monitoring/unified-implementation.webp
tracking_id: docs-azure-monitoring-bootstrapping-strategy-implementation-guide
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Implementation Guide</h2>
<p><img src="https://signoz.io/img/docs/azure-monitoring/unified-implementation.webp" alt="Azure Monitoring Strategy" /></p>
<p>_</p>
<p>Azure Unified Monitoring Strategy</p>
<p>_</p>
<h3>## Platform-Level Instrumentation</h3>
<p>Azure provides the capability to export system logs, infrastructure metrics, and attributes. These can be streamed and collected by Azure services such as App Service, Function App, and Container App.</p>
<p>Please refer to the <a href="../../">Azure Monitoring documentation</a> for detailed information on how to export logs and metrics to SigNoz for your particular service.</p>
<h3>## Application-Level Instrumentation</h3>
<p>For application metrics and traces, use the OpenTelemetry instrumentation SDK. For example, in a Node.js application, you can install the necessary packages using npm:</p>
<pre><code>npm install @opentelemetry/api
npm install @opentelemetry/auto-instrumentations-node
</code></pre>
<p>Please refer to our <a href="https://signoz.io/blog/">Blog</a> or <a href="../../../instrumentation/">SigNoz Tutorials</a> for detailed information on how to instrument your application like Spring, FastAPI, NextJS, Langchain, Node.js, Flask, Django, etc. with OpenTelemetry.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#next-steps
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-strategy-next-steps
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Next Steps</h2>
<p>Please complete the bootstrapping process to get started with monitoring your application running on Azure.</p>
<ul>
<li>
<p><a href="../data-ingestion">Event Hub Setup</a></p>
</li>
<li>
<p><a href="../collector-setup">Central Collector Setup</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/bootstrapping/strategy/#frequently-asked-questions-faq
tag_set: azure-monitoring, bootstrapping, strategy
image_urls: 
tracking_id: docs-azure-monitoring-bootstrapping-strategy-frequently-asked-questions-faq
group_tracking_ids: docs-azure-monitoring-bootstrapping-strategy
<h2>Azure Monitoring Strategy: Frequently Asked Questions (FAQ)</h2>
<h3>## How do I monitor traces in Azure?</h3>
<p>To monitor traces in Azure, you need to instrument your application using the OpenTelemetry SDK. Once instrumented, the traces can be collected and exported to a SigNoz Cloud for analysis.</p>
<ol>
<li><strong>Instrument Your Application</strong>: Use the OpenTelemetry SDK to add tracing to your application code.</li>
<li><strong>Configure Exporter</strong>: Set up the OpenTelemetry Collector to export traces to your <a href="../collector-setup">Central Collector</a> in SigNoz Cloud.</li>
</ol>
<p>By following these steps, you can achieve comprehensive trace monitoring in Azure, helping you to identify and resolve performance issues effectively.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/azure-monitoring/az-container-apps/metrics/
tag_set: azure-monitoring, az-container-apps, metrics
image_urls: https://signoz.io/img/docs/azure-monitoring/azure-container-app-metrics.webp
tracking_id: docs-azure-monitoring-az-container-apps-metrics
group_tracking_ids: docs-azure-monitoring-az-container-apps-metrics
<h2>Container App Metrics</h2>
<h2>QuickStart</h2>
<p>==========================</p>
<p>To monitor Azure Container App's system metrics like CPU Percentage, Memory Percentage, Replica Count with SigNoz, you just need to set up the OpenTelemetry Collector with the Azure Monitor exporter. No changes are needed to your application code.</p>
<h2>Overview</h2>
<p>======================</p>
<p>In this guide, you will learn how to monitor Azure Container App's system metrics like CPU Percentage, Memory Percentage, Data In, and Data Out with SigNoz. By monitoring these metrics, you can keep track of your application's resource utilization and performance.</p>
<p>For application-level traces and metrics, you can use the DNS name of the OpenTelemetry Collector you set up earlier. Simply configure your application to send traces and metrics to the Central Collector, and they will be forwarded to SigNoz automatically.</p>
<h2>Prerequisites</h2>
<p>================================</p>
<p>Before you can monitor your Azure Container App with SigNoz, you need to ensure the following prerequisites are met:</p>
<ol>
<li>You have an Azure subscription and an Azure Container App instance running.</li>
<li>You have set up the Central Collector with the Azure Monitor exporter. If you haven't set it up yet, follow the instructions in the¬†<a href="../../bootstrapping/collector-setup">Central Collector Setup</a></li>
</ol>
<h2>Dashboard Example</h2>
<p>========================================</p>
<p>Once you have completed the prerequisites, you can start monitoring your Azure Container App's system metrics with SigNoz. Here's how you can do it:</p>
<ol>
<li>Log in to your SigNoz account.</li>
<li>Navigate to the Dashboards, and add an dashboard</li>
<li>Add a Timeseries Panel</li>
<li>In <em>Metrics</em>, select <code>azure_replicas_count</code> and <em>Avg By</em> select tag <code>name</code></li>
<li>In Filter say <code>type = Microsoft.App/containerApps</code></li>
<li>Hit ‚ÄúSave Changes‚Äù You now have Memory Usage of your Container App in a Dashboard for reporting and alerting</li>
</ol>
<p><img src="https://signoz.io/img/docs/azure-monitoring/azure-container-app-metrics.webp" alt="Time Series Panel for Container Replica Count" /></p>
<p>_</p>
<p>Time Series Panel for Container Replica Count</p>
<p>_</p>
<p>That's it! You have successfully set up monitoring for your Azure Container App's system metrics with SigNoz.</p>
<p>Note: You don't need to make any changes to your application code to monitor the system metrics. The OpenTelemetry Collector with the Azure Monitor exporter takes care of collecting and sending the metrics to SigNoz.</p>
<h2>Troubleshooting</h2>
<p>====================================</p>
<p>If you encounter any issues while setting up monitoring for your Azure Container App's system metrics with SigNoz, here are a few troubleshooting steps you can try:</p>
<ol>
<li>Check if the OpenTelemetry Collector is running and properly configured with the Azure Monitor exporter.</li>
<li>Verify that your Azure Container App instance is running and accessible.</li>
<li>Ensure that you have the necessary permissions to access the metrics in your Azure subscription.</li>
</ol>
<p>By following this guide, you should be able to easily monitor your Azure Container App's system metrics with SigNoz and gain valuable insights into your application's performance and resource utilization.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#overview
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-overview
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<h2>Collecting Tomcat Access and Garbage Collector Logs: Collecting Tomcat Access and Garbage Collector Logs - Overview</h2>
<p>This documentation provides detailed instructions about configuring the OpenTelemetry Collector to read Tomcat Server Access and Garbage Collector logs and push them to SigNoz, enabling you to analyze them effectively.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#sample-log
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-sample-log
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<h2>Collecting Tomcat Access and Garbage Collector Logs: Sample Log</h2>
<p>Here is how the Tomcat Access logs and Garbage Collector logs look like:</p>
<h3>## Sample Access Log</h3>
<pre><code>0:0:0:0:0:0:0:1 - - [18/Apr/2024:13:45:29 +0530] &quot;GET /demo1/add?num1=1&amp;num2=2 HTTP/1.1&quot; 200 11
0:0:0:0:0:0:0:1 - - [18/Apr/2024:13:45:30 +0530] &quot;GET /demo1/add?num1=2&amp;num2=3 HTTP/1.1&quot; 200 11
</code></pre>
<h3>## Sample Garbage Collector log</h3>
<pre><code>[0.724s][info][gc] GC(3) Concurrent Mark Cycle 6.218ms
[0.772s][info][gc] GC(4) Pause Young (Prepare Mixed) (G1 Preventive Collection) 28M-&gt;8M(40M) 1.891ms
[591.215s][info][gc] GC(5) Pause Young (Mixed) (G1 Evacuation Pause) 10M-&gt;8M(40M) 8.173ms
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#collect-logs-in-signoz-cloud
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: https://signoz.io/img/docs/logs-management/send-logs/tomcat_access_logs.webp, https://signoz.io/img/docs/logs-management/send-logs/tomcat_gc_logs.webp
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-collect-logs-in-signoz-cloud
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<h2>Collecting Tomcat Access and Garbage Collector Logs: Collect Logs in SigNoz Cloud</h2>
<h3>## Prerequisite</h3>
<ul>
<li>SigNoz <a href="https://signoz.io/teams/">cloud</a> account</li>
</ul>
<p>Sending logs to SigNoz cloud can be achieved by following these simple steps:</p>
<ul>
<li>Installing OpenTelemetry Collector</li>
<li>Configuring filelog receiver</li>
</ul>
<h3>## Install OpenTelemetry Collector</h3>
<p>The OpenTelemetry collector provides a vendor-neutral way to collect, process, and export your telemetry data such as logs, metrics, and traces.</p>
<p>You can install OpenTelemetry collector (OTel collector) as an agent by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">guide</a>.</p>
<h3>## Configure Filelog receiver</h3>
<p>Modify the <code>config.yaml</code> file that you created while installing OTel collector in the previous step to include the filelog receiver. This involves specifying the path to your access and garbage collector logs and setting the <code>start_at</code> parameter, which specifies where to start reading logs from the log file. For more fields that are available for filelog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">this link</a>.</p>
<pre><code>receivers:
  ...
  filelog/access_logs:
    include: [ /&lt;path_to_access_log&gt;/localhost_access_log.*] #include the path to your access logs
    start_at: end
  filelog/gc_logs:
    include: [ /&lt;path_to_garbage_collector_log&gt;/garbage-collection.log.*] #include the path to your garbage collector logs
    start_at: end
...
</code></pre>
<p>:::note</p>
<p>The <code>start_at: end</code> configuration ensures that only newly added logs are transmitted. If you wish to include historical logs from the file, remember to modify <code>start_at</code> to <code>beginning</code>.</p>
<p>:::</p>
<p>:::note If you want to change the path of where your access logs are stored you can change it by adding the following in your server arguments <code>-Dcatalina.base=&lt;new_access_logs_path&gt;</code></p>
<p>If you want to change the path of where your garbage collector logs are stored you can change it by <code>-Xloggc:&lt;new_garbage_collection_logs_path&gt;</code> :::</p>
<h3>## Update Pipelines Configuration</h3>
<p>In the same <code>config.yaml</code> file, update the pipeline settings to include the new filelog receiver. This step is crucial for ensuring that the logs are correctly processed and sent to SigNoz.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, filelog/access_logs, filelog/gc_logs]
        processors: [batch]
        exporters: [otlp]
</code></pre>
<p>Now restart the OTel collector so that new changes are applied. The steps to run the OTel collector can be found <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">here</a></p>
<h3>## Verify Export</h3>
<p>The logs will be exported to SigNoz and will be visible in SigNoz UI.</p>
<p><img src="https://signoz.io/img/docs/logs-management/send-logs/tomcat_access_logs.webp" alt="access logs visible in SigNoz" /></p>
<p><em>Sample tomcat access logs data shown in SigNoz Logs Explorer</em></p>
<p><img src="https://signoz.io/img/docs/logs-management/send-logs/tomcat_gc_logs.webp" alt="garbage logs visible in SigNoz" /></p>
<p><em>Sample tomcat garbage collector logs data shown in SigNoz Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#collect-logs-in-self-hosted-signoz
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-collect-logs-in-self-hosted-signoz
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<h2>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Collect logs in self-hosted SigNoz</h2>
<p>Collecting Tomcat Access and Garbage Collector logs in Self-Hosted SigNoz can have two scenarios:</p>
<ul>
<li>SigNoz running on the same host</li>
<li>SigNoz running on different host</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#running-on-the-same-host
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-running-on-the-same-host
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<p>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Running on the same host</p>
<p>If your self-hosted SigNoz is running on the same host, then you can follow these steps to collect your application logs.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#install-signoz
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-install-signoz
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<p>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Install SigNoz</p>
<p>You can install Self-Hosted SigNoz using the instructions <a href="https://signoz.io/docs/install/docker/">here</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#modify-docker-compose-file
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-modify-docker-compose-file
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<p>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Modify Docker Compose file</p>
<p>In your self-hosted SigNoz setup, locate and edit the <code>docker-compose.yaml</code> file found in the <code>deploy/docker/clickhouse-setup</code> directory. You'll need to mount the log file of your application to the <code>tmp</code> directory of SigNoz OTel collector.</p>
<pre><code>  ...
  otel-collector:
  image: signoz/signoz-otel-collector:0.88.11
  command: [&quot;--config=/etc/otel-collector-config.yaml&quot;]
  volumes:
    - ~/&lt;access_logs_path&gt;/:/&lt;access_logs_path&gt;/
    - ~/&lt;garbage_collector_logs_path&gt;/:/&lt;garbage_collector_logs_path&gt;/
  ....
</code></pre>
<p>Replace <code>&lt;access_logs_path&gt;</code> , <code>&lt;garbage_collector_logs_path&gt;</code> with the path where your access log and garbage collector log files are present. Please ensure that the file path is correctly specified.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#add-filelog-receiver
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-add-filelog-receiver
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<p>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Add Filelog receiver</p>
<p>Add the filelog reciever to <code>otel-collector-config.yaml</code> which is present inside <code>deploy/docker/clickhouse-setup</code> directory in your self-hosted SigNoz setup. The configuration below tells the collector where to find your log file and how to start processing it.</p>
<pre><code>receivers:
  ...
  filelog/access_logs:
    include: [ /&lt;path&gt;/localhost_access_log.*] #include the path to your access logs
    start_at: end
  filelog/gc_logs:
    include: [ /&lt;path&gt;/garbage-collection.log.*] #include the path to your garbage collector logs
    start_at: end
...
</code></pre>
<p>:::note</p>
<p>The <code>start_at: end</code> configuration ensures that only newly added logs are transmitted. If you wish to include historical logs from the file, remember to modify <code>start_at</code> to <code>beginning</code>.</p>
<p>:::</p>
<p>:::note If you want to change the path of where your access logs are stored you can change it by adding the following in your server arguments <code>-Dcatalina.base=&lt;new_access_logs_path&gt;</code></p>
<p>If you want to change the path of where your garbage collector logs are stored you can change it by <code>-Xloggc:&lt;new_garbage_collection_logs_path&gt;</code> :::</p>
<p>For more fields that are available for filelog receiver please check <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">this link</a>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#update-pipeline-configuration
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-update-pipeline-configuration
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<p>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Update Pipeline configuration</p>
<p>Modify the pipeline inside <code>otel-collector-config.yaml</code> to include the filelog receiver. This step is crucial for ensuring that the logs are correctly processed and sent to SigNoz.</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, filelog/access_logs, filelog/gc_logs]
        processors: [batch]
        exporters: [clickhouselogsexporter]
</code></pre>
<p>Now, restart the OTel collector so that new changes are applied. You can find instructions to run OTel collector <a href="https://signoz.io/docs/install/docker/">here</a></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#verify-export-1
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: https://signoz.io/img/docs/logs-management/send-logs/tomcat_access_logs.webp, https://signoz.io/img/docs/logs-management/send-logs/tomcat_gc_logs.webp
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-verify-export-1
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<p>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Verify Export</p>
<p>The logs will be exported to SigNoz UI if there are no errors.</p>
<p><img src="https://signoz.io/img/docs/logs-management/send-logs/tomcat_access_logs.webp" alt="Access logs visible in SigNoz" /></p>
<p><em>Sample tomcat access logs data shown in SigNoz Logs Explorer</em></p>
<p><img src="https://signoz.io/img/docs/logs-management/send-logs/tomcat_gc_logs.webp" alt="garbage collector logs visible in SigNoz" /></p>
<p><em>Sample tomcat garbage collector logs data shown in SigNoz Logs Explorer</em></p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#running-on-a-different-host
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-running-on-a-different-host
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<p>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Running on a different host</p>
<p>If you have a SigNoz running on a different host then you will have to run a OTel collector to export logs from your host to the host where SigNoz is running.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/send-logs/collect-tomcat-access-and-garbage-collector-logs/#create-otel-collector-configuration
tag_set: logs-management, send-logs, collect-tomcat-access-and-garbage-collector-logs
image_urls: 
tracking_id: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs-create-otel-collector-configuration
group_tracking_ids: docs-logs-management-send-logs-collect-tomcat-access-and-garbage-collector-logs
<p>Collecting Tomcat Access and Garbage Collector Logs: Collect logs in self-hosted SigNoz: Create OTel collector configuration</p>
<p>You need to create an <code>otel-collector-config.yaml</code> file, this file defines how the OTel collector will process and forward logs to your SigNoz instance.</p>
<pre><code>receivers:
  filelog/access_logs:
    include: [ /&lt;path&gt;/localhost_access_log.*] #include the path to your access logs
    start_at: end
  filelog/gc_logs:
    include: [ /&lt;path&gt;/garbage-collection.log.*] #include the path to your garbage collector logs
    start_at: end
processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s
exporters:
  otlp/log:
    endpoint: http://&lt;host&gt;:&lt;port&gt;
    tls:
      insecure: true
service:
  pipelines:
    logs:
      receivers: [filelog/access_logs, filelog/gc_logs]
      processors: [batch]
      exporters: [ otlp/log ]
</code></pre>
<p>The parsed logs are batched up using the batch processor and then exported to the host where SigNoz is deployed. For finding the right host and port for your SigNoz cluster please follow the guide <a href="../../install/troubleshooting.md#signoz-otel-collector-address-grid">here</a>.</p>
<p>:::note</p>
<p>The <code>otlp/log</code> exporter in the above configuration file uses a <code>http</code> endpoint but if you want to use <code>https</code> you will have to provide the certificate and the key. You can read more about it <a href="https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/otlpexporter/README.md">here</a></p>
<p>:::</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/fluentbit_to_signoz/
tag_set: userguide, fluentbit_to_signoz
image_urls: 
tracking_id: docs-userguide-fluentbit_to_signoz
group_tracking_ids: docs-userguide-fluentbit_to_signoz
<h2>FluentBit to SigNoz</h2>
<p>If you use fluentBit to collect logs in your stack with this tutotrial you will be able to send logs from fluentBit to SigNoz.</p>
<p>At SigNoz we use opentelemetry collector to recieve logs which supports the fluentforward protocol. So you can forward your logs from your fluentBit agent to opentelemetry collector using fluentforward protocol.</p>
<h3>## Collect Logs Using FluentBit in SigNoz cloud</h3>
<ul>
<li>
<p>Add otel collector binary to your VM by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">guide</a>
.</p>
</li>
<li>
<p>Add fluentforward reciever to your <code>config.yaml</code></p>
<pre><code>receivers:
  fluentforward:
    endpoint: 0.0.0.0:24224
</code></pre>
<p>Here we have used port 24224 for listening in fluentforward protocol, but you can change it to a port you want. You can read more about fluentforward receiver <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/fluentforwardreceiver">here</a>
.</p>
</li>
<li>
<p>Modify your <code>config.yaml</code> and add the above receiver</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, fluentforward]
        processors: [batch]
        exporters: [otlp]
</code></pre>
</li>
<li>
<p>Add the following to your fluentBit config to forward the logs to otel collector.</p>
<pre><code>[OUTPUT]
  Name          forward
  Match         *
  Host          localhost
  Port          24224
</code></pre>
</li>
</ul>
<p>In this config we are forwarding the logs to the otel collector which is listening on port 24224. Also we are assuming that you are running the fluentBit binary on the host. If not, the value of <code>host</code> might change depending on your environment.</p>
<ul>
<li>Once you make this changes you can restart fluentBit and otel-binary, and you will be able to see the logs in SigNoz.</li>
<li>To properly transform your existing log model into opentelemetry <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md">log</a> model you can use the different processors provided by opentelemetry. <a href="/docs/userguide/logs/#processors-available-for-processing-logs">link</a></li>
</ul>
<p>eg:-</p>
<pre><code>processors:
  logstransform:
    operators:
      - type: trace_parser
        trace_id:
          parse_from: attributes.trace_id
        span_id:
          parse_from: attributes.span_id
      - type: remove
        field: attributes.trace_id
      - type: remove
        field: attributes.span_id
</code></pre>
<p>The operations in the above processor will parse the trace_id and span_id from log to opentelemetry log model and remove them from attributes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/fluentbit_to_signoz/#collect-logs-using-fluentbit-in-self-hosted-signoz
tag_set: userguide, fluentbit_to_signoz
image_urls: 
tracking_id: docs-userguide-fluentbit_to_signoz-collect-logs-using-fluentbit-in-self-hosted-signoz
group_tracking_ids: docs-userguide-fluentbit_to_signoz
<h2>FluentBit to SigNoz: Collect Logs Using FluentBit in Self-Hosted SigNoz</h2>
<ul>
<li>
<p>Add fluentforward reciever to your <code>otel-collector-config.yaml</code> which is present inside <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>receivers:
  fluentforward:
    endpoint: 0.0.0.0:24224
</code></pre>
<p>Here we have used port 24224 for listening in fluentforward protocol, but you can change it to a port you want. You can read more about fluentforward receiver <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/fluentforwardreceiver">here</a>
.</p>
</li>
<li>
<p>Update the pipleline for logs by making the following change in <code>otel-collector-config.yaml</code></p>
<pre><code>service:
  ...

  logs:
    receivers: [ otlp, fluentforward ]
    processors: [ batch ]
    exporters: [  clickhouselogsexporter ]
</code></pre>
<p>Here we are updating the logs pipeline which will collect logs from <code>fluentforward</code> and <code>otlp</code> receiver, processing it using batch processor and export it to clickhouse.</p>
</li>
<li>
<p>Expose the port in port for otel-collector in <code>docker-compose.yaml</code> file present in <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>otel-collector:
  ...
  ports:
    - &quot;24224:24224&quot;
</code></pre>
</li>
<li>
<p>Change the fluentBit config to forward the logs to otel collector.</p>
<pre><code>[INPUT]
  Name   dummy
  Tag    dummy.log
  Dummy {&quot;message&quot;: &quot;mylog&quot;, &quot;trace_id&quot;: &quot;0000000000000000f4dbb3edd765f620&quot;, &quot;span_id&quot;: &quot;43222c2d51a7abe3&quot;}

[OUTPUT]
  Name          forward
  Match         *
  Host          &lt;otel-collector-host&gt;
  Port          24224
</code></pre>
<p>In this example we are generating sample logs and then forwarding them to the otel collector which is listening on port 24224. <code>&lt;otel-collector-host&gt;</code> has to be replaced by the host where otel-collector is running. For more info check <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">troubleshooting</a>
.</p>
</li>
<li>
<p>Once you make this changes you can restart fluentBit and SignNoz, and you will be able to see the logs in SigNoz.</p>
</li>
<li>
<p>To properly transform your existing log model into opentelemetry <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md">log</a> model you can use the different processors provided by opentelemetry. <a href="/docs/userguide/logs/#processors-available-for-processing-logs">link</a></p>
<p>eg:-</p>
<pre><code>processors:
  logstransform:
    operators:
      - type: trace_parser
        trace_id:
          parse_from: attributes.trace_id
        span_id:
          parse_from: attributes.span_id
      - type: remove
        field: attributes.trace_id
      - type: remove
        field: attributes.span_id
</code></pre>
<p>The operations in the above processor will parse the trace_id and span_id from log to opentelemetry log model and remove them from attributes.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/overview/
tag_set: logs-management, logs-api, overview
image_urls: 
tracking_id: docs-logs-management-logs-api-overview
group_tracking_ids: docs-logs-management-logs-api-overview
<h2>Logs API</h2>
<h2>Overview</h2>
<hr />
<p>The SigNoz Logs API is a robust interface which enables developers to manage and analyze log data efficiently. This API facilitates various operations:</p>
<ul>
<li>
<p><strong>Searching Logs:</strong> Allows users to search through log data based on specific criteria. For example, finding all logs where the error code is 500.</p>
</li>
<li>
<p><strong>Paginating Logs:</strong> Helps in navigating through large sets of log data in a manageable way. For instance, retrieving logs in batches of 100 for easy viewing.</p>
</li>
<li>
<p><strong>Aggregating Logs:</strong> Enables summarizing log data to extract meaningful insights, such as aggregating logs to count the number of errors per day.</p>
</li>
</ul>
<h2>API Endpoint</h2>
<hr />
<p>Endpoint for Logs API:</p>
<p><code>POST</code> <code>https://{URL}/api/v3/query_range</code></p>
<p>Replace <code>{URL}</code> with your instance URL, e.g., example.signoz.io.</p>
<h2>Prerequisites</h2>
<hr />
<p>Access Token: To access this API, you need an Access Token. Navigate to the <code>Settings</code> page in the SigNoz UI and create a new Access Token.</p>
<p><img src="https://github.com/SigNoz/signoz/assets/9512100/9ca6c788-b60d-4051-8c14-c260a31bdb7d" alt="image" /></p>
<p>Access Tokens can only be created/managed by users with the <code>Admin</code> role. If you don't have the <code>Admin</code> role, contact your organization's admin to create an Access Token for you.</p>
<h2>Authentication</h2>
<hr />
<p>Using the Access Token: Add the Access token to your request header as follows:</p>
<pre><code>SIGNOZ-API-KEY:{YOUR_ACCESS_TOKEN}
</code></pre>
<p>üí° Tip</p>
<p>Secure storage and handling of your Access Token is crucial to prevent unauthorized access.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/logs-url-for-explorer-page/
tag_set: logs-management, logs-api, logs-url-for-explorer-page
image_urls: 
tracking_id: docs-logs-management-logs-api-logs-url-for-explorer-page
group_tracking_ids: docs-logs-management-logs-api-logs-url-for-explorer-page
<h2>Create Logs URL for Explorer page</h2>
<p>This section explains how to generate a URL for the SigNoz Explorer page. This URL allows users to directly access the Explorer page with predefined filters and a custom time range, facilitating quick and efficient log analysis.</p>
<h3>## Params for URL</h3>
<p>The generation of the final URL involves appending certain parameters to the base URL of the Explorer page. The base URL route for explorer page looks like:</p>
<pre><code>/logs-explorer?
</code></pre>
<p>The parameters includes:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
<th>URI encoded</th>
</tr>
</thead>
<tbody>
<tr>
<td>panelTypes</td>
<td>&quot;list&quot;/&quot;graph&quot;/&quot;table&quot;</td>
<td>once</td>
</tr>
<tr>
<td>compositeQuery</td>
<td>It's a JSON structure consisting of <code>builderQueries</code></td>
<td>twice</td>
</tr>
<tr>
<td>startTime</td>
<td>Timestamp start in ms</td>
<td>no</td>
</tr>
<tr>
<td>endTime</td>
<td>Timestamp end in ms</td>
<td>no</td>
</tr>
</tbody>
</table>
<p>These parameters have to be encoded before being appended to the base URL. The <code>URI encoded</code> column describes the <strong>number of times</strong> the parameter has to be URI encoded. <code>no</code>indicates that these parameters don't have to be encoded.</p>
<p>You can use this <a href="https://meyerweb.com/eric/tools/dencoder/">tool</a> to encode your parameters.</p>
<h3>## Example of Composite Query</h3>
<p><code>compositeQuery</code> is one of the parameter that is used to create the final URL. Here's an example composite query that is configured to fetch logs from a specified data source where the <code>container_name</code> equals &quot;hotrod&quot;:</p>
<pre><code>{
  &quot;queryType&quot;: &quot;builder&quot;,
  &quot;builder&quot;: {
    &quot;queryData&quot;: [\
      {\
        &quot;dataSource&quot;: &quot;logs&quot;,\
        &quot;queryName&quot;: &quot;A&quot;,\
        &quot;aggregateOperator&quot;: &quot;noop&quot;,\
        &quot;aggregateAttribute&quot;: {\
        },\
        &quot;filters&quot;: {\
          &quot;items&quot;: [\
            {\
              &quot;id&quot;: &quot;81c40cb5&quot;,\
              &quot;key&quot;: {\
                &quot;key&quot;: &quot;container_name&quot;,\
                &quot;dataType&quot;: &quot;string&quot;,\
                &quot;type&quot;: &quot;tag&quot;,\
                &quot;isColumn&quot;: false,\
                &quot;isJSON&quot;: false,\
                &quot;id&quot;: &quot;container_name--string--tag--false&quot;\
              },\
              &quot;op&quot;: &quot;=&quot;,\
              &quot;value&quot;: &quot;hotrod&quot;\
            }\
          ],\
          &quot;op&quot;: &quot;AND&quot;\
        },\
        &quot;expression&quot;: &quot;A&quot;,\
        &quot;disabled&quot;: false,\
        &quot;having&quot;: [],\
        &quot;stepInterval&quot;: 240,\
        &quot;limit&quot;: null,\
        &quot;orderBy&quot;: [\
          {\
            &quot;columnName&quot;: &quot;timestamp&quot;,\
            &quot;order&quot;: &quot;desc&quot;\
          }\
        ],\
        &quot;groupBy&quot;: [],\
        &quot;legend&quot;: &quot;&quot;,\
        &quot;reduceTo&quot;: &quot;sum&quot;\
      }\
    ],
    &quot;queryFormulas&quot;: []
  },
  &quot;id&quot;: &quot;af9df71b-b6eb-48e5-b889-f4d0946c6eaa&quot;
}
</code></pre>
<p>In the above query, the <code>builderQueries</code> map is structured as an array within the <code>queryData</code> field to align with the frontend's state management format.</p>
<h3>## Generating the URL</h3>
<p>Once you have all the parameters defined and encoded as mentioned in the <a href="#params-for-url">Param for URL</a> section, we can append them to the base URL of the SigNoz Explorer page.</p>
<p>This creates a direct link that pre-loads the Explorer page with the specified query parameters.</p>
<h4>## Sample Final URL</h4>
<p>After appending the encoded parameters to the base URL, this is how a complete URL would look like:</p>
<pre><code>https://[Your-SigNoz-Domain]/logs-explorer?startTime=[start-time]&amp;endTime=[end-time]&amp;panelTypes=[encoded-panelTypes]&amp;compositeQuery=[encoded-compositeQuery]
</code></pre>
<p>The complete URL when visited, will open the Explorer page with the log query and time range already set up as per the parameters. This feature is particularly useful for bookmarking specific log views or sharing them with others.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/fluentd_to_signoz/
tag_set: userguide, fluentd_to_signoz
image_urls: 
tracking_id: docs-userguide-fluentd_to_signoz
group_tracking_ids: docs-userguide-fluentd_to_signoz
<h2>FluentD to SigNoz</h2>
<p>If you use fluentD to collect logs in your stack with this tutotrial you will be able to send logs from fluentD to SigNoz.</p>
<p>At SigNoz we use opentelemetry collector to recieve logs which supports the fluentforward protocol. So you can forward your logs from your fluentD agent to opentelemetry collector.</p>
<h3>## Collect Logs Using FluentD in SigNoz cloud</h3>
<ul>
<li>
<p>Add otel collector binary to your VM by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">guide</a>
.</p>
</li>
<li>
<p>Add fluentforward reciever to your <code>config.yaml</code></p>
<pre><code>receivers:
  fluentforward:
    endpoint: 0.0.0.0:24224
</code></pre>
<p>Here we have used port 24224 for listening in fluentforward protocol, but you can change it to a port you want. You can read more about fluentforward receiver <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/fluentforwardreceiver">here</a>
.</p>
</li>
<li>
<p>Modify your <code>config.yaml</code> and add the above receiver</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, fluentforward]
        processors: [batch]
        exporters: [otlp]
</code></pre>
</li>
<li>
<p>Add the following to your fluentD config to forward the logs to otel collector.</p>
<pre><code>&lt;match &lt;directive&gt;&gt;
  @type forward
  send_timeout 60s
  recover_wait 10s
  hard_timeout 60s

  &lt;server&gt;
    name myserver1
    host localhost
    port 24224
  &lt;/server&gt;
&lt;/match&gt;
</code></pre>
<p>In this config we are matching a directive and forwarding logs to the otel collector which is listening on port 24224. Replace <code>&lt;directive&gt;</code> with your directive name. Also we are assuming that you are running the fluentD binary on the host. If not, the value of <code>host</code> might change depending on your environment.</p>
</li>
<li>
<p>Once you make this changes you can restart fluentD and otel-binary, and you will be able to see the logs in SigNoz.</p>
</li>
<li>
<p>To properly transform your existing log model into opentelemetry <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md">log</a> model you can use the different processors provided by opentelemetry. <a href="/docs/userguide/logs/#processors-available-for-processing-logs">link</a></p>
</li>
</ul>
<p>eg:-</p>
<pre><code>processors:
  logstransform:
    operators:
      - type: trace_parser
        trace_id:
          parse_from: attributes.trace_id
        span_id:
          parse_from: attributes.span_id
      - type: remove
        field: attributes.trace_id
      - type: remove
        field: attributes.span_id
</code></pre>
<p>The operations in the above processor will parse the trace_id and span_id from log to opentelemetry log model and remove them from attributes.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/fluentd_to_signoz/#collect-logs-using-fluentd-in-self-hosted-signoz
tag_set: userguide, fluentd_to_signoz
image_urls: 
tracking_id: docs-userguide-fluentd_to_signoz-collect-logs-using-fluentd-in-self-hosted-signoz
group_tracking_ids: docs-userguide-fluentd_to_signoz
<h2>FluentD to SigNoz: Collect Logs Using FluentD in Self-Hosted SigNoz</h2>
<h3>## Steps to recieve logs from FluentD:</h3>
<ul>
<li>
<p>Add fluentforward reciever to your <code>otel-collector-config.yaml</code> which is present inside <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>receivers:
  fluentforward:
    endpoint: 0.0.0.0:24224
</code></pre>
<p>Here we have used port 24224 for listening in fluentforward protocol, but you can change it to a port you want. You can read more about fluentforward receiver <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/fluentforwardreceiver">here</a>
.</p>
</li>
<li>
<p>Uncomment the exporter and pipleline for logs and make the following change in <code>otel-collector-config.yaml</code></p>
<pre><code>exporters:
  ...
  
  clickhouselogsexporter:
  dsn: tcp://clickhouse:9000/
  timeout: 5s
  sending_queue:
    queue_size: 100
  retry_on_failure:
    enabled: true
    initial_interval: 5s
    max_interval: 30s
    max_elapsed_time: 300s
  
  ...

service:
  ...

  logs:
    receivers: [ otlp, fluentforward ]
    processors: [ batch ]
    exporters: [  clickhouselogsexporter ]
</code></pre>
<p>Here we are adding our clickhouse exporter and creating a pipeline which will collect logs from <code>fluentforward</code> receiver, processing it using batch processor and export it to clickhouse.</p>
</li>
<li>
<p>Expose the port in port for otel-collector in <code>docker-compose.yaml</code> file present in <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>otel-collector:
  ...
  ports:
    - &quot;24224:24224&quot;
</code></pre>
</li>
<li>
<p>Change the fluentD config to forward the logs to otel collector.</p>
<pre><code>&lt;source&gt;
  @type sample
  sample [{&quot;message&quot;: &quot;my log data&quot;, &quot;source&quot;: &quot;myhost&quot;}, {&quot;message&quot;: &quot;my log data 1&quot;, &quot;source&quot;: &quot;myhost1&quot;}]
  tag sample
  rate 10000
&lt;/source&gt;

&lt;match sample&gt;
  @type forward
  send_timeout 60s
  recover_wait 10s
  hard_timeout 60s

  &lt;server&gt;
    name myserver1
    host &lt;otel-collector-host&gt;
    port 24224
  &lt;/server&gt;
&lt;/match&gt;
</code></pre>
<p>In this example we are generating sample logs and then forwarding them to the otel collector which is listening on port 24224. <code>&lt;otel-collector-host&gt;</code> has to be replaced by the host where otel-collector is running. For more info check <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">troubleshooting</a>
.</p>
</li>
<li>
<p>Once you make this changes you can restart fluentD and SignNoz, and you will be able to see the logs in SigNoz.</p>
</li>
<li>
<p>To properly transform your existing log model into opentelemetry <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md">log</a> model you can use the different processors provided by opentelemetry. <a href="/docs/userguide/logs/#processors-available-for-processing-logs">link</a></p>
<p>eg:-</p>
<pre><code>processors:
  logstransform:
    operators:
      - type: trace_parser
        trace_id:
          parse_from: attributes.trace_id
        span_id:
          parse_from: attributes.span_id
      - type: remove
        field: attributes.trace_id
      - type: remove
        field: attributes.span_id
</code></pre>
<p>The operations in the above processor will parse the trace_id and span_id from log to opentelemetry log model and remove them from attributes.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/payload-model/
tag_set: logs-management, logs-api, payload-model
image_urls: 
tracking_id: docs-logs-management-logs-api-payload-model
group_tracking_ids: docs-logs-management-logs-api-payload-model
<h2>Logs API Payload Model</h2>
<p>The SigNoz Logs API uses a JSON payload for queries, which includes various fields and nested fields. This document provides a detailed explanation of each field to help users construct effective queries.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/payload-model/#top-level
tag_set: logs-management, logs-api, payload-model
image_urls: 
tracking_id: docs-logs-management-logs-api-payload-model-top-level
group_tracking_ids: docs-logs-management-logs-api-payload-model
<p>Logs API Payload Model: : Top-level</p>
<p>The top-level of the payload model has the following fields:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>start</td>
<td>Epoch timestamp marking the start of the query range (in milliseconds or nanoseconds)</td>
</tr>
<tr>
<td>end</td>
<td>Epoch timestamp marking the end of the query range (in milliseconds or nanoseconds)</td>
</tr>
<tr>
<td>step</td>
<td>Aggregation interval for the query, specified in seconds</td>
</tr>
<tr>
<td>compositeQuery</td>
<td>This contains the <a href="#composite-query"><strong>compositeQuery</strong></a>&lt;br&gt; which is explained below</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/payload-model/#composite-query
tag_set: logs-management, logs-api, payload-model
image_urls: 
tracking_id: docs-logs-management-logs-api-payload-model-composite-query
group_tracking_ids: docs-logs-management-logs-api-payload-model
<p>Logs API Payload Model: : Composite Query</p>
<p>The <code>compositeQuery</code> field consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>queryType</td>
<td>Type of query (e.g., builder, clickhouse, prometheus).Scope of this documentation is limited to <strong>builder</strong> type</td>
</tr>
<tr>
<td>panelType</td>
<td>Type of panel (e.g., list, graph, table)</td>
</tr>
<tr>
<td>offset</td>
<td>Offset used in pagination</td>
</tr>
<tr>
<td>pageSize</td>
<td>Number of items to fetch, used in list view</td>
</tr>
<tr>
<td>limit</td>
<td>For list view: - Maximum number of items to be paginate, i.e., offset + pageSize cannot exceed limit, For aggregation:- limit on the results</td>
</tr>
<tr>
<td>builderQueries</td>
<td>Map of <a href="#builder-query"><strong>builderQuery</strong></a></td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/payload-model/#builder-query
tag_set: logs-management, logs-api, payload-model
image_urls: 
tracking_id: docs-logs-management-logs-api-payload-model-builder-query
group_tracking_ids: docs-logs-management-logs-api-payload-model
<p>Logs API Payload Model: : Builder Query</p>
<p>A <code>builderQuery</code> consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>stepInterval</td>
<td>Aggregation interval for query in seconds</td>
</tr>
<tr>
<td>queryName</td>
<td>Name of the query, should match the key to this map value</td>
</tr>
<tr>
<td>dataSource</td>
<td>Source of data, e.g., logs</td>
</tr>
<tr>
<td>aggregateOperator</td>
<td>Type of aggregation - noop, count, count_distinct, sum, avg, min, max, p05, p10, p20, p25, p50, p75, p90, p95, p99, rate, sum_rate, avg_rate, min_rate, max_rate, rate_sum, rate_avg, rate_min, rate_max</td>
</tr>
<tr>
<td>aggregateAttribute</td>
<td>The <a href="#attribute"><strong>attribute</strong></a>&lt;br&gt; against which the aggregateOperator is applied</td>
</tr>
<tr>
<td>filters</td>
<td>Array of <a href="#filter"><strong>filter</strong></a>&lt;br&gt; used for filtering data</td>
</tr>
<tr>
<td>groupBy</td>
<td>Array of <a href="#attribute"><strong>attribute</strong></a>&lt;br&gt; used for groupBy</td>
</tr>
<tr>
<td>expression</td>
<td>Will be same as query name but different in case of formulas</td>
</tr>
<tr>
<td>disabled</td>
<td>Specifies if the query is disabled</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/payload-model/#filter
tag_set: logs-management, logs-api, payload-model
image_urls: 
tracking_id: docs-logs-management-logs-api-payload-model-filter
group_tracking_ids: docs-logs-management-logs-api-payload-model
<p>Logs API Payload Model: : Filter</p>
<p>A <code>filter</code> consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>items</td>
<td>Array of <a href="#filter-item"><strong>filterItem</strong></a></td>
</tr>
<tr>
<td>op</td>
<td>Operator defining how filter items are joined (e.g., AND).</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/payload-model/#filter-item
tag_set: logs-management, logs-api, payload-model
image_urls: 
tracking_id: docs-logs-management-logs-api-payload-model-filter-item
group_tracking_ids: docs-logs-management-logs-api-payload-model
<p>Logs API Payload Model: : Filter Item</p>
<p>The <code>filterItem</code> includes:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>key</td>
<td>Corresponding <a href="#attribute"><strong>attribute</strong></a></td>
</tr>
<tr>
<td>op</td>
<td>Operators - =, !=, &gt;, &gt;=, &lt;, &lt;=, in, nin, contains, ncontains, regex, nregex, like, nlike, exists, nexists, has, nhas</td>
</tr>
<tr>
<td>value</td>
<td>Value for the filter, can be empty for some <strong>op</strong></td>
</tr>
</tbody>
</table>
<p>üìù Note</p>
<p>The <code>value</code> parameter will be empty for <code>exists</code> and <code>nexists</code>.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/payload-model/#attribute
tag_set: logs-management, logs-api, payload-model
image_urls: 
tracking_id: docs-logs-management-logs-api-payload-model-attribute
group_tracking_ids: docs-logs-management-logs-api-payload-model
<p>Logs API Payload Model: : Attribute</p>
<p>An <code>attribute</code> consists of:</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>key</td>
<td>Name of the key</td>
</tr>
<tr>
<td>type</td>
<td>Type of the key, i.e., tag/resource. It is empty for top level fields. (e.g., tag = method, resource = k8s_deployment_name, (empty) = trace_id)</td>
</tr>
<tr>
<td>dataType</td>
<td>Data type of the key (e.g., string, int64, float64, bool)</td>
</tr>
<tr>
<td>isColumn</td>
<td>Indicates if it has a materialized column, i.e., selected field</td>
</tr>
<tr>
<td>isJson</td>
<td>Specifies if the key is a JSON key</td>
</tr>
</tbody>
</table>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/payload-model/#sample-payload
tag_set: logs-management, logs-api, payload-model
image_urls: 
tracking_id: docs-logs-management-logs-api-payload-model-sample-payload
group_tracking_ids: docs-logs-management-logs-api-payload-model
<h2>Logs API Payload Model: : Sample Payload</h2>
<p>This sample payload contains the different fields that we looked at above. It queries the SigNoz Logs API and illustrates how to count distinct <code>component</code> values and group them by <code>container_id</code>.</p>
<pre><code>{
    &quot;start&quot;: 1700734490000,
    &quot;end&quot;: 1700738090000,
    &quot;step&quot;: 60,
    &quot;variables&quot;: {},
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;table&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;dataSource&quot;: &quot;logs&quot;,
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;aggregateOperator&quot;: &quot;count_distinct&quot;,
                &quot;aggregateAttribute&quot;: {
                    &quot;key&quot;: &quot;component&quot;,
                    &quot;dataType&quot;: &quot;string&quot;,
                    &quot;type&quot;: &quot;tag&quot;,
                    &quot;isColumn&quot;: false
                },
                &quot;filters&quot;: {
                    &quot;items&quot;: [],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;stepInterval&quot;: 60,
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ],
                &quot;groupBy&quot;: [\
                    {\
                        &quot;key&quot;: &quot;container_id&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true,\
                        &quot;isJSON&quot;: false\
                    }\
                ],
                &quot;offset&quot;: 0
            }
        }
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/aggregate-logs/
tag_set: logs-management, logs-api, aggregate-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-aggregate-logs
group_tracking_ids: docs-logs-management-logs-api-aggregate-logs
<h2>Aggregate Logs</h2>
<p>This section demonstrates how to perform aggregation operations on log data using the SigNoz Logs API. The example provided focuses on counting distinct components and grouping the results by <code>container_id</code></p>
<h2>Example of Aggregating Logs</h2>
<hr />
<p>The following example illustrates how to count distinct <code>component</code> values and group them by <code>container_id</code>. This can be useful for understanding the distribution of different components across various containers.</p>
<h3>## Sample Payload</h3>
<p>This is the JSON payload for the example described above for Aggregating Logs</p>
<pre><code>{
    &quot;start&quot;: 1700734490000,
    &quot;end&quot;: 1700738090000,
    &quot;step&quot;: 60,
    &quot;variables&quot;: {},
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;table&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;dataSource&quot;: &quot;logs&quot;,
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;aggregateOperator&quot;: &quot;count_distinct&quot;,
                &quot;aggregateAttribute&quot;: {
                    &quot;key&quot;: &quot;component&quot;,
                    &quot;dataType&quot;: &quot;string&quot;,
                    &quot;type&quot;: &quot;tag&quot;,
                    &quot;isColumn&quot;: false
                },
                &quot;filters&quot;: {
                    &quot;items&quot;: [],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;stepInterval&quot;: 60,
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ],
                &quot;groupBy&quot;: [\
                    {\
                        &quot;key&quot;: &quot;container_id&quot;,\
                        &quot;dataType&quot;: &quot;string&quot;,\
                        &quot;type&quot;: &quot;tag&quot;,\
                        &quot;isColumn&quot;: true,\
                        &quot;isJSON&quot;: false\
                    }\
                ],
                &quot;offset&quot;: 0
            }
        }
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs
group_tracking_ids: docs-logs-management-logs-api-search-logs
<h2>Search Logs</h2>
<p>This section provides example of how to search logs using the SigNoz Logs API. The example demonstrates querying logs with specific attributes and using pagination to navigate through the results.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/#example-query
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs-example-query
group_tracking_ids: docs-logs-management-logs-api-search-logs
<h2>Search Logs: Example Query</h2>
<p>The following example searches for logs where <code>deployment_name=hotrod</code>, <code>method=get</code>, and <code>severity_text=info</code>. Here, <code>deployment_name</code> is a resource attribute, <code>method</code> is a tag attribute, and <code>severity_text</code> is a top-level field. You can choose the start and end timestamp according to your use case.</p>
<h3>## Sample Payload</h3>
<p>This is the JSON payload for the example query described above for Searching Logs.</p>
<pre><code>{
    &quot;start&quot;: 1700733979000,
    &quot;end&quot;: 1700737579000,
    &quot;step&quot;: 60,
    &quot;variables&quot;: {},
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;list&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;dataSource&quot;: &quot;logs&quot;,
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;aggregateOperator&quot;: &quot;noop&quot;,
                &quot;aggregateAttribute&quot;: {},
                &quot;filters&quot;: {
                    &quot;items&quot;: [\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;deployment_name&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;type&quot;: &quot;resource&quot;,\
                                &quot;isColumn&quot;: false\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;hotrod&quot;\
                        },\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;method&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;type&quot;: &quot;tag&quot;,\
                                &quot;isColumn&quot;: false\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;get&quot;\
                        },\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;severity_text&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;type&quot;: &quot;&quot;,\
                                &quot;isColumn&quot;: true\
                            },\
                            &quot;op&quot;: &quot;=&quot;,\
                            &quot;value&quot;: &quot;info&quot;\
                        }\
                    ],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;stepInterval&quot;: 60,
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ],
                &quot;pageSize&quot;: 100
            }
        }
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/#pagination-in-log-search
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs-pagination-in-log-search
group_tracking_ids: docs-logs-management-logs-api-search-logs
<h2>Search Logs: Pagination in Log Search: Pagination in Log Search</h2>
<p>Pagination is crucial for efficiently navigating through large sets of log data. The SigNoz Logs API supports pagination, allowing you to retrieve logs in manageable batches. Below are examples demonstrating how to implement pagination in your log search queries.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/#ordering-by-timestamp
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs-ordering-by-timestamp
group_tracking_ids: docs-logs-management-logs-api-search-logs
<p>Search Logs: Pagination in Log Search: Ordering by Timestamp</p>
<p>If we are ordering by <code>timestamp</code>, then we will use <code>id</code> and <code>pageSize</code> for pagination.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/#fetching-the-latest-10-logs
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs-fetching-the-latest-10-logs
group_tracking_ids: docs-logs-management-logs-api-search-logs
<p>Search Logs: Pagination in Log Search: Fetching the Latest 10 logs</p>
<p>To retrieve the most recent 10 logs, you set the <code>pageSize</code> to 10 and order the results by <code>timestamp</code> in descending order. This ensures that you get the latest logs first.</p>
<pre><code>{
    &quot;start&quot;: 1700734490000,
    &quot;end&quot;: 1700738090000,
    &quot;step&quot;: 60,
    &quot;dataSource&quot;: &quot;logs&quot;,
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;list&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;dataSource&quot;: &quot;logs&quot;,
                &quot;aggregateOperator&quot;: &quot;noop&quot;,
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;pageSize&quot;: 10,
                &quot;stepInterval&quot;: 60,
                &quot;filters&quot;: {
                    &quot;items&quot;: [\
                    ],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ]
            }
        }
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/#fetching-the-previous-10-logs
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs-fetching-the-previous-10-logs
group_tracking_ids: docs-logs-management-logs-api-search-logs
<p>Search Logs: Pagination in Log Search: Fetching the Previous 10 logs</p>
<p>To fetch the 10 logs preceding the most recent batch, add a filter for the log ID using the <code>&lt;</code> operator with the value set to the ID of the last log received in the previous request.</p>
<pre><code>{
    &quot;start&quot;: 1700734490000,
    &quot;end&quot;: 1700738090000,
    &quot;step&quot;: 60,
    &quot;dataSource&quot;: &quot;logs&quot;,
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;list&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;dataSource&quot;: &quot;logs&quot;,
                &quot;aggregateOperator&quot;: &quot;noop&quot;,
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;pageSize&quot;: 10,
                &quot;filters&quot;: {
                    &quot;items&quot;: [\
                        {\
                            &quot;key&quot;: {\
                                &quot;key&quot;: &quot;id&quot;,\
                                &quot;type&quot;: &quot;&quot;,\
                                &quot;dataType&quot;: &quot;string&quot;,\
                                &quot;isColumn&quot;: true\
                            },\
                            &quot;op&quot;: &quot;&lt;&quot;,\
                            &quot;value&quot;: &quot;2QSbeXlRK0dyXIwJhLJBBtrZzxu&quot;\
                        }\
                    ],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;timestamp&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ]
            }
        }
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/#ordering-by-any-other-key
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs-ordering-by-any-other-key
group_tracking_ids: docs-logs-management-logs-api-search-logs
<p>Search Logs: Pagination in Log Search: Ordering by Any Other Key</p>
<p>In addition to fetching logs based on timestamps, the SigNoz Logs API allows you to paginate through logs using other keys. This method can be faster or slower depending on the key used.</p>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/#fetching-the-latest-10-logs-1
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs-fetching-the-latest-10-logs-1
group_tracking_ids: docs-logs-management-logs-api-search-logs
<p>Search Logs: Pagination in Log Search: Fetching the Latest 10 logs</p>
<p>To fetch the latest 10 logs based on a specific key (e.g., <code>response_time</code>), set the pageSize to 10 and order by that key in descending order. This retrieves the most recent logs according to the specified key.</p>
<pre><code>{
    &quot;start&quot;: 1700734490000,
    &quot;end&quot;: 1700738090000,
    &quot;step&quot;: 60,
    &quot;dataSource&quot;: &quot;logs&quot;,
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;list&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;dataSource&quot;: &quot;logs&quot;,
                &quot;aggregateOperator&quot;: &quot;noop&quot;,
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;pageSize&quot;: 10,
                &quot;offset&quot;: 0,
                &quot;limit&quot;: 100,
                &quot;filters&quot;: {
                    &quot;items&quot;: [\
                    ],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;response_time&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ]
            }
        }
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/logs-management/logs-api/search-logs/#fetching-the-previous-10-logs-1
tag_set: logs-management, logs-api, search-logs
image_urls: 
tracking_id: docs-logs-management-logs-api-search-logs-fetching-the-previous-10-logs-1
group_tracking_ids: docs-logs-management-logs-api-search-logs
<p>Search Logs: Pagination in Log Search: Fetching the Previous 10 logs</p>
<p>To fetch the previous 10 logs, you can use the <code>offset</code> parameter. For example, if you have already fetched the first 10 logs, set <code>offset</code> to 10 to retrieve the next 10 logs based on the specified key.</p>
<pre><code>{
    &quot;start&quot;: 1700734490000,
    &quot;end&quot;: 1700738090000,
    &quot;step&quot;: 60,
    &quot;dataSource&quot;: &quot;logs&quot;,
    &quot;compositeQuery&quot;: {
        &quot;queryType&quot;: &quot;builder&quot;,
        &quot;panelType&quot;: &quot;list&quot;,
        &quot;builderQueries&quot;: {
            &quot;A&quot;: {
                &quot;queryName&quot;: &quot;A&quot;,
                &quot;dataSource&quot;: &quot;logs&quot;,
                &quot;aggregateOperator&quot;: &quot;noop&quot;,
                &quot;expression&quot;: &quot;A&quot;,
                &quot;disabled&quot;: false,
                &quot;pageSize&quot;: 10,
                &quot;offset&quot;: 10,
                &quot;limit&quot;: 100,
                &quot;filters&quot;: {
                    &quot;items&quot;: [\
                    ],
                    &quot;op&quot;: &quot;AND&quot;
                },
                &quot;orderBy&quot;: [\
                    {\
                        &quot;columnName&quot;: &quot;response_time&quot;,\
                        &quot;order&quot;: &quot;desc&quot;\
                    }\
                ]
            }
        }
    }
}
</code></pre>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logstash_to_signoz/
tag_set: userguide, logstash_to_signoz
image_urls: 
tracking_id: docs-userguide-logstash_to_signoz
group_tracking_ids: docs-userguide-logstash_to_signoz
<h2>Logstash to SigNoz</h2>
<p>If you use logstash to collect logs in your stack with this tutotrial you will be able to send logs from logstash to SigNoz.</p>
<p>At SigNoz we use opentelemetry collector to recieve logs which supports the TCP protocol. So you can forward your logs from your logstash agent to opentelemetry collector</p>
<h3>## Collect Logs Using Logstash in SigNoz cloud</h3>
<ul>
<li>
<p>Add otel collector binary to your VM by following this <a href="https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/">guide</a>
.</p>
</li>
<li>
<p>Add the reciever to your <code>config.yaml</code></p>
<pre><code>receivers:
  tcplog/logstash:
    max_log_size: 1MiB
    listen_address: &quot;0.0.0.0:2255&quot;
    attributes: {}
    resource: {}
    add_attributes: false
    operators: []
</code></pre>
<p>Here we have used port 2255 for listening in TCP protocol, but you can change it to a port you want. You can read more about tcplog reciver <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/tcplogreceiver">here</a>
.</p>
</li>
<li>
<p>Modify your <code>config.yaml</code> and add the above receiver</p>
<pre><code>service:
    ....
    logs:
        receivers: [otlp, tcplog/logstash]
        processors: [batch]
        exporters: [otlp]
</code></pre>
</li>
<li>
<p>Change the logstash config to forward the logs to otel collector.</p>
<pre><code>output {
  tcp {
    codec =&gt; json_lines # this is required otherwise it will send eveything in a single line
    host =&gt; &quot;localhost&quot;
    port =&gt; 2255
  }
}
</code></pre>
<p>Here we are configuring logstash to send logs to otel-collector that we ran in the previous step, which is listening on port 2255. Also we are assuming that you are running the logstash binary on the host. If not, the value of <code>host</code> might change depending on your environment.</p>
</li>
<li>
<p>Once you make this changes you can otel binary and logstash, and you will be able to see the logs in SigNoz.</p>
</li>
<li>
<p>To properly transform your existing log model into opentelemetry <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md">log</a> model you can use the different processors provided by opentelemetry <a href="/docs/userguide/logs/#processors-available-for-processing-logs">link</a>
.</p>
</li>
</ul>

--------------------------------------------------------------------------------

link: https://signoz.io/docs/userguide/logstash_to_signoz/#collect-logs-using-logstash-in-self-hosted-signoz
tag_set: userguide, logstash_to_signoz
image_urls: 
tracking_id: docs-userguide-logstash_to_signoz-collect-logs-using-logstash-in-self-hosted-signoz
group_tracking_ids: docs-userguide-logstash_to_signoz
<h2>Logstash to SigNoz: Collect Logs Using Logstash in Self-Hosted SigNoz</h2>
<ul>
<li>
<p>Add the reciever to your <code>otel-collector-config.yaml</code> which is present inside <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>receivers:
  tcplog/logstash:
    max_log_size: 1MiB
    listen_address: &quot;0.0.0.0:2255&quot;
    attributes: {}
    resource: {}
    add_attributes: false
    operators: []
</code></pre>
<p>Here we have used port 2255 for listening in TCP protocol, but you can change it to a port you want. You can read more about tcplog reciver <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/tcplogreceiver">here</a>
.</p>
</li>
<li>
<p>Update the pipleline for logs and make the following change in <code>otel-collector-config.yaml</code></p>
<pre><code>service:
  ...

  logs:
    receivers: [ otlp, tcplog/logstash ]
    processors: [ batch ]
    exporters: [  clickhouselogsexporter ]
</code></pre>
<p>Here we are adding our clickhouse exporter and creating a pipeline which will collect logs from <code>tcplog/logstash</code> receiver, processing it using batch processor and export it to clickhouse.</p>
</li>
<li>
<p>Expose the port in port for otel-collector in <code>docker-compose.yaml</code> file present in <code>deploy/docker/clickhouse-setup</code></p>
<pre><code>otel-collector:
  ...
  ports:
    - &quot;2255:2255&quot;
</code></pre>
</li>
<li>
<p>Change the logstash config to forward the logs to otel collector.</p>
<pre><code>output {
  tcp {
    codec =&gt; json_lines # this is required otherwise it will send eveything in a single line
    host =&gt; &quot;&lt;otel-collector-host&gt;&quot;
    port =&gt; 2255
  }
}
</code></pre>
<p>In this example we are generating sample logs and then forwarding them to the otel collector which is listening on port 2255. <code>&lt;otel-collector-host&gt;</code> has to be replaced by the host where otel-collector is running. For more info check <a href="/docs/install/troubleshooting/#signoz-otel-collector-address-grid">troubleshooting</a>
.</p>
</li>
<li>
<p>Once you make this changes you can restart logstash and SignNoz, and you will be able to see the logs in SigNoz.</p>
</li>
<li>
<p>To properly transform your existing log model into opentelemetry <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md">log</a> model you can use the different processors provided by opentelemetry. <a href="/docs/userguide/logs/#processors-available-for-processing-logs">link</a></p>
</li>
</ul>

--------------------------------------------------------------------------------

